it may be possible to come up with a comparison sort algorithm that has lower constants, but the asymptotic complexity can't be any better.this of course is more efficient than the naive theta(n log n) method first described above but like quicksort the running time is not guaranteed.hence, merge sort in general and quicksort on average, are as good as we can hope for complexity.for example, to sort something like a set of strings, the radix could be the number of possible characters (say 256), but d would have to be the length of the longest string.this behavior should remind you of the running time of hash table operations; good distributions mean small buckets/chains on average.all of these work by comparing individual items against each other (thus why they're called comparison sorts).it turns out we can do better by using a divide and conquer style algorithm.we can represent any comparison sort on a certain sized sequence as a decision tree.by this manner, eventually we'll reach a leaf, where a certain permutation of the input is the resulting sorted sequence.here we will do something similar, except instead of processing both new subproblems, we'll only need to process one of them.but like quicksort the running time is not guaranteed.since we've shown that the height of the tree is in omega(n log n), that means the number of comparisons for the worst case traversal (representing the worst case for the comparison sort method), is also in omega(n log n).d = 3 056 592 512 056 234 512 234 234 592 234 056 512 512 056 592 592 interestingly, the run time of radix sort is not limited by omega(n log n) because it is not a comparison sort.instead, the values are distributed into buckets by digit.it turns out that the worst case of any comparison sort cannot run any faster than this limit.
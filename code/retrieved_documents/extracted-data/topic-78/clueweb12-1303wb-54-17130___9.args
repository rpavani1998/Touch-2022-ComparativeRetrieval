it turns out that the worst case of any comparison sort cannot run any faster than this limit.
here we will do something similar, except instead of processing both new subproblems, we'll only need to process one of them.
this behavior should remind you of the running time of hash table operations; good distributions mean small buckets/chains on average.
for example, to sort something like a set of strings, the radix could be the number of possible characters (say 256), but d would have to be the length of the longest string.
it may be possible to come up with a comparison sort algorithm that has lower constants, but the asymptotic complexity can't be any better.
this of course is more efficient than the naive theta(n log n) method first described above but like quicksort the running time is not guaranteed.
instead, the values are distributed into buckets by digit.
it turns out we can do better by using a divide and conquer style algorithm.
all of these work by comparing individual items against each other (thus why they're called comparison sorts).
for example, to sort something like a set of strings, the radix could be the number of possible characters (say 256), but d would have to be the length of the longest string.
however, if the values are well distributed throughout the fixed range, b will tend towards some constant (in the best case 1, since we have n items in n buckets), giving a theta(n) run time.
we can represent any comparison sort on a certain sized sequence as a decision tree.
since we've shown that the height of the tree is in omega(n log n), that means the number of comparisons for the worst case traversal (representing the worst case for the comparison sort method), is also in omega(n log n).
here we will do something similar, except instead of processing both new subproblems, we'll only need to process one of them.
it turns out that the worst case of any comparison sort cannot run any faster than this limit.
it turns out we can do better by using a divide and conquer style algorithm.
this behavior should remind you of the running time of hash table operations; good distributions mean small buckets/chains on average.
hence, merge sort in general and quicksort on average, are as good as we can hope for complexity.
since we've shown that the height of the tree is in omega(n log n), that means the number of comparisons for the worst case traversal (representing the worst case for the comparison sort method), is also in omega(n log n).
we can represent any comparison sort on a certain sized sequence as a decision tree.
however, if the values are well distributed throughout the fixed range, b will tend towards some constant (in the best case 1, since we have n items in n buckets), giving a theta(n) run time.
if the sort is a correct sorting algorithm, all the leaves will be reachable, thus n!
it may be possible to come up with a comparison sort algorithm that has lower constants, but the asymptotic complexity can't be any better.
all of these work by comparing individual items against each other (thus why they're called comparison sorts).
but like quicksort the running time is not guaranteed.
by this manner, eventually we'll reach a leaf, where a certain permutation of the input is the resulting sorted sequence.
d = 3 056 592 512 056 234 512 234 234 592 234 056 512 512 056 592 592 interestingly, the run time of radix sort is not limited by omega(n log n) because it is not a comparison sort.
individual items are not compared against each other.
by this manner, eventually we'll reach a leaf, where a certain permutation of the input is the resulting sorted sequence.
instead, the values are distributed into buckets by digit.
hence, merge sort in general and quicksort on average, are as good as we can hope for complexity.
if the sort is a correct sorting algorithm, all the leaves will be reachable, thus n!
d = 3 056 592 512 056 234 512 234 234 592 234 056 512 512 056 592 592 interestingly, the run time of radix sort is not limited by omega(n log n) because it is not a comparison sort.

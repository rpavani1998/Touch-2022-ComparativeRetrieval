an example is supporting uniprocessor and  multiprocessor modes.
dynamic programming is suitable for chained matrix multiplication and optimizing searching binary trees.
you should also be aware that algorithm choice is not merely a question of one uniprocessor architecture versus one multiprocessor architecture.
it is faster to apply a cut-off problem size, at which point recursion is stopped.
some applications need a problem solved more than once for the same  instance.
most of the time spent in the execution of a program is in a small proportion of the code.
other bottlenecks provide an incentive for several alternative algorithms  to be used in the same application for the same problem (sometimes even for the  same inputs!).
you should be aware that the best uniprocessor algorithm is not necessarily the best multiprocessor algorithm.
unfortunately, a practical demand for speed punishes having  strictly one algorithm per problem.
special features of different inputs may be  exploitable for a faster algorithm.
an approach based on simplicity is suitable for the majority of a codebase.
this work is licensed under a creative commons attribution 3retrieved from " http://programmer.97things.oreilly.com/wiki/index.php/execution_speed_versus_maintenance_effort "
the algorithmic complexity is unchanged but the  speed is improved.
following amdahl's law, to make the program fast we should  concentrate on those few lines which are run most of the time.
it is faster to apply a cut-off problem  size, at which point recursion is stopped.
a nonrecursive algorithm can finish off the remaining work.
check whether the graph has many edges.
find the right balance for you.
an approach based on simplicity is suitable for the  majority of a codebase.
it is maintainable without adversely slowing down the application.
when sequential, quicksort is preferable to merge sort, but for concurrency, more research effort has been devoted to producing excellent merge sort and radix sorts than quicksort.
different kinds of multiprocessors exist.
special features of different inputs may be exploitable for a faster algorithm.
littering a codebase with unintelligible tricks throughout would be bad, as would letting the application run too slowly.
determining the vertex-chromatic index is np-hard.
an example is supporting uniprocessor and multiprocessor modes.
e.g., a practical method for multiplying general square matrices would beo(n>2.376) but the special case of (tri)diagonal matrices admits ano(n) method.
given vertex coloring, sometimes graph coloring should be directly performed, sometimes clique partitioning should be performed instead.
e.g., a practical method for multiplying  general square matrices would beo(n>2.376) but the special  case of (tri)diagonal matrices admits ano(n) method.
check whether the graph has  many edges.
dynamic  programming is suitable for chained matrix multiplication and optimizing  searching binary trees.
some applications need a problem solved more than once for the same instance.
in 2009, the best published algorithm for multiplying typical m×n matrices (by p. d'alberto and a. nicolau) was designed for a small quantity of desktop multicore machines, whereas other algorithms are viable for machine clusters.
graph coloring is applicable to networking and numerical  differentiation.
determine  bottlenecks by empirically profiling representative runs instead of merely  relying on algorithmic complexity theory or a hunch.
you should also be aware that algorithm choice is not merely a  question of one uniprocessor architecture versus one multiprocessor  architecture.
unfortunately, a practical demand for speed punishes having strictly one algorithm per problem.
other bottlenecks provide an incentive for several alternative algorithms to be used in the same application for the same problem (sometimes even for the same inputs!).
given typical hardware with no  choice to use better hardware, division should (if efficiency is important) be  performed by multiplication, even though the algorithmic complexity of division  and multiplication are identical.
changing the quantity or architecture of processors is not the only  motivation for diverse algorithms.
find a minimum clique partitioning of the complement.
typical  dividers are slower than multipliers, so it is faster to multiply by the  reciprocal of the divisor than to divide by it.
exploit dynamic programming instead of recomputing.
in 2009, the best  published algorithm for multiplying typical m×n matrices (by p. d'alberto  and a. nicolau) was designed for a small quantity of desktop multicore  machines, whereas other algorithms are viable for machine clusters.
given typical hardware with no choice to use better hardware, division should (if efficiency is important) be performed by multiplication, even though the algorithmic complexity of division and multiplication are identical.
given vertex coloring, sometimes graph coloring should be directly  performed, sometimes clique partitioning should be performed instead.
find a minimum clique  partitioning of the complement.
this work is licensed under a creative commons attribution 3 retrieved from " http://programmer.97things.oreilly.com/wiki/index.php/execution_speed_versus_maintenance_effort "
typical dividers are slower than multipliers, so it is faster to multiply by the reciprocal of the divisor than to divide by it.
measure the performance.
you should be aware that  the best uniprocessor algorithm is not necessarily the best multiprocessor  algorithm.
divide-and-conquer algorithms, such as quicksort, start out well but suffer  from excessive subprogram call overhead when their recursive invocations  inevitably reach small subproblems.
changing the quantity or architecture of processors is not the only motivation for diverse algorithms.
execution speed versus maintenance effort from programmer 97-things jump to: navigation, search most of the time spent in the execution of a program is in a small  proportion of the code.
unlike a web browser's cache, it guarantees correctness.
following amdahl's law, to make the program fast we should concentrate on those few lines which are run most of the time.
the need for speed can encourage the use of an unobvious algorithm.
if so, get the graph's complement.
a nonrecursive algorithm can finish  off the remaining work.
graph coloring is applicable to networking and numerical differentiation.
littering a codebase with unintelligible tricks throughout would be bad, as  would letting the application run too slowly.
it is maintainable without adversely slowing down the  application.
for example, a primitive (and hence cheaper) embedded  uniprocessor may lack a branch predictor so a radix sort algorithm may not be  advantageous.
when sequential, quicksort is preferable to merge sort,  but for concurrency, more research effort has been devoted to producing  excellent merge sort and radix sorts than quicksort.
determine bottlenecks by empirically profiling representative runs instead of merely relying on algorithmic complexity theory or a hunch.
for example, a primitive (and hence cheaper) embedded uniprocessor may lack a branch predictor so a radix sort algorithm may not be advantageous.
divide-and-conquer algorithms, such as quicksort, start out well but suffer from excessive subprogram call overhead when their recursive invocations inevitably reach small subproblems.
consult books and papers on algorithms.
the algorithmic complexity is unchanged but the speed is improved.
by paul colin gloster
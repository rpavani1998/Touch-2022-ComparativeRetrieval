for example, sorting transparent objects before rendering or sorting objects by state attribute (e.g. texture) to improve batching.
// nothing to sort if(count 20k arrays using merge sort (the speed-ups with quicksort are not as good).
this can be done  with the __offload keyword and a simple queue: template void sortchunkqueued(t *data, size_t start,  size_t end, threadqueue *queue) { size_t count = (end - start + 1); if(count  waitforslot(); // execute the block on a spu and immediately  return an execution handle offloadthread_t handle = __offload(data, start, end,  count) { size_t size
// nothing to sort if(count  < 2) return; threadqueue queue; t *temp =
blocks the function we use to offload sorting on 1 spu (sortoffloadedlocalmem) is  blocking, which means that the ppu is blocked while the spu is sorting the  array.
the entire series is also available as a single pdf document.
we have to change this function if we want to use it in a parallel merge, because we need multiple spus to work at the same time.
doing two sorting operations on different spus at the same time is then quite easy: t *array1, array2; size_t size1, size2; threadqueue queue; sortqueued(array1, 0, size1 - 1, &queue); sortqueued(array2, 0, size2 - 1, &queue); queue.joinallthreads(); now that we can execute code on multiple spus at the same time, we can start designing our parallel sort implementation.
part 4 (offloading on 1 spu) part 5 (parallel sort on 2 spus) part 6 (final optimizations)
but we still need to address the array size limitation on the spu.
part 1 (quicksort) part 2 (merge) part 3 (merge sort) part 4 (offloading on 1 spu) part 5 (parallel sort on 2 spus) part 6 (final optimizations) the entire series is also available as a single pdf document.
the number of concurrent blocks can be specified in the  constructor.
in simple cases an array of handles could be used, but threadqueue is  easier to use with recursive functions.
doing two sorting operations on different spus at the same time is then  quite easy: t *array1, array2; size_t size1, size2;
builtin_memcpy(data +  start, temp + start,(end - start + 1) * sizeof(t)); } template void parallelsort(t *data, size_t count) {
threadqueue queue;  sortqueued(array1, 0, size1 - 1, &queue); sortqueued(array2, 0, size2 - 1,  &queue); queue.joinallthreads(); parallel sort on 2 spus now that we can execute code on multiple spus at the same time, we can  start designing our parallel sort implementation.
with multi-core architectures like the ps3 it makes sense to parallelize this operation to maximize the use of these cores.
in the next and last post in the series we will see how to improve our parallel sort so that it can run on 4 spus instead of 2.
the easiest solution would be to divide  the array into chunks that can fit in spu memory and then merge them on the  ppu.
builtin_memcpy(data + start, temp + start,(end - start + 1) * sizeof(t)); } template void parallelsort(t *data, size_t count) {
recall how recursive merge sort is implemented: template void mergesort(t *data, t *temp, size_t start, size_t end) { size_t count = end - start + 1; if(count  void parallelsort(t *data, t *temp, size_t start, size_t end, threadqueue *queue) { size_t count = end - start + 1; if(count joinallthreads(); // merge two parts (data) into one (temp) merge(temp + start, data, start, mid, data, mid + 1, end); // copy one part from temp to data __
we have to change this function if we want to use it in a parallel  merge, because we need multiple spus to work at the same time.
the number of concurrent blocks can be specified in the constructor.
(t *)memalign(16, count *  sizeof(t)); if(!temp) { failed_malloc("temp"); return; }  parallelsort(data, temp, 0, count - 1, &queue); free(temp); } this implementation behaves identically to the sequential one, except that  when the size of the array gets small enough, sorting is offloaded on a spu.
in the previous part we have started running these implementations on a spu to improve performance even further.
sorting is a simple but important concept for implementing games.
in simple cases an array of handles could be used, but threadqueue is easier to use with recursive functions.
in addition, despite using the vectorized merge function presented earlier in the series, we do not see any significant difference in performance between sorting floats on integers on the spu.
the function we use to offload sorting on 1 spu (sortoffloadedlocalmem) is blocking, which means that the ppu is blocked while the spu is sorting the array.
but we still need to address  the array size limitation on the spu.
can be used to enqueue multiple blocks and join (i.e. wait for) them  afterwards.
the easiest solution would be to divide the array into chunks that can fit in spu memory and then merge them on the ppu.
in this part we will run our sort functions on multiple spus and create a parallel sort implementation.
since there are two recursive calls in parallelsort and the blocks have to be  joined right after that, at most two blocks will be executing in parallel on  spus.
= count * sizeof(t); if(count enqueue(handle); } executing an __offload block returns a handle which can be used to wait  until the block has been executed.
however we will have to address the array size limitation.
this can be done with the __offload keyword and a simple queue: template void sortchunkqueued(t *data, size_t start, size_t end, threadqueue *queue) { size_t count = (end - start + 1); if(count waitforslot(); // execute the block on a spu and immediately return an execution handle offloadthread_t handle = __offload(data, start, end, count) { size_t size = count * sizeof(t); if(count enqueue(handle); } executing an __offload block returns a handle which can be used to wait until the block has been executed.
this effectively limits this implementation to use at most 2 spus.
in addition, we will show how to parallelize merge to run on any number of spus which will improve performance even further.
in that case waitforslot() should be called before enqueueing a block).
in that case waitforslot() should be called before enqueueing a  block).
because of the way most algorithms work (comparing and swapping pairs of items) sorting often takes precious time.
recall how recursive merge sort is implemented: template void mergesort(t *data, t *temp, size_t start,  size_t end) { size_t count = end - start + 1; if(count  void parallelsort(t *data, t *temp, size_t  start, size_t end, threadqueue *queue) { size_t count = end - start + 1;  if(count joinallthreads(); //  merge two parts (data) into one (temp) merge(temp + start, data, start, mid,  data, mid + 1, end); // copy one part from temp to data __
the threadqueue class (not part of offload) can be used to enqueue multiple blocks and join (i.e. wait for) them afterwards.
the threadqueue class (not part of offload)
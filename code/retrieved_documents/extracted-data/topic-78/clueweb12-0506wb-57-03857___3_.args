unfortunately, merge sort requires significantly more memory than do the other sorting routines (you can spend some time trying to come up with an ``in place'' merge sort, but you are quite likely to fail).if we always chose the largest element as the pivot, this algorithm would be equivalent toselection sort, and would take time o(n*n).other versions we've written merge sort so that it does not affect the original array.however, bad choice of pivots can give significantly worse running time.in bucket sort, you create separate ``buckets'' for kinds of elements, put each element into the appropriate bucket, sort each bucket, and then take them out again.if each partition is perfect (splits it exactly in half), we can stop the process after o(log_2(n)) levels.but if you're willing to use extra space and know something about the original data, then you can do better.with a little work, you can do this partitioning in place, so that there is no overhead (and so that ``glueing'' is basically a free operation).if you can arrange things so that each bucket contains only a few elements (say no more than four), then the main cost is putting in to buckets and taking out of buckets.running time can be a constant (as long as you can guarantee the number of items in any bucket)!held friday, october 8, 1999 overview today we continue our discussion of sorting by visiting some more efficient sorting algorithms that usedivide and conquer as their underlying design strategy.on average, we don't quite do half, but it's close enough that it doesn't make a significant difference.pre: the elements in the array can be compared to each other.any or all of the information on the pages may be incorrect.pre: all elements in the array can be compared to each other.
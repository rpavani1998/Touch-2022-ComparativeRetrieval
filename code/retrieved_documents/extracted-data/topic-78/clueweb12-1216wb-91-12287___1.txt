the o() hides too  much information, instead we need to prove f(n)
so average case analysis of a randomized  algorithm gives a randomized recurrence: n-1 c(n)
the divide and conquer idea: find natural subproblems, solve them  recursively, and combine them to get an overall solution.
so average case analysis of a randomized algorithm gives a randomized recurrence: n-1 c(n)
to start with, we can set up a binary tree of the right size and shape, and  put the objects into the tree in any old order.
so we can write a recurrence for the total number of comparisons  done by quicksort: c(n) =
(for instance, let x be  the median of three randomly chosen values rather than just one value).
to make the whole thing a heap, we merely have to percolate that value down to a lower level in the tree.
ics 161 -- dept. information & computer science -- uc irvine last update: 02 may 2000, 20:17:36 pdt
ics 161: design and analysis of algorithms lecture notes for january  18, 1996 three divide and conquer sorting algorithms today we'll finish  heapsort, and describe both mergesort and quicksort.
so we can conclude that c(n)
quicksort also uses few comparisons (somewhat more than the other two).
it also uses even fewer  comparisons than heapsort, and is especially suited for data stored as linked  lists.
how can we make it less bad?
= sum (1/n)[n - 1 + c(a) + c(n-a-1)]
+ c(b)where a and b are the sizes of l1 and l2, generally satisfying a+b=n-1.
this is all easy and doesn't  require any comparisons.
n-1 +  2a/n (n^2 log n / 2 - n^2/4 - 2 ln 2 + 1)
n-1 + a n log n - an/2 - o(1)and this will work if n-1 < an/2, and in particular if a=2.
<= 2 n log n. note that this is worse than either merge sort or heap sort, and requires  random number generator to avoid being really bad.
remember that a heap is  just a balanced binary tree in which the value at any node is smaller than the  values at its children.
the total number  of comparisons is n log n + however many are needed to make h.
in the worst case, we might pick x to be the minimum element in l. then a=0, b=n-1, and the recurrence simplifies to c(n)=n-1 + c(n-1) = o(n^2).
but we don't know  what value a should take.
heapification recall the idea of heapsort: heapsort(list l) { make heap h from l make empty list x while h nonempty  remove smallest from h and add it to x return x }
we work it out with a left as a variable then use the  analysis to see what values of a work.
n-1 + 2a/n  sum(i=2 to n-1)
now we have to switch objects around to get them back in order.
to do average case analysis, we write out the sum over possible random choices of the probability of that choice times the time for that choice.
it can be done in a way that uses very little extra  memory.
different methods work better in different applications.
but we don't know what value a should take.
here the obvious  subproblems are the subtrees.
ics 161 -- dept. information & computer science -- uc irvine last  update: 02 may 2000, 20:17:36 pdt
we can also take the (n-1) parts out of the sum since the sum of 1/n  copies of 1/n times n-1 is just n-1.
one useful idea here: we want to prove f(n) is o(g(n)).
y in l : y = x } quicksort(l1) quicksort(l2) return concatenation of l1, l3, and l2 } }(we don't need to sort l3 because everything in it is equal).
n-1 + c(a) + c(b) where a and b are the sizes of l1 and l2,  generally satisfying a+b=n-1.
n-1 + a n log n - an/2 - o(1) and this will work if n-1 < an/2, and in particular if a=2.
the o() hides too much information, instead we need to prove f(n)
heapify(tree t) { if (t is nonempty) { heapify(left subtree) heapify(right subtree) let x = value at tree root while node containing x doesn't satisfy heap propert switch values of node and its smallest child } }the while loop performs two comparisons per iteration, and takes at most log n iterations, so the time for this satisfies a recurrence t(n)  x } l3 = {
n - 1 + sum (2/n) c(a) a=0
(for instance, let x be the median of three randomly chosen values rather than just one value).
= sum (1/n)[n - 1 + c(a) + c(n-a-1)] a=0to simplify the recurrence, note that if c(a) occurs one place in the sum, the same number will occur as c(n-a-1) in another term -- we rearrange the sum to group the two together.
if we solve them recursively, we get something  that is close to being a heap, except that perhaps the root doesn't satisfy the  heap property.
heapsort uses close to the right number of comparisons but needs to move  data around quite a bit.
this is all easy and doesn't require any comparisons.
heapsort(list l) { make heap h from l make empty list x while h nonempty remove smallest from h and add it to x return x }remember that a heap is just a balanced binary tree in which the value at any node is smaller than the values at its children.
<= 2 n log n. note that this is worse than either merge sort or heap sort, and requires random number generator to avoid being really bad.
the only missing step: how to make a heap?
merge sort is good for data that's too big to have in memory at once,  because its pattern of storage access is very regular.
<= n-1 + 2a/n integral(i=2 to n)(i log i) =
the divide and conquer idea: find natural subproblems, solve them recursively, and combine them to get an overall solution.
n-1 + 2a/n (n^2 log n / 2 - n^2/4 - 2 ln 2 + 1) =
why do we need multiple  sorting algorithms?
we work it out with a left as a variable then use the analysis to see what values of a work.
why do we call it quicksort?
here the obvious subproblems are the subtrees.
the only missing  step: how to make a heap?
we went over most of this last time.
it's probably good when memory is tight, and you are sorting many small  items that come stored in an array.
like heapsort it can sort "in place" by moving data in an array.
a=0 to simplify the  recurrence, note that if c(a) occurs one place in the sum, the same number will  occur as c(n-a-1) in another term -- we rearrange the sum to group the two  together.
the book gives two proofs  that this is o(n log n).
n - 1 + sum (2/n) c(a) a=0the book gives two proofs that this is o(n log n).
n-1 + sum(2/n) c(a) <= n-1 + sum(2/
heapify(tree t) { if (t is nonempty) { heapify(left subtree)  heapify(right subtree) let x = value at tree root while node containing x  doesn't satisfy heap propert switch values of node and its smallest child } } the while loop performs two comparisons per iteration, and takes at most  log n iterations, so the time for this satisfies a recurrence t(n)  x } l3 = { y in l : y = x }  quicksort(l1) quicksort(l2) return concatenation of l1, l3, and l2 } } (we  don't need to sort l3 because everything in it is equal).
= a (1 log 1) for all a. suppose c(i)
but it's pretty commonly used, and can be tuned in various ways to work better.
then any value of a is  equally likely from 0 to n-1.
then any value of a is equally likely from 0 to n-1.
here the choices are the values of k, the probabilities are all 1/n, and the times can be described by formulas involving the time for the recursive calls to the algorithm.
n-1 + 2a/n sum(i=2 to n-1) (i log i)
of these, induction is easier.
suppose we pick x=a[k] where k is chosen randomly.
if we solve them recursively, we get something that is close to being a heap, except that perhaps the root doesn't satisfy the heap property.
to do average case analysis, we write out the sum  over possible random choices of the probability of that choice times the time  for that choice.
now we have to switch objects around to get them back  in order.
quicksort analysis the partition step of quicksort takes n-1  comparisons.
we can also take the (n-1) parts out of the sum since the sum of 1/n copies of 1/n times n-1 is just n-1.
to start with, we can set up a binary tree of the right size and shape, and put the objects into the tree in any old order.
to make the whole thing a heap, we merely have to percolate that  value down to a lower level in the tree.
so this seems like a very bad algorithm.
a i log i for  some a, all
so we can  conclude that c(n)
the total number of comparisons is n log n + however many are needed to make h.
a i log i for some a, all
here the choices are the values of k, the probabilities are  all 1/n, and the times can be described by formulas involving the time for the  recursive calls to the algorithm.
but it's pretty commonly  used, and can be tuned in various ways to work better.
in the worst case, we might pick x to be the  minimum element in l. then a=0, b=n-1, and the recurrence simplifies to  c(n)=n-1 + c(n-1) = o(n^2).
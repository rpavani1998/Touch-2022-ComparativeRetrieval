an example is supporting uniprocessor and multiprocessor modes.
dynamic programming is suitable for chained matrix multiplication and optimizing searching binary trees.
special features of different inputs may be exploitable for a faster algorithm.
the algorithmic complexity is unchanged but the speed is improved.
following amdahl's law, to make the program fast we should concentrate on those few lines which are run most of the time.
a nonrecursive algorithm can finish off the remaining work.
it is maintainable without adversely slowing down the application.
when sequential, quicksort is preferable to merge sort, but for concurrency, more research effort has been devoted to producing excellent merge sort and radix sorts than quicksort.
special features of different inputs may be exploitable for a faster algorithm.
littering a codebase with unintelligible tricks throughout would be bad, as would letting the application run too slowly.
an example is supporting uniprocessor and multiprocessor modes.
dynamic programming is suitable for chained matrix multiplication and optimizing searching binary trees.
in 2009, the best published algorithm for multiplying typical m×n matrices (by p. d'alberto and a. nicolau) was designed for a small quantity of desktop multicore machines, whereas other algorithms are viable for machine clusters.
given typical hardware with no choice to use better hardware, division should (if efficiency is important) be performed by multiplication, even though the algorithmic complexity of division and multiplication are identical.
changing the quantity or architecture of processors is not the only motivation for diverse algorithms.
typical dividers are slower than multipliers, so it is faster to multiply by the reciprocal of the divisor than to divide by it.
in 2009, the best published algorithm for multiplying typical m×n matrices (by p. d'alberto and a. nicolau) was designed for a small quantity of desktop multicore machines, whereas other algorithms are viable for machine clusters.
given typical hardware with no choice to use better hardware, division should (if efficiency is important) be performed by multiplication, even though the algorithmic complexity of division and multiplication are identical.
typical dividers are slower than multipliers, so it is faster to multiply by the reciprocal of the divisor than to divide by it.
divide-and-conquer algorithms, such as quicksort, start out well but suffer from excessive subprogram call overhead when their recursive invocations inevitably reach small subproblems.
changing the quantity or architecture of processors is not the only motivation for diverse algorithms.
execution speed versus maintenance effort from programmer 97-things jump to: navigation, search most of the time spent in the execution of a program is in a small proportion of the code.
unlike a web browser's cache, it guarantees correctness.
following amdahl's law, to make the program fast we should concentrate on those few lines which are run most of the time.
the need for speed can encourage the use of an unobvious algorithm.
a nonrecursive algorithm can finish off the remaining work.
littering a codebase with unintelligible tricks throughout would be bad, as would letting the application run too slowly.
it is maintainable without adversely slowing down the application.
for example, a primitive (and hence cheaper) embedded uniprocessor may lack a branch predictor so a radix sort algorithm may not be advantageous.
when sequential, quicksort is preferable to merge sort, but for concurrency, more research effort has been devoted to producing excellent merge sort and radix sorts than quicksort.
for example, a primitive (and hence cheaper) embedded uniprocessor may lack a branch predictor so a radix sort algorithm may not be advantageous.
divide-and-conquer algorithms, such as quicksort, start out well but suffer from excessive subprogram call overhead when their recursive invocations inevitably reach small subproblems.
the algorithmic complexity is unchanged but the speed is improved.

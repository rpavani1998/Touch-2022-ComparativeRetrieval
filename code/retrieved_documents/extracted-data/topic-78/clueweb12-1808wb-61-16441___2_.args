divide-and-conquer algorithms, such as quicksort, start out well but suffer from excessive subprogram call overhead when their recursive invocations inevitably reach small subproblems.dynamic programming is suitable for chained matrix multiplication and optimizing searching binary trees.when sequential, quicksort is preferable to merge sort, but for concurrency, more research effort has been devoted to producing excellent merge sort and radix sorts than quicksort.given typical hardware with no choice to use better hardware, division should (if efficiency is important) be performed by multiplication, even though the algorithmic complexity of division and multiplication are identical.it is maintainable without adversely slowing down the application.for example, a primitive (and hence cheaper) embedded uniprocessor may lack a branch predictor so a radix sort algorithm may not be advantageous.unlike a web browser's cache, it guarantees correctness.in 2009, the best published algorithm for multiplying typical m√ón matrices (by p. d'alberto and a. nicolau) was designed for a small quantity of desktop multicore machines, whereas other algorithms are viable for machine clusters.changing the quantity or architecture of processors is not the only motivation for diverse algorithms.typical dividers are slower than multipliers, so it is faster to multiply by the reciprocal of the divisor than to divide by it.the need for speed can encourage the use of an unobvious algorithm.the algorithmic complexity is unchanged but the speed is improved.littering a codebase with unintelligible tricks throughout would be bad, as would letting the application run too slowly.a nonrecursive algorithm can finish off the remaining work.special features of different inputs may be exploitable for a faster algorithm.
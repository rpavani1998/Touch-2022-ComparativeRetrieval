unfortunately, merge sort requires significantly more memory than do the other sorting routines (you can spend some time trying to come up with an ``in place'' merge sort, but you are quite likely to fail).if we always chose the largest element as the pivot, this algorithm would be equivalent toselection sort, and would take time o(n*n).other versions we've written merge sort so that it does not affect the original array.however, bad choice of pivots can give significantly worse running time.if each partition is perfect (splits it exactly in half), we can stop the process after o(log_2(n)) levels.but if you're willing to use extra space and know something about the original data, then you can do better.with a little work, you can do this partitioning in place, so that there is no overhead (and so that ``glueing'' is basically a free operation).if you can arrange things so that each bucket contains only a few elements (say no more than four), then the main cost is putting in to buckets and taking out of buckets.running time can be a constant (as long as you can guarantee the number of items in any bucket)!on average, we don't quite do half, but it's close enough that it doesn't make a significant difference.pre: the elements in the array can be compared to each other.any or all of the information on the pages may be incorrect.[instructions] [search] [current] [news] [syllabus] [glance] [links] [ handouts] [project] [outlines] [labs] [assignments] [quizzes] [exams] [examples ] [eij] [jpds] [tutorial] [api] back to some sorting algorithms.pre: all elements in the array can be compared to each other.[instructions] [search] [current] [news] [syllabus] [glance] [links] [ handouts] [project] [outlines] [labs] [assignments] [quizzes] [exams] [examples ] [eij] [jpds] [tutorial] [api] disclaimer often, these pages were created "on the fly" with little, if any, proofreading.
if the purpose of a decision tree is to yield insight into a causal process or into the structure of a database, splitting rules of similar accuracy can yield trees that vary greatly in their usefulness for interpreting and understanding the data.given that there is no one best splitting rule for all problem types or all purposes, it is important that the decision-tree tool you employ offer several proven splitting rules with distinct styles and operating characteristics.although we can make recommendations as to which splitting rule is best suited to which type of problem, it is good practice to always use several splitting rules and compare the results.when you are trying to detect fraud, identify borrowers who will declare bankruptcy in the next 12 months, target a direct mail campaign, or tackle other real-world business problems that do not admit of 90+ percent accuracy rates (with currently available data), the splitting rule you choose could materially affect the accuracy and value of your decision tree.as you work with different types of data and problems, you will begin to learn which splitting rules typically work best for specific problem types.gini then tackles the last heterogeneous node, striving to separate class c from class d. if the gini rule is successful, the final tree would contain four "pure" child nodes: a pure decision tree such as the above is attainable only in very rare circumstances; in most real world applications, database fields that clearly partition class from class are not available.contrary to popular opinion in data mining circles, our experience indicates that splitting criteria do matter; in fact, the difference between using the right rule and the wrong rule could add up to millions of dollars of lost opportunity.even when the overall accuracy of two trees grown by different splitting rules is identical, the usefulness of the trees for revealing data structure can be quite different.there will also be times when the value of a decision tree is determined by how rich the best nodes are in certain values of the target variable, and the overall accuracy of the tree is irrelevant.the list of variables in this exercise was quite limited; in particular, the trees needed to be grown without reference to important drivers of choice such as income or price, so the level of accuracy attainable was expected to be low.although the second tree also has quite a high error rate in percentage of error terms, it is a dramatic improvement over the first.variations on twoing an important variation of the twoing rule is power-modified twoing, which places an even heavier weight on splitting the data in a node into two equal-sized partitions.further, even when different splitting rules yield similarly accurate classifiers, the differences between them may still matter.unfortunately, previous examinations of splitting rule performance, the ones that found no differences, did not look at data-mining problems with large data sets where obtaining a good answer is genuinely difficult.a data-mining project concerning consumer choice from a set of 28 vehicles is a typical example of the substantial differences that can be observed across alternative splitting rules.
decision trees have a number of abilities that make them valuable for boosting, namely the ability to handle data of mixed type and the ability to model complex functions.as a result of this randomness, the bias of the forest usually slightly increases (with respect to the bias of a single non-random tree) but, due to averaging, its variance also decreases, usually more than compensating for the increase in bias, hence yielding an overall better model.the advantages of gbrt are: natural handling of data of mixed type (= heterogeneous features) predictive power robustness to outliers in input space (via robust loss functions)the advantages of gbrt are: - natural handling of data of mixed type (= heterogeneous features) - predictive power - robustness to outliers in input space (via robust loss functions)significant speedup can still be achieved though when building a large number of trees, or when building a single tree requires a fair amount of time (e.g., on large datasets).gradient tree boosting uses decision trees of fixed size as weak learners.in random forests (see randomforestclassifier and randomforestregressor classes), each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set.when training on large datasets, where runtime and memory requirements are important, it might also be beneficial to adjust themin_density parameter, that controls a heuristic for speeding up computations in each tree.this means a diverse set of classifiers is created by introducing randomness in the classifier construction.on average, the combined model is usually better than any of the single model because its variance is reduced.in contrast to the original publication [b2001], the scikit-learn implementation combines classifiers by averaging their probabilistic prediction, instead of letting each classifier vote for a single class.smaller values oflearn_rate require larger numbers of weak learners to maintain a constant training error.in addition, when splitting a node during the construction of the tree, the split that is chosen is no longer the best split among all features.gradientboostingclassifier supports both binary and multi-class classification via the deviance loss function (loss='deviance').the disadvantages of gbrt are: scalability, due to the sequential nature of boosting it can hardly be parallelized.
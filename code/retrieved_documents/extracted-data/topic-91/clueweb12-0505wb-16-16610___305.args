experiments on data sets with different properties (reuters-21578, patent abstracts and patent applications) and with two different algorithms (winnow and rocchio) show that uc-based term selection is not the most aggressive term selection criterium, but that its effect is quite stable across data sets and algorithms.
empirical comparison with several other existing feature selection methods shows that the backward elimination variant of csa leads to the most accurate classification results on an array of datasets."
our results on two datasets of scanned journals from the making of america collection confirm the importance of using whole page sequences.
we verify experimentally that the integration of wordnet helps ssahc improve its performance, effectively addresses the classification of documents into categories with few training documents.
as output, the system evaluates a set of weighted hypotheses about the type of the actual letter.
we provide experimental results demonstrating that the approach can significantly improve performance, and that it does not impair it.}, } @inproceedings{dagan96, author = {dagan, ido and feldman, ronen and hirsh, haym}, title = {keyword-based browsing and analysis of large document sets}, booktitle =
the utilized concepts are automatically extracted from documents via probabilistic latent semantic analysis.
however, many systems set a relatively high threshold to reduce retrieval of non-relevant documents, which results in the ignorance of many relevant documents.
we provide background, present procedures for building metaclassifiers that take into consideration both reliability indicators and classifier outputs, and review a set of comparative studies undertaken to evaluate the methodology.}, } @inproceedings{bennett03, author = {paul n. bennett}, title = {using asymmetric distributions to improve text classifier probability estimates}, booktitle = {proceedings of sigir-03, 26th acm international conference on research and development in information retrieval}, editor = {jamie callan and gordon cormack and charles clarke and david hawking and alan smeaton}, publisher = {acm press, new york, us}, address = {toronto, ca}, year = {2003}, pages = {111--118}, url = {http://doi.acm.org/10.1145/860435.860457},
the best performance was achieved by the feature selection methods based on the feature scoring measure called odds ratio that is known from information retrieval.}, } @phdthesis{mladenic98c, author = {
by doing so, model probability and classification accuracy come into correspondence, allowing unlabeled data to improve classification performance.
many corpora, such as internet directories, digital libraries, and patent databases are manually organized into topic hierarchies, also called taxonomies.
text classification algorithms were used to automatically classify arbitrary search results into an existing category structure on-the-fly.
the discrimination between informative keywords and functional keywords is not crisp.
in this paper, we systematically compare combination strategies in the context of document filtering, using queries from the tipster reference corpus.
our results show that ridge regression seems to be the most promising candidate for rare class problems.}, } @inproceedings{zhdanova02, author = {anna v. zhdanova and denis v. shishkin}, title = {classification of email queries by topic: approach based on hierarchically structured subject domain}, booktitle = {proceedings of ideal-02, 3rd international conference on intelligent data engineering and automated learning}, editor = {hujun yin and nigel allinson and richard freeman and john keane and simon hubbard}, publisher = {springer verlag, heidelberg, de}, address = {manchester, uk}, year = {2002}, pages = {99--104}, note = {published in the ``lecture notes in computer science'' series, number 2412}, url = {http://link.springer.de/link/service/series/0558/papers/2412/24120099.pdf}, abstract =
also the role of linguistic preprocessing seems to provide positive effects on the performance.
in this paper, we study the properties of phrasal and clustered indexing languages on a text categorization task, enabling us to study their properties in isolation from query interpretation issues.
generalization is an important ability specific to inductive learning that will predict unseen data with high accuracy based on learned concepts from training examples.
in addition to being highly accurate, this method utilizes the hamming distance from ecoc to provide high-precision results.
our experiments with the acm computing classification scheme, using documents from the acm digital library, indicate that gp can discover similarity functions superior to those based solely on a single type of evidence.
we conduct an empirical study on several document classification tasks which confirms the value of our methods in large scale semi-supervised settings."
these text messages are scanned and then distributed to one of several expert agents according to a certain task criterium.
our experiments use reuters 21578 database and consist of binary classifications for categories selected from the 89 topics classes of the reuters collection.
so far, documents have been classified according to their contents manually.
categorizing documents with the aid of knowledge-based features leverages information that cannot be deduced from the documents alone.
our experiments demonstrate that this new approach is able to learn more accurate classifiers than either of its constituent methods alone.}, } @inproceedings{slonim01, author = {noam slonim and naftali tishby}, title = {
our experiments show that this method significantly outperforms the combination of single label approach."
this article uses the structure that is present in the semantic space of topics in order to improve performance in text categorization: according to their meaning, topics can be grouped together into ``meta-topics'', e.g., gold, silver, and copper are all metals.
links clearly contain high-quality semantic clues that are lost upon a purely term-based classifier, but exploiting link information is non-trivial because it is noisy.
taipei, tw}, url = {http://acl.ldc.upenn.edu/coling2002/proceedings/data/area-28/co-269.pdf}, abstract = {automatic text categorization is a problem of automatically assigning text documents to predefined categories.
naive use of terms in the link neighborhood of a document can even degrade accuracy.
the system is also applicable to text classification problems other than telex classification.}, } @inproceedings{li97, author = {
a framework for filtering news and managing distributed data}, journal = {journal of universal computer science}, year = {1997}, number = {8}, volume = {3}, pages = {1007--1021}, url =
such categories offer a standardized and universal way for referring to or describing the nature of real world objects, activities, documents and so on, and may be used (we suggest) to semantically characterize the content of documents.
the algorithm is automatic and unsupervised in both training and application: senses are induced from a corpus without labeled training instances or other external knowledge sources.
the classifiers provide heuristics to the crawler thus biasing it towards certain portions of the web graph.
we report an experiment to study the impact of term redundancy on the performance of text classifier.
recurrent plausibility networks with local memory are developed and examined for learning robust text routing.
all the presented experiments are based on unrestricted text downloaded from the world wide web without any manual text preprocessing or text sampling.
the key idea is to automatically adjust the window size so that the estimated generalization error is minimized.
the method maintains a window on the training data.
our results show that an index can be constructed on a desktop workstation with little effect on categorisation accu-racy compared to a memory-based approach.
evaluation against the reuters-21578 collection shows both techniques have levels of performance that approach benchmark methods, and the ability of one of the classifiers to produce realistic measures of confidence in its decisions is shown to be useful for prioritizing relevant documents.}, } @inproceedings{lee02c, author = {kang hyuk lee and judy kay and byeong ho kang and uwe rosebrock}, title = {a comparative study on statistical machine learning algorithms and thresholding strategies for automatic text categorization}, booktitle = {proceedings of pricai-02, 7th pacific rim international conference on artificial intelligence}, editor = {mitsuru ishizuka and abdul sattar}, publisher = {springer verlag, heidelberg, de}, address = {
profile allows the user to update on-line the profile and to check the discrepancy between the assessment and the prediction of relevance of the system.
the final classification of test documents is determined by a majority voting from the individual classifications of each feature.
the described experimental study shows that the idf transform considerably effects the distribution of classification performance over feature selection reduction rates, and offers an evaluation method which permits the discovery of relationships between different document representations and feature selection methods which is independent of absolute differences in classification performance.}
in this paper, we present pva, an adaptive personal view information agent system to track, learn and manage, user's interests in internet documents.
informative keywords are the ones which reflect the contents of a document.
term selection according to this criterium is performed by the elimination of noisy terms on a class-by-class basis, rather than by selecting the most significant ones.
once the system has learned this information, a gaussian function is shaped for each term of a category, in order to assign the term a weight that estimates the level of its importance for that particular category.
it enables drawing on results from statistics and machine learning in explaining the effectiveness of alternate representations of text, and specifies desirable characteristics of text representations.
when choosing optimal pairs of metrics for each of the four performance goals, bns is consistently a member of the pair---e.g., for greatest recall, the pair bns + f1-measure yielded the best performance on the greatest number of tasks by a considerable margin.}, } @inproceedings{forman04, author = {
extensive experimental evidence has been derived on real test data and also from well-established academic test sets.
given a visual content, the occurrences of visual keywords are detected, summarized spatially, and coded via singular value decomposition to arrive at a concise coded description.
the r-measure can be effectively computed using the suffix array data structure.
support vector machines (svm), by an average of 2\% to 7\%.
collaboration with friends of the earth allows us to test our ideas in a non-academic context involving high volumes of documents.
text classification tasks, like text categorization, help the users to access to the great amount of text they find in the internet and their organizations.
we present the results of a number of experiments designed to evaluate the effectiveness and behavior of different compression-based text classification methods on english text.
we present a set of small-scale but reasonable experiments in text genre detection, author identification, and author verification tasks and show that the proposed method performs better than the most popular distributional lexical measures, i.e., functions of vocabulary richness and frequencies of occurrence of the most frequent words.
we present experimental results obtained on the standard \textsf{reuters-21578} benchmark with one classifier learning method (support vector machines), three term selection functions (information gain, chi-square, and gain ratio), and both local and global term selection and weighting.}, } @inproceedings{debole04, author = {franca debole and fabrizio sebastiani}, title = {an analysis of the relative difficulty of reuters-21578 subsets}, year = {2004}, booktitle = {proceedings of lrec-04, 4th international conference on language resources and evaluation}, address = {lisbon, pt}, pages = {}, url = {http://www.math.unipd.it/~fabseb60/publications/lrec04.pdf}, abstract = {
this article describes an approach to tc based on the integration of a training collection (reuters-21578) and a lexical database (wordnet 1.6) as knowledge sources.
this is still true even for the maximum entropy (me) method, whose flexible modeling capability has alleviated data sparseness more successfully than the other probabilistic models in many nlp tasks.
the system is also applicable to text classification problems other than telex classification.}, } @inproceedings{li97, author = {
this task has several applications, including automated indexing of scientific articles according to predefined thesauri of technical terms, filing patents into patent directories, selective dissemination of information to information consumers, automated population of hierarchical catalogues of web resources, spam filtering, identification of document genre, authorship attribution, survey coding, and even automated essay grading.
our algorithm has the following features: it does not need any natural language processing technique and it is robust for noisy data.
the originality of stretch lies principally in the possibility for unskilled users to define the indexes relevant to the document domains of their interest by simply presenting visual examples and applying reliable automatic information extraction methods (document classification, flexible reading strategies) to index the documents automatically, thus creating archives as desired.
experimental results show that our neuro-genetic algorithm is able to perform as well as, if not better than, the best results of neural networks to date, while using fewer input features.}, } @inproceedings{zaiane02, author = {osmar r. za{\"{\i}}ane and maria-luiza antonie}, title = {
though it is not a heavy requirement to rely on some existing pn dictionary (often these resources are available on the web), its coverage of a domain corpus may be rather low, in absence of manual updating.
this approach has the advantage of being able to recommend previously unrated items to users with unique interests and to provide explanations for its recommendations.
the changes may occur both on the transmission side (the nature of the streams can change), and on the reception side (the interest of a user can change).
however, as the number of positive training data decreases, the boundary of svmc starts overfitting at some point and end up generating very poor results.
in this case, the label of one entity (e.g., the topic of the paper) is often correlated with the labels of related entities.
performance of rankboost is somewhat inferior to that of a state-of-the-art routing algorithm which is, however, more complex and less theoretically justified than rankboost.
using corpus statistics to remove redundant words in text categorization}, journal = {journal of the american society for information science}, year = {1996}, volume = {47}, number = {5}, pages = {357--369}, url = {http://www3.interscience.wiley.com/cgi-bin/fulltext?id=57757&placebo=ie.pdf}, abstract =
automatic text classification is necessary to store documents like that.
the best variant, which we call lazyboosting, is tested on the largest sense-tagged corpus available containing 192,800 examples of the 191 most frequent and ambiguous
integrating {wordnet} knowledge to supplement training data in semi-supervised agglomerative hierarchical clustering for text categorization}, journal = {international journal of intelligent systems}, pages = {929--947}, year = {2001}, volume = {16}, number = {8}, url = {http://www3.interscience.wiley.com/cgi-bin/fulltext?id=84503376&placebo=ie.pdf}, abstract = {
with this extended model, we also have improved the well-known probabilistic classification method based on the bernoulli document generation model.
we have developed a text classifier that misclassifies only 13\% of the documents in the reuters benchmark; this is comparable to the best results ever obtained.
in this paper, we measure the importance of sentences using text summarization techniques.
amherst, us}, year = {1994}, url = {http://www.cs.utah.edu/~riloff/psfiles/single-thesis.ps}, abstract = {knowledge-based natural language processing systems have achieved good success with many tasks, but they often require many person-months of effort to build an appropriate knowledge base.
organizing search results allows users to focus on items in categories of interest rather than having to browse through all the results sequentially.}, } @inproceedings{chen00a, author = {hao chen and tin kam ho}, title = {evaluation of decision forests on text categorization}, booktitle = {proceedings of the 7th spie conference on document recognition and retrieval}, publisher =
experiments with the purely theoretical approach and with several heuristic variations show that heuristic assumptions may yield significant improvements.}, } @article{fuhr94, author = {norbert fuhr and ulrich pfeifer}, title =
we discovered that misclassifications by the citation-link based classifiers are in fact difficult cases, hard to classify even for humans."
using various data sets, their performances are investigated and compared to a standard centroid-based classifier (tdidf) and a centroid-based classifier modified with information gain.
this means that two (or several) different techniques are used to optimize the performances even if a single algorithm may have more chances to operate the right choices.
the results show that the method outperforms svm at multi-class categorization, and interestingly, that results correlate strongly with compression-based methods.}, } @inproceedings{kim00, author = {yu-hwan kim and shang-yoon hahn and byoung-tak zhang}, title = {text filtering by boosting naive bayes classifiers}, booktitle = {proceedings of sigir-00, 23rd acm international conference on research and development in information retrieval}, editor = {nicholas j. belkin and peter ingwersen and mun-kew leong}, publisher = {acm press, new york, us}, address = {athens, gr}, year = {2000}, pages =
in this paper, we present a method for automatic genre classification that is based on statistically selected features obtained from both subject-classified and genre classified training data.
experiments on a large-scale chinese document collection with 71,674 texts show that the f1 metric of categorization performance of bwm-nbs gets to 94.9\% in the best case, which is 26.4\% higher than that of tf*idf, 19.1\% higher than that of tf*idf*ig, and 5.8\% higher than that of bwm under the same condition.
experimental results show that the methods are a significant improvement over previously used methods in a number of areas.
simulation results on both toy-data settings and an actual application on internet chat line discussion analysis is presented by way of demonstration.}, } @inproceedings{kao03, author = {anne kao and lesley quach and steve poteet and steve woods}, title = {
as the result of collaborative research with friends of the earth, an environmental issues campaigning organisation, we have developed a general purpose information classification agent architecture and have applied it to the problem of document classification and routing.
finally, the relative performance of the classifiers being tested provided insights into their strengths and limitations for solving classification problems involving diverse and often noisy web pages.}, } @inproceedings{yang03, author =
we also present preliminary results showing how this model could classify documents with dtds not represented in the training set.}, } @inproceedings{denoyer03a, author = {ludovic denoyer and jean-no{\"{e}}l vittaut and patrick gallinari and sylvie brunessaux and stephan brunessaux}, title = {structured multimedia document classification}, booktitle = {proceedings of doceng-03, acm symposium on document engineering}, publisher = {acm press, new york, us}, editor = {}, year = {2003}, address = {grenoble, fr}, pages = {153--160}, url = {http://doi.acm.org/10.1145/958220.958249}, abstract = {
this paper presents an empirical comparison of twelve feature selection methods (e.g.\ information gain) evaluated on a benchmark of 229 text classification problem instances that were gathered from reuters, trec, ohsumed, etc.
the rapid growth of data in large databases, such as text databases and scientific databases, requires efficient computer methods for automating analyses of the data with the goal of acquiring knowledges or making discoveries.
second, undirected models are well suited for discriminative training, where we optimize the conditional likelihood of the labels given the features, which generally improves classification accuracy.
it is therefore worthwhile to understand whether such good performance is unique to the svm design, or if it can also be achieved by other linear classification methods.
published in the ``lecture notes in computer science'' series, number 2431}, url = {http://springerlink.metapress.com/openurl.asp?genre=article&issn=0302-9743&volume=2431&spage=150}, abstract = {good feature selection is essential for text classification to make it tractable for machine learning, and to improve classification performance.
the analysis gives theoretical insight into the heuristics used in the rocchio algorithm, particularly the word weighting scheme and the similarity metric.
furthermore, by combining these methods, we significantly reduced the variance in performance of our event tracking system over different data collections, suggesting a robust solution for parameter optimization.}, } @inproceedings{yang00a, author =
our method is unique in that decision lists are automatically constructed on the basis of the principle of minimizing extended stochastic complexity (esc), and with it we are able to construct decision lists that have fewer errors in classification.
it is used here for classifying xml documents.
it is concluded that, while there is no significant difference in the predictive efficiency between the bayesian and the factor score methods, automatic document classification is enhanced by the use of a factor-analytically-derived classification schedule.
our results show that overall, svms and k-nn lsa perform better than the other methods, in a statistically significant way.}, } @incollection{caropreso01, author = {maria fernanda caropreso and stan matwin and fabrizio sebastiani}, title = {a learner-independent evaluation of the usefulness of statistical phrases for automated text categorization}, year = {2001}, booktitle = {text databases and document management: theory and practice}, editor = {amita g. chin}, publisher = {idea group publishing}, address =
experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classification hierarchies consisting of hundreds of nodes.
in this paper we present a method for detecting the text genre quickly and easily following an approach originally proposed in authorship attribution studies which uses as style markers the frequencies of occurrence of the most frequent words in a training corpus (burrows, 1992).
the results show that filtering significantly improves the recall of the method, and that also has the effect of significantly improving the overall performance.} } @article{combarromdrm05, title =
we describe here an n-gram-based approach to text categorization that is tolerant of textual errors.
we find that adding the words in the linked neighborhood to the page having those links (both inlinks and outlinks) were helpful for all our classifiers on one data set, but more harmful than helpful for two out of the three classifiers on the remaining datasets.
in spite of these differences, both ripper and sleeping-experts perform extremely well across a wide variety of categorization problems, generally outperforming previously applied learning methods.
the biological literature presents a difficult challenge to information processing in its complexity, diversity, and in its sheer volume.
using a massively parallel supercomputer, we leverage the information already contained in the thousands of coded stories and are able to code a story in about 2 seconds.
classifying text documents by associating terms with text categories}, booktitle = {proceedings of the 13th australasian conference on database technologies}, publisher = {acm press, new york, us}, year = {2002}, pages = {215--222}, address = {melbourne, au}, volume = {5}, url = {}, note =
exploiting hierarchy in text categorization}, journal = {information retrieval}, number = {3}, volume = {1}, pages =
morgan kaufmann publishers, san francisco, us}, url = {http://www.ai.mit.edu/~jrennie/papers/icml03-nb.pdf}, abstract = {naive bayes is often used as a baseline in text classification because it is fast and easy to implement.
the system is based on calculating and comparing profiles of n-gram frequencies.
abis compares documents with the past situations and finds the similarity scores on the basis of a memory-based reasoning approach.}, } @article{amati99, author = {
published in the ``lecture notes in computer science'' series, number 2291}, pages = {213--228}, url = {http://link.springer.de/link/service/series/0558/papers/2291/22910213.pdf}, abstract = {typical text classifiers learn from example and training documents that have been manually categorized.
at the end, we also share the results of a survey conducted with this years cup participants.
we take advantage of this fact by using a hierarchically organized neural network, built up from a number of independent self-organizing maps in order to enable the true establishment of a document taxonomy.
text-classification methods have thus far not easily incorporated numerical features.
this problem can be tackled since a couple of recent learners (ripper and scar) do not require a preprocessing step.
we also report results of making actual use of the selected $n$-grams in the context of a linear classifier induced by means of the rocchio method.}, } @inproceedings{carreras01, author =
we do so by applying feature selection to the pool of all $k$-grams ($k\leq n$), and checking how many $n$-grams score high enough to be selected in the top $\sigma$ $k$-grams.
in many cases, users would like to search for information of a certain 'object', rather than a web page containing the query terms.
experiments on 20 newsgroups (20ng), reuters corpus volume 1 (rcv1) and open directory project (odp) data show that ocfs is consistently better than ig and chi with smaller computation time especially when the reduced dimension is extremely small."
the model is evaluated on the reuters test collection and compared to the multinomial naive bayes model.
while previous work in hierarchical classification focused on virtual category trees where documents are assigned only to the leaf categories, we propose a top-down level-based classification method that can classify documents to both leaf and internal categories.
we conclude by comparing our approach of representing texts and rules as logic programs to others.}, } @inproceedings{junker01, author = {markus junker and andreas dengel}, title = {preventing overfitting in learning text patterns for document categorization}, booktitle = {proceedings of icapr-01, 2nd international conference on advances in pattern recognition}, publisher = {springer verlag, heidelberg, de}, note =
these metrics are shown to be good predictors of categorization accuracy that can be achieved on a dataset, and serve as efficient heuristics for generating datasets subject to user's requirements.
this situation occurs, for instance, in declassifying documents that have been previously considered important to national security and thus are currently being kept as secret.
in this paper we present empirical results on the performance of a bayesian classifier and a decision tree learning algorithm on two text categorization data sets.
in the first stage, the queries are enriched such that for each query, its related web pages together with their category information are collected through the use of search engines.
because the sense distinctions made are coarse, the disambiguation can be accomplished without the expense of knowledge bases or inference mechanisms.
experimental results show that this variant provides an order of magnitude further improvement in training efficiency.
furthermore, they are fully automatic, eliminating the need for manual parameter tuning.}, } @inproceedings{joachims99, author =
the first set of results applies rankboost to a text representation produced using modern term weighting methods.
rather than performing lsi's singular value decomposition (svd) process solely on the training data, we instead use an expanded term-by-document matrix that includes both the labeled data as well as any available and relevant background text.
the linear combination approach makes use of limited knowledge in the training document set.
we report the results of a study on topic spotting in conversational speech.
it is easier to find similar documents which use different nomenclature.
the agreement between the automated grader and the final manual grade was as good as the agreement between human graders.}, } @inproceedings{larkey99, author =
using a massively parallel supercomputer, we leverage the information already contained in the thousands of coded stories and are able to code a story in about 2 seconds.
this can be considered as the effective combination of documents with no topic or class labels (unlabeled data), labeled documents, and prior domain knowledge (in the form of the known hierarchic structure), in providing enhanced document classification performance.}, } @inproceedings{vinot03, author = {
our algorithm automatically induces a very natural behavior, where our knowledge about one instance helps us classify related ones, which in turn help us classify others.
a combination of all the above results in a multi-stage ned system that performs much better than baseline single-stage ned systems.}, } @inproceedings{kwok98, author = {james t. kwok}, title = {automated text categorization using support vector machine}, booktitle = {proceedings of iconip'98, 5th international conference on neural information processing},
our experiments on human subjects indicate that human feedback on feature relevance can identify a sufficient proportion of the most relevant features (over 50% in our experiments).
despite a limited number of training examples, combining an effective feature selection with the chi(2) learning algorithm for training the text classifier results in an adequate categorization of new magazine articles.}, } @inproceedings{mooney00, author = {raymond j. mooney and loriene roy}, title = {content-based book recommending using learning for text categorization}, booktitle = {proceedings of dl-00, 5th acm conference on digital libraries}, editor = {}, publisher = {acm press, new york, us}, year =
although we usually estimate the model so that it completely satisfies the equality constraints on feature expectations with the me method, complete satisfaction leads to undesirable overfitting, especially for sparse features, since the constraints derived from a limited amount of training data are always uncertain.
the results showed that our method performs well for reuters text collection when enough training documents are given and the new measures have indeed considered the contributions of misclassified documents.}, } @article{sun03, author = {aixin sun and ee-peng lim and wee-keong ng}, title = {performance measurement framework for hierarchical text classification}, journal = {journal of the american society for information science and technology}, year = {2003}, volume = {54}, number = {11}, pages = {1014--1028}, url = {http://www.cais.ntu.edu.sg/~sunaixin/paper/sun_jasist03.pdf}, abstract = {hierarchical text classification or simply hierarchical classification refers to assigning a document to one or more suitable categories from a hierarchical category space.
first, a chain augmented naive bayes model relaxes some of the independence assumptions of naive bayes--allowing a local markov chain dependence in the observed variables--while still permitting efficient inference and learning.
the simplicity of the model, the high recall precision rates, and the efficient computation together make expnet preferable as a practical solution for real world applications.}, } @inproceedings{yang95, author = {
the experiments compare the performance of a counterpropagation network against a backpropagation neural network.
the results reveal that a new feature selection metric we call 'bi-normal separation' (bns), outperformed the others by a substantial margin in most situations.
"it is often useful to classify email according to the intent of the sender (e.g., 'propose a meeting', 'deliver information').
on the other hand, other techniques naturally extensible to handle multi-class classification are generally not as accurate as svm.
second, we present the word-augmented relevancy signatures algorithm that uses lexical items to represent domain-specific role relationships instead of semantic features.
however, published work on combining text categorizers suggests that, for this particular application, improvements in performance are hard to attain.
we present detailed experimental results using naive bayes and support vector machines on the 20 newsgroups data set and a 3-level hierarchy of html documents collected from dmoz open directory.}, } @article{dhillon03, author = {inderjit dhillon and subramanyam mallela and rahul kumar}, title = {
however, it does show that tfidf conversion and document length normalization are important.
our results confirm, quantify, and extend previous research using web structure in these areas, introducing new methods for classification and description of pages.}, } @inproceedings{godbole04, author = {shantanu godbole and abhay harpale and sunita sarawagi and soumen chakrabarti}, title = {document classification through interactive supervision of document and term labels}, booktitle = {proceedings of pkdd-04, 8th european conference on principles of data mining and knowledge discovery}, editor = {jean-fran{\c{c}}ois boulicaut and floriana esposito and fosca giannotti and dino pedreschi}, address = {
the accuracy of classification achieved with our method appears better than or comparable to those of existing rule-based methods.}, } @inproceedings{liao02, author = {yihua liao and v. rao vemuri}, title = {
we are facing a new challenge due to the fact that web documents have a rich structure and are highly heterogeneous.
thus, there is the problem of determining what information is relevant to the user and how this decision can be taken by a supporting system.
in the second phase, to clarify the impact of this performance on filtering, different types of user profiles were created by grouping subsets of classes based on their individual classification accuracy rates.
in these experiments we show that, after only one epoch of training, our algorithm performs much better than perceptron-based hierarchical classifiers, and reasonably close to a hierarchical support vector machine.} } @article{diaz04, author = {irene d{\'{\i}}az and jos{\'{e}} ranilla and elena monta{\~{n}}es and javier fern{\'{a}}ndez and el{\'{\i}}as f. combarro}, title = {
we propose a new hierarchical generative model for textual data, where words may be generated by topic specific distributions at any level in the hierarchy.
the final classification of test documents is determined by a majority voting from the individual classifications of each feature.
{integrating linguistic resources in an uniform way for text classification tasks}, booktitle = {proceedings of lrec-98, 1st international conference on language resources and evaluation}, publisher = {}, editor = {antonio rubio and natividad gallardo and rosa castro and antonio tejada}, address = {grenada, es}, pages = {1197--1204}, year = {1998}, url = {http://www.esi.uem.es/laboratorios/sinai/postscripts/lrec98.ps}, abstract = {applications based on automatic text classification tasks, like text categorization (tc), word sense disambiguation (wsd), text filtering or routing, monolingual or multilingual information retrieval, and text summarization could obtain serious improvements by integrating linguistic resources in the current methods.
the data-driven nature of tcs allows it is to satisfy fully the requirements of ease of application development, portability to other applications and maintainability.}, } @article{he03, author = {
this paper presents a new method for graph-based classification, with particular emphasis on hyperlinked text documents but broader applicability.
our experiments on hierarchical classification methods based on svm classifiers and binary naive bayes classifiers showed that svm classifiers perform better than naive bayes classifiers on reuters-21578 collection according to the extended measures.
the goal of these experiments was to automatically discover classification patterns that can be used for assignment of topics to the individual newswires.
the described experimental study shows that the idf transform considerably effects the distribution of classification performance over feature selection reduction rates, and offers an evaluation method which permits the discovery of relationships between different document representations and feature selection methods which is independent of absolute differences in classification performance.}
it connects the statistical properties of text-classification tasks with the generalization performance of a svm in a quantitative way.
we present experimental results on the \textsf{reuters-21578} text categorization collection, showing that for both algorithms the version with discretized continuous attributes outperforms the version with traditional binary representations.}, } @inproceedings{ng97, author =
in this study, we adapt a simple text classifier (rocchio), using weakly supervised clustering techniques.
in the traditional setting, text categorization is formulated as a concept learning problem where each instance is a single isolated document.
furthermore we demonstrate that the hsom is able to map large text collections in a semantically meaningful way and therefore allows a ``semantic browsing'' of text databases.}, } @article{paijmans98, author = {paijmans, hans}, title = {
our experiments, make use of the reuters 21578 database of documents and consist of a binary classification for each of the ten most populous categories of the reuters database.
we propose a theory of genres as bundles of facets, which correlate with various surface cues, and argue that genre detection based on surface cues is as successful as detection based on deeper structural properties.}, } @inproceedings{khmelev03, author =
visual keywords can be constructed automatically from samples of visual data through supervised/unsupervised learning.
the results show that our new approach outperforms the latest lnn approach and linear classifiers in all experiments.}, } @inproceedings{lam99, author = {savio l. lam and dik l. lee}, title = {feature reduction for neural network based text categorization}, booktitle = {proceedings of dasfaa-99, 6th ieee international conference on database advanced systems for advanced application}, editor = {
this approach allows us to tune the combination system on available but less-representative validation data and obtain smaller performance degradation of this system on the evaluation data than using a single-method classifier alone.
the feature sets are based on the ``latent semantics'' of a reference library - a collection of documents adequately representing the desired concepts.
as we show, each of these smaller problems can be solved accurately by focusing only on a very small set of features, those relevant to the task at hand.
we show how to learn such models efficiently from data.
the user interface helps users search in fields without requiring the knowledge of inquery query operators.
experimental results show that the choice of thresholding strategy can significantly influence the performance of knn, and that the "optimal" strategy may vary by application.
documents are assigned to categories based on these rankings.
new filtering and disambiguation methods are used as pre-processing to solve the problems caused by the use of the thesaurus.
empirical evaluation results indicate that the proposed technique, mice, was more effective than the category discovery approach and was insensitive to the quality of original categories.}, } @article{weigend99, author = {andreas s. weigend and erik d. wiener and jan o. pedersen}, title = {
as a classifier, we adopted a variant of k-nearest neighbor (knn) with supervised term weighting schemes to improve the performance, making our method among the top-performing systems in the trec official evaluation.
then the final output of the pca is combined with the feature vectors from the class-profile which contains the most regular words in each class.
in practice, the assumption is too restrictive since a web page itself may not always correspond to a concept instance of some semantic concept (or category) given to the classification task.
our results show that naive bayes is a weak choice for guiding a topical crawler when compared with support vector machine or neural network.
these algorithms make extremely fast decisions, because they need to examine only a small number of words in each text document.
once again, lsi slightly improves performance.
we present experimental results obtained on the standard \textsf{reuters-21578} benchmark with one classifier learning method (support vector machines), three term selection functions (information gain, chi-square, and gain ratio), and both local and global term selection and weighting.}, } @inproceedings{debole04, author = {franca debole and fabrizio sebastiani}, title = {an analysis of the relative difficulty of reuters-21578 subsets}, year = {2004}, booktitle = {proceedings of lrec-04, 4th international conference on language resources and evaluation}, address = {lisbon, pt}, pages = {}, url = {http://www.math.unipd.it/~fabseb60/publications/lrec04.pdf}, abstract = {
the system has been tested in two applications in particular, one concerning passive invoices and the other bank documents.
a probabilistic classification procedure computes indexing weights for each relevance description.
this paper presents an empirical comparison of twelve feature selection methods (e.g.\ information gain) evaluated on a benchmark of 229 text classification problem instances that were gathered from reuters, trec, ohsumed, etc.
athena satisfies these requirements through linear-time classification and clustering engines which are applied interactively to speed the development of accurate models.
to classify a database, our algorithm does not retrieve or in-spect any documents or pages from the database, but rather just exploits the number of matches that each query probe generates at the database in question.
it is easier to find similar documents which use different nomenclature.
in comparison to the previously proposed agglomerative strategies our divisive algorithm is much faster and achieves comparable or higher classification accuracies.
first, a chain augmented naive bayes model relaxes some of the independence assumptions of naive bayes--allowing a local markov chain dependence in the observed variables--while still permitting efficient inference and learning.
we devise an algorithm that interleaves labeling features and documents which significantly accelerates standard active learning in our simulation experiments.
in addition, this paper investigates how these term distributions contribute to weight each term in documents, e.g., a high term distribution of a word promotes or demotes importance or classification power of that word.
an application of the network model is described, followed by an indexing example and some experimental results about the indexing performance of the network model.}, } @article{uren02, author = {victoria s. uren and thomas r. addis}, title = {how weak categorizers based upon different principles strengthen performance}, journal = {the computer journal}, year = {2002}, volume = {45}, number = {5}, pages = {511--524}, url = {http://www3.oup.co.uk/computer_journal/hdb/volume_45/issue_05/pdf/450511.pdf}, abstract =
this advantage is achieved by executing morphological and semantic analyses of an incoming text.
we report experiments that compare its performance with that of a well-known probabilistic classifier.
the personal view constructor mines user interests and maps them to a class hierarchy (i.e., personal view).
using restrictive classification and meta classification for junk elimination}, booktitle = {proceedings of ecir-05, 27th european conference on information retrieval}, publisher = {springer verlag}, editor = {david e. losada and juan m. fern{'{a}}ndez-luna}, address = {santiago de compostela, es}, year = {2005}, pages = {287--299}, url = {}, abstract = {}, } @inproceedings{siolas00, author = {siolas, georges and d'alche-buc, florence}, title = {support vector machines based on a semantic kernel for text categorization}, booktitle = {proceedings of ijcnn-00, 11th international joint conference on neural networks}, publisher = {ieee computer society press, los alamitos, us}, editor = {amari, shun-ichi and giles, c. lee and gori, marco and piuri, vincenzo}, year = {2000}, address = {como, it}, volume = {5}, pages =
we also investigate the usability of our automated learning approach by actually developing a system that categorizes texts into a tree of categories.
this paper reports a system that hierarchically classifies chinese web documents without dictionary support and segmentation procedure.
as the standard performance measures assume independence between categories, they have not considered the documents incorrectly classified into categories that are similar or not far from the correct ones in the category tree.
the technique can effectively rank the words in the unlabeled set according to their importance.
recurrent plausibility networks with local memory are developed and examined for learning robust text routing.
adaboost produces better classifiers than rocchio when the training collection contains a very large number of relevant documents.
in order to select a good hypothesis language (or model) from a collection of possible models, one has to assess the generalization performance of the hypothesis which is returned by a learner that is bound to use that model.
the technique can effectively rank the words in the unlabeled set according to their importance.
our experiments show our algorithms represent a good trade-off between speed and accuracy in most applications.}, } @inproceedings{cheong02, author = {cheong fung, gabriel p. and jeffrey x. yu and hongjun lu}, title = {discriminative category matching:
our results show that the new method is highly effective and promising." } @article{bang:2006:hdc, author = {s.l. bang and j.d. yang and h.j. yang}, title = {hierarchical document categorization with k-nn and concept-based thesauri}, journal = {information processing and management}, year = {2006}, volume = {42}, number = {2}, pages = {387--406}, abstract = {in this paper, we propose a new algorithm, which incorporates the relationships of concept-based thesauri into the document categorization using the k-nn classifier (k-nn).
integrating background knowledge into text classification}, pages = {1448--1449}, url = {}, booktitle = {proceedings of ijcai-03, 18th international joint conference on artificial intelligence}, editor =
{3}, month = {march}, pages = {1265--1287}, year = {2003}, url = {http://www.jmlr.org/papers/volume3/dhillon03a/dhillon03a.pdf}, abstract = {high dimensionality of text can be a deterrent in applying complex learners such as support vector machines to the task of text classification.
it is demonstrated that the performance of such a classifier is further enhanced when employing the kernel derived from an appropriate hierarchic mixture model used for partitioning a document corpus rather than the kernel associated with a at non-hierarchic mixture model.
such systems can be deployed in various applications where instantaneous interactive learning is necessary such as on-line email or news categorization, text summarization and information filtering in general.}, } @inproceedings{ghani00, author = {rayid ghani}, title = {using error-correcting codes for text classification},
we also analyse the relationships of news headlines and their contents of the new reuters corpus by a series of experiments.
these theoretical findings are supported by our experiments, which show that hyperbolic soms can successfully be applied to text categorization and yield results comparable to other state-of-the-art methods.
using density estimation over the raw tf*idf values, we obtain a classification accuracy of 82\%, a number that outperforms baseline estimates and earlier, image-based approaches, at least in the domain of news articles, and that nears the accuracy of humans who perform the same task with access to comparable information.}, } @inproceedings{sable01, author = {carl sable and ken church}, title = {using bins to empirically estimate term weights for text categorization}, booktitle = {proceedings of emnlp-01, 6th conference on empirical methods in natural language processing}, year = {2001}, publisher =
a stochastic decision list is an ordered sequence of if-then rules, and our method can be viewed as a rule-based method for text clsssification having advantages of readability and refinability of acquired knowledge.
as the results, the micro averaged f1 measure for reuters-21578 improved from 83.69 to 87.27\%.}, } @article{kehagias03, title =
i have discovered a bug in my experimental software which caused the relevance sampling results reported in the paper to be incorrect.
machine learning techniques are used on data collected from yahoo, a large text hierarchy of web documents.
we approached the task with careful consideration of the specialized terminology and paid special attention to dealing with various forms of gene synonyms, so as to exhaustively locate the occurrences of the target gene.
to this end, we propose a set of style markers including analysis-level measures that represent the way in which the input text has been analyzed and capture useful stylistic information without additional cost.
{seattle, us}, pages = {130--136}, url = {http://www.cs.utah.edu/~riloff/psfiles/sigir95.ps}, abstract = {most information retrieval systems use stopword lists and stemming algorithms.
we propose to exploit the natural hierarchy of topics, or taxonomy, that many corpora, such as internet directories, digital libraries, and patent databases enjoy.
document collections from the medline database and mayo patient records are used for studies on the effectiveness of our approach, and on how much the effectiveness depends on the choices of training data, indexing language, word-weighting scheme, and morphological canonicalization.
unsolicited commercial e-mail, or "spam", floods mailboxes, causing frustration, wasting bandwidth, and exposing minors to unsuitable content.
in our experiments we demonstrate significantly superior performance both over a single classifier as well as over the use of the traditional weighted-sum voting approach.
to advance research on this challenging but important problem, we promote a natural, experimental framework-the daily classification task-which can be applied to large time-based datasets, such as reuters rcv1.
managing the hierarchical organization of data is starting to play a key role in the knowledge management community due to the great amount of human resources needed to create and maintain these organized repositories of information.
"large scale learning is often realistic only in a semi-supervised setting where a small set of labeled examples is available together with a large collection of unlabeled data.
the former hints a generative model of news articles, and the latter provides data enriched environments to perform red.
we also observed that extracting meta data from related web sites was extremely useful for improving classification accuracy in some of those domains.
finally, we apply the ssahc algorithm to the reuters database of documents and show that its performance is superior to the bayes classifier and to the expectation-maximization algorithm combined with bayes classifier.
we report an experiment to study the impact of term redundancy on the performance of text classifier.
even though most of these features are relevant, the underlying concepts can be concisely captured using only a few features, while keeping all of them has substantially detrimental effect on categorization accuracy.
we report on experiences with the reuters newswire benchmark, the us patent database, and web document samples from yahoo!.
{205--209}, url = {http://dlib.computer.org/conferen/ijcnn/0619/pdf/06193581.pdf}, abstract = {we propose to solve a text categorization task using a new metric between documents, based on a priori semantic knowledge about words.
we treat a document as a set of phrases, which the learning algorithm must learn to classify as positive or negative examples of keyphrases.
different transformations of input data: stemming, normalization, logtf and idf, together with dimensionality reduction, are found to have a statistically significant improving or degrading effect on classification performance measured by classical metrics -- accuracy, precision, recall, f$_1$ and f$_2$. the emphasis of the study is not on determining the best document representation which corresponds to each classifier, but rather on describing the effects of every individual transformation on classification, together with their mutual relationships.} } @inproceedings{radovanovic:2006:ibd, author =
integrating background knowledge into nearest-neighbor text classification}, pages = {1--5}, url = {}, booktitle = {proceedings of eccbr-02, 6th european conference on case-based reasoning}, editor = {
however the existing clustering, techniques are agglomerative in nature and result in (i) suboptimal word clusters and (ii) high computational cost.
further, the categorization system can be trained on noisy ocr output, without need for the true text of any image, or for editing of ocr output.
furthermore, detailed analysis of the retrieval performance on each individual test query is provided.}, } @inproceedings{lang95, author = {ken lang}, title = {{\sc newsweeder}: learning to filter netnews}, booktitle = {proceedings of icml-95, 12th international conference on machine learning}, editor = {armand prieditis and stuart j. russell}, address = {lake tahoe, us}, pages = {331--339}, year = {1995}, publisher =
typical text classifiers learn from example texts that are manually categorized.
last but not least, we describe an evaluation experiment that classifies professional nature scenery photographs to demonstrate the effectiveness and efficiency of visual keywords for automatic categorization of images in digital libraries.}, } @article{liu01, author =
a generic system for text categorization is presented which is based on statistical analysis of representative text corpora.
the ability to cheaply train text classifiers is critical to their use in information retrieval, content analysis, natural language processing, and other tasks involving data which is partly or fully textual.
intuitively, one would like each of the two kernels to contribute information that is not available to the other.
in this paper, we present pva, an adaptive personal view information agent system for tracking, learning and managing user interests in internet documents.
we compare the effectiveness of five different automatic learning algorithms for text categorization in terms of learning speed, real-time classification speed, and classification accuracy.
this method might, however, degrade classification results, since the distributions it employs are not always precise enough for representing the differences between categories.
the system employs several knowledge sources including a letter database, word frequency statistics for german, lists of message type specific words, morphological knowledge as well as the underlying document structure.
the goal of these methods is to discover automatically classification patterns that can be used for general document categorization or personalized filtering of free text.
comparisons with an optimized version of the traditional rocchio's algorithm adapted for text categorization, as well as flat neural network classifiers are provided.
for example, rather than choosing one set decision threshold, they can be used in a bayesian risk model to issue a run-time decision which minimizes a user-specified cost function dynamically chosen at prediction time.
we describe the algorithm and present experimental results on applying it to the document routing problem.
this dissertation introduces a new theoretical model for text classification systems, including systems for document retrieval, automated indexing, electronic mail filtering, and similar tasks.
our experiments clearly indicate that automatic categorization improves the retrieval performance compared with no categorization.
such topics can be used as descriptors, similarly to the way librarians use for example, the library of congress cataloging system to annotate and categorize books.
therefore, the proposed novelty detection approach focuses on the identification of previously unseen query-related patterns in sentences.
we verify experimentally that ssfcm both outperforms and takes less time than the fuzzy-c-means (fcm) algorithm.
on the one hand, news articles are always aroused by events; on the other hand, similar articles reporting the same event often redundantly appear on many news sources.
the method is evaluated using backpropagation neural networks, as the machine learning algorithm, that learn to assign mesh categories to a subset of medline records.
a direct computation of this feature vector would involve a prohibitive amount of computation even for modest values of k, since the dimension of the feature space grows exponentially with k.
our analysis and empirical evaluation show substantial improvement in the accuracy of catalog integration.}, } @inproceedings{aizawa00, author = {akiko aizawa}, title = {
the algorithms analyze inter- and intra- document dynamics by considering how information evolves over time from article to article, as well as within individual articles.
it is built on top of a text-categorization paradigm where text articles are annotated with keywords organized in a hierarchical structure.
using an already coded training database of about 50,000 stories from the dow jones press release news wire, and seeker [stanfill] (a text retrieval system that supports relevance feedback) as the underlying match engine, codes are assigned to new, unseen stories with a recall of about 80\% and precision of about 70\%.
the learning algorithm combines an adaptive phase which instantly updates dictionary and weights during interaction and a tuning phase which fine tunes for performance using previously seen data.
we implemented pagetypesearch system based on our approach.
this task is easy to understand, but the lack of straightforward training set, subjective user intents of queries, poor information in short queries, and high noise level make the task very challenge.
on the other hand, a discriminative measure is proposed for term selection and is combined with the plu-based likelihood ratio to determine the text category.
instead of using a randomly selected training set, the learner has access to a pool of unlabeled instances and can request the labels for some number of them.
our experimental results on a large dataset confirm that the use of the implicit links is better than using explicit links in classification performance, with an increase of more than 10.5\% in terms of the macro-f1 measurement."
we present results comparing the performance of boostexter and a number of other text-categorization algorithms on a variety of tasks.
our results with the english newswire collection show a very large gain in performance as compared to published benchmarks, while our initial results with the german newswires appear very promising.
other linguistic resources that are emerging, like lexical databases, can also be used for classification tasks.
our experimental results demonstrate that the technique of refining the training set reduces from one-third to two-thirds of the storage.
the system is used as part of a commercial news clipping and retrieval product.
based on two chinese corpora, a series of controlled experiments evaluated their learning capabilities and efficiency in mining text classification knowledge.
using n-gram frequency profiles provides a simple and reliable way to categorize documents in a wide range of classification tasks.}, } @inproceedings{ceci03, author = {michelangelo ceci and donato malerba}, title = {hierarchical classification of html documents with {webclassii}}, booktitle = {proceedings of ecir-03, 25th european conference on information retrieval}, publisher =
by providing a formal analysis of the computational complexity of each classification method, followed by an investigation on the usage of different classifiers in a hierarchical setting of categorization, we show how the scalability of a method depends on the topology of the hierarchy and the category distributions.
therefore, an accurate classifier is an essential component of a hypertext database.
a document includes informative keywords and non-informative keywords.
text classification from labeled and unlabeled documents using em}, journal = {machine learning}, year = {2000}, number = {2/3}, volume = {39}, pages = {103--134}, url = {http://www.cs.cmu.edu/~knigam/papers/emcat-mlj99.ps}, abstract = {this paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents.
works in text retrieval through internet suggest that embedding linguistic information at a suitable level within traditional quantitative approaches (e.g. sense distinctions for query expansion as in [14]) is the crucial issue able to bring the experimental stage to operational results.
we find that simple averaging strategies do indeed improve performance, but that direct averaging of probability estimates is not the correct approach.
in concrete, we have evaluated a range of machine learning methods for the task (c4.5, naive bayes, part, support vector machines and rocchio), made cost sensitive through several methods (threshold optimization, instance weighting, and meta-cost).
we demonstrate that, although this categorization problem is quite different from 'topical' text classification, certain categories of messages can nonetheless be detected with high precision (above 80%) and reasonable recall (above 50%) using existing text-classification learning methods.
we present efficient approximate inference techniques based on variational methods and an em algorithm for empirical bayes parameter estimation.
the wpcm uses a neural network with inputs obtained by both the principal components and class profile-based features.
empirical results indicate that our approach outperforms the best published results on this reuters collection.
in addition, centralized approaches are more vulnerable to attacks or system failures and less robust in dealing with them.
very accurate text classifiers can be learned automatically from training examples.
the results show that the use of the hierarchical structure improves text categorization performance with respect to an equivalent flat model.
published in the ``lecture notes in computer science'' series, number 2291}, pages = {213--228}, url = {http://link.springer.de/link/service/series/0558/papers/2291/22910213.pdf}, abstract = {typical text classifiers learn from example and training documents that have been manually categorized.
we analyze the relative utility of document text, and the text in citing documents near the citation, for classification and description.
{springer science}, year = {2004}, volume = {7}, number = {3-4}, pages = {347--368}, abstract = {topic detection and tracking (tdt) is a research initiative that aims at techniques to organize news documents in terms of news events.
we also introduce decision functions for the centroid-based classification algorithm and support vector classifiers to handle the classification problem where a document may belong to multiple classes.
we have conducted an extensive experimental evaluation of our technique over collections of real documents, including over one hundred web-accessible databases.
experimental results show that winnow with thesauri attains high accuracy and that the proposed filtering and disambiguation methods also contribute to the improved accuracy.}, } @inproceedings{yang00, author = {yiming yang and thomas ault and thomas pierce and charles w. lattimer}, title = {
{elsevier science publishers, amsterdam, nl}, editor = {andr{\'e} lichnerowicz}, address = {barcelona, es}, year = {1991}, pages = {606--623}, url = {http://www.darmstadt.gmd.de/~tzeras/fullpapers/gz/fuhr-etal-91.ps.gz}, abstract = {air/x is a rule-based system for indexing with terms (descriptors) from a prescribed vocabulary.
maintaining catalogues manually is becoming increasingly difficult, due to the sheer amount of material on the web; it is thus becoming necessary to resort to techniques for the automatic classification of documents.
all of these methods showed significant improvement (up to 71\% reduction in weighted error rates) over the performance of the original knn algorithm on tdt benchmark collections, making knn among the top-performing systems in the recent tdt3 official evaluation.
the system has been tested in two applications in particular, one concerning passive invoices and the other bank documents.
however, separate databases of short system call sequences have to be built for different programs, and learning program profiles involves time-consuming training and testing processes.
c-evolve first finds highly accurate cluster digests (partial clusters), gets user feedback to merge and correct these digests, and then uses the classification algorithm to complete the partitioning of the data.
we present a unified view of text categorization systems, focusing on the selection of features.
our experiments with the acm computing classification scheme, using documents from the acm digital library, indicate that gp can discover similarity functions superior to those based solely on a single type of evidence.
the scheme for automatic text classification proposed in the paper, is based on document indexing, where a document is represented as a list of keywords.
we experimentally evaluate the quality of the lower dimensional spaces both in the context of document categorization and improvements in retrieval performance on a variety of different document collections.
this new synthesis of neural networks, learning and information retrieval techniques allows us to scale up to a real-world task and demonstrates a lot of potential for hybrid plausibility networks for semantic text routing agents on the internet.}, } @inproceedings{wermter99a, author = {stefan wermter and garen arevian and christo panchev}, title = {
instead of concentrating on a small set of ambiguous words, as done in most of the related previous work, all content words of the examined corpus are disambiguated.
new event detection is a challenging task that still offers scope for great improvement after years of effort.
an extended version appears as~\cite{cohen99}}, url = {http://www.research.whizbang.com/~wcohen/postscript/sigir-96.ps}, abstract = {two machine learning algorithms, ripper and sleeping experts for phrases, are evaluated on a number of large text categorization problems.
joining statistics with nlp for text categorization}, booktitle = {proceedings of anlp-92, 3rd conference on applied natural language processing}, publisher = {association for computational linguistics, morristown, us}, editor = {marcia bates and oliviero stock}, year = {1992}, address = {trento, it}, pages = {178--185}, url = {}, abstract = {automatic news categorization systems have produced high accuracy, consistency, and flexibility using some natural language processing techniques.
rcut is most natural for online response but is too coarse-grained for global or local optimization.
the study showed that the category interface is superior both in objective and subjective measures.
we focus on the robustness of these methods in dealing with a skewed category distribution, and their performance as function of the training-set category frequency.
a chinese documents classification system following above described techniques is implemented with naive bayes, knn and hierarchical classification methods.
the empirical evaluation indicates that the error rate (as obtained by running the naive bayes classifier on isolated pages) can be significantly reduced if contextual information is incorporated.}, } @inproceedings{frommholz01, author = {ingo frommholz}, title = {
we have used the receiver operating characteristic convex hull method for the evaluation, that best suits classification problems in which target conditions are not known, as it is the case.
we investigate a meta-model approach, called meta-learning using document feature characteristics (mudof), for the task of automatic textual document categorization.
the experimental results show that our new approaches give better results for both micro-averaged f1 and macro-averaged f1 scores.}, } @article{lehnert94, author = {
although our automated learning approach still gives a lower accuracy, by appropriately incorporating a set of manually chosen words to use as features, the combined, semi-automated approach yields accuracy close to the rule-based approach.}, } @article{nieto02, author = {salvador nieto s{\'{a}}nchez and evangelos triantaphyllou and donald kraft}, title = {
we compare pagetypesearch using the document type-indices with a conventional keyword-based search system in experiments.
it is found that decision forest outperforms both c4.5 and knn in all cases, and that category dependent term selection yields better accuracies.
the authors found that topic identification performance was maintained or slightly improved using character shape codes derived from images.}, } @article{stamatatos00, author = {efstathios stamatatos and nikos fakotakis and george kokkinakis}, title = {automatic text categorization in terms of genre and author}, journal = {computational linguistics}, pages = {471--495}, year = {2000}, number = {4}, volume = {26}, url = {}, abstract = {
therefore, it is very costly to assign a category for them because humans investigate their contents.
the best f1 value of our two solutions is 9.6% higher than the best of all other participants solutions.
mh boosting algorithm is applied to the word sense disambiguation (wsd) problem.
our experimental results indicate that the naive bayes classifier and the subspace method outperform the other two classifiers on our data sets.
the results showed that our method performs well for reuters text collection when enough training documents are given and the new measures have indeed considered the contributions of misclassified documents.}, } @article{sun03, author = {aixin sun and ee-peng lim and wee-keong ng}, title = {performance measurement framework for hierarchical text classification}, journal = {journal of the american society for information science and technology}, year = {2003}, volume = {54}, number = {11}, pages = {1014--1028}, url = {http://www.cais.ntu.edu.sg/~sunaixin/paper/sun_jasist03.pdf}, abstract = {hierarchical text classification or simply hierarchical classification refers to assigning a document to one or more suitable categories from a hierarchical category space.
we study the problem of classifying data in a given taxonomy when classifications associated with multiple and/or partial paths are allowed.
whereas most statistical approaches to text categorization derive classification knowledge based on training examples alone, aram performs supervised learning and integrates user-defined classification knowledge in the form of if-then rules.
experiments also were conducted to compare the performance between gp techniques and other fusion techniques such as genetic algorithms (ga) and linear fusion.
we applied the technique to several standard text collections and found that they contained a significant number of duplicate and plagiarised documents.
we report experiments that compare its performance with that of a well-known probabilistic classifier.
after training, the incoming news articles are classified based on their similarity to the existing newsgroup categories.
these levels control the ability of the categories to attract documents during the categorization process.
in this study, we adapt a simple text classifier (rocchio), using weakly supervised clustering techniques.
the experimental results show that the proposed method outperforms a direct application of a statistical learner often used for subject classification.
www indices, like {{\sc yahoo!}}\ provide a huge hierarchy of categories (topics) that touch every aspect of human endeavors.
furthermore, the supervised lower dimensional space greatly improves the retrieval performance when compared to lsi.}, } @inproceedings{kawatani02, author = {
for the hierarchical approach, we found the same accuracy using a sequential boolean decision rule and a multiplicative decision rule.
in addition, unlike other feature selection models --- which typically require different feature selection parameters for categories at different hierarchical levels --- our technique works equally well for all categories in a hierarchical structure.
finally, the system computes a distance measure between the document's profile and each of the category profiles.
we introduce a model for learning patterns for text categorization (the lpt-model) that does not rely on an attribute-value representation of documents but represents documents essentially "as they are".
since the sequential approach is much more efficient, requiring only 14\%-16\% of the comparisons used in the other approaches, we find it to be a good choice for classifying text into large hierarchical structures.} } @inproceedings{dumais98, author = {
on the one hand, news articles are always aroused by events; on the other hand, similar articles reporting the same event often redundantly appear on many news sources.
upon close examination of the algorithm, we concluded that the algorithm is most successful in correctly classifying more positive documents, but may cause more negative documents to be classified incorrectly.}, } @inproceedings{taskar01, author = {benjamin taskar and eran segal and daphne koller}, title = {probabilistic classification and clustering in relational data}, booktitle =
our implementation uses an inverted file to store the trained term structures of each newsgroup, and uses a list similar to the inverted file to buffer the newly arrival articles, for efficient routing and updating purposes.
"2005", pages = "294--303", abstract = "a web object is defined to represent any meaningful object embedded in web pages (e.g. images, music) or pointed to by hyperlinks (e.g. downloadable files).
further, the weak performance of naive bayes can be partly explained by extreme skewness of posterior probabilities generated by it.
our experiments on a variety of categorization tasks indicate that there is significant potential in improving classifier performance by feature re-weighting, beyond that achieved via membership queries alone (traditional active learning) if we have access to an oracle that can point to the important (most predictive) features.
we report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic lsi model.}, } @article{bloedorn98, author = {eric bloedorn and ryszard s. michalski}, title = {data-driven constructive induction}, journal = {ieee intelligent systems}, year = {1998}, number = {2}, volume = {13}, pages = {30--37}, url = {}, abstract = {an inductive learning program's ability to find an accurate hypothesis can depend on the quality of the representation space.
our results show that ridge regression seems to be the most promising candidate for rare class problems.}, } @inproceedings{zhdanova02, author = {anna v. zhdanova and denis v. shishkin}, title = {classification of email queries by topic:
we modify the query-by-committee (qbc) method of active learning to use the unlabeled pool for explicitly estimating document density when selecting examples for labeling.
we show that link information can be useful when the document collection has a sufficiently high link density and links are of sufficiently high quality.
we experiment with various types of descriptions for the {{\sc yahoo!}}\ categories and the webpages to be categorized.
hbc can reconstruct the original clusters more accurately than other non-probabilistic algorithms.
our experimental results using real netnews articles and newsgroups demonstrate (1) applying feature reduction to the training set improves the routing accuracy, efficiency, and database storage; (2) updating improves the routing accuracy; and (3) the batch technique improves the efficiency of the updating operation.}, } @inproceedings{huffman94, author = {stephen huffman and marc damashek}, title = {acquaintance: a novel vector-space n-gram technique for document categorization}, booktitle = {proceedings of trec-3, 3rd text retrieval conference}, publisher =
experiments using a number of e-mail documents generated by different authors on a set of topics gave promising results for both aggregated and multi-topic author categorisation.}, } @inproceedings{dhillon02, author = {inderjit dhillon and subramanyam mallela and rahul kumar}, title =
experiments also were conducted to compare the performance between gp techniques and other fusion techniques such as genetic algorithms (ga) and linear fusion.
it is called gaussian weighting and it is a supervised learning algorithm that, during the training phase, estimates two very simple and easily computable statistics which are: the presence \emphp, how much a term \emph{t} is present in a category \emph{c}; the expressiveness \emphe, how much \emph{t} is present outside \emph{c} in the rest of the domain.
however, once a misclassification occurs at a high level class, it may result in a class that is far apart from the correct one.
we address this open challenge by using a combination of classifiers with different performance characteristics to effectively reduce the performance variance on average of the overall system across all classes, including those not seen before.
our approach is especially suitable for applications of on-line text classification.}, } @inproceedings{zhou02a, author = {shuigeng zhou and jihong guan}, title = {chinese documents classification based on n-grams},
furthermore, the performance of the refined centroid classifier implemented is comparable, if not better, to that of state-of-the-art support vector machine (svm)-based classifier, but offers a much lower computational cost."
we show that by using large feature vectors in combination with feature reduction, we can train linear support vector machines that achieve high classification accuracy on data that present classification challenges even for a human annotator.
categorization by context}, journal = {journal of universal computer science}, year = {1998}, number = {9}, volume = {4}, pages = {719--736}, url = {http://www.jucs.org/jucs_4_9/categorisation_by_context}, abstract = {assistance in retrieving of documents on the world wide web is provided either by search engines, through keyword based queries, or by catalogues, which organise documents into hierarchical collections.
the wpcm uses a neural network with inputs obtained by both the principal components and class profile-based features.
the paper describes the technique of categorisation by context, which exploits the context perceivable from the structure of html documents to extract useful information for classifying the documents they refer to.
further, we show that selecting a best classification method using text-only features and then adding numerical features to the problem (as might happen if numerical features are only later added to a pre existing text-classification problem) gives performance that rivals a more time-consuming approach of evaluating all classification methods using the full set of both text and numerical features.}, } @article{maderlechner97, author = {maderlechner, g. and suda, p. and bruckner, t.}, title = {classification of documents by form and content}, journal =
volume = {36}, number = {3}, pages = {415--444}, url = {}, abstract = {document classifiers can play an intermediate role in multilevel filtering systems.
this can be seen as a complementary tool for topic detection and tracking applications.
{158}, pages = {69--88}, url = {http://dx.doi.org/10.1016/j.ins.2003.03.003}, abstract = {automatic categorization is the only viable method to deal with the scaling problem of the world wide web (www).
this is a novel approach for document classification, where each agent evolves a parse-tree representation of a user's particular information need.
both textual and non-textual information associated with the projects are used in the learning and classification phases.
using the same representation of categories, experiments show a significant improvement when the above mentioned method is used.
the authors show how this term-frequency approach supports a range of kdd operations, providing a general framework for knowledge discovery and exploration in collections of unstructured text.}, } @inproceedings{dagan97, author = {ido dagan and yael karov and dan roth}, title = {mistake-driven learning in text categorization}, booktitle = {proceedings of emnlp-97, 2nd conference on empirical methods in natural language processing}, publisher =
we report experiments on text classification of the cora and webkb data sets using probabilistic latent semantic analysis and probabilistic hypertext induced topic selection.
the research described in this paper combines weighted trigram analysis, clustering, and a special two-pool evolutionary algorithm, to create an adaptive information filtering system with such useful properties as domain independence, spelling error insensitivity, adaptability, and optimal use of user feedback while minimizing the amount of user feedback required to function properly.
furthermore, these experiments show that feature labeling takes much less (about 1/5th) time than document labeling.
however, the algorithm is much more efficient (because the learner does not have to be invoiced at all) and thus solves model selection problems with as many as a thousand relevant attributes and 12000 examples.}, } @inproceedings{schneider03, author = {{karl-michael} schneider}, year = {2003}, title = {a comparison of event models for naive bayes anti-spam e-mail filtering}, pages = {}, address = {}, editor = {}, booktitle = {proceedings of eacl-03, 11th conference of the european chapter of the association for computational linguistics}, url = {http://www.phil.uni-passau.de/linguistik/mitarbeiter/schneider/pub/eacl2003.pdf}, abstract = {}, } @inproceedings{schutze95, author = {hinrich sch{\"{u}}tze and david a. hull and jan o. pedersen}, title = {
we report on experiences with the reuters newswire benchmark, the us patent database, and web document samples from {{\sc yahoo!}}\.}, } @inproceedings{chakrabarti98b, author = {soumen chakrabarti and byron e. dom and piotr indyk}, title = {enhanced hypertext categorization using hyperlinks}, booktitle = {proceedings of sigmod-98, acm international conference on management of data},
we report here on experiments using a committee of winnow-based learners and demonstrate that this approach can reduce the number of labeled training examples required over that used by a single winnow learner by 1-2 orders of magnitude.}, } @inproceedings{liere98, author = {ray liere and prasad tadepalli}, title = {active learning with committees: preliminary results in comparing winnow and perceptron in text categorization}, booktitle = {proceedings of conald-98, 1st
using {wordnet} to complement training information in text categorization}, booktitle = {proceedings of ranlp-97, 2nd international conference on recent advances in natural language processing}, publisher = {}, editor = {ruslan milkov and nicolas nicolov and nilokai nikolov}, address =
on the use of bernoulli mixture models for text classification}, journal = {pattern recognition}, year = {2002}, volume = {35}, number = {12}, pages = {2705--2710}, url = {}, abstract = {mixture modelling of class-conditional densities is a standard pattern recognition technique.
latent semantic indexing (lsi) has been successfully used for ir purposes, as a technique for capturing semantic relations between terms and inserting them into the similarity measure between two documents.
in our approach, training data are represented as the projections of training documents on each feature.
combining the results of classifiers has shown much promise in machine learning generally.
"large scale learning is often realistic only in a semi-supervised setting where a small set of labeled examples is available together with a large collection of unlabeled data.
we discuss the role of importance-weights (e.g. document frequency and redundancy), which is not yet fully understood in the light of model complexity and calculation cost, and we show that time consuming lemmatization or stemming can be avoided even when classifying a highly inflectional language like german.}, } @article{lertnattee04, author = {verayuth lertnattee and thanaruk theeramunkong}, title = {effect of term distributions on centroid-based text categorization}, journal = {information sciences}, year = {2004}, number = {1}, volume = {158}, pages = {89--115}, url = {http://dx.doi.org/10.1016/j.ins.2003.07.007}, abstract = {
however, the exponential growth in the volume of on-line textual information makes it nearly impossible to maintain such taxonomic organization for large, fast-changing corpora by hand.
naive bayes classifiers are recognized to be among the best for classifying text.
in previous work, we developed several algorithms that use information extraction techniques to achieve high-precision text categorization.
the categorization, which consists in assigning an international code of disease (icd) to the medical document under examination, is based on well-known information retrieval techniques.
such a system will have to be adaptive to the user changing interest.
{acm press, new york, us}, address = {toronto, ca}, year = {2003}, pages = {26--32}, url = {http://doi.acm.org/10.1145/860435.860443}, abstract = {question classification is very important for question answering.
improved estimates of the term distributions are made by differentiation of words in the hierarchy according to their level of generality/specificity.
in this paper, we apply lsi to the routing task, which operates under the assumption that a sample of relevant and non-relevant documents is available to use in constructing the query.
our approach to text classification uses case-based reasoning to represent natural language contexts that can be used to classify texts with extremely high precision.
we showed also that ssahc helps ahc techniques to improve their performance.}, } @inproceedings{slattery00, author = {
classifier of pagetypesearch classifies web pages into the document types by comparing their pages with typical structural characteristics of the types.
the documents are retrieved by considering both the predicted relevance and its value as a training observation.
first, we train the linear svm on a subset of training data and retain only those features that correspond to highly weighted components (in absolute value sense) of the normal to the resulting hyperplane that separates positive and negative examples.
{http://www.cs.utah.edu/~riloff/psfiles/single-thesis.ps}, abstract = {knowledge-based natural language processing systems have achieved good success with many tasks, but they often require many person-months of effort to build an appropriate knowledge base.
the methods to create, detect, summarize, select, and code visual keywords will be detailed.
to avoid their being overflowed by the incoming data, methods of information filtering are required.
the results demonstrated that representative sampling offers excellent learning performance with fewer labeled documents and thus can reduce human efforts in text classification tasks.}, } @inproceedings{xue03, author = {dejun xue and maosong sun}, title =
it allows an effective exploitation of the available linguistic information that better emphasizes this latter with significant both data compression and accuracy.
this is because when the positive training data is too few, the boundary over-iterates and trespasses the natural gaps between positive and negative class in the feature space and thus ends up fitting tightly around the few positive training data.}, } @inproceedings{yu98, author = {
the result shows that term redundancy behaves very similar to noise and may degrade the classifier performance.
by using the hierarchically structured subject domain and classification rules, the classifier's engine assigns an email query to the most relevant category or categories.}, } @article{zheng04, author = {zhaohui zheng and xiaoyun wu and rohini srihari}, title = {
our experiments on the reuters-21578 benchmark show that boosting is not effective in improving the performance of the base classifiers on common categories.
next, we investigate the application of automatic categorization to text retrieval.
hyperlinks pose new problems not addressed in the extensive text classification literature.
our feature selection approach employs distributional clustering of words via the recently introduced information bottleneck method, which generates a more efficient word-cluster representation of documents.
the crucial question of the quality of automatic classification is treated at considerable length, and empirical data are introduced to support the hypothesis that classification quality improves as more information about each document is used for input to the classification program.
as the number of unique words in the collection set is big, the principal component analysis (pca) has been used to select the most relevant features for the classification.
a central problem in good text classification for information filtering and retrieval (if/ir) is the high dimensionality of the data.
furthermore, the effectiveness of word sense disambiguation for different parts of speech (nouns and verbs) is examined empirically.}, } @inproceedings{pang02, author = {
we also propose a simple and effective way of combining a traditional text based classifier with a citation-link based classifier.
the paper shows how a text classifier's need for labeled training documents can be reduced by taking advantage of a large pool of unlabeled documents.
these performance measures often assume independence between categories and do not consider documents misclassified into categories that are similar or not far from the correct categories in the category tree.
this model allows us to simultaneously take into account structure and content information.
it is developed for the naive bayesian classifier applied on text data, since it combines well with the addressed learning problems.
in addition, this paper investigates how these term distributions contribute to weight each term in documents, e.g., a high term distribution of a word promotes or demotes importance or classification power of that word.
however, published work on combining text categorizers suggests that, for this particular application, improvements in performance are hard to attain.
classifying text documents by associating terms with text categories}, booktitle = {proceedings of the 13th australasian conference on database technologies}, publisher = {acm press, new york, us}, year = {2002}, pages = {215--222}, address = {melbourne, au}, volume = {5}, url = {}, note =
effective algorithm for text corpus pruning is designed.
in all cases we show that our approach either outperforms other methods tried for these tasks or performs comparably to the best.}, } @article{ruiz02, author = {miguel ruiz and padmini srinivasan}, title = {hierarchical text classification using neural networks}, journal = {information retrieval}, number = {1}, volume = {5}, pages = {87--118}, year = {2002}, url = {http://www.wkap.nl/article.pdf?383232}, abstract = {
in order to reflect the subtopic structure of a document, we propose a new passage-level or passage-based text categorization model, which segments a test document into several passages, assigns categories to each passage, and merges the passage categories to the document categories.
we make available detailed, per-category experimental results, as well as corrected versions of the category assignments and taxonomy structures, via online appendices.}, } @inproceedings{lewis91, author = {lewis, david d.}, title = {data extraction as text categorization: an experiment with the {muc-3} corpus.}, booktitle =
our feature selection method strives to reduce redundancy between features while maintaining information gain in selecting appropriate features for text categorization.
reducing the dimensionality, or selecting a good subset of features, without sacrificing accuracy, is of great importance for neural networks to be successfully applied to the area.
our experimental results show that lb, an association-based lazy classifier can achieve a good tradeoff between high classification accuracy and scalability to large document collections and large feature sizes.}, } @article{merkl98, author = {merkl, dieter}, title = {
however, owing to the use of context-sensitive features, the classifier is very accurate.
experimental results are encouraging overall; in particular, document classification results fulfill the requirements of high-volume application.
our investigation leads to conclude that association rule mining is a good and promising strategy for efficient automatic text categorization.}, } @inproceedings{zelikovitz00, author = {sarah zelikovitz and haym hirsh}, title = {
we demonstrate that precision and recall can be significantly improved by solving the categorization problem taking hierarchy into account.
the crawling process is modeled as a parallel best-first search over a graph defined by the web.
we conclude by examining factors that make the sentiment classification problem more challenging.}, } @article{park04, author = {seong-bae park and byoung-tak zhang}, title = {co-trained support vector machines for large scale unstructured document classification using unlabeled data and syntactic information}, journal = {information processing and management}, year = {2004}, volume = {40}, number = {3}, pages = {421--439}, url = {}, abstract = {}, } @inproceedings{peng03, author = {fuchun peng and dale schuurmans}, title =
the performance of the system using the neural network classifier was generally satisfactory and, as expected, the filtering performance varied with regard to the accuracy rates of classes.}, } @inproceedings{moulinier96, author = {isabelle moulinier and gailius ra{\u{s}}kinis and jean-gabriel ganascia}, title = {
though it is not a heavy requirement to rely on some existing pn dictionary (often these resources are available on the web), its coverage of a domain corpus may be rather low, in absence of manual updating.
senses are interpreted as groups (or clusters) of similar contexts of the ambiguous word.
we present results from text classification experiments that compare relevancy signatures, which use local linguistic context, with corresponding indexing terms that do not.
these tests also have investigated finding the best indexing terms that could be used in making these classification decisions.
a major challenge in indexing unstructured hypertext databases is to automatically extract meta-data that enables structured searching using topic taxonomies, circumvents keyword ambiguity and improves the quality of searching and profile-based routing and filtering.
finally, representing each newsgroup by k vectors (with k = 2 or 3) using clustering yields the most significant improvement in routing accuracy, ranging from 60\% to loo\%, while causing only slightly higher storage requirements.}, } @inproceedings{hsu99a, author =
comparing classification schemes}, journal = {acm transactions on information systems}, year = {2005}, volume = {23}, number = {4}, pages = {430--462}, url = {http://doi.acm.org/10.1145/1095872.1095875}, abstract = {topical crawling is a young and creative area of research that holds the promise of benefiting from several sophisticated data mining techniques.
the system is a component of a more extensive intelligent agent for adaptive information filtering on the web.
the other is that there are no training data for this classification problem.
this approach is motivated by the observation that hyperbolic spaces possess a geometry where the size of a neighborhood around a point increases exponentially and therefore provides more freedom to map a complex information space such as language into spatial relations.
the results of these computational experiments on a sample of 2897 text documents from the tipster collection indicate that the first approach has many advantages over the vsm approach for solving this type of text document classification problem.
profile filters the netnews and uses a scale of 11 predefined values of relevance.
the experimental results support the claim that a custom-designed algorithm (genex), incorporating specialized procedural domain knowledge, can generate better keyphrases than a general-purpose algorithm (c4.5).
although the corpus contains documents written in chinese, the proposed approach can be applied to documents written in any language and such documents can be transformed into a list of separated terms.}, } @inproceedings{yang01, author = {yiming yang}, title = {a study on thresholding strategies for text categorization}, booktitle = {proceedings of sigir-01, 24th acm international conference on research and development in information retrieval}, editor =
in spite of these differences, both ripper and sleeping experts perform extremely well across a wide variety of categorization problems, generally outperforming previously applied learning methods.
our system can be used in any application that requires fast and easily adaptable text categorization in terms of stylistically homogeneous categories.
if a category's recall exceeds its precision, the category is too strong and its level is reduced.
our algorithm automatically induces a very natural behavior, where our knowledge about one instance helps us classify related ones, which in turn help us classify others.
this could be due to the fact that when a word along with its adjoining word - a phrase - is considered towards building a category profile, it could be a good discriminator.
the category that contains the largest categorical points is selected as the category of a document.
we use support vector machine (svm) classifiers, which have been shown to be efficient and effective for classification, but not previously explored in the context of hierarchical classification.
improving linear classifier for chinese text categorization}, journal = {information processing and management}, year = {2004}, volume = {40}, number = {2}, pages = {223--237}, url = {}, abstract = {}, } @article{turney00, author = {peter d. turney}, title = {learning algorithms for keyphrase extraction}, journal = {information retrieval}, number = {4}, volume = {2}, pages = {303--336}, year = {2000}, url = {http://extractor.iit.nrc.ca/reports/ir2000.ps.z}, abstract = {many academic journals ask their authors to provide a list of about five to fifteen keywords, to appear on the first page of each article.
our approach also indicates a new promising way to use trust-worthy deep web knowledge to help organize dispersive information of surface web."
highly accurate text segmentation is also possible - the accuracy of the ppm-based chinese word segmenter is close to 99\% on chinese news text; similarly, a ppm-based method of segmenting text by language achieves an accuracy of over 99\%.}, } @inproceedings{teytaud01, author =
empirical evaluation results indicate that the proposed technique, mice, was more effective than the category discovery approach and was insensitive to the quality of original categories.}, } @article{weigend99, author = {andreas s. weigend and erik d. wiener and jan o. pedersen}, title = {
moreover, by ranking words and phrases in the citing documents according to expected entropy loss, we are able to accurately name clusters of web pages, even with very few positive examples.
mh$^kr$} is based on the idea to build, at every iteration of the learning phase, not a single classifier but a sub-committee of the $k$ classifiers which, at that iteration, look the most promising.
{65--72}, url = {http://www.cs.cmu.edu/~yiming/papers.yy/sigir00.ps}, abstract = {automated tracking of events from chronologically ordered document streams is a new challenge for statistical text classification.
the experimental results show that the proposed pattern-based approach significantly outperforms all three baselines in terms of precision at top ranks."
we find that our simple corrections result in a fast algorithm that is competitive with state-of-the-art text classification algorithms such as the support vector machine.}, } @inproceedings{rennie99, author = {jason rennie and andrew k. mccallum}, title = {using reinforcement learning to spider the web efficiently}, booktitle = {proceedings of icml-99, 16th international conference on machine learning}, editor = {ivan bratko and saso dzeroski}, year = {1999}, address = {bled, sl}, publisher = {morgan kaufmann publishers, san francisco, us}, pages = {335--343}, url = {http://www.watson.org/~jrennie/papers/icml99.ps.gz}, abstract = {consider the task of exploring the web in order to find pages of a particular kind or on a particular topic.
since the single layers of self-organizing maps represent different aspects of the document collection at different levels of detail, the neural network shows the document collection in a form comparable to an atlas where the user may easily select the most appropriate degree of granularity depending on the actual focus of interest during the exploration of the document collection.}, } @inproceedings{meyer04, author = {
to classify a database, our algorithm does not retrieve or in-spect any documents or pages from the database, but rather just exploits the number of matches that each query probe generates at the database in question.
by applying tdfa to the document set that belongs to a given class and a set of documents that is misclassified as belonging to that class by an existent classifier, we can obtain features that take large values in the given class but small ones in other classes, as well as features that take large values in other classes but small ones in the given class.
this paper proposes topic difference factor analysis (tdfa) as a method to extract projection axes that reflect topic differences between two document sets.
on investigating the root cause, we find that a large class of feature scoring methods suffers a pitfall: they can be blinded by a surplus of strongly predictive features for some classes, while largely ignoring features needed to discriminate difficult classes.
an empirical evaluation of the system was performed by means of a confidence interval technique.
the system does not perform a complete semantic or syntactic analyses of the input stories.
despite this, we show for one data set that fax quality images can be categorized with nearly the same accuracy as the original text.
as a practical matter, we also explain how the text classification system can be easily ported across domains.}, } @phdthesis{riloff94a, author =
in this research, our experiment dealt with the classification of news wire articles using category profiles.
such a system will have to be adaptive to the user changing interest.
comparisons with an optimized version of the traditional rocchio's algorithm adapted for text categorization, as well as flat neural network classifiers are provided.
{using text classification to predict the gene knockout behaviour of {s.\ cerevisiae}}, booktitle = {proceedings of apbc-03, 1st asia-pacific bioinformatics conference}, editor = {yi-ping p. chen}, publisher
however, most of them are useless for document categorization because of the weakness in representing document contents.
however, we have found that recognizing singular and plural nouns, verb forms, negation, and prepositions can produce dramatically different text classification results.
we demonstrate that the idc algorithm is especially advantageous when the data exhibits high attribute noise.
the construction steps often involve human efforts and are not completely automated.
such systems can be deployed in various applications where instantaneous interactive learning is necessary such as on-line email or news categorization, text summarization and information filtering in general.}, } @inproceedings{ghani00, author = {rayid ghani}, title = {using error-correcting codes for text classification},
overall both algorithms are comparable and are quite effective.
this method might, however, degrade classification results, since the distributions it employs are not always precise enough for representing the differences between categories.
in contrast, for the reuters collection, we only achieve mediocre results.
we use different feature sets and integrate neural network learning into the method.
when combined with the classification power of the svm, this method yields high performance in text categorization.
current text learning techniques for combining labeled and unlabeled, such as em and co-training, are mostly applicable for classification tasks with a small number of classes and do not scale up well for large multiclass problems.
experimental results obtained on three real-world data sets show that we can reduce the feature dimensionality by three orders of magnitude and lose only 2\% accuracy, significantly better than latent semantic indexing, class-based clustering, feature selection by mutual information, or markov-blanket-based feature selection.
with a smaller number of features, ssfcm's performance is also superior to that of ssahc's.
we examine more complex combination strategies but find them less successful due to the high correlations among our filtering methods which are optimized over the same training data and employ similar document representations.}, } @inproceedings{hull98, author = {david a. hull}, title = {
the second problem is that even with a representative model, the improvements given by unlabeled data do not sufficiently compensate for a paucity of labeled data.
the main idea of ferrety algorithm can be generalized for mapping one taxonomy to another if training documents are available."
text categorization algorithms usually represent documents as bags of words and consequently have to deal with huge numbers of features.
here, the challenges are: a) obtain a high accuracy classification model; b) consume low computational time for both model training and operation; and c) occupy low storage space.
in addition, more than 80\% of automatically extracted terms are meaningful.
we address these problems with a system for topical information space navigation that combines the query-based and taxonomic systems.
to our knowledge, this work is the first to report performance results with the entire new reuters corpus.}, } @article{craven00, author = {mark craven and dan dipasquo and dayne freitag and andrew k. mccallum and tom m. mitchell and kamal nigam and se{\'{a}}n slattery}, title = {
today, text categorization is a necessity due to the very large amount of text documents that we have to deal with daily.
our evaluations show that for web collections, the megadocument method clearly outperformes other classification methods.
in this paper, we propose a novel text classification approach, called discriminative category matching, which could achieve all of the stated characteristics.
as a result, they are not portable across domains.
* publications thet discuss related topics sometimes confused with % % atc; these include, in particular, text clustering (i.e. text % % classification by unsupervised learning) and text indexing; % % % % * technical reports and workshop papers.
a text is represented as a set of cases and we classify a text as relevant if any of its cases is deemed to be relevant.
naive use of terms in the link neighborhood of a document can even degrade accuracy.
experiments also show that the proportion of meaningful terms extracted from training data is relative to the classification accuracy in outside testing.}, } @inproceedings{lam01, author = {wai lam and kwok-yin lai}, title = {a meta-learning approach for text categorization}, booktitle = {proceedings of sigir-01, 24th acm international conference on research and development in information retrieval}, editor =
we report on experiences with the reuters newswire benchmark, the us patent database, and web document samples from {{\sc yahoo!}}\.}, } @inproceedings{chakrabarti98b, author = {soumen chakrabarti and byron e. dom and piotr indyk}, title = {enhanced hypertext categorization using hyperlinks}, booktitle = {proceedings of sigmod-98, acm international conference on management of data},
{information sciences}, year = {2004}, number = {1}, volume = {158}, pages = {69--88}, url = {http://dx.doi.org/10.1016/j.ins.2003.03.003}, abstract = {automatic categorization is the only viable method to deal with the scaling problem of the world wide web (www).
significant features are automatically derived from training texts by selecting substrings from actual word forms and applying statistical information and general linguistic knowledge.
it was found that typical categorization approaches produce predictions which are too similar for combining them to be effective since they tend to fail on the same records.
also the role of linguistic preprocessing seems to provide positive effects on the performance.
an important aspect of this dissertation is that autoslog and the text classification systems can be easily ported across domains.}, } @inproceedings{riloff95, author = {ellen riloff}, title = {
the impact of rule insertion is most significant for categories with a small number of relevant documents.}, } @article{tan02, author =
we found ig and chi most effective in our experiments.
words, contexts and senses are represented in word space, a high-dimensional real-valued space in which closeness corresponds to semantic similarity.
however, even the most successful techniques are defeated by many real-world applications that have a strong time-varying component.
ssahc is (i) a clustering algorithm that (ii) uses a finite design set of labeled data to (iii) help agglomerative hierarchical clustering (ahc) algorithms partition a finite set of unlabeled data and then (iv) terminates without the capability to label other objects.
the ddc method for subject indexing is very close to operational status for a data base which grows at the rate of two million words of text per year.}, } @inproceedings{klinkenberg00, author = {ralf klinkenberg and thorsten joachims}, title = {
this risk is found to be most severe when little data is available.
in order to accomplish this testing, we employ the em algorithm which helps efficiently estimate parameters in a finite mixture model.
in particular, we demonstrate (1) how variation in document length can be tolerated by either normalizing feature weights or by using negative weights, (2) the positive effect of applying a threshold range in training, (3) alternatives in considering feature frequency, and (4) the benefits of discarding features while training.
our evaluations show that for web collections, the megadocument method clearly outperformes other classification methods.
when tested on 50 user profiles and 550 megabytes of documents, results indicate that the feature set that is the basis of the text categorization module and the algorithm that establishes the boundary of categories of potentially relevant documents accomplish their tasks with a high level of performance.
the study showed that the category interface is superior both in objective and subjective measures.
optimizing and estimating effectiveness is greatly aided if classifiers that explicitly estimate the probability of class membership are used.}, } @article{lewis95a, author = {lewis, david d.}, title = {
we observed that our new method made a significant improvement in all classifiers and both data sets.}, } @article{ko04, author = {youngjoong ko and jinwoo park and jungyun seo}, title = {
in this paper, we compare learning techniques based on statistical classification to traditional methods of relevance feedback for the document routing problem.
kld method achieve substantial improvements over the tfidf performing method.}, } @article{blei03, author = {david m. blei and andrew y. ng and michael i. jordan}, title = {latent dirichlet allocation}, journal = {journal of machine learning research}, volume = {3}, pages = {993--1022}, year =
the proposed method shows a similar degree of performance, compared with the traditional supervised learning methods.
the accuracy of the supervised classifier was established by comparing its performance with a baseline system that uses human classification information.
this basic em procedure works well when the data conform to the generative assumptions of the model.
[1], and shows that some of the modifications included in twcnb may not be necessary to achieve optimum performance on some datasets.
our results show that an index can be constructed on a desktop workstation with little effect on categorisation accu-racy compared to a memory-based approach.
significant improvements in computational efficiency without losing categorization accuracy were evident in the testing results.}, } @article{yang96, author =
in our approach, first, layered lsi spaces are built for a better representation of the hierarchically structured domain knowledge, in order to emphasize the specific semantics and term space in each layer of the domain knowledge.
upon the fact that refined statistics may have more chance to meet sparse data problem, we re-evaluate the role of the binary weighting model (bwm) in tc for further consideration.
{chris hutchison and gaetano lanzarone}, year = {1999}, address = {varese, it}, pages = {105--119}, url = {http://www.math.unipd.it/~fabseb60/publications/thai99.pdf}, abstract = {assistance in retrieving documents on the world wide web is provided either by search engines, through keyword-based queries, or by catalogues, which organize documents into hierarchical collections.
such engines can build giant indices that let you quickly retrieve the set of all web pages containing a given word or string.
twenty subjects from expert or novice literary reading experience backgrounds were, in two experiments, required to rate two parallel sets of graphically and phonetically manipulated poems.
our experiments use reuters 21578 database and consist of binary classifications for categories selected from the 115 topics classes of the reuters collection.
the algorithms analyze inter- and intra- document dynamics by considering how information evolves over time from article to article, as well as within individual articles.
we find that simple averaging strategies do indeed improve performance, but that direct averaging of probability estimates is not the correct approach.
"like many purely data-driven machine learning methods, support vector machine (svm) classifiers are learned exclusively from the evidence presented in the training dataset; thus a larger training dataset is required for better performance.
in particular, boosting methods such as adaboost have shown good performance applied to real text data.
this calls for using a feature selection method, not only to reduce the number of features but also to increase the sparsity of document vectors.
this task is challenging since incoming messages may contain constructions which have not been anticipated.
the algorithm, which we proposed, operates in a fully automatic mode and requires no supervision or training data.
{acm press, new york, us}, address = {toronto, ca}, year = {2003}, pages = {182--189}, url = {http://doi.acm.org/10.1145/860435.860470}, abstract = {term-based representations of documents have found wide-spread use in information retrieval.
the importance of different features is reported.
the proposed architecture relies on hidden markov models whose emissions are bag-of-words according to a multinomial word event model, as in the generative portion of the naive bayes classifier.
this property causes k-nnfp to eliminate possible adverse effects of irrelevant features on the classification accuracy.
the method is quantitative analysis of the glosses of such definitions that these terms are given in on- dictionaries, and on the use of the resulting term representations semi-supervised term classification.
we present efficient approximate inference techniques based on variational methods and an em algorithm for empirical bayes parameter estimation.
we find that using words in web pages alone often yields suboptimal performance of classifiers, compared to exploiting additional sources of information beyond document content.
thus the classifier has a small model size and is very fast.
previous work used a similar clustering procedure to show that word-clusters can significantly reduce the feature space dimensionality, with only a minor change in classification accuracy.
the method combines information derived from n-grams (consecutive sequences of n characters) with a simple vector-space technique that makes sorting, categorization, and retrieval feasible in a large multilingual collection of documents.
to index successfully in the defense documentation center's environment, an automated system must chose single words or phrases (dependent upon context) rapidly and economically.
an object-oriented design allows easy addition of new preprocessors, machine learning algorithms, and classifier types.}, } @article{li02, author
the accuracy of modern text classification systems rivals that of trained human professionals, thanks to a combination of information retrieval (ir) technology and machine learning (ml) technology.
here, limited labeled data provide em initializations that lead to low-probability models.
in this paper we present a systematic, comparative experimental study of the three subsets of \textsf{reuters-21578} that have been most popular among tc researchers.
combined with the classification power of an svm, this method yields high performance text categorization that can outperform other recent methods in terms of categorization accuracy and representation efficiency.
different from existing categorization methods, mudof can automatically recommend a suitable algorithm for each category based on the category-specific statistical characteristics.
subjects liked the category interface much better than the list interface, and they were 50\% faster at finding information that was organized into categories.
svm is highly efficient in learning from well organized samples of moderate size, although on relatively large and noisy data the efficiency of svm and aram are comparable.}, } @article{heaps73, author = {h.s. heaps}, title = {a theory of relevance for automatic document classification}, year = {1973}, journal = {information and control}, volume = {22}, number = {3}, pages = {268-278}, url = {}, abstract = {}, } @inproceedings{hearst91, author = {marti a. hearst}, title = {
the experimental evaluation demonstrates that the wpcm method provides acceptable classification accuracy with the sports news datasets.}, } @inproceedings{sevillano04, author = {sevillano dominguez, xavier and alias pujol, francesc and socoro carrie, joan c.}, title = {ica-based hierarchical text classification for multi-domain text-to-speech synthesis}, booktitle = {proceedings of icassp-04, proceedings of the 29th ieee international conference on acoustics, speech, and signal processing}, editor = {}, publisher = {ieee computer society press, los alamitos, us}, address = {
the simple method of conducting hypothesis testing over word-based distributions in categories suffers from the data sparseness problem.
the mcnemar test shows that in most categories the increases are very significant.
the motivation is that there are statistical problems associated with natural language text when it is applied as input to existing machine learning algorithms (too much noise, too many features, skewed distribution).
in this paper we present a systematic, comparative experimental study of the three subsets of \textsf{reuters-21578} that have been most popular among tc researchers.
only relatively recently did detailed studies on the impact of various document representations step into the spotlight, showing that there may be statistically significant differences in classifier performance even among variations of the classical bag-of-words model.
we also demonstrate some experimental results using our algorithm on the problem of classifying bibliographic data and extracting keywords in order to show the effectiveness of our approach.}, } @inproceedings{sakkis01, author = {georgios sakkis and ion androutsopoulos and georgios paliouras and vangelis karkaletsis and constantine d. spyropoulos and panagiotis stamatopoulos}, title = {stacking classifiers for anti-spam filtering of e-mail},
finally ssfcm results in improved performance and faster execution time as more weight is given to training documents.}, } @inproceedings{bennett02, author = {paul n. bennett and susan t. dumais and eric horvitz}, title = {probabilistic combination of text classifiers using reliability indicators: models and results}, booktitle = {proceedings of sigir-02, 25th acm international conference on research and development in information retrieval}, editor =
yet, for many text classification tasks, providing labeled training documents is expensive, while unlabeled documents are readily available in large quantities.
we show that judicious use of a hierarchy can significantly improve both the speed and effectiveness of the categorization process.
for both data sets, boosting trees and svms had acceptable test performance in terms of accuracy and speed.
further, we show that selecting a best classification method using text-only features and then adding numerical features to the problem (as might happen if numerical features are only later added to a pre existing text-classification problem) gives performance that rivals a more time-consuming approach of evaluating all classification methods using the full set of both text and numerical features.}, } @article{maderlechner97, author = {maderlechner, g. and suda, p. and bruckner, t.}, title = {classification of documents by form and content}, journal =
thus, unlike some other unsupervised dimensionality-reduction techniques, such as latent semantic indexing, we are able to compress the feature space much more aggressively, while still maintaining high document classification accuracy.
however, centralized classification approaches often are limited due to constraints on knowledge and computing resources.
on a text categorization task, using the reuters-22,173 collection, we give empirical evidence that feature selection is useful: first, the size of the collection index can be drastically reduced without causing a significant loss in categorization effectiveness.
we use the technique of genetic programming (gp), (koza and rice 1992), to evolve classifying agents.
in application, the adapted text categorizers are reliable, fast, and completely automatic.
if term occurrences are random then there will be no correlation and the strength will be zero, but if for any subject, the term is either always present or never present its strength will be one.
however, one of the main shortcomings of such methods is that they largely disregard lexical semantics and, as a consequence, are not sufficiently robust with respect to variations in word usage.
the learning approach presented is attribute-efficient and, therefore, appropriate for domains having very large number of attributes.
we show that even on purely numerical-valued data the results of text-classification on the derived text-like representation outperforms the more naive numbers-as-tokens representation and, more importantly, is competitive with mature numerical classification methods such as c4.5 and ripper.}, } @article{macskassy03, author =
the ecoc method scales well to large data sets with a large number of classes.
the text categorization module described in the paper provides a front-end filtering function for the larger dr-link text retrieval system (liddy and myaeng 1993).
we conclude that our techniques permit automatic categorisation using very large train-ing collections, vocabularies, and numbers of categories.}, } @article{shin01, author =
highly accurate text segmentation is also possible - the accuracy of the ppm-based chinese word segmenter is close to 99\% on chinese news text; similarly, a ppm-based method of segmenting text by language achieves an accuracy of over 99\%.}, } @inproceedings{teytaud01, author =
a combined use of the projections on and the distances to the dlsi spaces introduced from the differential document vectors improves the adaptability of the lsi (latent semantic indexing) method by capturing unique characteristics of documents.
initial experiments on a set of 15 selected polysemous words show that the boosting approach surpasses naive bayes and exemplar-based approaches, which represent state-of-the-art accuracy on supervised wsd.
even though most of these features are relevant, the underlying concepts can be concisely captured using only a few features, while keeping all of them has substantially detrimental effect on categorization accuracy.
the problem of automatically determining the gender of a document's author would appear to be a more subtle problem than those of categorization by topic or authorship attribution.
in this work we present a comparative study of digital library citations and web links, in the context of automatic text classification.
using a machine learning approach, we build classifiers that accept an audio file of conversational human speech as input, and output an estimate of the topic being discussed.
the proposed approach does not require professional librarians or that the end users have extensive training.
in addition to being highly accurate, this method utilizes the hamming distance from ecoc to provide high-precision results.
the goal of the research described here is to automatically create a computer understandable knowledge base whose content mirrors that of the world wide web.
we evaluate the inequality me models on text categorization datasets, and demonstrate their advantages over standard me estimation, similarly motivated gaussian map estimation of me models, and support vector machines (svms), which are one of the state-of-the-art methods for text categorization.} } @article{kim:2005:drt, author = {
we found that pages in some genres change rarely if at all and can be used in present-day research experiments without requiring an updated version.
domain knowledge is used to specify a prior distribution for the parameters of a logistic regression model, and labeled training data is used to produce a posterior distribution, whose mode we take as the final classifier.
it is shown to be more robust with respect to the training set size and to improve the performance both for ranking and classification, specially for classes with few training examples.}, } @inproceedings{denoyer03, author = {ludovic denoyer and patrick gallinari}, title = {
as a global observation, knn, llsf and a neural network method had the best performance; except for a naive bayes approach, the other learning algorithms also performed relatively well.}, } @inproceedings{yavuz98, author = {yavuz, tuba and g{\"u}venir, h. altay}, title = {application of k-nearest neighbor on feature projections classifier to text categorization}, booktitle = {proceedings of iscis-98, 13th
documents are represented as feature-vectors that include word sequences instead of including only single words as commonly used when learning on text data.
{journal of intelligent information systems}, year = {2002}, note = {special issue on automated text categorization}, volume = {18}, number = {2/3}, pages = {127--152}, url = {http://www.wkap.nl/article.pdf?391243}, abstract = {kernel methods like support vector machines have successfully been used for text categorization.
these connections do not depend on whether there are shared terms among the queries and documents; therefore, they are especially effective for a mapping from queries to the documents where the concepts are relevant but the terms used by article authors happen to be different from the terms of database users.
in this paper, we introduce a new information gain and divergence-based feature selection method for statistical machine learning-based text categorization without relying on more complex dependence models.
while previous work in hierarchical classification focused on virtual category trees where documents are assigned only to the leaf categories, we propose a top-down level-based classification method that can classify documents to both leaf and internal categories.
however, the svm methods turned out to be quite sensitive to the choice of representation and kernel in ways which are not well understood; therefore, for the time being leaving the neural network approach as the most robust.}, } @inbook{manning99a, author = {
in this article, we evaluate the retrieval performance of an algorithm that automatically categorizes medical documents.
we suggest the utilization of additional resources like lexical databases to increase the amount of information that tc systems make use of, and thus, to improve their performance.
an analysis of patterns in sentences was performed with data from the trec 2002 novelty track and experiments on novelty detection were carried out on data from the trec 2003 and 2004 novelty tracks.
this paper presents a text categorization system, capable of analyzing html/text documents collected from the web.
describes an approach to document routing on the trec corpus that employs a technique for the automatic construction of classification trees.
by noisy it is meant any text obtained through an extraction process (affected by errors) from media different than digital texts.
its severe assumptions make such efficiency possible but also adversely affect the quality of its results.
we present experimental results showing that employing our active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings.}, note = {an extended version appears as \cite{tong01}}, } @article{tong01, author = {simon tong and daphne koller}, title = {support vector machine active learning with applications to text classification}, journal = {journal of machine learning research}, volume = {2}, month = {november}, pages = {45--66}, year = {2001}, url = {http://www.ai.mit.edu/projects/jmlr/papers/volume2/tong01a/tong01a.pdf}, abstract = {support vector machines have met with significant success in numerous real-world learning tasks.
the results show that a significant improvement can be derived using the proposed inference model.
in an unsupervised setting, our models produced coherent clusters with a very natural interpretation, even for instance types that do not have any attributes.}, } @inproceedings{taskar02, author = {ben taskar and pieter abbeel and daphne koller}, title = {discriminative probabilistic models of relational data}, booktitle = {proceedings of uai-02, 18th conference on uncertainty in artificial intelligence}, year = {2002}, address = {edmonton, ca}, pages = {485--492}, publisher = {morgan kaufmann publishers, san francisco, us}, editor = {}, url = {}, abstract = {
feature feedback can complement traditional active learning in applications such as news filtering, e-mail classification, and personalization, where the human teacher can have significant knowledge on the relevance of features.}, } @inproceedings{forman:2006:tcd, author =
adaboost produces better classifiers than rocchio when the training collection contains a very large number of relevant documents.
our new classifier misclassified 36\% of the patents, indicating that classifying hypertext can be more difficult than classifying text.
empirical results show that gp was able to discover better similarity functions than ga or other fusion techniques." }
linear support vector machines (svms) are particularly promising because they are very accurate, quick to train, and quick to evaluate.} } @inproceedings{elyaniv01, author = {
since information retrieval is a domain where such data sets are widespread, it provides an ideal application area for machine learning.
the algorithm, which we proposed, operates in a fully automatic mode and requires no supervision or training data.
we analyze a few of the commonly used statistics based and machine learning algorithms for natural language disambiguation tasks and observe that they can be recast as learning linear separators in the feature space.
published in the ``lecture notes in computer science'' series, number 2276}, year = {2002}, address = {mexico city, mx}, pages = {405--414}, url = {http://link.springer.de/link/service/series/0558/papers/2276/22760405.pdf}, abstract = {traditional chinese documents classifiers are based on keywords in the documents, which need dictionaries support and efficient segmentation procedures.
the algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents.
links clearly contain high-quality semantic clues that are lost upon a purely term-based classifier, but exploiting link information is non-trivial because it is noisy.
requirements of any such system include speed and minimal end-user effort.
the results confirm that our meta-model approach can exploit the advantage of its component algorithms, and demonstrate a better performance than existing algorithms.}, } @inproceedings{lam97, author = {wai lam and kon f. low and chao
in training the $i$-th classifier special emphasis is placed on the correct categorization of the training documents which have proven harder for the previously trained classifiers.
we tested our learning method on the task of single-label classification using the reuters-21578 benchmark.
using synthetically generated data we empirically demonstrate that whenever the dc procedure is successful in recovering some of the structure hidden in the data, the extended idc procedure can incrementally compute a dramatically better classification, with minor additional computational resources.
experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 30\%.}, } @inproceedings{nigam00a, author = {kamal nigam and rayid ghani}, title =
statistical methods for categorization, on the other hand, are easy to implement and require little or no human customization.
this disser- tation demonstrates that supervised learning algorithms that use a small number of labeled examples and many inexpensive unlabeled examples can create high-accuracy text classifiers.
we developed a user interface that organizes web search results into hierarchical categories.
six nonjudgmental criteria are used in testing the hypothesis for 100 keyword lists (each list representing a document) for a series of computer runs in which the number of words per document is increased progressively from 12 to 36.
however, on these tasks, rocchio runs much faster than adaboost.}, } @inproceedings{scheffer99, author = {tobias scheffer and thorsten joachims}, title = {expected error analysis for model selection}, booktitle = {proceedings of icml-99, 16th international conference on machine learning}, editor = {ivan bratko and saso dzeroski}, year = {1999}, address = {bled, sl}, publisher = {morgan kaufmann publishers, san francisco, us}, pages = {361-370}, url = {http://www-ai.cs.uni-magdeburg.de/~scheffer/papers/icml99.ps}, abstract = {
the goal of these methods is to discover automatically classification patterns that can be used for general document categorization or personalized filtering of free text.
it is evident in this study that automated word removal based on corpus statistics has a practical and significant impact on the computational tractability of categorization methods in large databases.}, } @inproceedings{yang96a, author =
we study to what extent ocr errors affect stylistic text classification from scanned documents.
in this paper, we examine the effects of page evolution on genre classification of web pages.
in the text domain there are likely to exist many gaps in the feature space because a document is usually mapped to a sparse and high dimensional feature space.
"support vector machines (svms) have been very successful in text classification.
in this paper, we propose a novel text classification approach, called discriminative category matching, which could achieve all of the stated characteristics.
while the decision tree based classifier outperforms the bayesian classifier when features and training size are selected optimally for both, a carefully designed naive bayesian classifier is more robust.}, } @inproceedings{diaz98, author = {d{\'{\i}}az esteban, alberto and de buenaga rodr{\'{\i}}guez, manuel and ure{\~n}a l{\'o}pez, l. alfonso and garc{\'{\i}}a vega, manuel}, title = {integrating linguistic resources in an uniform way for text classification tasks}, booktitle = {proceedings of lrec-98, 1st international conference on language resources and evaluation}, publisher = {}, editor = {antonio rubio and natividad gallardo and rosa castro and antonio tejada}, address = {grenada, es}, pages = {1197--1204}, year = {1998}, url = {http://www.esi.uem.es/laboratorios/sinai/postscripts/lrec98.ps}, abstract = {applications based on automatic text classification tasks, like text categorization (tc), word sense disambiguation (wsd), text filtering or routing, monolingual or multilingual information retrieval, and text summarization could obtain serious improvements by integrating linguistic resources in the current methods.
we tested this method on both retrieval and indexing with a set of medline documents which has been used by other information retrieval systems for evaluations.
however, the effect of boosting for rare categories varies across classifiers: for svms and decision trees, we achieved a 13-17\% performance improvement in macro-averaged f1 measure, but did not obtain substantial improvement for the other two classifiers.
on the other hand, other techniques naturally extensible to handle multi-class classification are generally not as accurate as svm.
% % % % everyone is also welcome to let me know either additional % % references or corrections and additions (e.g. urls, where % % they are not already present) to the existing ones.
all the experiments result in a significant improvement with respect to other purely statistical methods (e.g. [yang, 1999]), thus stressing the relevance of the available linguistic information.
{kluwer academic publishers}, address = {dordrecht, nl}, url = {http://www.cais.ntu.edu.sg/~sunaixin/paper/sun_hcl.pdf}, abstract = {hierarchical text classification refers to assigning text documents to the categories in a given category tree based on their content.
exploiting structural information for semi-structured document categorization}, journal = {information processing and management}, volume = {42}, number = {3}, pages =
document retrieval, categorization, routing and filtering can all be formulated as classification problems.
an algorithm for sequential sampling during machine learning of statistical classifiers was developed and tested on a newswire text categorization task.
% % everyone is welcome to download the bibliography as a whole and % % distribute it, provided that it is distributed untouched.
a text-categorization application developed with tcs consists of the tcs run-time system and a rule base.
instead of concentrating on a small set of ambiguous words, as done in most of the related previous work, all content words of the examined corpus are disambiguated.
we verify experimentally that ssfcm both outperforms and takes less time than the fuzzy-c-means (fcm) algorithm.
experimental data is presented showing widrow-hoff and eg to be more effective than the widely used rocchio algorithm on several categorization and routing tasks.}, } @misc{lewis97a, author = {lewis, david d.}, title = {reuters-21578 text categorization test collection.
this article describes our general approach, several machine learning algorithms for this task, and promising initial results with a prototype system that has created a knowledge base describing university people, courses, and research projects.}, } @article{craven01, author = {craven, mark and slattery, se{\'{a}}n}, title = {relational learning with statistical predicate invention: better models for hypertext}, journal = {machine learning}, pages = {97--119}, year = {2001}, volume = {43}, number = {1/2}, url = {http://www.wkap.nl/article.pdf?321079},
to convert the documents into vector form, we experiment with different numbers of features, which we select, based on an information gain criterion.
combining the results of classifiers has shown much promise in machine learning generally.
we further show that feature clustering is an effective technique for building smaller class models in hierarchical classification.
it is evident that the llsf approach uses the relevance information effectively within human decisions of categorization and retrieval, and achieves a semantic mapping of free texts to their representations in an indexing language.
experiments on a real-world data set show a reduction in classification error by up to 66\% over the traditional naive bayes classifier.
scut is potentially better for fine-tuning but risks overfitting.
an experimental evaluation of ocr text representations for learning document classifiers}, journal = {international journal on document analysis and recognition}, pages = {116--122}, year = {1998}, number = {2}, volume = {1}, url = {http://link.springer.de/link/service/journals/10032/papers/8001002/80010116.ps.gz}, abstract = {
we present experimental results in learning to classify email in this fashion, where each class corresponds to a verb-noun pair taken from a predefined ontology describing typical 'email speech acts'.
senses are interpreted as groups (or clusters) of similar contexts of the ambiguous word.
extensive experiments have been conducted on a real-world document collection and satisfactory performance is obtained.}, } @article{lai02, author = {yu-sheng lai and chung-hsien wu}, title = {meaningful term extraction and discriminative term selection in text categorization via unknown-word methodology}, journal = {acm transactions on asian language information processing}, year = {2002}, number = {1}, volume = {1}, pages =
to specify a user's problem solving task, we introduce the concept of document types that directly relate to the problem solving tasks; with this approach, users can easily designate problem solving tasks.
for example, in a scientific paper domain, papers are related to each other via citation, and are also related to their authors.
it is therefore worthwhile to understand whether such good performance is unique to the svm design, or if it can also be achieved by other linear classification methods.
this can aid if/ir systems that rely on the acquisition of large numbers of term weights or other measures of relevance.
we show that our learning algorithm can be used for automatic extraction of keywords for text retrieval and automatic text categorization.
previous work used a similar clustering procedure to show that word-clusters can significantly reduce the feature space dimensionality, with only a minor change in classification accuracy.
the algorithm's predictive accuracy is competitive with other recently introduced hierarchical multi-category or multilabel classification learning algorithms."
experiments on a web directory show that best results are achieved when links from pages outside the directory are considered.
trento, it}, pages = {178--185}, url = {}, abstract = {automatic news categorization systems have produced high accuracy, consistency, and flexibility using some natural language processing techniques.
the paper describes the technique of categorisation by context, which exploits the context perceivable from the structure of html documents to extract useful information for classifying the documents they refer to.
the results are compared to 1-of-n coding (i.e.\ one svm for each text category).
since a multi-dimensional model can be converted to flat and hierarchical models, three classification strategies are possible, i.e., classifying directly based on the multi-dimensional model and classifying with the equivalent flat or hierarchical models.
in [3] it has been suggested that classifiers based on generalized rocchio formula can be used to weight features in category profiles in order to exploit the selectivity of linguistic information techniques in text classification.
we find that both algorithms achieve reasonable performance and allow controlled tradeoffs between false positives and false negatives.
because the analyses of data are generally so expensive, most parts in databases remains as raw, unanalyzed primary data.
the changes may occur both on the transmission side (the nature of the streams can change) and on the reception side (the interests of a user can change).
in this paper, we describe a novel text classifier that can effectively cope with structured documents.
in this article, we evaluate the retrieval performance of an algorithm that automatically categorizes medical documents.
moreover, the derived classifier reachs the performance (about 85\%) of the best known models (i.e. support vector machines (svm) and k-nearest neighbour (knn)) characterized by an higher computational complexity for training and processing.}, } @inproceedings{basili01a, author = {
however, support vector machines are so far considered special in that they have been demonstrated to achieve the state of the art performance.
in addition, nlp systems can increase the information contained in keyword fields by separating keywords into segments, or distinct fields that capture certain discriminating content or relations among keywords.
information fusion almost always gives better results than the individual constituent feature sets, with certain combinations doing better than the others.}, } @inproceedings{davidov04, author = {dmitry davidov and evgeniy gabrilovich and shaul markovitch}, title = {parameterized generation of labeled datasets for text categorization based on a hierarchical directory}, booktitle = {proceedings of sigir-04, 27th acm international conference on research and development in information retrieval}, editor =
experimental results show that our system can effectively and efficiently classify chinese web documents.}, } @inproceedings{zhou02, author = {shuigeng zhou and jihong guan}, title = {an approach to improve text classification efficiency}, booktitle = {proceedings of adbis-02, 6th east-european conference on advances in databases and information systems}, publisher = {springer verlag, heidelberg, de}, editor = {yannis manolopoulos and pavol n{\'a}vrat}, year = {2002}, address = {bratislava, sk}, pages = {65--79}, url = {http://link.springer.de/link/service/series/0558/papers/2435/24350065.pdf}, abstract =
this note presents the corrected results, along with additional data supporting the original claim that uncertainty sampling has an advantage over relevance sampling in most training situations.}, } @inproceedings{lewis95b, author = {david d. lewis}, title = {the {trec-4} filtering track: description and analysis}, booktitle = {proceedings of trec-4, 4th text retrieval conference}, publisher = {national institute of standards and technology, gaithersburg, us}, editor =
for example, in hypertext classification, the labels of linked pages are highly correlated.
experiments with syntactic phrase indexing, however, have never yielded significant improvements in text retrieval performance.
these experiments suggest that stopword lists and stemming algorithms may remove or conflate many words that could be used to create more effective indexing terms.}, } @inproceedings{riloff96, author = {ellen riloff}, title = {using learned extraction patterns for text classification}, booktitle = {connectionist, statistical, and symbolic approaches to learning for natural language processing}, editor = {stefan wermter and ellen riloff and gabriele scheler}, pages = {275--289}, year = {1996}, publisher = {springer verlag, heidelberg, de}, note = {
one difficulty in handling some classes of documents is the presence of different kinds of textual errors, such as spelling and grammatical errors in email, and character recognition errors in documents that come through ocr.
in contrast, programs that filter text streams, software that categorizes documents, agents which alert users, and many other ir systems must make decisions without human input or supervision.
the pattern extraction is aimed at providing descriptions (in the form of two logical expressions) of the two classes of positive and negative examples.
our approach uses a natural language processing task called information extraction as a basis for high-precision text classification.
our experiments show that the lower dimensional spaces computed by our algorithm consistently improve the performance of traditional algorithms such as c4.5, k-nearest-neighbor, and support vector machines (svm), by an average of 2\% to 7\%.
a central problem in information retrieval is the automated classification of text documents.
we verify experimentally that the integration of wordnet helps ssfcm improve its performance, effectively addresses the classification of documents into categories with few training documents and does not interfere with the use of training data.}, } @article{benkhalifa01a, author = {mohammed benkhalifa and abdelhak mouradi and houssaine bouyakhf}, title = {
this paper presents the infoclas system applying statistical methods of information retrieval for the classification of german business letters into corresponding message types such as order, offer, enclosure, etc.
the learning algorithm combines an adaptive phase which instantly updates dictionary and weights during interaction and a tuning phase which fine tunes for performance using previously seen data.
we use a training set of manually categorized documents to learn word-category associations, and use these associations to predict the categories of arbitrary documents.
mh$^kr$}, an improved boosting algorithm, and its application to text categorization.
maui, us}, pages = {}, url = {http://dlib.computer.org/conferen/hicss/0981/pdf/09817061.pdf}, abstract = {with rapid expansion of the numbers and sizes of text repositories and improvements in global connectivity, the quantity of information available online as free-format text is growing exponentially.
text mining applies the same analytical functions of data mining to the domain of textual information, relying on sophisticated text analysis techniques that distill information from free-text documents.
however, on these tasks, rocchio runs much faster than adaboost.}, } @inproceedings{scheffer99, author = {tobias scheffer and thorsten joachims}, title = {expected error analysis for model selection}, booktitle = {proceedings of icml-99, 16th international conference on machine learning}, editor = {ivan bratko and saso dzeroski}, year = {1999}, address = {bled, sl}, publisher = {
the paper shows how a text classifier's need for labeled training documents can be reduced by taking advantage of a large pool of unlabeled documents.
by referencing various relationships in the thesaurus corresponding to the structured categories, k-nn can be prominently improved, removing the ambiguity.
a chinese documents classification system following above described techniques is implemented with naive bayes, knn and hierarchical classification methods.
it analyzes the particular properties of learning with text data and identifies why svms are appropriate for this task.
the problem is not easy to tackle due to the semi-structured or even unstructured nature of those texts under consideration.
an appraisal group is represented as a set of attribute values in several task-independent semantic taxonomies, based on appraisal theory.
we compare pagetypesearch using the document type-indices with a conventional keyword-based search system in experiments.
svms have been trained on data sets with several thousand instances, but web directories today contain millions of instances which are valuable for mapping billions of web pages into yahoo!-like directories.
we consider a family of models that take into account the fact that relevant documents may contain irrelevant passages; the originality of the model is that it does not explicitly segment documents but rather considers all possible segmentations in its final score.
in comparison with other machine-learning techniques, results on a key benchmark from the reuters collection show a large gain in performance, from a previously reported 67\% recall/precision breakeven point to 80.5\%.
the results show that a significant improvement can be derived using the proposed inference model.
on a text categorization task, using the reuters-22,173 collection, we give empirical evidence that feature selection is useful: first, the size of the collection index can be drastically reduced without causing a significant loss in categorization effectiveness.
finally, the naive bayesian filter is compared, in terms of performance, to a filter that uses keyword patterns, and which is part of a widely used e-mail reader.}, } @article{appiani01, author = {enrico appiani and francesca cesarini and annamaria colla and massimiliano diligenti and marco
the paper describes the kdt system for knowledge discovery in texts.
our techniques considerably improve the robustness and accuracy of the classification outcome, as shown in systematic experimental comparisons with previously published methods on three different real-world datasets."
to accomplish this task, each substantive word in a text is first categorized using a feature set based on the semantic subject field codes (sfcs) assigned to individual word senses in a machine-readable dictionary.
the paper demonstrates good performance of context-group discrimination for a sample of natural and artificial ambiguous words.}, } @mastersthesis{scott98, author = {sam scott}, title = {
the construction steps often involve human efforts and are not completely automated.
phrases, word senses and syntactic relations derived by natural language processing (nlp) techniques were observed ineffective to increase retrieval accuracy.
using hcl, a hierarchical classification method can be materialized easily with the help of a method generator system.}, } @inproceedings{sun03b, author = {aixin sun and ee-peng lim}, title = {web unit mining: finding and classifying subgraphs of web pages}, booktitle = {proceedings of cikm-03, 12th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, editor = {}, year = {2003}, address = {new orleans, us}, pages = {108--115}, url = {http://doi.acm.org/10.1145/956863.956885}, abstract = {
at their core, our algorithms employ recently developed modified finite newton techniques.
a framework for filtering news and managing distributed data}, journal = {journal of universal computer science}, year = {1997}, number = {8}, volume = {3}, pages = {1007--1021}, url = {http://www.jucs.org/jucs_3_8/a_framework_for_filtering}, abstract = {with the development and diffusion of the internet worldwide connection, a large amount of information is available to the users.
we found small advantages in accuracy for hierarchical models over flat models.
we are particularly interested in domain transfer: how well the learned classifiers generalize from the training corpus to a new document corpus.
latent semantic indexing (lsi) has been successfully used for ir purposes, as a technique for capturing semantic relations between terms and inserting them into the similarity measure between two documents.
since there is a difference between important sentences and unimportant sentences in a document, the features from more important sentences should be considered more than other features.
such a scheme has several potential advantages because it does not require any pre-processing of the input text.
given these inputs, the system learns to extract information from other pages and hyperlinks on the web.
experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classification hierarchies consisting of hundreds of nodes.
the determination of category themes and their hierarchical structures were most done by human experts.
in this paper an original classification model sensitive to document syntactic information and characterized by a novel inference method is described.
we show how to train these models effectively, and how to use approximate probabilistic inference over the learned model for collective classification of multiple related entities.
the multi-classifier approach helps us leverage all the relevant textual features and meta data, and appears to generalize to related classification tasks.}, } @inproceedings{amati96, author = {gianni amati and daniela d'aloisi and vittorio giannini and flavio ubaldini}, title =
we present a system for searching and classifying u.s. patent documents, based on inquery.
results obtained from evaluation show that the integration of wordnet clearly outperforms training approaches, and that an integrated technique can effectively address the classification of low frequency categories.}, } @inproceedings{delima98, author = {de lima, luciano r. and laender, alberto h. and ribeiro-neto, berthier a.}, title = {
we present the results of a number of experiments designed to evaluate the effectiveness and behavior of different compression-based text classification methods on english text.
the results show that our approach outperformed the bayesian independence classifier as measured by a metric that combines precision and recall measures.}, } @inproceedings{lam98, author = {wai lam and chao y. ho}, title = {using a generalized instance set for automatic text categorization}, booktitle = {proceedings of sigir-98, 21st acm international conference on research and development in information retrieval}, editor = {
we report on experiments with different kernels for both of these implementations and with different representations of the data, including binary vectors, tf-idf representation and a modification called ``hadamard" representation.
in an effort to deal more effectively with this large vocabulary and improve information processing, a method of focus has been developed which allows one to classify terms based on a measure of their importance in describing the content of the documents in which they occur.
it allows an effective exploitation of the available linguistic information that better emphasizes this latter with significant both data compression and accuracy.
this filtering process is a word sense disambiguation task.
one difficulty in handling some classes of documents is the presence of different kinds of textual errors, such as spelling and grammatical errors in email, and character recognition errors in documents that come through ocr.
knowledge discovery is performed by analyzing the co-occurrence frequencies of keywords from this hierarchy in the various documents.
with the increasing availability of lexical resources in electronic form (including lexical databases (ldbs), machine readable dictionaries, etc.), there is an interesting opportunity for the integration of them in learning-based atc.
however, the svm methods turned out to be quite sensitive to the choice of representation and kernel in ways which are not well understood; therefore, for the time being leaving the neural network approach as the most robust.}, } @inbook{manning99a, author = {christopher manning and hinrich sch{\"{u}}tze}, title = {
text-classification methods have thus far not easily incorporated numerical features.
both textual and non-textual information associated with the projects are used in the learning and classification phases.
first, undirected models do not impose the acyclicity constraint that hinders representation of many important relational dependencies in directed models.
we augment naive bayes models with statistical n-gram language models to address short-comings of the standard naive bayes text classifier.
[stanfill] (a text retrieval system that supports relevance feedback) as the underlying match engine, codes are assigned to new, unseen stories with a recall of about 80\% and precision of about 70\%.
so far, documents have been classified according to their contents manually.
the stepwise feature selection in the decision tree algorithm is particularly effective in dealing with the large feature sets common in text categorization.
this paper studies the ability of symbolic learning algorithms to perform a text categorization task.
we suggest that one (or a collection) of names of {{\sc yahoo!}}\ (or any other www indexer's) categories can be used to describe the content of a document.
www indices, like {{\sc yahoo!}}\ provide a huge hierarchy of categories (topics) that touch every aspect of human endeavors.
our method is unique in that decision lists are automatically constructed on the basis of the principle of minimizing extended stochastic complexity (esc), and with it we are able to construct decision lists that have fewer errors in classification.
text collections in different domains were used for evaluation.
in this paper, we show how inverted indexes can be used for scalable training in categorisation, and propose novel heuristics for a fast, accurate, and memory efficient approach.
our results show simple windows are best for all test collections tested in these experiments.
their only potential drawback is their training time and memory requirement.
a user study compared our new category interface with the typical ranked list interface of search results.
using the reuters collection, we show that adaptive resampling techniques can improve decision-tree performance and that relatively small, pooled local dictionaries are effective.
the utilized concepts are automatically extracted from documents via probabilistic latent semantic analysis.
standard term clustering strategies from information retrieval (ir), based on cooccurence of indexing terms in documents or groups of documents, were tested on a syntactic indexing phrase representation.
moreover, the number of irrelevant references gathered by our system is about one-thirteenth that of traditional keyword-based search systems.
we provide experimental results demonstrating that the approach can significantly improve performance, and that it does not impair it.}, } @inproceedings{dagan96, author = {dagan, ido and feldman, ronen and hirsh, haym}, title = {keyword-based browsing and analysis of large document sets}, booktitle = {proceedings of sdair-96, 5th annual symposium on document analysis and information retrieval}, publisher = {}, editor = {}, year = {1996}, address = {las vegas, us}, pages = {191--207}, url = {}, abstract = {knowledge discovery in databases (kdd) focuses on the computerized exploration of large amounts of data and on the discovery of interesting patterns within them.
in order to classify text documents, we must extract good features from them.
an extensive empirical study of feature selection metrics for text classification}, journal = {journal of machine learning research}, volume = {3}, month = {march}, pages =
if term occurrences are random then there will be no correlation and the strength will be zero, but if for any subject, the term is either always present or never present its strength will be one.
this method, which we call uncertainty sampling, reduced by as much as 500-fold the amount of training data that would have to be manually classified to achieve a given level of effectiveness.}, } @article{lewis94b, author = {lewis, david d. and philip j. hayes}, title = {guest editors' introduction to the special issue on text categorization}, journal = {acm transactions on information systems}, volume = {12}, number = {3}, pages = {231}, year = {1994}, } @inproceedings{lewis94c, author = {lewis, david d. and jason catlett}, title = {heterogeneous uncertainty sampling for supervised learning}, booktitle = {proceedings of icml-94, 11th international conference on machine learning}, editor =
an experimental evaluation of ocr text representations for learning document classifiers}, journal = {international journal on document analysis and recognition}, pages = {116--122}, year = {1998}, number = {2}, volume = {1}, url = {http://link.springer.de/link/service/journals/10032/papers/8001002/80010116.ps.gz}, abstract = {
thus, they do not fully make use of the weight information provided by standard term weighting methods.
year = {1998}, address = {bethesda, us}, pages = {148--155}, url = {http://robotics.stanford.edu/users/sahami/papers-dir/cikm98.pdf}, abstract = {text categorization - the assignment of natural language texts to one or more predefined categories based on their content - is an important component in many information organization and management tasks.
all three algorithms achieved high precision on both test sets, with the augmented relevancy signatures algorithm and the case-based algorithm reaching 100\% precision with over 60\% recall on one set.
the classifiers and regression equations were then applied to a new set of essays.
it is evident in this study that automated word removal based on corpus statistics has a practical and significant impact on the computational tractability of categorization methods in large databases.}, } @inproceedings{yang96a, author = {yiming yang}, title = {
we improve a high-accuracy maximum entropy classifier by combining an ensemble of classifiers with neural network voting.
the experiments were conducted on the standard reuters data set.
the comparison of the examined models demonstrates that techniques from information retrieval integrated into recurrent plausibility networks performed well even under noise and for different corpora.}, } @inproceedings{wermter02, author = {stefan wermter and chihli hung}, title = {
morgan kaufmann publishers, san francisco, us}, url = {http://robotics.stanford.edu/users/sahami/papers-dir/ml97-hier.ps}, abstract = {the proliferation of topic hierarchies for text documents has resulted in a need for tools that automatically classify new documents within such hierarchies.
these methods can greatly reduce the number of instances that an expert need label.
in this paper, we study the properties of phrasal and clustered indexing languages on a text categorization task, enabling us to study their properties in isolation from query interpretation issues.
in this paper, we suggest, for text categorization, the integration of external wordnet lexical information to supplement training data for a semi-supervised clustering algorithm which can learn from both training and test documents to classify new unseen documents.
we thereby treat the problem of classifying documents as that of conducting statistical hypothesis testing over finite mixture models.
by allowing this interactivity in the clustering process, c-evolve achieves considerably higher clustering accuracy (10 to 20\% absolute increase in our experiments) than the popular k-means and agglomerative clustering methods.}, } @inproceedings{agrawal01, author =
the filtering engine memorizes both user preferences and past situations.
we show how a naive bayes classification can be enhanced to incorporate the similarity information present in source catalogs.
morgan kaufmann publishers, san francisco, us}, url = {http://www.cs.cmu.edu/~yiming/papers.yy/hypertext-icml01.ps.gz}, abstract = {hypertext poses new text classification research challenges as hyperlinks, content of linked documents, and meta data about related web sites all provide richer sources of information for hypertext classification that are not available in traditional text classification.
combined with the classification power of an svm, this method yields high performance text categorization that can outperform other recent methods in terms of categorization accuracy and representation efficiency.
this basic em procedure works well when the data conform to the generative assumptions of the model.
in this paper, we introduce hyperlink ensembles, a novel type of ensemble classifier for classifying hypertext documents.
this level of complexity makes an ``off-the-shelf'' database-management and information-retrieval solution impossible.
while this weighting method seems very appropriate for ir, it is not clear that it is the best choice for tc problems.
an automatic document classification system using pattern recognition techniques}, booktitle = {proceedings of asis-78, 41st
we demonstrate that the classifiers perform 10-15\% better than relevance feedback via rocchio expansion for the trec-2 and trec-3 routing tasks.
when using a state-of-the-art classifier, knn, the average accuracy is 96.40\%, outperforming all the other systems evaluated on the same collection, including the traditional term-word by knn (88.52\%); sleeping-experts (82.22\%); sparse phrase by four-word sleeping-experts (86.34\%); and boolean combinations of words by ripper (87.54\%).
the high number of features is reduced by taking into account the hierarchical structure and using feature subset selection based on the method known from information retrieval.
traditionally, the categories are arranged in hierarchical manner to achieve effective searching and indexing, as well as easy comprehension for humans.
while it is easy to collect the unlabeled documents, it is not so easy to manually categorize them for creating training documents.
however, many popular feature selection techniques such as information gain (ig) and $\chi^2$-test (chi) are all greedy in nature and thus may not be optimal according to some criterion.
as we show, each of these smaller problems can be solved accurately by focusing only on a very small set of features, those relevant to the task at hand.
each news web page is represented by the term-weighting scheme.
we show that even on purely numerical-valued data the results of text-classification on the derived text-like representation outperforms the more naive numbers-as-tokens representation and, more importantly, is competitive with mature numerical classification methods such as c4.5 and ripper.}, } @article{macskassy03, author =
a document includes informative keywords and non-informative keywords.
performance can be significantly improved by using active learning to select high-quality initializations, and by using alternatives to em that avoid low-probability local maxima.}, } @inproceedings{nigam98, author = {kamal nigam and andrew k. mccallum and sebastian thrun and tom m. mitchell}, title = {
abstract = {automatic categorization of multimedia documents is an important function for a digital library system.
second, they permit straightforward application of sophisticated smoothing techniques from statistical language modeling, which allows one to obtain better parameter estimates than the standard laplace smoothing used in naive bayes classification.
we believe this approach is effective in reducing the development time to implement classification systems involving large number of topics for the purpose of classification, message routing etc.}, } @incollection{masand94, author = {briji masand}, title = {optimising confidence of text classification by evolution of symbolic expressions}, booktitle = {advances in genetic programming}, publisher = {
automatic text classification is needed to store documents like that.
* publications thet discuss related topics sometimes confused with % % atc; these include, in particular, text clustering (i.e. text % % classification by unsupervised learning) and text indexing; % % % % * technical reports and workshop papers.
feature feedback can complement traditional active learning in applications such as news filtering, e-mail classification, and personalization, where the human teacher can have significant knowledge on the relevance of features.}, } @inproceedings{forman:2006:tcd, author =
the hypothesis that cdm`s performance exceeds two non-domain specific algorithms, bayesian classification and decision tree learners, is empirically tested.}, } @article{goldberg96, author = {goldberg, jeffrey l.}, title = {cdm: an approach to learning in text categorization}, journal = {international journal on artificial intelligence tools}, year = {1996}, number = {1/2}, volume = {5}, pages = {229--253}, url = {}, abstract = {
finally, we apply the ssahc algorithm to the reuters database of documents and show that its performance is superior to the bayes classifier and to the expectation-maximization algorithm combined with bayes classifier.
we also show that, surprisingly, the addition of deep linguistic analysis features to a set of surface level word n-gram features contributes consistently to classification accuracy in this domain."
we showed also that ssahc helps ahc techniques to improve their performance.}, } @inproceedings{slattery00, author = {se{\'{a}}n slattery and mark craven}, title = {discovering test set regularities in relational domains}, booktitle = {proceedings of icml-00, 17th international conference on machine learning}, editor = {pat langley}, year = {2000}, address = {stanford, us}, pages = {895--902}, publisher = {morgan kaufmann publishers, san francisco, us}, url = {http://www.cs.cmu.edu/~sean/papers/icml2000.ps}, abstract = {machine learning typically involves discovering regularities in a training set, then applying these learned regularities to classify objects in a test set.
then the system computes a profile for a particular document that is to be classified.
the system is small, fast and robust.
the authors found that topic identification performance was maintained or slightly improved using character shape codes derived from images.}, } @article{stamatatos00, author =
european colloquium on information retrieval research}, editor = {}, year = {2001}, address = {darmstadt, de}, publisher = {}, pages = {24--40}, url = {http://cis.paisley.ac.uk/vino-ci0/fisher_hierarchic.ps}, abstract = {this paper demonstrates that the probabilistic corpus model which emerges from the automatic or unsupervised hierarchical organisation of a document collection can be further exploited to create a kernel which boosts the performance of state-of-the-art support vector machine document classifiers.
in contrast to most ir methods, theoretical analysis provides performance guarantees and guidance on parameter settings for these algorithms.
newsjunkie employs novelty-analysis algorithms that represent articles as words and named entities.
as the result of collaborative research with friends of the earth, an environmental issues campaigning organisation, we have developed a general purpose information classification agent architecture and have applied it to the problem of document classification and routing.
our experiments in a transductive classification setting indicate that accuracy can be significantly improved by modeling relational dependencies.
our new classifier misclassified 36\% of the patents, indicating that classifying hypertext can be more difficult than classifying text.
in particular, we show that in our environment, ocr errors have no effect on categorization when we use a classifier based on the naive bayes model.
the motivation is that there are statistical problems associated with natural language text when it is applied as input to existing machine learning algorithms (too much noise, too many features, skewed distribution).
effectiveness of the similarity functions discovered through simple majority voting is better than that of content-based as well as combination-based support vector machine classifiers.
learning algorithms for keyphrase extraction}, journal = {information retrieval}, number = {4}, volume = {2}, pages = {303--336}, year = {2000}, url = {http://extractor.iit.nrc.ca/reports/ir2000.ps.z}, abstract = {many academic journals ask their authors to provide a list of about five to fifteen keywords, to appear on the first page of each article.
we present detailed experimental results using naive bayes and support vector machines on the 20newsgroups data set and a 3-level hierarchy of html documents collected from the open directory project (www.dmoz.org).}, } @inproceedings{diao00, author = {yanlei diao and hongjun lu and dekai wu}, title = {
furthermore, the online approach offers the advantage of continuous learning in the batch-adaptive text filtering task.}, } @inproceedings{adami03, author = {giordano adami and paolo avesani and diego sona}, title = {
in this paper we show an adaptive incremental learning algorithm that learns interactively to classify text messages (here: emails) into categories without the need for lengthy batch training runs.
as the number of unique words in the collection set is big, the principal component analysis (pca) has been used to select the most relevant features for the classification.
it of applications, ranging from tracking usersõ products or about political candidates as expressed forums, to customer relationship management.
we have performed extensive experiments on the use of ppm compression models for categorization using the standard reuters-21578 dataset.
our experimental results on three real world data sets show that we achieve substantial improvements over standard naive bayes classification, while also achieving state of the art performance that competes with the best known methods in these cases.}, } @inproceedings{peng03a, author = {fuchun peng and dale schuurmans and shaojun wang}, title = {language and task independent text categorization with simple language models}, booktitle = {proceedings of hlt-03, 3rd human language technology conference}, publisher = {}, editor = {}, address = {
last but not least, we describe an evaluation experiment that classifies professional nature scenery photographs to demonstrate the effectiveness and efficiency of visual keywords for automatic categorization of images in digital libraries.}, } @article{liu01, author = {zhi-qiang liu and ya-jun zhang}, title = {a competitive neural network approach to web-page categorization}, journal =
the personal view maintainer synchronizes user interests and the personal view periodically.
the other unusual features of our research are the longevity of our agents and the fact that they undergo a continual training process; feedback from the user enables the agent to adapt to the user's long-term information requirements.}, } @inproceedings{cohen95, author =
it also makes use of bayesian classification techniques to classify new documents within an existing categorization scheme.
we show that, even with an average word error rate of around 50\%, the categorization performance loss with respect to the clean version of the same documents is negligible.}, url = {ftp://ftp.idiap.ch/pub/reports/2003/rr03-61.pdf}, } @inproceedings{vinokourov01, author = {alexei vinokourov and mark girolami}, title = {document classification employing the fisher kernel derived from probabilistic hierarchic corpus representations},
the linear combination approach makes use of limited knowledge in the training document set.
we introduce appropriate cost-sensitive measures, investigating at the same time the effect of attribute-set size, training-corpus size, lemmatization, and stop lists, issues that have not been explored in previous experiments.
a dynamic probabilistic model to visualise topic evolution in text streams}, journal = {journal of intelligent information systems}, year = {2002}, note = {special issue on automated text categorization}, volume =
documents are represented as feature-vectors that include word sequences instead of including only single words as commonly used when learning on text data.
we show that our learning algorithm can be used for automatic extraction of keywords for text retrieval and automatic text categorization.
we present experimental results on the \textsf{reuters-21578} text categorization collection, showing that for both algorithms the version with discretized continuous attributes outperforms the version with traditional binary representations.}, } @inproceedings{ng97, author =
document retrieval, categorization, routing and filtering can all be formulated as classification problems.
it is important to define what constitutes good effectiveness for these autonomous systems, tune the systems to achieve the highest possible effectiveness, and estimate how the effectiveness changes as new data is processed.
we compare the accuracy of our learning approach to a rule-based, expert system approach that uses a text categorization shell built by carnegie group.
as control experiment best human categorization performance was established at 79.4\% for this task.
our approach includes some original ideas for handling large number of features, categories and documents.
we present here a transductive boosting method for text categorization in order to make use of the large amount of unlabeled data efficiently.
for each category, all corresponding document texts from the training sample are concatenated to a megadocument, which is indexed using standard methods.
this paper studies how link information can be used to improve classification results for web collections.
in addition, pva considers the aging problem of user interests.
the method is evaluated using the umls metathesaurus as the underlying hierarchical structure, and the ohsumed test set of medline records.
an empirical evaluation of the system was performed by means of a confidence interval technique.
the first concerns the information filtering system profile based on an adaptation of the generalized probabilistic model of information retrieval.
we show how to update such databases with new documents with high speed and accuracy.
as a first step toward automatic go annotation, we aim to assign go domain codes given a specific gene and an article in which the gene appears, which is one of the task challenges at the trec 2004 genomics track.
although the test corpus contains documents written in chinese, the proposed approach can be applied to documents written in any language and such documents can be transformed into a list of separated terms.} } @inproceedings{esuli:2005:dso, author =
we also examine training set size, and alternative document representations.
knowledge discovery is performed by analyzing the co-occurrence frequencies of keywords from this hierarchy in the various documents.
mainly non-informative keywords play the roles of grammatical functions in sentences; such keywords, what are called functional keywords, reflect its contents very little, so they should be removed in the process of document indexing.
rankboost achieves comparable performance to the state-of-the-art algorithm when combined with feature or example selection heuristics.
the first level of the architecture predicts the probabilities of the meta-topic groups.
subjects liked the category interface much better than the list interface, and they were 50\% faster at finding information that was organized into categories.
these search engines are, however, unsuited for a wide range of equally important tasks.
we find that using words in web pages alone often yields suboptimal performance of classifiers, compared to exploiting additional sources of information beyond document content.
we extend the traditional active learning framework to include feedback on features in addition to labeling instances, and we execute a careful study of the effects of feature selection and human feedback on features in the setting of text categorization.
our experiments on hierarchical classification methods based on svm classifiers and binary naive bayes classifiers showed that svm classifiers perform better than naive bayes classifiers on reuters-21578 collection according to the extended measures.
the existence, public availability, and widespread acceptance of a standard benchmark for a given information retrieval (ir) task are beneficial to research on this task, since they allow different researchers to experimentally compare their own systems by comparing the results they have obtained on this benchmark.
reprinted in karen sparck jones and peter willett (eds.), ``readings in information retrieval'', morgan kaufmann, san francisco, us, 1997, pp.\ 513--517.}, url = {http://www.acm.org/pubs/articles/proceedings/ir/62437/p333-biebricher/p333-biebricher.pdf}, abstract = {since october 1985, the automatic indexing system air/phys has been used in the input production of the physics data base of the fachinformationszentrum karlsruhe/west germany.
to avoid their being overflowed by the incoming data, methods of information filtering are required.
this tight packaging of word pairs could bring in some semantic value.
it is based on a hybrid case-based architecture, where two multilayer perceptrons are integrated into a case-based reasoner.
previous researches on advanced representations for document retrieval have shown that statistical state-of-the-art models are not improved by a variety of different linguistic representations.
we also demonstrate some experimental results using our algorithm on the problem of classifying bibliographic data and extracting keywords in order to show the effectiveness of our approach.}, } @inproceedings{sakkis01, author = {georgios sakkis and ion androutsopoulos and georgios paliouras and vangelis karkaletsis and constantine d. spyropoulos and panagiotis stamatopoulos}, title = {stacking classifiers for anti-spam filtering of e-mail}, booktitle = {proceedings of emnlp-01, 6th conference on empirical methods in natural language processing}, year = {2001}, publisher =
even though the learning ability and computational complexity of training in support vector machines may be independent of the dimension of the feature space, reducing computational complexity is an essential issue to efficiently handle a large number of terms in practical applications of text classification.
the best results were obtained using the m-estimate as search heuristics combined with the likelihood-ratio-statics for pruning.
the proposed method shows a similar degree of performance, compared with the traditional supervised learning methods.
we investigate a meta-model approach, called meta-learning using document feature characteristics (mudof), for the task of automatic textual document categorization.
we show that our method is especially useful for text classification tasks involving a large number of categories and outperforms other semi-supervised learning techniques such as em and co-training.
we report experiments on text classification of the cora and webkb data sets using probabilistic latent semantic analysis and probabilistic hypertext induced topic selection.
a preliminary experimentation proved that the logic approach is able to capture the semantics underlying some kind of sentences, even if the assessment of the efficiency of such a method, as well as a comparison with other related approaches, has still to be carried out.}, } @article{field75, author = {b.j. field}, title = {towards automatic indexing: automatic assignment of controlled-language indexing and classification from free indexing}, year = {1975}, journal = {journal of documentation}, volume = {31}, number = {4}, pages = {246--265}, url = {}, abstract = {}, } @inproceedings{finn02, author = {aidan finn and nicholas kushmerick and barry smyth}, title = {genre classification and domain transfer for information filtering}, booktitle =
genre or style, on the other hand, is a different and important property of text, and automatic text genre classification is becoming important for classification and retrieval purposes as well as for some natural language processing research.
in this paper, we introduce a new information gain and divergence-based feature selection method for statistical machine learning-based text categorization without relying on more complex dependence models.
this paper presents a text categorization system, capable of analyzing html/text documents collected from the web.
comparing classification schemes}, journal = {acm transactions on information systems}, year = {2005}, volume = {23}, number = {4}, pages = {430--462}, url = {http://doi.acm.org/10.1145/1095872.1095875}, abstract = {topical crawling is a young and creative area of research that holds the promise of benefiting from several sophisticated data mining techniques.
the accuracy of classification achieved with our method appears better than or comparable to those of existing rule-based methods.}, } @inproceedings{liao02, author = {yihua liao and v. rao vemuri}, title = {
in this categorization process recall is considered more important than precision.
text categorization presents unique challenges due to the large number of attributes present in the data set, large number of training samples, attribute dependency, and multi-modality of categories.
since a multi-dimensional model can be converted to flat and hierarchical models, three classification strategies are possible, i.e., classifying directly based on the multi-dimensional model and classifying with the equivalent flat or hierarchical models.
it also shows that support vector machines can, in fact, sometimes very significantly outperform both methods.
this margin widened in tasks with high class skew, which is rampant in text classification problems and is particularly challenging for induction algorithms.
the interpretation shows the strengths and weaknesses of using thesaurus knowledge and gives hints for future research.}, } @article{junker98, author = {markus junker and rainer hoch}, title = {
text categorization: a symbolic approach}, booktitle = {proceedings of sdair-96, 5th annual symposium on document analysis and information retrieval}, publisher = {}, editor = {}, address = {las vegas, us}, year = {1996}, pages = {87--99}, url = {http://www-poleia.lip6.fr/~moulinie/sdair.ps.gz}, abstract = {recent research in machine learning has been concerned with scaling-up to large data sets.
three dictionaries produced by autoslog for different domains performed well in the author`s text classification experiments.}, } @incollection{riloff99, author =
extensive experimental evidence has been derived on real test data and also from well-established academic test sets.
each node's vocabulary is filtered and its words assigned weights with respect to the specific category.
document feature characteristics, derived from the training document set, capture some inherent category-specific properties of a particular category.
applying the multiple cause mixture model to text categorization}, booktitle = {proceedings of icml-96, 13th international conference on machine learning}, editor = {lorenza saitta}, year = {1996}, address = {bari, it}, pages = {435--443}, publisher = {morgan kaufmann publishers, san francisco, us}, url = {http://robotics.stanford.edu/users/sahami/papers-dir/ml96-mcmm.ps}, abstract = {the paper introduces the use of the multiple cause mixture model for automatic text category assignment.
in many settings, we also have the option of using pool-based active learning.
these theoretical findings are supported by experiments on three test collections.
the personal view maintainer synchronizes user interests and the personal view periodically.
neural networks perform equally well with either set of features and can take advantage of the additional information available when both feature sets are used as input.}, } @article{schutze98, author = {hinrich sch{\"{u}}tze}, title = {automatic word sense discrimination}, journal = {computational linguistics}, year = {1998}, volume = {24}, number = {1}, pages = {97--124}, url = {}, abstract = {this paper presents context-group discrimination, a disambiguation algorithm based on clustering.
the packing of word pairs also filters out words occurring frequently in isolation that do not bear much weight towards characterizing that category.}, } @article{koppel02, author = {
new event detection is a challenging task that still offers scope for great improvement after years of effort.
it is developed for the naive bayesian classifier applied on text data, since it combines well with the addressed learning problems.
it also shows that support vector machines can, in fact, sometimes very significantly outperform both methods.
by contrast, content-based methods use information about an item itself to make suggestions.
therefore, it is very costly to assign a category to them because a human investigates their contents.
we are using a custom workflow management system as the base for a range of services which are offered via a multimodal portal, using a language-based approach to extracting information from html forms, email, and sms.
the specifics of our classifier is that it allows accurate categorization of short messages containing only a few words.
unfortunately, the paradigm of supervised machine learning is ill-suited to this task, as it assumes that the training examples are classified by a teacher - usually a human.
different from existing categorization methods, mudof can automatically recommend a suitable algorithm for each category based on the category-specific statistical characteristics.
using density estimation over the raw tf*idf values, we obtain a classification accuracy of 82\%, a number that outperforms baseline estimates and earlier, image-based approaches, at least in the domain of news articles, and that nears the accuracy of humans who perform the same task with access to comparable information.}, } @inproceedings{sable01, author = {carl sable and ken church}, title = {using bins to empirically estimate term weights for text categorization}, booktitle = {proceedings of emnlp-01, 6th conference on empirical methods in natural language processing}, year = {2001}, publisher = {association for computational linguistics, morristown, us}, editor = {lillian lee and donna harman}, pages = {58--66}, address = {pittsburgh, us}, url = {http://www.cs.columbia.edu/~sable/research/emnlp01.ps},
for example, rather than choosing one set decision threshold, they can be used in a bayesian risk model to issue a run-time decision which minimizes a user-specified cost function dynamically chosen at prediction time.
the combination of evidence from a document and citing documents can improve on either information source alone.
the back data stores the information about keywords: the frequency for each category, the number of documents for each category.
in this paper, we introduce hyperlink ensembles, a novel type of ensemble classifier for classifying hypertext documents.
our test collection consists of the real news articles and the 519 subnewsgroups under the rec newsgroup of netnews in a period of 3 months.
text classifiers that give probability estimates are more readily applicable in a variety of scenarios.
standard term clustering strategies from information retrieval (ir), based on cooccurence of indexing terms in documents or groups of documents, were tested on a syntactic indexing phrase representation.
however the high performance of most existing pn classifiers heavily depends upon the availability of large dictionaries of domain-specific proper nouns, and a certain amount of manual work for rule writing or manual tagging.
using these techniques, we can automatically build text categorization systems that benefit from domain-specific natural language processing.}, } @article{robertson84, author = {stephen e. robertson and p. harding}, title = {probabilistic automatic indexing by learning from human indexers}, year = {1984}, journal = {journal of documentation}, volume = {40}, number = {4}, pages = {264--270}, url = {}, abstract = {}, } @inproceedings{rose02, author = {tony rose and mark stevenson and miles whitehead}, title = {
mh$^kr$} is both more efficient to train and more effective than the original {\sc adaboost.
we focus upon the routing of case law summaries to various secondary law volumes in which they should be cited.
an additional test submitted 250 trec queries to a search engine and successfully categorized 66\% of the top 100 using the odp and 61\% of the top 350.
this restructuring makes it possible for svms to focus on the latent semantic space without losing information given by the original feature space.
thus, feature reduction is often performed in order to increase the efficiency and effectiveness of the classification.
finally, the interactive nature of the system results in a more correct and precise description of each document than a fully automatic system would.}, } @article{kar78, author = {gautam kar and lee j. white}, title = {a distance measure for automated document classification by sequential analysis}, journal = {information processing and management}, pages = {57--69}, year = {1978}, number = {2}, volume = {14}, url = {}, abstract = {}, } @inproceedings{karypis00, author = {george karypis and eui-hong han}, title = {fast supervised dimensionality reduction algorithm with applications to document categorization and retrieval},
similarity in word space is based on second-order co-occurrence: two tokens (or contexts) of the ambiguous word are assigned to the same sense cluster if the words they co-occur with in turn occur with similar words in a training corpus.
the problem is not easy to tackle due to the semi-structured or even unstructured nature of those texts under consideration.
it accommodates both single and multiple topic assignments for each document.
the fragments it looks for are determined by a set of knowledge-based rules.
we also propose a simple and effective way of combining a traditional text based classifier with a citation-link based classifier.
the packing of word pairs also filters out words occurring frequently in isolation that do not bear much weight towards characterizing that category.}, } @article{koppel02, author = {
more importantly, our investigation suggests that meta data which is often available, or can be acquired using information extraction techniques, can be extremely useful for improving classification accuracy.
the research project agnet develops agents for neural text routing in the internet.
the outcome of the result was quite impressive: in different experimental setups, we reached a micro-averaged f1-measure of 0.89, with a peak of 0.899.
further, the weak performance of naive bayes can be partly explained by extreme skewness of posterior probabilities generated by it.
significant features are automatically derived from training texts by selecting substrings from actual word forms and applying statistical information and general linguistic knowledge.
{121--140}, url = {http://www.elsevier.nl/gej-ng/10/23/143/56/27/27/article.pdf}, abstract = {information filtering is concerned with filtering data streams in such a way as to leave only pertinent data (information) to be perused.
experimental results on standard benchmarks confirm the validity of our approach, showing that adaboost achieves consistent improvements by including additional semantic features in the learned ensemble.}, } @inproceedings{cai04, author = {lijuan cai and thomas hofmann}, title =
neural networks allow us to model higher-order interaction between document terms and to simultaneously predict multiple topics using shared hidden features.
this system, named stretch (storage and retrieval by content of imaged documents), is based on an archiving and retrieval engine, which overcomes the bottleneck of document profiling bypassing some limitations of existing pre-defined indexing schemes.
computationally, expnet has an o(n log n) time complexity which is much more efficient than the cubic complexity of the llsf method.
the experiments show substantial improvements over inductive methods, especially for small training sets, cutting the number of labeled training examples down to a 20th on some tasks.
arbee l. chen and frederick h. lochovsky}, publisher = {ieee computer society press, los alamitos, us}, year = {1999}, address = {hsinchu, tw}, pages = {195--202}, url = {http://dlib.computer.org/conferen/dasfaa/0084/pdf/00840195.pdf}, abstract = {in a text categorization model using an artificial neural network as the text classifier scalability is poor if the neural network is trained using the raw feature space since textural data has a very high-dimension feature space.
in this paper we present a method for detecting the text genre quickly and easily following an approach originally proposed in authorship attribution studies which uses as style markers the frequencies of occurrence of the most frequent words in a training corpus (burrows, 1992).
we believe that database categorization can be a potentially effective technique for good database selection, especially in the internet environment, where
{ieee computer society press, los alamitos, us}, year = {2000}, address = {snowbird, us}, pages = {200--209}, url = {http://dlib.computer.org/conferen/dcc/0592/pdf/05920555.pdf}, abstract = {text categorization is the assignment of natural language texts to predefined categories based on their content.
extensive experiments reported in the paper shows that this new weighting method improves significantly the classification accuracy as measured on many categorization tasks."
we also try combining classifiers based on different representations using a majority voting technique, and this improves performance on both test collections.
performance of rankboost is somewhat inferior to that of a state-of-the-art routing algorithm which is, however, more complex and less theoretically justified than rankboost.
the architecture is based on the metaphor of the software agents and incorporates innovative hints from other fields: distributed architectures, relevance feedback and active interfaces.
in addition, aram predictive accuracy and learning efficiency can be improved by incorporating a set of rules derived from the reuters category description.
we show how to update such databases with new documents with high speed and accuracy.
we evaluate four different measures of subject similarity, derived from the web link structure, and determine how accurate they are in predicting document categories.
it is evident that the llsf approach uses the relevance information effectively within human decisions of categorization and retrieval, and achieves a semantic mapping of free texts to their representations in an indexing language.
we present an approach using the vector space model to integrate two different kind of resources: a lexical database and training collections, in text content analysis tasks.
since the whole system is rule-based, it can be adapted to different subject fields by appropriate modifications of the rule bases.
% everyone is welcome to download the bibliography as a whole and % % distribute it, provided that it is distributed untouched.
in this paper, we present an approach to the integration of lexical knowledge extracted from the ldb wordnet in learning-based atc, based on stacked generalization (sg).
"like many purely data-driven machine learning methods, support vector machine (svm) classifiers are learned exclusively from the evidence presented in the training dataset; thus a larger training dataset is required for better performance.
the best performance was achieved by the feature selection based on a feature scoring measure known from information retrieval called odds ratio and using relatively small number of features.}, } @inproceedings{mladenic04, author = {
empirical results show that gp was able to discover better similarity functions than ga or other fusion techniques." }
maintaining catalogues manually is becoming increasingly difficult due to the sheer amount of material on the web, and therefore it will be soon necessary to resort to techniques for automatic classification of documents.
the results show that the probabilistic algorithms are preferable to the heuristic rocchio classifier not only because they are more well-founded, but also because they achieve better performance.}, } @inproceedings{joachims97b, author = {thorsten joachims and dayne freitag and tom m. mitchell}, title = {{\sc webwatcher}: a tour guide for the word wide web}, booktitle = {proceedings of ijcai-97, 15th international joint conference on artificial intelligence}, editor = {martha e. pollack}, publisher = {morgan kaufmann publishers, san francisco, us}, year = {1997}, address = {nagoya, jp}, pages = {770--775}, url = {http://www.cs.cmu.edu/afs/cs/user/dayne/www/ps/ijcai97.ps.z}, abstract = {
we report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic lsi model.}, } @article{bloedorn98, author = {eric bloedorn and ryszard s. michalski}, title = {data-driven constructive induction}, journal = {ieee intelligent systems}, year = {1998}, number = {2}, volume = {13}, pages = {30--37}, url = {}, abstract = {an inductive learning program's ability to find an accurate hypothesis can depend on the quality of the representation space.
on both corpora the algorithms we present outperform adaptations to topic-ranking of rocchio's algorithm and the perceptron algorithm.
text mining applies the same analytical functions of data mining to the domain of textual information, relying on sophisticated text analysis techniques that distill information from free-text documents.
hence, the creation of new directories or the modification of existing ones require strong investments.
the paper describes the kdt system for knowledge discovery in texts.
our experiments show that the transductive method outperforms conventional boosting techniques that employ only labeled data.}, } @inproceedings{taira99, author =
hierarchical classifi- cation is a more efficient method - instead of a single classifier, we use a set of classifiers distributed over a class taxonomy, one for each internal node.
we propose a new hierarchical generative model for textual data, where words may be generated by topic specific distributions at any level in the hierarchy.
two example categorization tasks achieve recognition scores of approximately 80\% and are very robust against recognition or typing errors.}, } @inproceedings{bekkerman01, author =
in addition, they work much better at certain tasks, such as identifying major events in texts, than at others, such as determining what sort of business or product is involved in a news event.
from this perspective, bns was the top single choice for all goals except precision, for which information gain yielded the best result most often.
ibm's intelligent miner for text provides the necessary tools to unlock the business information that is ''trapped'' in email, insurance claims, news feeds, or other document repositories.
we also introduce decision functions for the centroid-based classification algorithm and support vector classifiers to handle the classification problem where a document may belong to multiple classes.
the system selects the category whose profile has the smallest distance to the document's profile.
results from experiments show that this filter has successfully rejected a sufficient number of non-relevant documents, resulting in an improvement of filtering performance.}, } @inproceedings{hoch94, author = {rainer hoch}, title = {using ir techniques for text classification in document analysis}, booktitle = {proceedings of sigir-94, 17th acm international conference on research and development in information retrieval}, editor = {w. bruce croft and van rijsbergen, cornelis j.}, publisher = {springer verlag, heidelberg, de}, year = {1994}, address = {dublin, ie}, pages = {31--40}, url = {http://www.acm.org/pubs/articles/proceedings/ir/188490/p31-hoch/p31-hoch.pdf}, abstract =
the simple tfidf classifier is chosen to train sample data and to classify other new data.
this process requires less effort than providing words with no help or manual labeling of documents.
we have evaluated expnet in categorization and retrieval on a document collection of the medline database, and observed a performance in recall and precision comparable to the linear least squares fit (llsf) mapping method, and significantly better than other methods tested.
as the standard performance measures assume independence between categories, they have not considered the documents incorrectly classified into categories that are similar or not far from the correct ones in the category tree.
we show that, in expectation, the excess cumulative h-loss grows at most logarithmically in the length of the data sequence.
in contrast to most ir methods, theoretical analysis provides performance guarantees and guidance on parameter settings for these algorithms.
this makes it a good candidate for a general "install-and-forget" term selection mechanism.
a probabilistic classification procedure computes indexing weights for each relevance description.
the combination with content-based methods can further improve the results, but too much noise may be introduced, since the text of web pages is a much less reliable source of information.
a major problem facing online information services is how to index and supplement large document collections with respect to a rich set of categories.
this can be considered as the effective combination of documents with no topic or class labels (unlabeled data), labeled documents, and prior domain knowledge (in the form of the known hierarchic structure), in providing enhanced document classification performance.}, } @article{vinokourov02, author = {
this simplifies the creation of knowledge-based if/ir systems, speeds up their operation, and allows easy editing of the rule bases employed.
we find that on average, labeling a feature takes much less time than labeling a document.
in our experiments we compare the effectiveness of the svm -based feature selection with that of more traditional feature selection methods, such as odds ratio and information gain, in achieving the desired tradeoff between the vector sparsity and the classification performance.
this architecture acquires its language model and dictionary adaptively and hence avoids handcoding of either.
it employs a meta-learning phase using document feature characteristics.
when a probabilistic text categorization is extended to a cluster-based one, the use of hbc offers better performance than the use of non-probabilistic algorithms.}, } @inproceedings{iwazume96, author = {michiaki iwazume and hideaki takeda and toyoaki nishida}, title = {ontology-based information gathering and text categorization from the internet}, booktitle = {proceedings of iea/aie-96, 9th international conference in industrial and engineering applications of artificial intelligence and expert systems}, editor = {}, publisher = {}, year = {1996}, address = {fukuoka, jp}, pages = {305--314}, url = {}, abstract = {}, } @inproceedings{iyer00, author = {raj d. iyer and david d. lewis and robert e. schapire and yoram singer and amit singhal}, title = {boosting for document routing}, booktitle = {proceedings of cikm-00, 9th acm international conference on information and knowledge management}, publisher =
the case base of natural language contexts is acquired automatically during sentence analysis using a training corpus of texts and their correct relevancy classifications.
the hypothesis that cdm`s performance exceeds two non-domain specific algorithms, bayesian classification and decision tree learners, is empirically tested.}, } @article{goldberg96, author = {goldberg, jeffrey l.}, title = {cdm: an approach to learning in text categorization}, journal = {international journal on artificial intelligence tools}, year = {1996}, number = {1/2}, volume = {5}, pages = {229--253}, url = {}, abstract = {
we introduce a novel hybrid system specifically designed for multi-page text documents.
since information retrieval is a domain where such data sets are widespread, it provides an ideal application area for machine learning.
published in the ``lecture notes in computer science'' series, number 2004}, pages = {423--436}, url = {http://link.springer.de/link/service/series/0558/papers/2004/20040423.pdf}, abstract = {a new way of representing texts written in natural language is introduced, as a conditional probability distribution at the letter level learned with a variable length markov model called adaptive context tree model.
works in text retrieval through internet suggest that embedding linguistic information at a suitable level within traditional quantitative approaches (e.g. sense distinctions for query expansion as in [14]) is the crucial issue able to bring the experimental stage to operational results.
we present an efficient algorithm for text classification using hierarchical classifiers based on a concept hierarchy.
further, the categorization system can be trained on noisy ocr output, without need for the true text of any image, or for editing of ocr output.
rankboost achieves comparable performance to the state-of-the-art algorithm when combined with feature or example selection heuristics.
experiments on data sets with different properties (reuters-21578, patent abstracts and patent applications) and with two different algorithms (winnow and rocchio) show that uc-based term selection is not the most aggressive term selection criterium, but that its effect is quite stable across data sets and algorithms.
finally, the relative performance of the different classifiers being tested gives us insights into the strengths and limitations of our algorithms for hypertext classification.}, } @inproceedings{ghani01a, author = {rayid ghani}, title =
the trec-4 (4th text retrieval conference) filtering track was an experiment in the evaluation of binary text classification systems.
effectiveness of the similarity functions discovered through simple majority voting is better than that of content-based as well as combination-based support vector machine classifiers.
once again, lsi slightly improves performance.
conventional methods such as decision trees have had competitive, but not optimal, predictive performance.
go annotation is a major activity in most model organism database projects and annotates gene functions using a controlled vocabulary.
in particular, our new feature selection method yields considerable improvement.
as the amount of data stored in storage media is increased exponentially, it becomes necessary to store documents according to their category, to access them easily.
firstly we introduce a novel kernel, whose gram matrix is the well known co-citation matrix from bibliometrics, and demonstrate on real data that it has a good performance.
to verify our new method, we conducted experiments on two language newsgroup data sets: one written by english and the other written by korean.
these algorithms relied on hand-coded training data, including annotated texts and a semantic dictionary.
in a different manner from topographical techniques previously utilized for static text collections, the topography is an outcome of the coherence in time of the data stream in the proposed model.
results show that the use of metadata is almost as good as the full-text version of papers.
most importantly, this regularized estimation enables the model parameters to become sparse.
experiments with syntactic phrase indexing, however, have never yielded significant improvements in text retrieval performance.
when training classifiers on large collections of documents, both the time and memory requirements connected with processing of these vectors may be prohibitive.
upon the fact that refined statistics may have more chance to meet sparse data problem, we re-evaluate the role of the binary weighting model (bwm) in tc for further consideration.
the utility of our approach is demonstrated on a set of web-pages that relate to computer science departments.}, } @inproceedings{furnkranz99, author = {johannes f{\"{u}}rnkranz}, title = {exploiting structural information for text classification on the www}, booktitle = {proceedings of ida-99, 3rd symposium on intelligent data analysis}, publisher = {springer verlag, heidelberg, de}, note = {
experiments show that feature selection using weights from linear svms yields better classification performance than other feature weighting methods when combined with the three explored learning algorithms.
we show that a careful hybrid integration of techniques from neural network architectures, learning and information retrieval can reach consistent recall and precision rates of more than 92\% on an 82,000 word corpus; this is demonstrated for 10,000 unknown news titles from the reuters newswire.
using such an architecture, the time needed for training is reduced substantially and the user is provided with an even more intuitive metaphor for visualization.
much information is nowadays stored as multilingual textual data; therefore advanced classification systems are currently considered as strategic components for effective knowledge management.
we also carefully analyze the case of those documents whose categorization is not in accordance with the one provided by the human specialists.
{acm press, new york, us}, editor = {}, year = {2003}, address = {new orleans, us}, pages = {232--239}, url = {http://doi.acm.org/10.1145/956863.956909}, abstract = {most existing studies of text classification assume that the training data are completely labeled.
more importantly, our investigation suggests that meta data which is often available, or can be acquired using information extraction techniques, can be extremely useful for improving classification accuracy.
the proposed approach is more efficient, does not require the specification of any parameters, and similarly to the parameter-based approach, boosts the performance of baseline svms by at least 20\% for standard information retrieval measures.}, } @inproceedings{shanks03, author = {vaughan r. shanks and hugh e. williams}, title = {index construction for linear categorisation}, booktitle = {proceedings of cikm-03, 12th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, editor = {}, year = {2003}, address = {new orleans, us}, pages = {334--341}, url = {http://doi.acm.org/10.1145/956863.956926}, abstract = {
combinations of multiple classifiers did not always improve the classification accuracy compared to the best individual classifier.
we show that the accuracy of a naive bayes classifier over text classification tasks can be significantly improved by taking advantage of the error-correcting properties of the code.
the authors show how this term-frequency approach supports a range of kdd operations, providing a general framework for knowledge discovery and exploration in collections of unstructured text.}, } @inproceedings{dagan97, author = {ido dagan and yael karov and dan roth}, title = {mistake-driven learning in text categorization}, booktitle = {proceedings of emnlp-97, 2nd conference on empirical methods in natural language processing}, publisher = {association for computational linguistics, morristown, us}, editor = {claire cardie and ralph weischedel}, year = {1997}, address = {providence, us}, pages = {55--63}, url = {http://l2r.cs.uiuc.edu/~danr/papers/categ.ps.gz}, abstract = {learning problems in the text processing domain often map the text to a space whose dimensions are the measured features of the text, e.g., its words.
pattern matching produces fairly accurate and fast categorisation over a large number of classes, while information extraction provides fine-grained classification for a reduced number of classes.
we describe the results of extensive machine learning experiments on large collections of reuters' english and german newswires.
however, many popular feature selection techniques such as information gain (ig) and $\chi^2$-test (chi) are all greedy in nature and thus may not be optimal according to some criterion.
for a slim fraction of all documents (0.77\% for category coding and 1.4\% for subcategory coding), the algorithm makes assignments that are clearly incorrect.
the experimental results show that the proposed pattern-based approach significantly outperforms all three baselines in terms of precision at top ranks."
we use techniques from statistical pattern recognition to efficiently separate the feature words or discriminants from the noise words at each node of the taxonomy.
experiments were conducted using a large scale document collection from reuters news articles.
the investigation includes different attribute and distance-weighting schemes, and studies on the effect of the neighborhood size, the size of the attribute set, and the size of the training corpus.
we analyze a few of the commonly used statistics based and machine learning algorithms for natural language disambiguation tasks and observe that they can be recast as learning linear separators in the feature space.
then, a filtering system with the neural network integrated into it was used to filter the medical documents and this performance was compared with the filtering results achieved using the baseline system.
this neural model is based on significance vectors and benefits from the presentation of document clusters.
in this work we reproduce these results and go further to show that when the training sample is small word clusters can yield significant improvement in classification accuracy (up to 18\%) over the performance using the words directly.}, } @inproceedings{soucy01, author = {pascal soucy and guy w. mineau}, title = {
we show how both algorithms can be adapted to maximize any general utility matrix that associates cost (or gain) for each pair of machine prediction and correct label.
our solution obtained creativity and precision runner-up awards at the competition.
these theoretical findings are supported by experiments on three test collections.
for best accuracy, f-measure or recall, the findings reveal an outstanding new feature selection metric, "bi-normal separation" (bns).
the data-driven nature of tcs allows it is to satisfy fully the requirements of ease of application development, portability to other applications and maintainability.}, } @article{he03, author = {
relation selection improves foil's performance as measured by any of recall, precision, f-measure, or error rate.
a stochastic decision list is an ordered sequence of if-then-else rules, and our method can be viewed as a rule-based method for text classification having advantages of readability and refinability of acquired knowledge.
the key idea is to automatically adjust the window size so that the estimated generalization error is minimized.
here, limited labeled data provide em initializations that lead to low-probability models.
but they don't offer any of the benefits of natural language processing, such as the ability to identify relationships and enforce linguistic constraints.
we compare the accuracy of our learning approach to a rule-based, expert system approach that uses a text categorization shell built by carnegie group.
the experiment results show that with only surface text features the svm outperforms the other four methods for this task.
our experimental comparison of eleven feature scoring measures show that considering domain and algorithm characteristics significantly improves the results of classification.}, } @article{moens00, author = {marie-francine moens and jos dumortier}, title = {
an extended set of e-mail document features including structural characteristics and linguistic patterns were derived and, together with a support vector machine learning algorithm, were used for mining the e-mail content.
the experiments compare the performance of a counterpropagation network against a backpropagation neural network.
they were tested on a corpus of articles from the dutch newspaper nrc, and pre-classified into four categories.
in our tests with three categorization methods on text collections from different domains/applications, significant numbers of words were removed without sacrificing categorization effectiveness.
finally, the relative performance of the classifiers being tested provided insights into their strengths and limitations for solving classification problems involving diverse and often noisy web pages.}, } @inproceedings{yang03, author =
in this paper, using several algorithms, we compare the categorization accuracy of classifiers based on words to that of classifiers based on senses.
our experiments using the webkb dataset showed that iwum improves the overall classification performance and works very well on the more structured parts of a web site.}, } @inproceedings{taghva00, author = {taghva, kazem and nartker, thomas a. and julie borsack and steven lumos and allen condit and ron young}, title = {
we tested this approach by applying k-nearest neighbor, rocchio and language modeling classifiers and their combination to the event tracking problem in the topic detection and tracking (tdt) domain, where new classes (events) are created constantly over time, and representative validation sets for new classes are often difficult to obtain on time.
our feature selection approach employs distributional clustering of words via the recently introduced information bottleneck method, which generates a more efficient word-cluster representation of documents.
the text classification algorithms classify texts with high accuracy by using an underlying information extraction system to represent linguistic phrases and contexts.
by referencing various relationships in the thesaurus corresponding to the structured categories, k-nn can be prominently improved, removing the ambiguity.
experimental evaluation on real-world data shows that the proposed approach gives good results.
mh$^kr$} is both more efficient to train and more effective than the original {\sc adaboost.
using the vector space model, each document is represented by its original feature vector augmented with external feature vector generated using wordnet.
in previous research, lsi has produced a small improvement in retrieval performance.
our test collection consists of the real news articles and the 519 subnewsgroups under the rec newsgroup of netnews in a period of 3 months.
the changes may occur both on the transmission side (the nature of the streams can change), and on the reception side (the interest of a user can change).
to save the storage space and computation time in text categorization, efficient and effective algorithms for reducing the data before analysis are highly desired.
abstract = {current trend in operational text categorization is the designing of fast classification tools.
using the vector space model (vsm), each document is represented by its original feature vector augmented with external feature vector generated using wordnet.
in [3] it has been suggested that classifiers based on generalized rocchio formula can be used to weight features in category profiles in order to exploit the selectivity of linguistic information techniques in text classification.
for this reason, the automatic construction of disambiguation rules is highly desirable.
we suggest the utilization of additional resources like lexical databases to increase the amount of information that tc systems make use of, and thus, to improve their performance.
while developing simpl, we also make a detailed experimental analysis of the cache performance of svms.}, } @inproceedings{chakrabarti97, author = {soumen chakrabarti and byron e. dom and rakesh agrawal and prabhakar raghavan}, title = {using taxonomy, discriminants, and signatures for navigating in text databases}, booktitle = {proceedings of vldb-97, 23rd international conference on very large data bases}, publisher =
by noisy it is meant any text obtained through an extraction process (affected by errors) from media different than digital texts.
this study benchmarks the performance of twelve feature selection metrics across 229 text classification problems drawn from reuters, ohsumed, trec, etc.
we found that it and other existing methods failed to produce good results on an industrial text classification problem.
it is shown that both classifiers can perform filtering with reasonable accuracy.
in this case, the label of one entity (e.g., the topic of the paper) is often correlated with the labels of related entities.
the described system can be efficiently adapted to new domains or different languages.
{dasigi, venu and mann, reinhold c. and protopopescu, vladimir a.}, title = {information fusion for text classification: an experimental comparison}, journal = {pattern recognition}, year = {2001}, volume = {34}, number = {12}, pages = {2413--2425}, url = {}, abstract = {this article reports on our experiments and results on the effectiveness of different feature sets and information fusion from some combinations of them in classifying free text documents into a given number of categories.
% % % % concerning urls from which to download on-line copies of the % % papers, where possible i have included urls with unrestricted % % access (e.g. home pages of authors).
on the other hand, we also observe that linked pages can be more harmful than helpful when the linked neighborhoods are highly ``noisy'' and that links have to be used in a careful manner.
bayesian independence classifiers and k-nearest-neighbor classifiers were trained to assign scores to manually-graded essays.
this process involves an activity of {\em supervised learning}, in which information on the membership of training documents in categories is used.
this paper presents a new method for graph-based classification, with particular emphasis on hyperlinked text documents but broader applicability.
the learning process constructs a relationship between an index term and the words relevant and irrelevant to it, based on the positive training set and negative training set, which are sample documents indexed by the index term, and those not indexed by it, respectively.
{acm press, new york, us}, year = {1996}, address = {z{\"{u}}rich, ch}, pages = {279--288}, url = {ftp://parcftp.xerox.com/pub/qca/papers/sigirfiltering96.ps}, abstract = {there is strong empirical and theoretic evidence that combination of retrieval methods can improve performance.
we report the results of our experiments, using various feature selection measures and varying values of $\sigma$, performed on the {\sc reuters-21578} standard tc benchmark.
in particular, we develop metrics that estimate the difficulty of a dataset by examining the host directory structure.
in addition, we carefully analyze the internal representation using cluster analysis and output representations using a new surface error technique.
experienced users can make effective use of such engines for tasks that can be solved by searching for tightly constrained keywords and phrases.
text collections in different domains were used for evaluation.
svms have been trained on data sets with several thousand instances, but web directories today contain millions of instances which are valuable for mapping billions of web pages into yahoo!-like directories.
we also demonstrate that the retrieval performance using automatic categorization achieves the same retrieval quality as the performance using manual categorization.
in this paper, we focus on using user assisted text classification in conjunction with a web portal, multiple document management systems and an ontology, to provide a powerful solution for organizing information about a company's technology.
the subsequences are weighted by an exponentially decaying factor of their full length in the text, hence emphasising those occurrences that are close to contiguous.
the algorithm uses the information gain metric, combined with various frequency thresholds.
the text categorization (tc) is the automated assignment of text documents to predefined categories based on document contents.
therefore, the contributions of this research are in learning and generalizing neural architectures for the robust interpretation of potentially noisy unrestricted messages.
the engine exploits a structured document representation and can activate appropriate methods to characterise and automatically index heterogeneous documents with variable layout.
this paper reports on a system that uses natural language text processing to derive keywords from free text news stories, separate these keywords into segments, and automatically build a segmented database.
our experiments are conducted in the text categorization domain, which is characterized by a large number of features, many of which are irrelevant.
however, a problem with iterative training techniques such as svm is that during their learning or training phase, they require the entire training collection to be held in main-memory; this is infeasible for large training collections such as dmoz or large news wire feeds.
{2000}, pages = {176--183}, url = {http://www.acm.org/pubs/articles/proceedings/ir/345508/p176-hoashi/p176-hoashi.pdf}, abstract = {document filtering is a task to retrieve documents relevant to a user's profile from a flow of documents.
we have developed a new effective probabilistic classifier for document classification by introducing the concept of differential document vectors and dlsi (differential latent semantic indexing) spaces.
we propose here the use of soft clustering of words, i.e., in which a word can be assigned to several different clusters and each cluster is characterized by a specific word probability distribution.
the baseline method combines the terms of all the articles of each newsgroup in the training set to represent the newsgroups as single vectors.
in this paper the suitability of different document representations for automatic document classification is compared, investigating a whole range of representations between bag-of-words and bag-of-phrases.
{acm press, new york, us}, year = {1996}, address = {z{\"{u}}rich, ch}, pages = {307--315}, note = {an extended version appears as~\cite{cohen99}}, url = {http://www.research.whizbang.com/~wcohen/postscript/sigir-96.ps}, abstract = {two machine learning algorithms, ripper and sleeping experts for phrases, are evaluated on a number of large text categorization problems.
% % this is a bibliography, in bibtex format, on automated text % % categorization (atc), defined as the activity of automatically % % building, by means of machine learning techniques, automatic text % % classifiers, i.e. systems capable of assigning to a text % % document one or more thematic categories from a predefined set.
the author presents the core technology of teklis, the results on the filtering and routing tasks and a discussion of the insights gained through participation in the exercise.}, } @inproceedings{cai03, author = {lijuan cai and thomas hofmann}, title = {
first, we use the system to compute profiles on training set data that represent the various categories, e.g., language samples or newsgroup content samples.
our experiments demonstrate that this new approach is able to learn more accurate classifiers than either of its constituent methods alone.}, } @inproceedings{slonim01, author = {
the accuracy of classification achieved with our method appears better than or comparable to those of existing rule-based methods.
while most work on kdd has been concerned with structured databases, there has been little work on handling the huge amount of information that is available only in unstructured textual form.
we present a method to detect automatically pornographic content on the web.
we show how a naive bayes classification can be enhanced to incorporate the similarity information present in source catalogs.
the automatic indexing system {air/phys}.
our key insight is that many of the data sources have their own categorization, and classification accuracy can be improved by factoring in the implicit information in these source categorizations.
coding (ecoc) for learning text classifiers.
in contrast, software for text categorization, message filtering, textual data mining, and related tasks is less common.
published in the ``lecture notes in computer science'' series, number 2997}, pages = {197--208}, url = {http://springerlink.metapress.com/openurl.asp?genre=article&issn=0302-9743&volume=2997&spage=197}, abstract = {high dimensionality of feature space is a main obstacle for text categorization (tc).
finally, the system computes a distance measure between the document's profile and each of the category profiles.
we present experimental results in learning to classify email in this fashion, where each class corresponds to a verb-noun pair taken from a predefined ontology describing typical 'email speech acts'.
although the corpus contains documents written in chinese, the proposed approach can be applied to documents written in any language and such documents can be transformed into a list of separated terms.}, } @inproceedings{yang01, author =
one important goal in text mining is automatic classification of electronic documents.
it outperforms existing systems by keeping most of their interesting properties (i.e. easy implementation, low complexity and high scalability).
"485--492", abstract = "automatic classification of data items, based on training samples, can be boosted by considering the neighborhood of data items in a graph structure (e.g., neighboring documents in a hyperlink environment or co-authors and their publications for bibliographic data entries).
the effect of using hierarchical classifiers in text categorization}, booktitle = {proceeding of riao-00, 6th international conference ``recherche d'information assistee par ordinateur''}, editor = {}, address = {paris, fr}, year = {2000}, pages = {302--313}, url = {http://www.iona.edu/cs/facultypublications/riao2000new.pdf}, abstract = {
the effect of using hierarchical classifiers in text categorization}, booktitle = {proceeding of riao-00, 6th international conference ``recherche d'information assistee par ordinateur''}, editor = {}, address = {paris, fr}, year = {2000}, pages = {302--313}, url = {http://www.iona.edu/cs/facultypublications/riao2000new.pdf}, abstract = {
meanwhile, the web object representation is constructed by hyperlink analysis, and further pruned to remove the noises.
based on a revision of self-organizing maps, namely taxsom, the proposed model performs an unsupervised classification, exploiting the a-priori knowledge encoded in a taxonomy structure both at the terminological and topological level.
this is because when the positive training data is too few, the boundary over-iterates and trespasses the natural gaps between positive and negative class in the feature space and thus ends up fitting tightly around the few positive training data.}, } @inproceedings{yu98, author = {kwok l. yu and wai lam}, title = {a new on-line learning algorithm for adaptive text filtering}, booktitle = {proceedings of cikm-98, 7th acm international conference on information and knowledge management}, publisher =
the training approaches we test are the rocchio (relevance feedback) and the widrow-hoff (machine learning) algorithms and wordnet as the lexical database.
we are facing a new challenge due to the fact that web documents have a rich structure and are highly heterogeneous.
tampere, fi}, year = {2002}, pages = {137--144}, url = {http://doi.acm.org/10.1145/564376.564402}, abstract = {to improve performance in text categorization, it is important to extract distinctive features for each class.
current text learning techniques for combining labeled and unlabeled, such as em and co-training, are mostly applicable for classification tasks with a small number of classes and do not scale up well for large multiclass problems.
a typical example is information filtering, i.e. the adaptive classification of documents with respect to a particular user interest.
the authors' initial work with this algorithm has demonstrated that probabilistic structures can be automatically acquired from a training set of documents with respect to a single target concept, or a set of related concepts.
machine learning techniques developed for learning on text data are used here on the hierarchical classification structure.
it was found that categorizers consisting of the words with highest tf.idf values scored best.}, } @inproceedings{paliouras99, author = {
our approach has practical advantages for problem solving by introducing the viewpoint of tasks to achieve higher performance.}, } @inproceedings{mccallum98, author = {andrew k. mccallum and kamal nigam}, title = {
in this paper, we present an approach to the integration of lexical knowledge extracted from the ldb wordnet in learning-based atc, based on stacked generalization (sg).
experimental results show that winnow with thesauri attains high accuracy and that the proposed filtering and disambiguation methods also contribute to the improved accuracy.}, } @inproceedings{yang00, author = {yiming yang and thomas ault and thomas pierce and charles w. lattimer}, title = {
a system called autoslog is presented which automatically constructs dictionaries for information extraction, given an appropriate training corpus.
moreover, the estimators developed here address the special performance measures needed for evaluating text classifiers.
{acm press, new york, us}, address = {toronto, ca}, year = {2003}, pages = {190--197}, url = {http://doi.acm.org/10.1145/860435.860471}, abstract = {real-world applications often require the classification of documents under situations of small number of features, mis-labeled documents and rare positive examples.
moreover, the number of irrelevant references gathered by our system is about one-thirteenth that of traditional keyword-based search systems.
for very long codes, the performance is in some cases further improved by kl-distance optimization.}, } @inproceedings{klas00, author = {klas, claus-peter and fuhr, norbert}, title = {a new effective approach for categorizing web documents}, booktitle = {proceedings of bcsirsg-00, the 22nd annual colloquium of the british computer society information retrieval specialist group}, editor = {}, address = {cambridge, uk}, year = {2000}, pages = {}, publisher = {}, url = {http://ls6-www.informatik.uni-dortmund.de/bib/fulltext/ir/klas_fuhr:00.ps.gz}, abstract = {categorization of web documents poses a new challenge for automatic classification methods.
in previous work, we developed several algorithms that use information extraction techniques to achieve high-precision text categorization.
our results confirm, quantify, and extend previous research using web structure in these areas, introducing new methods for classification and description of pages.}, } @inproceedings{godbole04, author = {shantanu godbole and abhay harpale and sunita sarawagi and soumen chakrabarti}, title = {document classification through interactive supervision of document and term labels}, booktitle = {proceedings of pkdd-04, 8th european conference on principles of data mining and knowledge discovery}, editor = {jean-fran{\c{c}}ois boulicaut and floriana esposito and fosca giannotti and dino pedreschi}, address = {
the empirical evaluation indicates that the error rate (as obtained by running the naive bayes classifier on isolated pages) can be significantly reduced if contextual information is incorporated.}, } @inproceedings{frommholz01, author = {ingo frommholz}, title = {
the results showed that the proposed model was able to achieve high categorization effectiveness as measured by precision and recall.
the msdn corpus is collected from an online news website maintained by the min-sheng daily news, taiwan.
the architecture is based on the metaphor of the software agents and incorporates innovative hints from other fields: distributed architectures, relevance feedback and active interfaces.
experiments in the terrorism domain suggest that increasing the amount of linguistic context can improve performance.
experimental results confirm improved performance, breaking through the plateau previously reached in the field."
this approach has the advantage of being able to recommend previously unrated items to users with unique interests and to provide explanations for its recommendations.
abis compares documents with the past situations and finds the similarity scores on the basis of a memory-based reasoning approach.}, } @article{amati99, author = {
classification experiments usually grab a snapshot (temporally and spatially) of the web for a corpus.
we show that an old corpus can be used for training when testing on new web pages, with only a marginal drop in accuracy rates on genre classification.
we show that a good hierarchical machine learning-based categoriser can be developed using small numbers of features from pre-categorised training documents.
we built these profiles by selecting feature words and phrases from the training documents.
furthermore, the use of shape coding is particularly advantageous over ocr in the processing of page images of poor quality.
our experimental results on a large dataset confirm that the use of the implicit links is better than using explicit links in classification performance, with an increase of more than 10.5\% in terms of the macro-f1 measurement."
"we demonstrate the value of using context in a new-information detection system that achieved the highest precision scores at the text retrieval conference's novelty track in 2004.
subjective human evaluation of the keyphrases generated by extractor suggests that about 80\% of the keyphrases are acceptable to human readers.
unrestricted potentially faulty text messages arrive at a certain delivery point (e.g. email address or world wide web address).
in text categorization (tc) based on the vector space model, feature weighting is vital for the categorization effectiveness.
in particular, we evaluate the vector and latent semantic analysis (lsa) methods, a classifier based on support vector machines (svm) and the k-nearest neighbor variations of the vector and lsa models.
we discuss the role of structural information for classification and describe experiments on a small collection of class labeled structured documents.
abis minimizes user's effort in selecting the huge amount of available documents.
the experiments reported in this paper deal with the relationship between specific formal textual features, i.e. graphic and phonetic information, and the reader's literary educational background in the categorization of poetic texts.
conversely, a category's level is increased to strengthen it if its precision exceeds its recall.
arbee l. chen and frederick h. lochovsky}, publisher = {ieee computer society press, los alamitos, us}, year = {1999}, address = {hsinchu, tw}, pages = {195--202}, url = {http://dlib.computer.org/conferen/dasfaa/0084/pdf/00840195.pdf}, abstract = {in a text categorization model using an artificial neural network as the text classifier scalability is poor if the neural network is trained using the raw feature space since textural data has a very high-dimension feature space.
these distributions are useful to increase classification accuracy by exploiting information of (1) term distribution among classes, (2) term distribution within a class and (3) term distribution in the whole collection of training data.
we experimented with pre-classified samples from {{\sc yahoo!}}\ and the us patent database.
text categorization experiments demonstrates the ability of this representation to catch information about the semantic content of the text.}, } @inproceedings{viechnicki98, author = {peter viechnicki}, title = {
alternative methods are also tested on these data collections for comparison.
{saarbr{\"{u}}cken, de}, url = {http://nlp3.korea.ac.kr/proceeding/coling2000/coling/ps/066.ps}, abstract = {the goal of text categorization is to classify documents into a certain number of pre-defined categories.
in contrast, mi had relatively poor performance due to its bias towards favoring rare terms, and its sensitivity to probability estimation errors.}, } @inproceedings{yang99, author =
our experiments use reuters 21578 database and consist of binary classifications for categories selected from the 89 topics classes of the reuters collection.
the mfom learning framework is evaluated on the reuters-21578 task with lsi-based feature extraction and a binary tree classifier.
although much research has been done on text categorization, this algorithm is novel in that it is unsupervised, i.e., it does not require pre-labeled training examples, and it can assign multiple category labels to documents.
in a batch mode, the programs to accomplish this indexing would require no more than fifteen minutes of cpu time per week.}, } @article{klingbiel73a, author = {paul h. klingbiel}, title = {a technique for machine-aided indexing}, journal = {information storage and retrieval}, year = {1973}, volume = {9}, number = {9}, pages =
specifically, we show that by adjusting the category levels in a principled way, that precision can be significantly improved, from 84\% to 91\%, on the much-studied reuters-21578 corpus organized in a three-level hierarchy of categories.}, } @article{damashek95, author = {marc damashek}, title = {
the method is evaluated using backpropagation neural networks, as the machine learning algorithm, that learn to assign mesh categories to a subset of medline records.
it produces inferior results because it is insensitive to subtle differences between articles that belong to a category and those that do not.
a generic system for text categorization is presented which is based on statistical analysis of representative text corpora.
in order to obtain class labels for training samples, we conducted a study where subjects ranked document pages with respect to their resemblance to representative page images.
in this paper we propose simple, heuristic solutions to some of the problems with naive bayes classifiers, addressing both systemic issues as well as problems that arise because text is not actually generated according to a multinomial model.
for example, we have previously shown how foil, a relational learner, can learn to classify web pages by discovering training set regularities in the words occurring on target pages, and on other pages related by hyperlinks.
we use image features such as percentages of text and non-text (graphics, images, tables, and rulings) content regions, column structures, relative point sizes of fonts, density of content area, and statistics of features of connected components which can be derived without class knowledge.
grammatical errors do not exceed five per cent of the output, so human screening is satisfactorily low.
when parameters tuned on an early benchmark tdt corpus were evaluated on a later tdt benchmark corpus with no overlapping events, we observed a 38-65\% reduction in tracking cost (a weighted combination of errors) by the combined system over the individual methods evaluated under the same conditions, strongly suggesting the robustness of this approach as a solution for improving cross-class performance consistency of statistical classifiers when standard cross-validation fails due to the lack of representative validation sets.}, } @inproceedings{yang00b, author = {hsin-chang yang and chung-hong lee}, title = {automatic category generation for text documents by self-organizing maps}, booktitle = {proceedings of ijcnn-00, 11th international joint conference on neural networks}, publisher =
as a consequence, these algorithms cannot take full advantage of the ``weighted'' representations (consisting of vectors of continuous attributes) that are customary in information retrieval tasks, and that provide a much more significant rendition of the document's content than binary representations.
a theoretical analysis and experiments show that the new method can effectively estimate the performance of svm text classifiers in an efficient way.}, } @inproceedings{joachims01b, author = {thorsten joachims and nello cristianini and john shawe-taylor}, title = {composite kernels for hypertext categorisation}, booktitle = {proceedings of icml-01, 18th international conference on machine learning}, editor = {carla brodley and andrea danyluk}, address = {williams college, us}, year = {2001}, pages = {250--257}, publisher = {morgan kaufmann publishers, san francisco, us}, url = {http://www.cs.cornell.edu/people/tj/publications/joachims_etal_01a.pdf}, abstract = {kernels are problem-specific functions that act as an interface between the learning system and the data.
after identifying the weakness and strength of each technique, we propose a new technique known as the generalized instance set (gis) algorithm by unifying the strengths of lnn and linear classifiers and adapting to characteristics of text categorization problems.
little words can make a big difference for text classification},
from these, we extract vocabulary, words that appear with high frequency within a given category, characterizing each subject area.
the results suggest that the spatial and the temporal similarity measures need to be improved.
we find that both algorithms achieve reasonable performance and allow controlled tradeoffs between false positives and false negatives.
in this paper we present an approach to discovering additional regularities in the test set, and show that in relational domains such test set regularities can be used to improve classification accuracy beyond that achieved using the training set alone.
our working hypothesis is that it is often easier to classify a hypertext page using information provided on pages that point to it instead of using information that is provided on the page itself.
this is a problem in which there are large amounts of data available, but the rules for classification are not explicitly available.
the indexing process determines whether an index term is assigned to a certain document, based on the relationship constructed by the learning process, and the text found in the document.
machine learning techniques are used on data collected from yahoo, a large text hierarchy of web documents.
our findings suggest that the best results occur when using the very brief descriptions of the {{\sc yahoo!}}\ categorized entries; these brief descriptions are provided either by the entries' submitters or by the {{\sc yahoo!}}\ human indexers and accompany most {{\sc yahoo!}}\-indexed entries.}, } @inproceedings{lai01, author = {kwok-yin lai and wai lam}, title = {meta-learning models for automatic textual document categorization}, booktitle = {proceedings of pakdd-01, 5th pacific-asia conferenece on knowledge discovery and data mining}, editor = {david cheung and qing li and graham williams}, year = {2001}, publisher = {springer verlag, heidelberg, de}, address = {hong kong, cn}, note = {
extensive experiments have been conducted on a real-world document collection and satisfactory performance is obtained.}, } @article{lai02, author = {yu-sheng lai and chung-hsien wu}, title = {meaningful term extraction and discriminative term selection in text categorization via unknown-word methodology}, journal = {acm transactions on asian language information processing}, year = {2002}, number = {1}, volume = {1}, pages = {34--64}, url = {http://doi.acm.org/10.1145/595576.595579}, abstract = {
the changes may occur both on the transmission side (the nature of the streams can change) and on the reception side (the interests of a user can change).
the proposed architecture relies on hidden markov models whose emissions are bag-of-words according to a multinomial word event model, as in the generative portion of the naive bayes classifier.
first, unlabeled data can hurt performance in domains where the generative modeling assumptions are too strongly violated.
k-nn is one of the most popular document categorization methods because it shows relatively good performance in spite of its simplicity.
from these web subgraphs, web units are constructed and classified into semantic concepts (or categories) in an iterative manner.
prior to text categorization, a feature generator analyzes the documents and maps them onto appropriate ontology concepts, which in turn induce a set of generated features that augment the standard bag of words.
in this paper we propose instead that learning from training data should also affect phase (ii), i.e.\ that information on the membership of training documents to categories be used to determine term weights.
the main goal of this research is to build neural networks and to train them in assigning mesh phrases based on term frequency of single words from title and abstract.
we also investigate codes with optimized kl distance between the text categories which are merged in the code-words.
the relevance values are interpreted as subjective probabilities and hence are mapped into the real interval [0; 1].
this has resulted in high accuracy, shorter customization time, and good prospects for the application of the statistical methods to problems in lexical acquisition.}, } @article{jacobs93, author = {paul s. jacobs}, title = {using statistical methods to improve knowledge-based news categorization}, journal = {ieee expert}, year = {1993}, number = {2}, volume = {8}, pages = {13--23}, url = {}, abstract = {}, } @inproceedings{jo99, author = {taeho c. jo}, title = {
our experiments show that our system has low overhead and achieves high classification ac-curacy across a variety of databases.}, } @inproceedings{ittner95, author =
we show how to learn such models efficiently from data.
to convert the documents into vector form, we experiment with different numbers of features, which we select, based on an information gain criterion.
two example categorization tasks achieve recognition scores of approximately 80\% and are very robust against recognition or typing errors.}, } @inproceedings{bekkerman01, author =
once the system has learned this information, a gaussian function is shaped for each term of a category, in order to assign the term a weight that estimates the level of its importance for that particular category.
in this paper, we apply lsi to the routing task, which operates under the assumption that a sample of relevant and non-relevant documents is available to use in constructing the query.
we applied the technique to several standard text collections and found that they contained a significant number of duplicate and plagiarised documents.
then an optimal matching between the web object and the domain knowledge is performed, in order to pick out the structure attributes of the web object from the knowledge.
the rocchio classifier, its probabilistic variant, and a naive bayes classifier are compared on six text categorization tasks.
text categorization experiments supported a number of predictions of the concept learning model about properties of phrasal representations, including dimensionality properties not previously measured for text representations.
the indexing process determines whether an index term is assigned to a certain document, based on the relationship constructed by the learning process, and the text found in the document.
we also observe that the deviation formula and discrimination formula using document frequency ratios also work as expected.
extensive tests of the model suggest its application as a viable and robust tool for large scale text classification and filtering, as well as a basic module for more complex scenarios.}, } @article{bayer98, author = {thomas bayer and ulrich kressel and heike mogg-schneider and ingrid renz}, title = {categorizing paper documents.
we provide an approach for automatically building the implicit links between web pages using web query logs, together with a thorough comparison between the uses of implicit and explicit links in web page classification.
svm is highly efficient in learning from well organized samples of moderate size, although on relatively large and noisy data the efficiency of svm and aram are comparable.}, } @article{heaps73, author = {h.s. heaps}, title =
an appraisal group is represented as a set of attribute values in several task-independent semantic taxonomies, based on appraisal theory.
we learn a hierarchical classifier that is guided by a layered semantic hierarchy of answer types, and eventually classifies questions into finegrained classes.
one important goal in text mining is automatic classification of electronic documents.
meanwhile, the web object representation is constructed by hyperlink analysis, and further pruned to remove the noises.
support vector machines provide the best accuracy on test data.}, } @article{skarmeta00, author = {
in this paper, we measure the importance of sentences using text summarization techniques.
we present empirical studies (controlled experiments on boolean decision trees and a large-scale text categorization problem) which show that the model selection algorithm leads to error rates which are often as low as those obtained by 10-fold cross validation (sometimes even lower).
in the text domain there are likely to exist many gaps in the feature space because a document is usually mapped to a sparse and high dimensional feature space.
our approach has the advantage of a very fast training phase, and the rules of the classifier generated are easy to understand and manually tuneable.
the system also worked reasonably well for classifying articles from a number of different computer-oriented newsgroups according to subject, achieving as high as an 80\% correct classification rate.
this is achieved by means of a data mining approach, called one clause at a time (ocat), which is based on mathematical logic.
our classifier based on these improvements performes significantly better on pre-classified samples from the web and the us patent database than the usual classifiers.}, } @inproceedings{yu03a, author = {hwanjo yu and chengxiang zhai and jiawei han}, title = {
for the application of the air/phys system a large-scale dictionary containing more than 600000 word-descriptor relations resp.
this is particularly useful when labeling text is a labor-intensive job and when there is a large amount of information available about a particular problem on the world wide web.
furthermore, we focus on the fact that document collections lend themselves naturally to a hierarchical structure defined by the subject matter of the documents.
this analysis determined, for example, that ig and chi-squared have correlated failures for precision, and that ig paired with bns is a better choice.}, } @article{forman03, author = {
moreover, the performance of these greedy methods may be deteriorated when the reserved data dimension is extremely low.
the hypernym relation in wordnet supplements the neural model in classification.
in our method not only words but also semantic categories given by the thesaurus are used as features in a classifier.
we identify document genre is an important factor in retrieving useful documents and focus on the novel document genre dimension of subjectivity.
this approach allows us to tune the combination system on available but less-representative validation data and obtain smaller performance degradation of this system on the evaluation data than using a single-method classifier alone.
the results show that the probabilistic algorithms are preferable to the heuristic rocchio classifier not only because they are more well-founded, but also because they achieve better performance.}, } @inproceedings{joachims97b, author = {
very accurate text classifiers can be learned automatically from training examples.
we apply the support vector machine (svm) to this problem, as it is able to cope with half a million of inputs it requires no feature selection and can process the frequency vector of all words of a text.
our approach to wsd is also based on the integration of two linguistic resources: a training collection (semcor and reuters-21578) and a lexical database (wordnet 1.6).}, } @inproceedings{vert01, author = {jean-philippe vert}, title = {
finally, the interactive nature of the system results in a more correct and precise description of each document than a fully automatic system would.}, } @article{kar78, author = {gautam kar and lee j. white}, title = {a distance measure for automated document classification by sequential analysis}, journal = {information processing and management}, pages = {57--69}, year = {1978}, number = {2}, volume = {14}, url = {},
published in the ``lecture notes in computer science'' series, number 2276}, year = {2002}, address = {mexico city, mx}, pages = {405--414}, url = {http://link.springer.de/link/service/series/0558/papers/2276/22760405.pdf}, abstract = {traditional chinese documents classifiers are based on keywords in the documents, which need dictionaries support and efficient segmentation procedures.
this result suggests that useful task-tracking tools could be constructed based on automatic classification into this taxonomy."
these levels control the ability of the categories to attract documents during the categorization process.
to discover the best fusion framework, we apply genetic programming (gp) techniques.
based on its performance on the classification of 800,000 example queries recorded from msn search, the system received the runner-up award for query categorization performance of the kdd cup 2005." } @article{li:2005:kddcup2005report, author =
the corresponding classifier parameters are learned by optimizing an overall objective function of interest.
the engine exploits a structured document representation and can activate appropriate methods to characterise and automatically index heterogeneous documents with variable layout.
in order to obtain class labels for training samples, we conducted a study where subjects ranked document pages with respect to their resemblance to representative page images.
this task is challenging since incoming messages may contain constructions which have not been anticipated.
however, in the web space, hyperlinks are usually sparse, noisy and thus in many situations can only provide limited help in classification.
the optimized rocchio algorithm achieves a performance comparable with that of the hierarchical neural networks.}, } @inproceedings{ruiz97, author = {miguel e. ruiz and padmini srinivasan}, title = {automatic text categorization using neural networks}, booktitle = {proceedings of the 8th asis/sigcr workshop on classification research}, editor = {efthimis efthimiadis}, publisher =
this system, named stretch (storage and retrieval by content of imaged documents), is based on an archiving and retrieval engine, which overcomes the bottleneck of document profiling bypassing some limitations of existing pre-defined indexing schemes.
unlike other machine learning techniques, it allows easy incorporation of new documents into an existing trained system.
experimental results show that our system can achieve satisfactory performance, which is comparable with other traditional classifiers.}, } @inproceedings{zhou03, author = {shuigeng zhou and tok wang ling and jihong guan and jiangtao hu and aoying zhou}, title = {fast text classification: a training-corpus pruning based approach}, booktitle = {proceedings of dasfaa-03, 8th ieee international conference on database advanced systems for advanced application}, editor = {}, publisher = {ieee computer society press, los alamitos, us}, year = {2003}, address = {kyoto, jp}, pages = {127--136}, url = {}, abstract = {}, } @inproceedings{zu03, author = {guowei zu and wataru ohyama and tetsushi wakabayashi and fumitaka kimura}, title = {accuracy improvement of automatic text classification based on feature transformation}, booktitle = {proceedings of doceng-03, acm symposium on document engineering}, publisher = {acm press, new york, us},
although the test corpus contains documents written in chinese, the proposed approach can be applied to documents written in any language and such documents can be transformed into a list of separated terms.} } @inproceedings{esuli:2005:dso, author =
we obtained some encouraging results on two-category situations, and the results on the general problem seem reasonably impressive---in one case outstanding.
{acm press, new york, us}, address = {toronto, ca}, year = {2003}, pages = {182--189}, url = {http://doi.acm.org/10.1145/860435.860470}, abstract = {term-based representations of documents have found wide-spread use in information retrieval.
{seattle, us}, pages = {130--136}, url = {http://www.cs.utah.edu/~riloff/psfiles/sigir95.ps}, abstract = {most information retrieval systems use stopword lists and stemming algorithms.
for the comparison, we run a series of experiments using a digital library of computer science papers and a web directory.
newsjunkie employs novelty-analysis algorithms that represent articles as words and named entities.
the proposed approach is more efficient, does not require the specification of any parameters, and similarly to the parameter-based approach, boosts the performance of baseline svms by at least 20\% for standard information retrieval measures.}, } @inproceedings{shanks03, author = {vaughan r. shanks and hugh e. williams}, title = {index construction for linear categorisation}, booktitle = {proceedings of cikm-03, 12th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, editor = {}, year = {2003}, address = {new orleans, us}, pages = {334--341}, url = {http://doi.acm.org/10.1145/956863.956926}, abstract = {categorisation is a useful method for organising documents into subcollections that can be browsed or searched to more accurately and quickly meet information needs.
since the whole system is rule-based, it can be adapted to different subject fields by appropriate modifications of the rule bases.
we show that link information can be useful when the document collection has a sufficiently high link density and links are of sufficiently high quality.
these text messages are scanned and then distributed to one of several expert agents according to a certain task criterium.
our approach views the task as one of information integration using whirl, a tool that combines database functionalities with techniques from the information retrieval literature.}, } @inproceedings{zelikovitz01, author = {
in particular, we present an extensive experimental comparison of our approach with other methods on several well studied lexical disambiguation tasks such as context-sensitive spelling correction, prepositional phrase attachment and part of speech tagging.
we show how both algorithms can be adapted to maximize any general utility matrix that associates cost (or gain) for each pair of machine prediction and correct label.
the system is small, fast and robust.
in this report, we try to prove that a previous filtering of the words used by svm in the classification can improve the overall performance.
in contrast, mi had relatively poor performance due to its bias towards favoring rare terms, and its sensitivity to probability estimation errors.}, } @inproceedings{yang99, author =
the estimates can be used in standard classification models to reduce error rates.
experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 33\%.}, } @inproceedings{oh00, author = {hyo-jung oh and sung hyon myaeng and mann-ho lee}, title = {a practical hypertext categorization method using links and incrementally available class information}, booktitle = {proceedings of sigir-00, 23rd acm international conference on research and development in information retrieval}, editor = {nicholas j. belkin and peter ingwersen and mun-kew leong}, publisher = {acm press, new york, us}, address = {athens, gr}, year = {2000}, pages = {264--271}, url = {http://www.acm.org/pubs/articles/proceedings/ir/345508/p264-oh/p264-oh.pdf}, abstract =
the research described in this paper combines weighted trigram analysis, clustering, and a special two-pool evolutionary algorithm, to create an adaptive information filtering system with such useful properties as domain independence, spelling error insensitivity, adaptability, and optimal use of user feedback while minimizing the amount of user feedback required to function properly.
the document collection was trained by a self-organizing map to form two feature maps.
we describe an automatic system that starts with a small sample of the corpus in which topics have been assigned by hand, and then updates the database with new documents as the corpus grows, assigning topics to these new documents with high speed and accuracy.
it is used here for classifying xml documents.
given these inputs, the system learns to extract information from other pages and hyperlinks on the web.
when the data streams are produced in a changing environment the filtering has to adapt too in order to remain effective.
experimental results are encouraging overall; in particular, document classification results fulfill the requirements of high-volume application.
the determination of categories and their hierarchical structures were most done by human experts.
naively using terms in neighboring documents increased the error to 38\%; our hypertext classifier reduced it to 21\%.
rakesh agrawal and paul e. stolorz and gregory piatetsky-shapiro}, publisher = {aaai press, menlo park, us}, year = {1998}, address = {new york, us}, pages = {169--173}, url = {http://www.research.whizbang.com/~wcohen/postscript/kdd-98.ps}, abstract = {whirl is an extension of relational databases that can perform ``soft joins'' based on the similarity of textual identifiers; these soft joins extend the traditional operation of joining tables based on the equivalence of atomic values.
in order to determine whether information within a sentence has been seen in material read previously, our system integrates information about the context of the sentence with novel words and named entities within the sentence, and uses a specialized learning algorithm to tune the system parameters."
we present evidence that this new algorithm leads to better test set precision and recall on three binary web classification tasks where the test set web pages are taken from different web sites than the training set.}, } @inproceedings{slattery98, author =
after posting a query, the user is offered an opportunity to refine the results by browsing through a category tree derived from the dmoz open directory topic hierarchy.
we benchmark several widely used supervised learning methods on rcv1-v2, illustrating the collection's properties, suggesting new directions for research, and providing baseline results for future studies.
rather than just throwing phrases and keywords together, we shall start with pure hm pairs and gradually add more keywords to the document representation.
{227--247}, url = {http://www.wkap.nl/article.pdf?391243}, abstract = {most of the text categorization algorithms in the literature represent documents as collections of words.
the first concerns the information filtering system profile based on an adaptation of the generalized probabilistic model of information retrieval.
other linguistic resources that are emerging, like lexical databases, can also be used for classification tasks.
{nagoya, jp}, url = {http://www.cs.strath.ac.uk/~fabioc/papers/97-ijcai.pdf}, abstract = {new methods and new systems are needed to filter or to selectively distribute the increasing volume of electronic information being produced nowadays.
experiments on a web directory show that best results are achieved when links from pages outside the directory are considered.
the results showed that the proposed model was able to achieve high categorization effectiveness as measured by precision and recall.
the categorization algorithm used is a supervised learning procedure that uses a linear classifier based on the category levels.
to discover the best fusion framework, we apply genetic programming (gp) techniques.
the system has a cooperative and supportive role: it understands the user's needs and learns from his behavior.
in order to verify our methods, we test a large body of tagged japanese newspaper articles created by rwcp.
but they don't offer any of the benefits of natural language processing, such as the ability to identify relationships and enforce linguistic constraints.
in these experiments we show that, after only one epoch of training, our algorithm performs much better than perceptron-based hierarchical classifiers, and reasonably close to a hierarchical support vector machine.} } @article{diaz04, author = {irene d{\'{\i}}az and jos{\'{e}} ranilla and elena monta{\~{n}}es and javier fern{\'{a}}ndez and el{\'{\i}}as f. combarro}, title = {
thus, unlike some other unsupervised dimensionality-reduction techniques, such as latent semantic indexing, we are able to compress the feature space much more aggressively, while still maintaining high document classification accuracy.
text categorization with many redundant features: using aggressive feature selection to make {svm}s competitive with {c4.5}}, booktitle = {proceedings of icml-04, 21st international conference on machine learning}, editor = {carla e. brodley}, year = {2004}, address = {banff, ca}, pages = {}, publisher =
significant improvements in computational efficiency without losing categorization accuracy were evident in the testing results.}, } @article{yang96, author =
however, in addition to relying on labeled training data, we improve classification accuracy by also using unlabeled data and other forms of available ``background" text in the classification process.
our experiments use reuters 21578 database and consist of binary classifications for categories selected from the 115 topics classes of the reuters collection.
it enables drawing on results from statistics and machine learning in explaining the effectiveness of alternate representations of text, and specifies desirable characteristics of text representations.
we also examine training set size, and alternative document representations.
this task has several applications, including automated indexing of scientific articles according to predefined thesauri of technical terms, filing patents into patent directories, selective dissemination of information to information consumers, automated population of hierarchical catalogues of web resources, spam filtering, identification of document genre, authorship attribution, survey coding, and even automated essay grading.
raising high-degree overlapped character bigrams into trigrams for dimensionality reduction in chinese text categorization}, booktitle = {proceedings of cicling-04, 5th international conference on computational linguistics and intelligent text processing}, year = {2004}, editor = {alexander f. gelbukh}, publisher = {springer verlag, heidelberg, de}, address = {seoul, ko}, note =
we apply the support vector machine (svm) to this problem, as it is able to cope with half a million of inputs it requires no feature selection and can process the frequency vector of all words of a text.
our methodology makes use of a well-known corpus of transcribed and topic-labeled speech (the switchboard corpus), and involves an interesting double use of the boostexter learning algorithm.
by assuming that documents are created by a parametric generative model, expectation-maximization (em) finds local maximum a posteriori models and classifiers from all the data|labeled and unlabeled.
first, we use the system to compute profiles on training set data that represent the various categories, e.g., language samples or newsgroup content samples.
these tests also have investigated finding the best indexing terms that could be used in making these classification decisions.
we show how to do this for binary text classification systems, emphasizing that different goals for the system lead to different optimal behaviors.
a central problem in good text classification for information filtering and retrieval (if/ir) is the high dimensionality of the data.
this paper presents work that uses latent semantic indexing (lsi) for text classification.
the idea is to identify sub-topics of the original classes which can help improve the categorization process.
however, support vector machines are so far considered special in that they have been demonstrated to achieve the state of the art performance.
{supervised learning techniques for text classification often require a large number of labeled examples to learn accurately.
for precision alone, however, information gain (ig) was superior.
in order to classify text documents, we must extract good features from them.
through our experiments on the reuters-21578 news database, we showed that aram performed reasonably well in mining categorization knowledge from sparse and high dimensional document feature space.
experimental evaluation on real-world data shows that the proposed approach gives good results.
mh boosting algorithm is applied to the word sense disambiguation (wsd) problem.
however, even this algorithm is aided by an initial prefiltering of features, confirming the results found by almuallim and dietterich on artificial data sets.
the system has a cooperative and supportive role: it understands the user's needs and learns from his behavior.
the hierarchical structure is initially used to train different second-level classifiers.
this restructuring makes it possible for svms to focus on the latent semantic space without losing information given by the original feature space.
the results indicate that in each case, the refined classifiers achieve significant performance improvement over the base classifiers used.
rather than just throwing phrases and keywords together, we shall start with pure hm pairs and gradually add more keywords to the document representation.
the alternative to supervised learning is usually viewed to be building classifiers by hand, using a domain expert's understanding of which features of the text are related to the class of interest.
as a classifier, we adopted a variant of k-nearest neighbor (knn) with supervised term weighting schemes to improve the performance, making our method among the top-performing systems in the trec official evaluation.
we classify movie reviews using features based upon these taxonomies combined with standard ``bag-of-words'' features, and report state-of-the-art accuracy of 90.2%.
our results show that the identification of hypertext regularities in the data and the selection of appropriate representations for hypertext in particular domains are crucial, but seldom obvious, in real-world problems.
author detection with svms on full word forms was remarkably robust even if the author wrote about different topics.}, } @inproceedings{dinunzio03, author =
it is demonstrated that the probabilistic corpus model which emerges from the automatic or unsupervised hierarchical organisation of a document collection can be further exploited to create a kernel which boosts the performance of state-of-the-art support vector machine document classifiers.
specifically, we view the expansion of such lexicons as a process of learning previously unknown associations between terms and \emph{domains}.
we study the problem of classifying data in a given taxonomy when classifications associated with multiple and/or partial paths are allowed.
the representations are evaluated using the ripper learning algorithm on the reuters-21578 and digitrad test corpora.
after defining our notion of ``text mining'', we focus on the differences between text and data mining and describe in some more detail the unique technologies that are key to successful text mining.}, } @article{doyle65, author = {lauren b. doyle}, title = {is automatic classification a reasonable application of statistical analysis of text?}, journal = {journal of the acm}, volume = {12}, number = {4}, year =
{18}, number = {11/13}, url = {}, abstract = {this paper presents a modular software system, which classifies a large variety of office documents according to layout form and textual content.
finally, we analyze the experimental performance of these models over the outputs of two text classifiers.
we also present a new interactive clustering algorithm, c-evolve, for topic discovery.
our substantial experimental results show that with several dimension reduction methods that are designed particularly for clustered data, higher efficiency for both training and testing can be achieved without sacrificing prediction accuracy of text classification even when the dimension of the input space is significantly reduced.} } @article{yang:2005:act, author = {hsin-chang yang and chung-hong lee}, title = {automatic category theme identification and hierarchy generation for chinese text categorization}, journal = {journal of intelligent information systems}, year = {2005}, volume = {25}, number = {1}, pages = {47--67}, abstract = {recently research on text mining has attracted lots of attention from both industrial and academic fields.
such methods can provide automatic indexing and keyword assignment capabilities that are at least as accurate as human indexers in many applications.
the paper describes how despite this fact the inner product can be efficiently evaluated by a dynamic programming technique.
it then uses these words to extract a set of documents for each class from a set of unlabeled documents to form the initial training set.
empirical comparison with several other existing feature selection methods shows that the backward elimination variant of csa leads to the most accurate classification results on an array of datasets."
{2001}, url = {http://www.ai.mit.edu/projects/jmlr/papers/volume2/tong01a/tong01a.pdf}, abstract = {support vector machines have met with significant success in numerous real-world learning tasks.
evaluating the performance of a two-level implementation on the reuters-22173 testbed of newswire articles shows the most significant improvement for rare classes.}, } @article{weiss99, author = {sholom m. weiss and chidanand apt\'{e} and fred j. damerau and david e. johnson and frank j. oles and thilo goetz and thomas hampp}, title = {maximizing text-mining performance}, journal = {ieee intelligent systems}, year = {1999}, number = {4}, volume = {14}, pages = {63--69}, url = {http://www.research.ibm.com/dar/papers/pdf/ieee99_mtmp.pdf}, abstract = {
cross validation over 4 different corpora in two languages allowed us to gather an overwhelming evidence that complex nominals, proper nouns and word senses are not adequate to improve tc accuracy.}, } @article{mostafa00, author = {javed mostafa and wai lam}, title = {automatic classification using supervised learning in a medical document filtering application}, journal = {information processing and management}, year = {2000}, volume = {36}, number = {3}, pages = {415--444}, url = {}, abstract = {document classifiers can play an intermediate role in multilevel filtering systems.
the paper shows that the accuracy of a naive bayes text classifier can be significantly improved by taking advantage of a hierarchy of classes.
the method harnesses reliability indicators---variables that provide a valuable signal about the performance of classifiers in different situations.
we further show that feature clustering is an effective technique for building smaller class models in hierarchical classification.
the results indicate that the use of hierarchical structures improves performance significantly.}, } @article{sable00, author = {carl l. sable and vasileios hatzivassiloglou}, title = {text-based approaches for non-topical image categorization}, journal = {international journal of digital libraries}, year = {2000}, number = {3}, volume = {3}, pages = {261--275}, url = {http://www.cs.columbia.edu/~sable/research/ijodl00.pdf}, abstract = {
published in the ``lecture notes in computer science'' series, number 2431}, url = {http://springerlink.metapress.com/openurl.asp?genre=article&issn=0302-9743&volume=2431&spage=150}, abstract = {good feature selection is essential for text classification to make it tractable for machine learning, and to improve classification performance.
however, we have found that recognizing singular and plural nouns, verb forms, negation, and prepositions can produce dramatically different text classification results.
specifically, a query is preprocessed and represented with patterns that include both query words and required answer types.
in our learning experiments, for each of the subproblems, naive bayesian classifier was used on text data.
to the extraction of opinions from text is the of the orientation of subjective terms contained i.e. the determination of whether a term that opinionated content has a positive or a negative connotation.
we also explore the use of different kinds of codes, namely error-correcting codes, random codes, and domain and data-specific codes and give experimental results for each of them.
we conclude that even the most careful term selection cannot overcome the differences in document frequency between phrases and words, and propose the use of term clustering to make phrases more cooperative.}, } @article{krier02, author = {marc krier and francesco zacc{\`a}}, title = {automatic categorization applications at the european patent office}, journal = {world patent information}, year = {2002}, volume = {24}, number = {}, pages = {187--196}, url = {}, abstract = {}, } @inproceedings{krishnapuram03, author = {raghu krishnapuram and krishna chitrapura and sachindra joshi}, title = {classification of text documents based on minimum system entropy},
our results do not show a dominant algorithm nor method for making algorithms cost-sensitive, but are the best reported on the test collection used, and approach real-world hand-crafted classifiers accuracy.}, } @inproceedings{goodman90, author = {marc goodman}, title = {{\sc prism}: a case-based telex classifier}, booktitle = {proceedings of iaai-90, 2nd conference on innovative applications of artificial intelligence}, publisher =
actually, this weighting method does not leverage the information implicitly contained in the categorization task to represent documents.
in text categorization (tc) based on the vector space model, feature weighting is vital for the categorization effectiveness.
in this paper, we compare learning techniques based on statistical classification to traditional methods of relevance feedback for the document routing problem.
the contents of many valuable web-accessible databases are only accessible through search interfaces and are hence in-visible to traditional web ``crawlers''.
these maps were then analyzed to obtain the category themes and their structure.
we further show that feature clustering is an effective technique for building smaller class models in hierarchical classification.
we show that a careful hybrid integration of techniques from neural network architectures, learning and information retrieval can reach consistent recall and precision rates of more than 92\% on an 82,000 word corpus; this is demonstrated for 10,000 unknown news titles from the reuters newswire.
in our experiments we show that, whenever an existing pn dictionary allows the identification of 50\% of the proper nouns within a corpus, our technique allows, without additional manual effort, the successful recognition of about 90\% of the remaining 50\%.}, } @inproceedings{peters02, author = {c. peters and cornelis h. koster}, title = {uncertainty-based noise reduction and term selection in text categorization}, booktitle = {proceedings of ecir-02, 24th european colloquium on information retrieval research}, editor = {fabio crestani and mark girolami and van rijsbergen, cornelis j.}, year = {2002}, address = {glasgow, uk}, publisher = {springer verlag, heidelberg, de}, note = {
in our tests with three categorization methods on text collections from different domains/applications, significant numbers of words were removed without sacrificing categorization effectiveness.
in particular, a topic of any breadth will typically contain several thousand or million relevant web pages.
our results show that svm, knn and llsf significantly outperform nnet and nb when the number of positive training instances per category are small (less than ten), and that all the methods perform comparably when the categories are sufficiently common (over 300 instances).}, } @article{yang99a, author = {yiming yang}, title = {an evaluation of statistical approaches to text categorization}, journal = {information retrieval}, year = {1999}, pages = {69--90}, volume = {1}, number = {1/2}, url = {http://www.cs.cmu.edu/~yiming/papers.yy/irj99.ps}, abstract = {this paper focuses on a comparative evaluation of a wide-range of text categorization methods, including previously published results on the reuters corpus and new results of additional experiments.
in this paper an original classification model sensitive to document syntactic information and characterized by a novel inference method is described.
in our approach, training data are represented as the projections of training documents on each feature.
we also show that, surprisingly, the addition of deep linguistic analysis features to a set of surface level word n-gram features contributes consistently to classification accuracy in this domain."
text classification by a neural network}, booktitle = {proceedings of the 23rd annual summer computer simulation conference}, editor = {}, publisher = {}, address = {baltimore, us}, pages = {313--318}, year = {1991}, url = {}, abstract = {when banks process their free-form telex traffic, the first task is the classification of the telexes.
in the results of an experiment with news article classification, precision is about 98\%.}, } @incollection{jo99b, author = {taeho c. jo}, title = {news articles classification based on representative keywords of categories}, booktitle = {computational intelligence for modelling, control and automation}, editor = {
this level of complexity makes an ``off-the-shelf'' database-management and information-retrieval solution impossible.
"this paper shows how citation-based information and structural content (e.g., title, abstract) can be combined to improve classification of text documents into predefined categories.
we formulate the problem of automated survey coding as a \emph{text categorization} problem, i.e.\ as the problem of learning, by means of supervised machine learning techniques, a model of the association between answers and codes from a training set of pre-coded answers, and applying the resulting model to the classification of new answers.
our approach includes some original ideas for handling large number of features, categories and documents.
at each node, this classifier can ignore the large number of ``noise'' words in a document.
based on its performance on the classification of 800,000 example queries recorded from msn search, the system received the runner-up award for query categorization performance of the kdd cup 2005." } @article{li:2005:kddcup2005report, author =
a series of experiments indicates that the use of senses does not result in any significant categorization improvement.}, } @inproceedings{kessler97, author = {brett kessler and geoff nunberg and hinrich sch{\"{u}}tze}, title = {automatic detection of text genre}, booktitle = {proceedings of acl-97, 35th annual meeting of the association for computational linguistics}, publisher = {morgan kaufmann publishers, san francisco, us}, editor =
we report the results of systematic experimentation of this method performed on the standard {\sf reuters-21578} benchmark.
our contribution is to propose robust statistical models and a relaxation labeling technique for better classification by exploiting link information in a small neighborhood around documents.
we present simpl, a nearly linear-time classification algorithm which mimics the strengths of svms while avoiding the training bottleneck.
firstly we introduce a novel kernel, whose gram matrix is the well known co-citation matrix from bibliometrics, and demonstrate on real data that it has a good performance.
"retrospective news event detection (red) is defined as the discovery of previously unidentified events in historical news corpus.
cross validation over 4 different corpora in two languages allowed us to gather an overwhelming evidence that complex nominals, proper nouns and word senses are not adequate to improve tc accuracy.}, } @article{mostafa00, author = {javed mostafa and wai lam}, title = {automatic classification using supervised learning in a medical document filtering application}, journal = {information processing and management}, year = {2000},
we show that, in expectation, the excess cumulative h-loss grows at most logarithmically in the length of the data sequence.
our approach views the task as one of information integration using whirl, a tool that combines database functionalities with techniques from the information retrieval literature.}, } @inproceedings{zelikovitz01, author = {sarah zelikovitz and haym hirsh}, title = {using lsi for text classification in the presence of background text}, booktitle = {proceedings of cikm-01, 10th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, editor = {henrique paques and ling liu and david grossman}, year = {2001}, address = {atlanta, us}, pages = {113--118}, url = {ftp://ftp.cs.rutgers.edu/pub/zelikovi/lsi01.ps}, abstract = {
in the domain of terrorism, autoslog created a dictionary using a training corpus and five person-hours of effort that achieved 98\% of the performance of a hand-crafted dictionary that took approximately 1500 person-hours to build.
we report the results of applying a variety of machine learning algorithms to the automated categorization of english-language patent documents.
put another way, genre classification is orthogonal to a classification based on the documents' contents.
in this paper, we show how an operator-based view of rule induction enables the easy integration of a thesaurus as background knowledge.
as control experiment best human categorization performance was established at 79.4\% for this task.
this reduced feature space is then used to train a classifier over a larger training set because more documents now fit into the same amount of memory.
drawing on interviews with reuters personnel and access to reuters documentation, we describe the coding policy and quality control procedures used in producing the rcv1 data, the intended semantics of the hierarchical category taxonomies, and the corrections necessary to remove errorful data.
nevertheless, it is shown that automated text categorization techniques can exploit combinations of simple lexical and syntactic features to infer the gender of the author of an unseen formal written document with approximately 80 per cent accuracy.
we present experiments on the classification of multilingual pornographic html pages using text and image data.
in this paper we demonstrate this pitfall hurts performance even for a relatively uniform text classification task.
the training approaches we test are the rocchio (relevance feedback) and the widrow-hoff (machine learning) algorithms and wordnet as the lexical database.
a belief networks-based generative model for structured documents.
pcut copes better with rare categories and exhibits a smoother trade-off in recall versus precision, but is not suitable for online decision making.
we devise an algorithm that interleaves labeling features and documents which significantly accelerates standard active learning in our simulation experiments.
by contrast, content-based methods use information about an item itself to make suggestions.
to the extraction of opinions from text is the of the orientation of subjective terms contained i.e. the determination of whether a term that opinionated content has a positive or a negative connotation.
we have conducted an extensive experimental evaluation of our technique over collections of real documents, including over one hundred web-accessible databases.
after training, the incoming news articles are classified based on their similarity to the existing newsgroup categories.
this means that the category of potentially relevant documents for most profiles would contain at least 80\% of all documents later determined to be relevant to the profile.
we verify experimentally that the integration of wordnet helps ssfcm improve its performance, effectively addresses the classification of documents into categories with few training documents and does not interfere with the use of training data.}, } @article{benkhalifa01a, author = {mohammed benkhalifa and abdelhak mouradi and houssaine bouyakhf}, title = {
it is concluded that, while there is no significant difference in the predictive efficiency between the bayesian and the factor score methods, automatic document classification is enhanced by the use of a factor-analytically-derived classification schedule.
much information is nowadays stored as multilingual textual data; therefore advanced classification systems are currently considered as strategic components for effective knowledge management.
moreover, the estimators developed here address the special performance measures needed for evaluating text classifiers.
users can search for pants or classify patent text.
for example, text can be classified with a very high degree of accuracy by authorship, language, dialect and genre.
the open and modularized system architecture makes our classifier be extendible.
the second problem is that even with a representative model, the improvements given by unlabeled data do not sufficiently compensate for a paucity of labeled data.
with a smaller number of features, ssfcm's performance is also superior to that of ssahc's.
using insights gained from examining the way humans make fast decisions when classifying text documents, two new text classification algorithms are developed based on sequential sampling processes.
one way to achieve this aim is by applying machine learning techniques to training data containing the various senses of the ambiguous words.
to advance research on this challenging but important problem, we promote a natural, experimental framework-the daily classification task-which can be applied to large time-based datasets, such as reuters rcv1.
we find that even a relatively high level of errors in the ocred documents does not substantially affect stylistic classification accuracy."
we show how these networks can support text routing of noisy newswire titles according to different given categories.
with nearly perfect reliability the svm was able to reject other authors and detected the target author in 60-80\% of the cases.
this paper presents a modular software system, which classifies a large variety of office documents according to layout form and textual content.
"1265--1270", abstract = "evaluating text fragments for positive and negative subjective expressions and their strength can be important in applications such as single- or multi- document summarization, document ranking, data mining, etc.
this word-cluster representation is computed using the recently introduced information bottleneck method, which generates a compact and efficient representation of documents.
we also try combining classifiers based on different representations using a majority voting technique, and this improves performance on both test collections.
{proceedings of sdair-96, 5th annual symposium on document analysis and information retrieval}, publisher = {}, editor = {}, year = {1996}, address = {las vegas, us}, pages = {191--207}, url = {}, abstract = {knowledge discovery in databases (kdd) focuses on the computerized exploration of large amounts of data and on the discovery of interesting patterns within them.
they use credible knowledge resources, including a us government organizational hierarchy, a thematic hierarchy from the open directory project (odp) web directory, and personal browse histories, to add valuable metadata to search results.
we also present preliminary results showing how this model could classify documents with dtds not represented in the training set.}, } @inproceedings{denoyer03a, author =
most of them are irrelevant and others introduce noise which could mislead the classifiers.
our results indicate that features based on latent semantic indexing are more effective for techniques such as linear discriminant analysis and logistic regression, which have no way to protect against overfitting.
our results indicate that the use of n-grams is an attractive technique which can even compare to techniques relying on a morphological analysis.
most of these methods are usually awkward and sometimes intractable in high-dimensional feature spaces.
we describe a method for classifying news stories using memory based reasoning (mbr) a k-nearest neighbor method), that does not require manual topic definitions.
this approach is motivated by the observation that hyperbolic spaces possess a geometry where the size of a neighborhood around a point increases exponentially and therefore provides more freedom to map a complex information space such as language into spatial relations.
an example-based mapping method for text categorization and retrieval}, journal = {acm transactions on information systems}, year = {1994}, number = {3}, volume = {12}, pages = {252--277}, url = {http://www.acm.org/pubs/articles/journals/tois/1994-12-3/p252-yang/p252-yang.pdf}, abstract = {a unified model for text categorization and text retrieval is introduced.
the analysis demonstrates that one of these models is theoretically attractive (introducing few new parameters while increasing flexibility), computationally efficient, and empirically preferable.}, } @article{bennett05, author = {paul n. bennett and susan t. dumais and eric horvitz}, title = {
we show that such unlabeled background knowledge can greatly decrease error rates, particularly if the number of examples or the size of the strings in the training set is small.
experiments with simulated concept drift scenarios based on real-world text data compare the new method with other window management approaches.
however, the three machine learning methods we employed (naive bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization.
the best results were obtained using the m-estimate as search heuristics combined with the likelihood-ratio-statics for pruning.
the results suggest that information extraction techniques can support high-precision text classification and, in general, using more extracted information improves performance.
ssahc is (i) a clustering algorithm that (ii) uses a finite design set of labeled data to (iii) help agglomerative hierarchical clustering (ahc) algorithms partition a finite set of unlabeled data and then (iv) terminates without the capability to label other objects.
stretch offers ease of use and application programming and the ability to dynamically adapt to new types of documents.
survey coding is a difficult task, since the code that should be attributed to a respondent based on the answer she has given is a matter of subjective judgment, and thus requires expertise.
the technique is completely language-independent, highly garble-resistant, and computationally simple.
moreover, the derived classifier reachs the performance (about 85\%) of the best known models (i.e. support vector machines (svm) and k-nearest neighbour (knn)) characterized by an higher computational complexity for training and processing.}, } @inproceedings{basili01a, author = {roberto basili and alessandro moschitti and maria t. pazienza}, title = {
the first level of the architecture predicts the probabilities of the meta-topic groups.
with nearly perfect reliability the svm was able to reject other authors and detected the target author in 60-80\% of the cases.
specifically, we show that by adjusting the category levels in a principled way, that precision can be significantly improved, from 84\% to 91\%, on the much-studied reuters-21578 corpus organized in a three-level hierarchy of categories.}, } @article{damashek95, author = {marc damashek}, title = {
however the high performance of most existing pn classifiers heavily depends upon the availability of large dictionaries of domain-specific proper nouns, and a certain amount of manual work for rule writing or manual tagging.
we study the use of support vector machines (svms) in classifying email as spam or nonspam by comparing it to three other classification algorithms: ripper, rocchio, and boosting decision trees.
experimental results have shown that the statistical learning approach and the learning feedback technique are practical means to automatic indexing of controlled index terms.}, } @inproceedings{lewis00, author = {lewis, david d.}, title = {machine learning for text categorization: background and characteristics}, booktitle = {proceedings of the 21st annual national online meeting}, editor = {williams, martha e.}, publisher =
the system includes a unique phrase help facility, which helps users find and add phrases and terms related to those in their query.}, } @inproceedings{lee00, author = {hahn-ming lee and chih-ming chen and cheng-wei hwang}, title = {a neural network document classifier with linguistic feature selection}, booktitle = {proceedings of iea/aie-00, 13th international conference on industrial and engineering applications of artificial intelligence and expert systems}, publisher = {}, editor = {}, year = {2003}, address = {new orleans, us}, pages = {555--560}, url = {}, abstract = {}, } @inproceedings{lee02, author = {yong-bae lee and sung h. myaeng}, title = {
this paper reports a system that hierarchically classifies chinese web documents without dictionary support and segmentation procedure.
an algorithm for sequential sampling during machine learning of statistical classifiers was developed and tested on a newswire text categorization task.
when parameters tuned on an early benchmark tdt corpus were evaluated on a later tdt benchmark corpus with no overlapping events, we observed a 38-65\% reduction in tracking cost (a weighted combination of errors) by the combined system over the individual methods evaluated under the same conditions, strongly suggesting the robustness of this approach as a solution for improving cross-class performance consistency of statistical classifiers when standard cross-validation fails due to the lack of representative validation sets.}, } @inproceedings{yang00b, author = {hsin-chang yang and chung-hong lee}, title = {automatic category generation for text documents by self-organizing maps}, booktitle = {proceedings of ijcnn-00, 11th international joint conference on neural networks}, publisher =
specifically, we view the expansion of such lexicons as a process of learning previously unknown associations between terms and \emph{domains}.
as a result, they are not portable across domains.
"we present a principled methodology for filtering news stories by formal measures of information novelty, and show how the techniques can be used to custom-tailor newsfeeds based on information that a user has already reviewed.
"in this paper, we use a blog corpus to demonstrate that we can often identify the author of an anonymous text even where there are many thousands of candidate authors.
in addition, the way in which learning with redundancy influences categorization performance is also studied.}, } @inproceedings{moulinier97, author = {
comparing efficiency, knn was notably more costly in terms of time and memory than the other two methods.
existing classification schemes which ignore the hierarchical structure and treat the topics as separate classes are often inadequate in text classification where the there is a large number of classes and a huge number of relevant features needed to distinguish between them.
the experimental results on the validation dataset help confirm our conjectures on the performance of the q2c@ust system.
experimental results on three real world data sets from usenet, yahoo, and corporate web pages show improved performance, with a reduction in error up to 29\% over the traditional flat classifier.}, } @inproceedings{mccallum98c, author = {andrew mccallum and k. nigam}, title = {
in this way, it allows users to navigate the results of a query at a more topical level rather than having to examine each document text separately.}, } @article{sakakibara96, author = {yasubumi sakakibara and kazuo misue and takeshi koshiba}, title = {a machine learning approach to knowledge acquisitions from text databases}, year = {1996}, journal = {
then an optimal matching between the web object and the domain knowledge is performed, in order to pick out the structure attributes of the web object from the knowledge.
text categorization is useful for indexing documents for information retrieval, filtering parts for document understanding, and summarizing contents of documents of special interests.
this technique used in the learning process modifies the relationship between an index term and its relevant and irrelevant words to improve the learning performance and, thus, the indexing performance.
we have empirically demonstrated that rule-based methods like ours result in high classification accuracy when the categories to which texts are to be assigned are relatively specific ones and when the texts tend to be short.
our results indicate that features based on latent semantic indexing are more effective for techniques such as linear discriminant analysis and logistic regression, which have no way to protect against overfitting.
for text categorization (tc) are available fewer and less definitive studies on the use of advanced document representations as it is a relatively new research area (compared to document retrieval).
this task arises in the construction of search engines and web knowledge bases.
the experimental results show that the proposed methods of incorporating prior knowledge is effective."
based on this consideration, the authors have built a neural network classification system, which has three subsystems: a user-maintainable feature definition subsystem, a feature extraction subsystem, and a neural network subsystem.
we present a unified view of text categorization systems, focusing on the selection of features.
we find that term selection and our modified lsi representations lead to similar topic spotting performance, and that this performance is equal to or better than other published results on the same corpus.}, } @mastersthesis{wiener95a, author = {erik d. wiener}, title = {
a major knowledge-engineering bottleneck for information extraction systems is the process of constructing an appropriate dictionary of extraction patterns.
with the former, we provide an enhanced document representation that incorporates the structural and heterogeneous nature of web documents.
the high number of features is reduced by feature subset selection and additionally by using `stop-list', pruning low-frequency features and using a short description of each document given in the hierarchy instead of using the document itself.
however, svms had significantly less training time.}
we report the results of systematic experimentation of this method performed on the standard {\sf reuters-21578} benchmark.
the experiments of the categorization of news articles show that the proposed schemes of text categorization outperform the schemes with crisp sets.}, } @incollection{jo99a, author = {taeho c. jo}, title = {news article classification based on categorical points from keywords in backdata}, booktitle = {computational intelligence for modelling, control and automation}, editor = {
we focus our discussion on the ability to discriminate between authors for the case of both aggregated e-mail topics as well as across different email topics.
we also investigate the usability of our automated learning approach by actually developing a system that categorizes texts into a tree of categories.
in this way, it allows users to navigate the results of a query at a more topical level rather than having to examine each document text separately.}, } @article{sakakibara96, author = {yasubumi sakakibara and kazuo misue and takeshi koshiba}, title = {
text classifiers that give probability estimates are more readily applicable in a variety of scenarios.
while this weighting method seems very appropriate for ir, it is not clear that it is the best choice for tc problems.
the traditional method of building a single classifier to do all the classification work would incur a high overhead.
previous researches have investigated the use of $n$-grams (or some variant of them) in the context of specific learning algorithms, and thus have not obtained general answers on their usefulness for tc.
the proxy logs the user's activities and extracts the user's interests without user intervention.
results show that for hierarchical techniques it is better to use hierarchical training sets.}, } @inproceedings{cerny83, author = {barbara a. cerny and anna okseniuk and j. dennis lawrence}, title = {
we report the results of applying a variety of machine learning algorithms to the automated categorization of english-language patent documents.
we therefore propose the category-similarity measures and distance-based measures to consider the degree of misclassification in measuring the classification performance.
experimental results indicate that the mfom classifier gives improved f1 and enhanced robustness over the conventional one.
the baseline method combines the terms of all the articles of each newsgroup in the training set to represent the newsgroups as single vectors.
therefore, the contributions of this research are in learning and generalizing neural architectures for the robust interpretation of potentially noisy unrestricted messages.
we describe a content-based book recommending system that utilizes information extraction and a machine-learning algorithm for text categorization.
computationally, expnet has an o(n log n) time complexity which is much more efficient than the cubic complexity of the llsf method.
we demonstrate the effectiveness of our categorization approach using two real-world document collections from the medline database.
the results show that the method outperforms svm at multi-class categorization, and interestingly, that results correlate strongly with compression-based methods.}, } @inproceedings{kim00, author = {yu-hwan kim and shang-yoon hahn and byoung-tak zhang}, title = {text filtering by boosting naive bayes classifiers}, booktitle = {proceedings of sigir-00, 23rd acm international conference on research and development in information retrieval}, editor = {nicholas j. belkin and peter ingwersen and mun-kew leong}, publisher = {acm press, new york, us}, address = {athens, gr}, year = {2000}, pages = {168--175}, url = {http://www.acm.org/pubs/articles/proceedings/ir/345508/p168-kim/p168-kim.pdf}, abstract = {several machine learning algorithms have recently been used for text categorization and filtering.
experiments were conducted using a large scale document collection from reuters news articles.
this is expensive, requires a degree of sophistication about linguistics and classification, and makes it difficult to use combinations of weak predictors.
when user interests change, in pva, not only the contents, but also the structure of the user profile are modified to adapt to the changes.
we tested our learning method on the task of single-label classification using the reuters-21578 benchmark.
given the large number (> 13,000) of closely related categories, this is a challenging task that is unlikely to succumb to a single algorithmic solution.
in the second phase, to clarify the impact of this performance on filtering, different types of user profiles were created by grouping subsets of classes based on their individual classification accuracy rates.
the indexing strategy first automatically classifies the document, thus avoiding pre-sorting, then locates and reads the information pertaining to the specific document class.
experimental results show that modulating the structure of user profile does increase the accuracy of personalization systems.}, } @article{chen02, author = {
using the results evaluated on the other versions of reuters which exclude the unlabelled documents, the performance of twelve methods are compared directly or indirectly.
the rapid expansion of multimedia digital collections brings to the fore the need for classifying not only text documents but their embedded non-textual parts as well.
this hybrid approach of neural selforganization and symbolic hypernym relationships is successful to achieve good classification rates on 100,000 full-text news articles.
we verify experimentally that the integration of wordnet helps ssahc improve its performance, effectively addresses the classification of documents into categories with few training documents.
these generative models do not capture all the intricacies of text; however on some domains this technique substan- tially improves classification accuracy, especially when labeled data are sparse.
year = {1998}, address = {bethesda, us}, pages = {148--155}, url = {http://robotics.stanford.edu/users/sahami/papers-dir/cikm98.pdf}, abstract = {text categorization - the assignment of natural language texts to one or more predefined categories based on their content - is an important component in many information organization and management tasks.
integrating background knowledge into nearest-neighbor text classification}, pages = {1--5}, url = {}, booktitle = {proceedings of eccbr-02, 6th european conference on case-based reasoning}, editor = {
kld method achieve substantial improvements over the tfidf performing method.}, } @article{blei03, author = {david m. blei and andrew y. ng and michael i. jordan}, title = {latent dirichlet allocation}, journal = {journal of machine learning research}, volume = {3}, pages = {993--1022}, year = {2003}, url = {http://www.ai.mit.edu/projects/jmlr/papers/volume3/blei03a/blei03a.pdf}, abstract =
this suggests that df thresholding, the simplest method with the lowest cost in computation, can be reliably used instead of ig or chi when the computation of these measures are too expensive.
most previous studies found that the majority of these features are relevant for classification, and that the performance of text categorization with support vector machines peaks when no feature selection is performed.
the main goal of this research is to build neural networks and to train them in assigning mesh phrases based on term frequency of single words from title and abstract.
we present an approach using the vector space model to integrate two different kind of resources: a lexical database and training collections, in text content analysis tasks.
in the traditional setting, text categorization is formulated as a concept learning problem where each instance is a single isolated document.
the experimental results on the validation dataset help confirm our conjectures on the performance of the q2c@ust system.
the bin-based method is intended for tasks where there is insufficient training data to estimate a separate weight for each word.
a sequential algorithm for training text classifiers: corrigendum and additional data}, journal = {sigir forum}, year = {1995}, pages = {13--19}, volume = {29}, number = {2}, url = {http://www.research.att.com/~lewis/papers/lewis95g.ps}, abstract = {
published in the ``lecture notes in computer science'' series, number 1910}, url = {http://link.springer.de/link/service/series/0558/papers/1910/19100490.pdf}, abstract = {supervised learning algorithms usually require large amounts of training data to learn reasonably accurate classifiers.
to evaluate classifiers in our multipath framework, we define a new hierarchical loss function, the h-loss, capturing the intuition that whenever a classification mistake is made on a node of the taxonomy, then no loss should be charged for any additional mistake occurring in the subtree of that node.
the results of this experiment were subsequently used to create a new large medical test collection, which was used in experiments with the smart retrieval system to obtain baseline performance data as well as compare smart with the other searchers.}, } @inproceedings{hoashi00, author = {keiichiro hoashi and kazunori matsumoto and naomi inoue and kazuo hashimoto}, title = {document filtering methods using non-relevant information profile}, booktitle = {proceedings of sigir-00, 23rd acm international conference on research and development in information retrieval},
the essential formula is cue validity borrowed from cognitive psychology, and used to select from all possible single word-based features the `best` predictors of a given category.
put another way, genre classification is orthogonal to a classification based on the documents' contents.
an analysis of patterns in sentences was performed with data from the trec 2002 novelty track and experiments on novelty detection were carried out on data from the trec 2003 and 2004 novelty tracks.
% % % % everyone is also welcome to let me know either additional % % references or corrections and additions (e.g. urls, where % % they are not already present) to the existing ones.
this is a novel approach for document classification, where each agent evolves a parse-tree representation of a user's particular information need.
a divisive information-theoretic feature clustering algorithm for text classification}, journal = {journal of machine learning research}, volume = {3}, month = {march}, pages = {1265--1287}, year = {2003}, url = {http://www.jmlr.org/papers/volume3/dhillon03a/dhillon03a.pdf}, abstract = {high dimensionality of text can be a deterrent in applying complex learners such as support vector machines to the task of text classification.
in this paper we show an adaptive incremental learning algorithm that learns interactively to classify text messages (here: emails) into categories without the need for lengthy batch training runs.
in this paper, we present a weight adjusted k-nearest neighbor (waknn) classification that learns feature weights based on a greedy hill climbing technique.
we describe the results of extensive machine learning experiments on large collections of reuters' english and german newswires.
the methods to create, detect, summarize, select, and code visual keywords will be detailed.
the results indicate that the use of hierarchical structures improves performance significantly.}, } @article{sable00, author = {carl l. sable and vasileios hatzivassiloglou}, title = {text-based approaches for non-topical image categorization}, journal =
the outcome of the result was quite impressive: in different experimental setups, we reached a micro-averaged f1-measure of 0.89, with a peak of 0.899.
in general, only references specific to atc are considered % % pertinent to this bibliography; in particular, references that % % *are* considered pertinent are: % % % % * publications that discuss novel atc methods, novel % % experimentation of previously known methods, or resources for % % atc experimentation; % % % % * publications that discuss applications of atc (e.g. % % automated indexing for boolean ir systems, filtering, etc.).
scut is potentially better for fine-tuning but risks overfitting.
in application, the adapted text categorizers are reliable, fast, and completely automatic.
a large collection of automatically generated datasets are made available for other researchers to use.}, } @inproceedings{debole03, author = {franca debole and fabrizio sebastiani}, title = {supervised term weighting for automated text categorization}, year = {2003}, booktitle = {proceedings of sac-03, 18th acm symposium on applied computing}, address = {melbourne, us}, publisher = {acm press, new york, us}, pages = {784--788}, url = {http://www.math.unipd.it/~fabseb60/publications/sac03b.pdf}, note = {
using a commercial medline product based on the vector space model, these physicians searched just as effectively as more experienced searchers using boolean searching.
the use of a controlled vocabulary allows for a more consistent description of corporate documents, and promotes easier access by people across the company.
one way to reduce the amount of labeled data required is to develop algorithms that can learn effectively from a small number of labeled examples augmented with a large number of unlabeled examples.
the module evaluates a large incoming stream of documents to determine which documents are sufficiently similar to a profile at the broad subject level to warrant more refined representation and matching.
in this paper we present an approach to discovering additional regularities in the test set, and show that in relational domains such test set regularities can be used to improve classification accuracy beyond that achieved using the training set alone.
analysis and empirical evidence suggest that the evaluation results on some versions of reuters were significantly affected by the inclusion of a large portion of unlabelled documents, making those results difficult to interpret and leading to considerable confusions in the literature.
machine learning techniques developed for learning on text data are used here on the hierarchical classification structure.
the logic representation of sentences required by the adopted learning algorithm is obtained by detecting structure in raw text trough a parser.
in particular, evaluations on ocr documents are very rare.
to use traditional feature-vector- based learning methods, one could treat the presence or ab-sence of a word as a boolean feature and use these binary-valued features together with the numerical features.
this model gives new representations of both news articles and news events.
we present an efficient algorithm for text classification using hierarchical classifiers based on a concept hierarchy.
the technique of periodical updates improves the routing accuracy ranging from 20\% to 100\% but incurs runtime overhead.
the bases of the cdm are research results about the way that humans learn categories and concepts vis-a-vis contrasting concepts.
ibm's intelligent miner for text provides the necessary tools to unlock the business information that is ''trapped'' in email, insurance claims, news feeds, or other document repositories.
our empirical results show that the proposed approach, text categorization using feature projections (tcfp), outperforms k-nn, rocchio, and naive bayes.
we describe the algorithm and present experimental results on applying it to the document routing problem.
the case base of natural language contexts is acquired automatically during sentence analysis using a training corpus of texts and their correct relevancy classifications.
in spite of these differences, both ripper and sleeping-experts perform extremely well across a wide variety of categorization problems, generally outperforming previously applied learning methods.
thus, the classifier has a small model size and is very fast.
the combination of text classifiers using reliability indicators}, journal = {information retrieval}, number = {1}, volume = {8}, pages = {67--100}, year = {2005}, url = {http://www.kluweronline.com/issn/1386-4564}, abstract = {
in particular, a topic of any breadth will typically contain several thousand or million relevant web pages.
we describe a text categorization task and an experiment using documents from the reuters and ohsumed collections.
the paper describes the novel technique of categorization by context, which instead extracts useful information for classifying a document from the context where a url referring to it appears.
managing the hierarchical organization of data is starting to play a key role in the knowledge management community due to the great amount of human resources needed to create and maintain these organized repositories of information.
unrestricted potentially faulty text messages arrive at a certain delivery point (e.g. email address or world wide web address).
the bases of the cdm are research results about the way that humans learn categories and concepts vis-a-vis contrasting concepts.
one way to reduce the amount of labeled data required is to develop algorithms that can learn effectively from a small number of labeled examples augmented with a large number of unlabeled examples.
we report on computational experience using this procedure.
in the work presented here, the decision tree learning algorithm c4.5 is applied on a corpus of financial news articles.
they may not perform effectively in adaptive text filtering which is a more realistic problem.
the results show that the use of the hierarchical structure improves text categorization performance significantly.}, } @inproceedings{ruiz99a, author = {miguel e. ruiz and padmini srinivasan}, title = {
the analysis gives theoretical insight into the heuristics used in the rocchio algorithm, particularly the word weighting scheme and the similarity metric.
in this paper we present a learning system for information filtering and selective information dissemination.
we discovered that the knowledge about relevance among queries and documents can be used to obtain empirical connections between query terms and the canonical concepts which are used for indexing the content of documents.
this advantage is achieved by executing morphological and semantic analyses of an incoming text.
we present evidence that this new algorithm leads to better test set precision and recall on three binary web classification tasks where the test set web pages are taken from different web sites than the training set.}, } @inproceedings{slattery98, author =
moreover, the frequencies of occurrence of the most common punctuation marks play an important role in terms of accurate text categorization as well as when dealing with training data of limited size.}, } @inproceedings{sun01, author = {aixin sun and ee-peng lim}, title = {hierarchical text classification and evaluation}, booktitle = {proceedings of icdm-01, ieee international conference on data mining}, publisher = {ieee computer society press, los alamitos, us}, editor = {nick cercone and tsau y. lin and xindong wu}, year = {2001}, address = {san jose, ca}, pages = {521--528}, url = {http://www.cais.ntu.edu.sg:8000/~sunaixin/paper/sun_icdm01.pdf}, abstract = {hierarchical classification refers to assigning of one or more suitable categories from a hierarchical category space to a document.
go annotation is a major activity in most model organism database projects and annotates gene functions using a controlled vocabulary.
whirl is also fast-up to 500 times faster than c4.5 on some benchmark problems.
this dissertation addresses the knowledge-engineering bottleneck for a natural language processing task called ``information extraction''.
we also present results with algorithms other than co-training in this framework and show that co-training is uniquely suited to work well within ecoc.}, } @inproceedings{giorgetti03, author = {daniela giorgetti and fabrizio sebastiani}, title = {multiclass text categorization for automated survey coding}, year = {2003}, address = {melbourne, us}, booktitle = {proceedings of sac-03, 18th acm symposium on applied computing}, publisher = {acm press, new york, us}, pages = {798--802}, url = {http://www.math.unipd.it/~fabseb60/publications/sac03a.pdf}, abstract = {\emph{survey coding} is the task of assigning a symbolic code from a predefined set of such codes to the answer given in response to an open-ended question in a questionnaire (aka \emph{survey}).
the experiment results show that with only surface text features the svm outperforms the other four methods for this task.
we develop a framework to incorporate unlabeled data in the error-correcting output coding (ecoc) setup by decomposing multiclass problems into multiple binary problems and then use co-training to learn the individual binary classification problems.
specifically, we apply the information bottleneck method to find word-clusters that preserve the information about document categories and use these clusters as features for classification.
issues of document indexing, classifier construction, and classifier evaluation, will be touched upon.}, } @article{selamat04, author = {ali selamat and sigeru omatu}, title = {web page feature selection and classification using neural networks}, journal = {information sciences}, year = {2004}, number = {1}, volume =
the model is based on the concept of `uncertainty sampling', a technique that allows for relevance feedback both on relevant and nonrelevant documents.
these results reach most of the state-of-the-art techniques of machine learning applied to text categorization, demonstrating that this new weighting scheme does perform well on this particular task.}, } @inproceedings{dinunzio04, author = {giorgio m. {di nunzio}}, title = {a bidimensional view of documents for text categorisation}, booktitle = {proceedings of ecir-04, 26th european conference on information retrieval research}, editor = {sharon mcdonald and john tait}, year = {2004}, address = {sunderland, uk}, publisher = {springer verlag, heidelberg, de}, note = {
"in this paper we present a novel strategy, dragpushing, for improving the performance of text classifiers.
we evaluate the algorithms on the reuters-21578 corpus and the new corpus released by reuters in 2000.
with large number of categories organized as a tree, hierarchical text classification helps users to find information more quickly and accurately.
the classifiers worked very well.
we also found that passages have different degrees of contribution to the main topic(s), depending on their location in the test document.}, } @inproceedings{kindermann01, author = {j{\"{o}}rg kindermann and gerhard paa{{\ss}} and edda leopold}, title = {error correcting codes with optimized kullback-leibler distances for text categorization}, booktitle = {
employing the thesaurus entails structuring categories into hierarchies, since their structure needs to be conformed to that of the thesaurus for capturing relationships between categories.
based on a revision of self-organizing maps, namely taxsom, the proposed model performs an unsupervised classification, exploiting the a-priori knowledge encoded in a taxonomy structure both at the terminological and topological level.
for the first time, we apply the scheme to text categorization with support vector machines (svms) on several large text corpora with more than 100 categories.
in this paper, we present a hierarchical text classifier based on independent component analysis (ica), which is capable of (i) organizing the contents of the corpus in a hierarchical manner and (ii) classifying the texts to be synthesized according to the learned structure.
we suggest that one (or a collection) of names of {{\sc yahoo!}}\ (or any other www indexer's) categories can be used to describe the content of a document.
this can be thought of as automatic feature selection, which is expected to improve generalization performance further.
moreover, the frequencies of occurrence of the most common punctuation marks play an important role in terms of accurate text categorization as well as when dealing with training data of limited size.}, } @inproceedings{sun01, author = {aixin sun and ee-peng lim}, title =
we find that term selection and our modified lsi representations lead to similar topic spotting performance, and that this performance is equal to or better than other published results on the same corpus.}, } @article{wong96, author = {jacqueline w. wong and wing-kay kan and gilbert h. young}, title = {{{\sc action}}: automatic classification for full-text documents}, journal = {sigir forum}, year = {1996}, volume = {30}, number = {1}, pages = {26--41}, url = {}, abstract = {}, } @article{wu04, author = {kuo-jui wu and menc-chang chen and yeali sun}, title = {automatic topics discovery from hyperlinked documents}, journal = {information processing and management}, year = {2004}, volume = {40}, number = {2}, pages = {239--255}, url = {}, abstract = {}, } @inproceedings{wu04a, author = {xiaoyun wu and rohini srihari and zhaohui zheng},
we report the results of our experiments, using various feature selection measures and varying values of $\sigma$, performed on the {\sc reuters-21578} standard tc benchmark.
{127--152}, url = {http://www.wkap.nl/article.pdf?391243}, abstract = {kernel methods like support vector machines have successfully been used for text categorization.
this paper presents work that uses latent semantic indexing (lsi) for text classification.
measuring the similarity of two documents is conducted by comparing a pair of their corresponding sub-vectors at a time.
we also observe that dimensionality reduction techniques eliminate a large number of ocr errors and improve categorization results.}, } @inproceedings{taira01, author =
we use different feature sets and integrate neural network learning into the method.
actually, this weighting method does not leverage the information implicitly contained in the categorization task to represent documents.
we also present two performance optimizations of waknn that improve the computational performance by a few orders of magnitude, but do not compromise on the classification quality.
moreover, bwm-nbs exhibits the strong stability in categorization performance.}, } @inproceedings{xue04, author =
we also show that less aggressive clustering sometimes results in improved classification accuracy over classification without clustering.}, } @inproceedings{bao01, author = {yongguang bao and satoshi aoyama and xiaoyong du and kazutaka yamada and naohiro ishii}, title = {a rough set-based hybrid method to text categorization}, booktitle = {proceedings of wise-01, 2nd international conference on web information systems engineering}, editor = {m. tamer {\"o}zsu and hans-j{\"{o}}rg schek and katsumi tanaka and yanchun zhang and yahiko kambayashi}, publisher = {ieee computer society press, los alamitos, us}, year = {2001}, address = {kyoto, jp}, pages = {254--261}, url = {}, abstract = {
most such methods are character-based, and thus have the potential to automatically capture non-word features of a document, such as punctuation, word-stems, and features spanning more than one word.
domain knowledge is used to specify a prior distribution for the parameters of a logistic regression model, and labeled training data is used to produce a posterior distribution, whose mode we take as the final classifier.
the alternative to supervised learning is usually viewed to be building classifiers by hand, using a domain expert's understanding of which features of the text are related to the class of interest.
the layout of a document contains a significant amount of information that can be used to classify it by type in the absence of domain-specific models.
for best accuracy, f-measure or recall, the findings reveal an outstanding new feature selection metric, "bi-normal separation" (bns).
we have developed a new effective probabilistic classifier for document classification by introducing the concept of differential document vectors and dlsi (differential latent semantic indexing) spaces.
whereas most statistical approaches to text categorization derive classification knowledge based on training examples alone, aram performs supervised learning and integrates user-defined classification knowledge in the form of if-then rules.
we find that term selection and our modified lsi representations lead to similar topic spotting performance, and that this performance is equal to or better than other published results on the same corpus.}, } @article{wong96, author =
initial experiments on a set of 15 selected polysemous words show that the boosting approach surpasses naive bayes and exemplar-based approaches, which represent state-of-the-art accuracy on supervised wsd.
the best of all connectionist architectures presented here achieves near human performance (79.1\%).
it was found that typical categorization approaches produce predictions which are too similar for combining them to be effective since they tend to fail on the same records.
without any computation-intensive resampling, the new estimators are computationally much more efficient than cross-validation or bootstrapping.
an effective information filtering system is one that provides the exact information that fulfills user's interests with the minimum effort by the user to describe it.
most of these methods are usually awkward and sometimes intractable in high-dimensional feature spaces.
preliminary results show improved accuracy, as well as reduced cost, resulting from these automated techniques.}, } @inproceedings{rennie03, author = {jason rennie and lawrence shih and jaime teevan and david karger}, title =
the results are analyzed from multiple goal perspectives-accuracy, f-measure, precision, and recall-since each is appropriate in different situations.
our techniques considerably improve the robustness and accuracy of the classification outcome, as shown in systematic experimental comparisons with previously published methods on three different real-world datasets.
in the first stage, the queries are enriched such that for each query, its related web pages together with their category information are collected through the use of search engines.
we describe a content-based book recommending system that utilizes information extraction and a machine-learning algorithm for text categorization.
for example, text can be classified with a very high degree of accuracy by authorship, language, dialect and genre.
since there is a difference between important sentences and unimportant sentences in a document, the features from more important sentences should be considered more than other features.
furthermore, these experiments show that feature labeling takes much less (about 1/5th) time than document labeling.
categorical points to each category are computed by summing the frequency of each keyword from back data, or the number of documents from it.
research on machine learning for text categorization, already advancing at a rapid pace, could be further accelerated if better test collections were available.}, } @article{lewis04, author =
experimental results have shown that the statistical learning approach and the learning feedback technique are practical means to automatic indexing of controlled index terms.}, } @inproceedings{lewis00, author = {lewis, david d.}, title = {machine learning for text categorization: background and characteristics}, booktitle = {proceedings of the 21st annual national online meeting}, editor = {williams, martha e.}, publisher = {
we demonstrate via a novel visualization that the recurrent themes subtype is present in rcv1.
the system could potentially scale up to an operational size of 10 million words of text per year - the equivalent of a dozen bibles or a third of the encyclopedia britannica.
when tested on 50 user profiles and 550 megabytes of documents, results indicate that the feature set that is the basis of the text categorization module and the algorithm that establishes the boundary of categories of potentially relevant documents accomplish their tasks with a high level of performance.
in this paper, using several algorithms, we compare the categorization accuracy of classifiers based on words to that of classifiers based on senses.
results obtained from evaluation show that the integration of wordnet can outperform approaches based only on training.}, } @article{diederich03, author = {
the results show that our new approach outperforms the latest lnn approach and linear classifiers in all experiments.}, } @inproceedings{lam99, author = {savio l. lam and dik l. lee}, title = {feature reduction for neural network based text categorization}, booktitle = {proceedings of dasfaa-99, 6th ieee international conference on database advanced systems for advanced application}, editor = {
integration of phonetic and graphic features in poetic text categorization judgements}, journal = {poetics}, year = {1996}, volume = {23}, number = {5}, pages = {363--380}, url = {}, abstract = {
our approach integrates wordnet information with two training approaches through the vector space model.
relevant phrases and contexts are acquired automatically using a training corpus.
we describe a methodology and system (named accio) for automatically acquiring labeled datasets for text categorization from the world wide web, by capitalizing on the body of knowledge encoded in the structure of existing hierarchical directories such as the open directory.
second, smoothing techniques from statistical language modeling can be used to recover better estimates than the laplace smoothing techniques usually used in naive bayes classification.
in addition, pva considers the aging problem of user interests.
this is achieved by means of a data mining approach, called one clause at a time (ocat), which is based on mathematical logic.
the main idea of ferrety algorithm can be generalized for mapping one taxonomy to another if training documents are available."
in this paper, we introduce can models and apply them to various text classification problems.
finally, the relative performance of the different classifiers being tested gives us insights into the strengths and limitations of our algorithms for hypertext classification.}, } @inproceedings{ghani01a, author = {rayid ghani}, title = {combining labeled and unlabeled data for text classification with a large number of categories}, booktitle = {proceedings of the ieee international conference on data mining}, editor = {nick cercone and tsau young lin and xindong wu}, address = {san jose, us}, year = {2001}, pages = {597--598}, publisher =
most of all, tcfp is about one hundred times faster than k-nn.
the advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert manpower, and straightforward portability to different domains.
a text-categorization application developed with tcs consists of the tcs run-time system and a rule base.
morgan kaufmann publishers, san francisco, us}, url = {http://www.ai.mit.edu/~jrennie/papers/icml03-nb.pdf}, abstract = {naive bayes is often used as a baseline in text classification because it is fast and easy to implement.
we provide background, present procedures for building metaclassifiers that take into consideration both reliability indicators and classifier outputs, and review a set of comparative studies undertaken to evaluate the methodology.}, } @inproceedings{bickel04, author = {steffen bickel and tobias scheffer}, title = {learning from message pairs for automatic email answering}, booktitle = {proceedings of ecml-04, 15th european conference on machine learning}, editor = {jean-fran{\c{c}}ois boulicaut and floriana esposito and fosca giannotti and dino pedreschi}, address = {
our first set of experiments applies the c4.5 decision tree induction algorithm to this learning task.
in comparison against a recently proposed technique that appears to be the only one of the kind, we obtained up to 18.5\% of improvement in effectiveness while reducing the processing time dramatically.
thus, there is the problem of determining what information is relevant to the user and how this decision can be taken by a supporting system.
our fully implemented and recently deployed system shows that a superior classification engine for this task can be constructed from a combination of classifiers.
we report on experiences with the reuters newswire benchmark, the us patent database, and web document samples from yahoo!.
the use of a vector space classifier and training method robust to large feature sets, combined with discarding of low frequency ocr output strings are the key to our approach.}, } @inproceedings{iwayama94, author = {makoto iwayama and takenobu tokunaga}, title = {a probabilistic model for text categorization: based on a single random variable with multiple values}, booktitle = {proceedings of anlp-94, 4th conference on applied natural language processing}, publisher = {association for computational linguistics, morristown, us}, editor = {}, year = {1994}, address = {
the user profile is a vector of weighted terms which are learned from the relevance assessment values given by the user on the training set.
we examine several variations to a tf*idf-based approach for this task, empirically analyze their effects, and evaluate our system on a large collection of images from current news newsgroups.
we show on three text categorization data sets that this approach can rescue what would otherwise be disastrously bad training situations, producing much more effective classifiers."
the interpretation shows the strengths and weaknesses of using thesaurus knowledge and gives hints for future research.}, } @article{junker98, author = {markus junker and rainer hoch}, title = {
c-evolve first finds highly accurate cluster digests (partial clusters), gets user feedback to merge and correct these digests, and then uses the classification algorithm to complete the partitioning of the data.
one problem with this approach is that the classifier best suited for an application may be too expensive to train or use during the selection of instances.
this paper presents the results of the application of an instance-based learning algorithm k-nearest neighbor method on feature projections (k-nnfp) to text categorization and compares it with k-nearest neighbor classifier (k-nn).
based on this understanding, we present solutions inspired by round-robin scheduling that avoid this pitfall, without resorting to costly wrapper methods.
"we address the problem dealing with a large collection of data, and investigate the use of automatically constructing category hierarchy from a given set of categories to improve classification of large corpora.
results on a real-world text datasets show that these learners may substantially benefit from using a large amount of unlabeled documents in addition to some labeled documents.}, } @inproceedings{larkey96, author =
in earlier work we described an approach for converting numerical features into bags of tokens so that text classification methods can be applied to numerical classification problems, and showed that the resulting learning methods are competitive with traditional numerical classification methods.
in particular, whirl generally achieves lower generalization error than c4.5, ripper, and several nearest-neighbor methods.
in this paper we propose instead that learning from the training data should also affect phase (ii), i.e.\ that information on the membership of training documents to categories be used to determine term weights.
our experiments show that the transductive method outperforms conventional boosting techniques that employ only labeled data.}, } @inproceedings{taira99, author = {hirotoshi taira and masahiko haruno}, title = {feature selection in svm text categorization},
the paper studies noise reduction for computational efficiency improvements in a statistical learning method for text categorization, the linear least squares fit (llsf) mapping.
we compare different search heuristics and pruning methods known from various symbolic rule learners on a set of representative text categorization problems.
"we address the problem dealing with a large collection of data, and investigate the use of automatically constructing category hierarchy from a given set of categories to improve classification of large corpora.
this can be seen as a complementary tool for topic detection and tracking applications.
the result shows that term redundancy behaves very similar to noise and may degrade the classifier performance.
we demonstrate via a novel visualization that the recurrent themes subtype is present in rcv1.
exploiting hierarchy in text categorization}, journal = {information retrieval}, number = {3}, volume = {1}, pages =
the described system can be efficiently adapted to new domains or different languages.
previous research on automated text categorization has mixed machine learning and knowledge engineering methods, making it difficult to draw conclusions about the performance of particular methods.
the importance of text mining stems from the availability of huge volumes of text databases holding a wealth of valuable information that needs to be mined.
how weak categorizers based upon different principles strengthen performance}, journal = {the computer journal}, year = {2002}, volume = {45}, number = {5}, pages = {511--524}, url = {http://www3.oup.co.uk/computer_journal/hdb/volume_45/issue_05/pdf/450511.pdf}, abstract = {
document collections from the medline database and mayo patient records are used for studies on the effectiveness of our approach, and on how much the effectiveness depends on the choices of training data, indexing language, word-weighting scheme, and morphological canonicalization.
the ability to cheaply train text classifiers is critical to their use in information retrieval, content analysis, natural language processing, and other tasks involving data which is partly or fully textual.
the proposed performance measures consist of category similarity measures and distance based measures that consider the contributions of misclassified documents.
the paper demonstrates that the addition of automatically selected word-pairs substantially increases the accuracy of text classification which is contrary to most previously reported research.
the simplicity of the model, the high recall precision rates, and the efficient computation together make expnet preferable as a practical solution for real world applications.}, } @inproceedings{yang95, author = {
using such an architecture, the time needed for training is reduced substantially and the user is provided with an even more intuitive metaphor for visualization.
on their own, the new representations are not found to produce significant performance improvements.
the rocchio classifier, its probabilistic variant, and a naive bayes classifier are compared on six text categorization tasks.
we show that the benefit of using a first-order representation in this domain is relatively modest; in particular, the performance difference between flipper and foil and their propositional counterparts is quite small, compared to the differences between foil and flipper.
therefore, the indexing system needs some fault-tolerating features.
although our automated learning approach still gives a lower accuracy, by appropriately incorporating a set of manually chosen words to use as features, the combined, semi-automated approach yields accuracy close to the rule-based approach.}, } @article{nieto02, author = {salvador nieto s{\'{a}}nchez and evangelos triantaphyllou and donald kraft}, title = {a feature mining based approach for the classification of text documents into disjoint classes}, journal = {information processing and management}, year = {2002}, volume = {38}, number = {4}, pages = {583--604}, url = {}, abstract = {
this has resulted in high accuracy, shorter customization time, and good prospects for the application of the statistical methods to problems in lexical acquisition.}, } @article{jacobs93, author = {paul s. jacobs}, title = {using statistical methods to improve knowledge-based news categorization}, journal = {ieee expert}, year = {1993}, number = {2}, volume = {8}, pages = {13--23}, url = {}, abstract = {}, } @inproceedings{jo99, author = {taeho c. jo}, title = {
using the vector space model (vsm), each document is represented by its original feature vector augmented with external feature vector generated using wordnet.
in order to select a good hypothesis language (or model) from a collection of possible models, one has to assess the generalization performance of the hypothesis which is returned by a learner that is bound to use that model.
we provide experimental results on a webpage classification task, showing that accuracy can be significantly improved by modeling relational dependencies.}, } @article{tauritz00, author = {daniel r. tauritz and joost n. kok and ida g. sprinkhuizen-kuyper}, title = {
experimental results obtained on three real-world data sets show that we can reduce the feature dimensionality by three orders of magnitude and lose only 2\% accuracy, significantly better than latent semantic indexing, class-based clustering, feature selection by mutual information, or markov-blanket-based feature selection.
the essential formula is cue validity borrowed from cognitive psychology, and used to select from all possible single word based features, the best predictors of a given category.
experimental results on standard benchmarks confirm the validity of our approach, showing that adaboost achieves consistent improvements by including additional semantic features in the learned ensemble.}, } @inproceedings{cai04, author = {lijuan cai and thomas hofmann}, title = {hierarchical document categorization with support vector machines},
technology from machine learning (ml) will offer efficient tools for the intelligent analyses of the data using generalization ability.
the technique is completely language-independent, highly garble-resistant, and computationally simple.
we examine several variations to a tf*idf-based approach for this task, empirically analyze their effects, and evaluate our system on a large collection of images from current news newsgroups.
in contrast to this approach we use the frequencies of occurrence of the most frequent words of the entire written language.
we have performed extensive experiments on the use of ppm compression models for categorization using the standard reuters-21578 dataset.
"common approaches to multi-label classification learn independent classifiers for each category, and employ ranking or thresholding schemes for classification.
the results show that the use of the hierarchical structure improves text categorization performance with respect to an equivalent flat model.
the combination of these techniques significantly influences the overall performance of text categorization.
the traditional method of building a single classifier to do all the classification work would incur a high overhead.
we are using a custom workflow management system as the base for a range of services which are offered via a multimodal portal, using a language-based approach to extracting information from html forms, email, and sms.
our approach to classification is based on "visual similarity" of layout structure and is implemented by building a supervised classifier, given examples of each class.
thus the classifier has a small model size and is very fast.
in this paper, we report on a set of experiments that explore the utility of making use of the structural information of www documents.
we also show that features found to be useful in one corpus do not transfer well to other corpora with different genres." } @article{vogel:2005:kddcup2005, author =
the estimates can be used in standard classification models to reduce error rates.
in comparison with other machine-learning techniques, results on a key benchmark from the reuters collection show a large gain in performance, from a previously reported 67\% recall/precision breakeven point to 80.5\%.
from these, we extract vocabulary, words that appear with high frequency within a given category, characterizing each subject area.
whirl is also fast-up to 500 times faster than c4.5 on some benchmark problems.
"in this paper we present a novel strategy, dragpushing, for improving the performance of text classifiers.
to select the proper number of , we use assures the degree of our disappointment in any differences between the true distribution over inputs and the learner's prediction.
previously, documents have been classified according to their contents manually.
specifically, a query is preprocessed and represented with patterns that include both query words and required answer types.
in this paper we present empirical results on the performance of a bayesian classifier and a decision tree learning algorithm on two text categorization data sets.
a preliminary experimentation proved that the logic approach is able to capture the semantics underlying some kind of sentences, even if the assessment of the efficiency of such a method, as well as a comparison with other related approaches, has still to be carried out.}, } @article{field75, author = {b.j. field}, title = {towards automatic indexing: automatic assignment of controlled-language indexing and classification from free indexing}, year = {1975}, journal =
the number of common keywords between keywords from the document itself and representative keywords from back data classifies documents.
it then uses these words to extract a set of documents for each class from a set of unlabeled documents to form the initial training set.
we examine more complex combination strategies but find them less successful due to the high correlations among our filtering methods which are optimized over the same training data and employ similar document representations.}, } @inproceedings{hull98, author = {david a. hull}, title = {
after posting a query, the user is offered an opportunity to refine the results by browsing through a category tree derived from the dmoz open directory topic hierarchy.
as a by-product, we can compute for each document a set of terms that occur significantly more often in it than in the classes to which it belongs.
we present experiments on the classification of multilingual pornographic html pages using text and image data.
we show accurate results on a large collection of free-form questions used in trec 10.}, } @inproceedings{li03, author = {cong li and ji-rong wen and hang li}, title = {text classification using stochastic keyword generation}, booktitle = {proceedings of icml-03, 20th international conference on machine learning}, editor = {}, year = {2003}, address = {washington, dc}, pages = {}, publisher =
however, the three machine learning methods we employed (naive bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization.
in this paper, we describe a comparative study on techniques of feature transformation and classification to improve the accuracy of automatic text classification.
our classifier based on these improvements performes significantly better on pre-classified samples from the web and the us patent database than the usual classifiers.}, } @inproceedings{yu03a, author = {hwanjo yu and chengxiang zhai and jiawei han}, title = {
results with an algorithm extended by thesaurus knowledge are presented and interpreted.
non-textual information is processed by a symbolic learning technique.
this analysis determined, for example, that ig and chi-squared have correlated failures for precision, and that ig paired with bns is a better choice.}, } @article{forman03, author = {
improving performance of text categorization by combining filtering and support vector machines}, journal = {journal of the american society for information science and technology}, year = {2004}, volume = {55}, number = {7}, pages = {578--592}, url = {http://dx.doi.org/10.1002/asi.10409}, abstract = {
for indirect comparisons, knn, llsf and word were used as baselines, since they were evaluated on all versions of reuters that exclude the unlabelled documents.
"when search results against digital libraries and web resources have limited metadata, augmenting them with meaningful and stable category information can enable better overviews and support user exploration.
the experimental result on the fbis document corpus shows that the atf algorithm outperforms the pure eg (exponentiated-gradient) algorithm.}, } @inproceedings{yu99, author = {
survey coding is a difficult task, since the code that should be attributed to a respondent based on the answer she has given is a matter of subjective judgment, and thus requires expertise.
in this paper, we explore correlations among categories with maximum entropy method and derive a classification algorithm for multi-labelled documents.
the contents of many valuable web-accessible databases are only accessible through search interfaces and are hence in-visible to traditional web ``crawlers''.
given a visual content, the occurrences of visual keywords are detected, summarized spatially, and coded via singular value decomposition to arrive at a concise coded description.
the analysis demonstrates that one of these models is theoretically attractive (introducing few new parameters while increasing flexibility), computationally efficient, and empirically preferable.}, } @article{bennett05, author = {paul n. bennett and susan t. dumais and eric horvitz}, title = {
second, undirected models are well suited for discriminative training, where we optimize the conditional likelihood of the labels given the features, which generally improves classification accuracy.
the system is a component of a more extensive intelligent agent for adaptive information filtering on the web.
feature generation is accomplished through contextual analysis of document text, implicitly performing word sense disambiguation.
i have discovered a bug in my experimental software which caused the relevance sampling results reported in the paper to be incorrect.
our experiments on a variety of categorization tasks indicate that there is significant potential in improving classifier performance by feature re-weighting, beyond that achieved via membership queries alone (traditional active learning) if we have access to an oracle that can point to the important (most predictive) features.
the results show multi-agent classification can achieve promising classification results while maintaining its other advantages."
its main feature is to bind the searching space so that optimal parameters can be selected quickly.
we show that optimal effectiveness occurs when using only a small proportion of the indexing terms available, and that effectiveness peaks at a higher feature set size and lower effectiveness level for a syntactic phrase indexing than for word-based indexing.
the mfom learning framework is evaluated on the reuters-21578 task with lsi-based feature extraction and a binary tree classifier.
while it is easy to collect the unlabeled documents, it is not so easy to manually categorize them for creating training documents.
{90--95}, url = {http://cobar.cs.umass.edu/pubfiles/ir-121.ps}, abstract = {several standard text-categorization techniques were applied to the problem of automated essay grading.
they are accurate, robust, and quick to apply to test instances.
our results show that the new method is highly effective and promising." } @article{bang:2006:hdc, author = {s.l. bang and j.d. yang and h.j. yang}, title = {hierarchical document categorization with k-nn and concept-based thesauri}, journal = {information processing and management}, year = {2006}, volume = {42}, number = {2}, pages = {387--406}, abstract = {
the co-training setting applies to datasets that have a natural separation of their features into two disjoint sets.
machine learning community has in part addressed this problem by developing hierarchical supervised classifiers that help maintainers to categorize new resources within given hierarchies.
experimental results show that it achieves nearly perfect performance on a set of hard cases.}, } @inproceedings{chen00, author = {hao chen and susan t. dumais}, title =
the first set of experiments tests a recurrent neural network for the task of library title classification.
such a knowledge base would enable much more effective retrieval of web information, and promote new uses of the web to support knowledge-based inference and problem solving.
our experiments show our algorithms represent a good trade-off between speed and accuracy in most applications.}, } @inproceedings{cheong02, author = {cheong fung, gabriel p. and jeffrey x. yu and hongjun lu}, title = {discriminative category matching: efficient text classification for huge document collections}, booktitle =
integrating background knowledge into text classification}, pages = {1448--1449}, url = {}, booktitle =
it accommodates both single and multiple topic assignments for each document.
in this paper, we present pva, an adaptive personal view information agent system to track, learn and manage, user's interests in internet documents.
the text classification algorithms classify texts with high accuracy by using an underlying information extraction system to represent linguistic phrases and contexts.
empirical results are given on a number of dataset, showing that our feature selection method is more effective than koller and sahami’s method [koller, d., & sahami, m. (1996).
machine learning community has in part addressed this problem by developing hierarchical supervised classifiers that help maintainers to categorize new resources within given hierarchies.
we tested this approach by applying k-nearest neighbor, rocchio and language modeling classifiers and their combination to the event tracking problem in the topic detection and tracking (tdt) domain, where new classes (events) are created constantly over time, and representative validation sets for new classes are often difficult to obtain on time.
we show that an old corpus can be used for training when testing on new web pages, with only a marginal drop in accuracy rates on genre classification.
using a database of 20,569 documents, we verify that the algorithm attains levels of average precision in the 70-80\% range for category coding and in the 60-70\% range for subcategory coding.
in training the $i$-th classifier special emphasis is placed on the correct categorization of the training documents which have proven harder for the previously trained classifiers.
finally, the obtained structure attributes are used to re-organize and index the web objects.
our experimental results show that lb, an association-based lazy classifier can achieve a good tradeoff between high classification accuracy and scalability to large document collections and large feature sizes.}, } @article{merkl98, author = {merkl, dieter}, title = {
mainly non-informative keywords play the roles of grammatical functions in sentences; such keywords, what are called functional keywords, reflect its contents very little, so they should be removed in the process of document indexing.
we show that for problems plagued with numerous redundant features the performance of c4.5 is significantly superior to that of svm, while aggressive feature selection allows svm to beat c4.5 by a narrow margin.}, } @inproceedings{galavotti00, author = {luigi galavotti and fabrizio sebastiani and maria simi}, title = {experiments on the use of feature selection and negative evidence in automated text categorization}, booktitle = {proceedings of ecdl-00, 4th european conference on research and advanced technology for digital libraries}, editor = {jos{\'e} l. borbinha and thomas baker}, publisher = {springer verlag, heidelberg, de}, note = {
it also can be used for creating training documents.}, } @inproceedings{ko02, author = {youngjoong ko and jinwoo park and jungyun seo}, title = {automatic text categorization using the importance of sentences}, booktitle =
the system is used as part of a commercial news clipping and retrieval product.
for the first time, we apply the scheme to text categorization with support vector machines (svms) on several large text corpora with more than 100 categories.
the user profile is a vector of weighted terms which are learned from the relevance assessment values given by the user on the training set.
we also present results suggesting that traditional term clustering methods are unlikely to provide significantly improved text representations.
in contrast to ranking systems, binary text classification systems may need to produce result sets of any size, requiring that sampling be used to estimate their effectiveness.
in this paper we describe extensive experiments for semantic text routing based on classified library titles and newswire titles.
in this paper we describe experiments that investigate the effects of ocr errors on text categorization.
this article describes our general approach, several machine learning algorithms for this task, and promising initial results with a prototype system that has created a knowledge base describing university people, courses, and research projects.}, } @article{craven01, author = {craven, mark and slattery, se{\'{a}}n}, title =
one key difficulty with text classification learning algorithms is that they require many hand-labeled examples to learn accurately.
using four subsets of the reuters text categorization test collection and a full-text test collection of which documents are varying from tens of kilobytes to hundreds, we evaluate the proposed model, especially the effectiveness of various passage types and the importance of passage location in category merging.
we use a training set of manually categorized documents to learn word-category associations, and use these associations to predict the categories of arbitrary documents.
in this paper, we report on a set of experiments that explore the utility of making use of the structural information of www documents.
{3339}, publisher = {springer-verlag}, url = {http://www.cs.waikato.ac.nz/~eibe/pubs/kibriya_et_al_cr.ps.gz}, abstract = {this paper presents empirical results for several versions of the multinomial naive bayes classifier on four text categorization problems, and a way of improving it using locally weighted learning.
despite being chosen by this heterogeneous approach, the uncertainty samples yielded classifiers with lower error rates than random samples ten times larger.}, } @inproceedings{lewis95, author = {lewis, david d.}, title = {evaluating and optmizing autonomous text classification systems}, booktitle = {proceedings of sigir-95, 18th acm international conference on research and development in information retrieval}, editor = {edward a. fox and peter ingwersen and raya fidel}, publisher = {acm press, new york, us}, year = {1995}, address = {
it is shown that both classifiers can perform filtering with reasonable accuracy.
in our method, independent components of document vectors are extracted using ica and concatenated with the original vectors.
text categorization of low quality images}, booktitle = {proceedings of sdair-95, 4th annual symposium on document analysis and information retrieval}, publisher = {}, editor = {}, year = {1995}, address = {las vegas, us}, pages = {301--315}, url = {http://www.research.att.com/~lewis/papers/ittner95.ps}, abstract = {categorization of text images into content-oriented classes would be a useful capability in a variety of document handling systems.
the corpus contains 44,675 articles with over 35 million words.
the advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert manpower, and straightforward portability to different domains.
we use telltale as our classifier; telltale uses n-grams to compute the similarity between documents.
the performance of our approach is promising, when tested on the questions from the trec qa track.}, } @inproceedings{zhang03a, author = {jian zhang and rong jin and yiming yang and alex hauptmann}, title = {modified logistic regression: an approximation to svm and its applications in large-scale text categorization}, booktitle = {proceedings of icml-03, 20th international conference on machine learning}, editor = {}, year = {2003}, address = {washington, dc}, pages = {}, publisher =
this note presents the corrected results, along with additional data supporting the original claim that uncertainty sampling has an advantage over relevance sampling in most training situations.}, } @inproceedings{lewis95b, author = {david d. lewis}, title = {the {trec-4} filtering track: description and analysis}, booktitle = {proceedings of trec-4, 4th text retrieval conference}, publisher = {national institute of standards and technology, gaithersburg, us}, editor = {donna k. harman and ellen m. voorhees}, year = {1995}, address = {gaithersburg, us}, pages = {165--180}, url = {http://www.research.att.com/~lewis/papers/lewis96b.ps}, abstract = {the trec-4 (4th text retrieval conference) filtering track was an experiment in the evaluation of binary text classification systems.
in all cases we show that our approach either outperforms other methods tried for these tasks or performs comparably to the best.}, } @article{ruiz02, author = {miguel ruiz and padmini srinivasan}, title = {hierarchical text classification using neural networks}, journal = {information retrieval}, number = {1}, volume = {5}, pages = {87--118}, year = {2002}, url = {http://www.wkap.nl/article.pdf?383232},
one problem with this approach is that the classifier best suited for an application may be too expensive to train or use during the selection of instances.
in particular, we present an extensive experimental comparison of our approach with other methods on several well studied lexical disambiguation tasks such as context-sensitive spelling correction, prepositional phrase attachment and part of speech tagging.
given the large number (> 13,000) of closely related categories, this is a challenging task that is unlikely to succumb to a single algorithmic solution.
experiments show that the models outperform their single-label counterparts on standard text corpora.
this set of relevant features varies widely throughout the hierarchy, so that, while the overall relevant feature set may be large, each classifier only examines a small subset.
the neural network is simulated on a vax computer with a fast learning algorithm, and is combined with some non-statistical knowledge from the feature definition system.
we describe the design and implementation of our system, stressing how to exploit standard, efficient relational operations like sorts and joins.
the use of naive bayes allows the boosting algorithm to utilize term frequency information while maintaining probabilistically accurate confidence ratio.
our results indicate that the use of n-grams is an attractive technique which can even compare to techniques relying on a morphological analysis.
however, owing to the use of context-sensitive features, the classifier is very accurate.
= {dublin, ie}, pages = {192--201}, year = {1994}, url = {http://www.acm.org/pubs/articles/proceedings/ir/188490/p192-hersh/p192-hersh.pdf}, abstract = {a series of information retrieval experiments was carried out with a computer installed in a medical practice setting for relatively inexperienced physician end-users.
despite its simplicity, results of experiments on web pages and tv closed captions demonstrate high classification accuracy.
the filtering engine memorizes both user preferences and past situations.
at each node, this classifier can ignore the large number of noise words in a document.
performances of all three classifiers degrade from the reuters collection to the ohsumed collection, but decision forest remains to be superior.}, } @inproceedings{chen01, author
as more information becomes available on-line, intelligent information retrieval will be crucial in order to navigate the information highway efficiently and effectively.
our approach also indicates a new promising way to use trust-worthy deep web knowledge to help organize dispersive information of surface web."
about 70\% of the web-documents are assigned to their true genre; note in this connection that no genre classification benchmark for web pages has been published so far.}, } @article{mladenic03, author = {
such topics can be used as descriptors, similarly to the way librarians use for example, the library of congress cataloging system to annotate and categorize books.
we implemented pagetypesearch system based on our approach.
on one of these datasets (the 20 newsgroups) the method based on word clusters significantly outperforms the word-based representation in terms of categorization accuracy or representation efficiency.
finally, representing each newsgroup by k vectors (with k = 2 or 3) using clustering yields the most significant improvement in routing accuracy, ranging from 60\% to loo\%, while causing only slightly higher storage requirements.}, } @inproceedings{hsu99a, author = {
in order to determine whether information within a sentence has been seen in material read previously, our system integrates information about the context of the sentence with novel words and named entities within the sentence, and uses a specialized learning algorithm to tune the system parameters."
these algorithms relied on hand-coded training data, including annotated texts and a semantic dictionary.
naive bayes classifiers are recognized to be among the best for classifying text.
the results we have obtained significantly outperform the results achieved by previous automated survey coding approaches.}, } @article{giorgetti03a, author = {daniela giorgetti and fabrizio sebastiani}, title = {automating survey coding by multiclass text categorization techniques}, journal = {journal of the american society for information science and technology}, year = {2003}, volume = {54}, number =
the optimized rocchio algorithm achieves a performance comparable with that of the hierarchical neural networks.}, } @inproceedings{ruiz97, author = {miguel e. ruiz and padmini srinivasan}, title = {automatic text categorization using neural networks}, booktitle = {proceedings of the 8th asis/sigcr workshop on classification research}, editor = {efthimis efthimiadis}, publisher =
our experiments on a variety of text categorization tasks indicate that there is significant potential in improving classifier performance by feature reweighting, beyond that achieved via selective sampling alone (standard active learning) if we have access to an oracle that can point to the important (most predictive) features.
year = {2001}, url = {http://link.springer.de/link/service/series/0558/papers/2167/21670454.pdf}, abstract = {the paper demonstrates that the addition of automatically selected word-pairs substantially increases the accuracy of text classification which is contrary to most previously reported research.
in reality, however, many information retrieval problems can be more accurately described as learning a binary classifier from a set of incompletely labeled examples, where we typically have a small number of labeled positive examples and a very large number of unlabeled examples.
to remedy the drawback, we employ concept-based thesauri in the categorization.
in the first phase, a multilayer feed-forward neural network was trained to classify medical documents in the area of cell biology.
since it does not require complicated parameterization, it is simpler to use and more robust than comparable heuristics.
we present empirical studies (controlled experiments on boolean decision trees and a large-scale text categorization problem) which show that the model selection algorithm leads to error rates which are often as low as those obtained by 10-fold cross validation (sometimes even lower).
we extend the traditional active learning framework to include feedback on features in addition to labeling instances, and we execute a careful study of the effects of feature selection and human feedback on features in the setting of text categorization.
in contrast to previous stylometric approaches, we attempt to take full advantage of existing natural language processing (nlp) tools.
however, one of the main shortcomings of such methods is that they largely disregard lexical semantics and, as a consequence, are not sufficiently robust with respect to variations in word usage.
finally, we analyze the experimental performance of these models over the outputs of two text classifiers.
integrating {wordnet} knowledge to supplement training data in semi-supervised agglomerative hierarchical clustering for text categorization}, journal = {international journal of intelligent systems}, pages = {929--947}, year = {2001}, volume = {16}, number = {8}, url = {http://www3.interscience.wiley.com/cgi-bin/fulltext?id=84503376&placebo=ie.pdf}, abstract = {
in our method not only words but also semantic categories given by the thesaurus are used as features in a classifier.
extensive experimentation on representative classifiers, rocchio and svm, as well as a careful analysis of the literature have been carried out to study how some nlp techniques used for indexing impact tc.
unfortunately, the paradigm of supervised machine learning is ill-suited to this task, as it assumes that the training examples are classified by a teacher - usually a human.
in this paper, we study such a problem of performing text classification without labeled negative data tc-won).
the experimental results show that modulating the structure of the user profile increases the accuracy of a personalization system.}, } @article{chen03, author = {chen l. and tokuda n. and nagai a.}, title = {a new differential lsi space-based probabilistic document classifier}, journal = {information processing letters}, pages = {203--212}, year = {2003}, volume = {88}, number = {5}, doi = {http://dx.doi.org/10.1016/j.ipl.2003.09.002}, abstract = {
this situation occurs, for instance, in declassifying documents that have been previously considered important to national security and thus are currently being kept as secret.
in this paper we propose an integration of a selforganizing map and semantic networks from wordnet for a text classification task using the new reuters news corpus.
for precision alone, however, information gain (ig) was superior.
we have tried our representation scheme for automatic document categorisation on the reuters' test set of documents.
the fixed number of regular words from each class will be used as a feature vectors together with the reduced principal components from the pca.
however, the benefits that this has brought about have somehow been limited by the fact that different researchers have ``carved'' different subsets out of this collection, and tested their systems on one of these subsets only; systems that have been tested on different \textsf{reuters-21578} subsets are thus not readily comparable.
in particular, our new feature selection method yields considerable improvement.
such a semantic mapping leads to a significant improvement in categorization and retrieval, compared to alternative approaches.}, } @inproceedings{yang94a, author = {yiming yang}, title = {expert network: effective and efficient learning from human decisions in text categorisation and retrieval}, booktitle = {
finally, we show empirically that this categorization system utilizing a machine-derived taxonomy performs as well as a manual categorization process, but at a far lower cost.}, } @inproceedings{agrawal00, author = {rakesh agrawal and roberto j. bayardo and ramakrishnan srikant}, title = {{\sc athena}: mining-based interactive management of text databases}, booktitle = {proceedings of edbt-00, 7th international conference on extending database technulogy}, editor = {carlo zaniolo and peter c. lockemann and marc h. scholl and torsten grust}, year = {2000}, address = {konstanz, de}, publisher = {springer verlag, heidelberg, de}, note = {
using these techniques, we can automatically build text categorization systems that benefit from domain-specific natural language processing.}, } @article{robertson84, author =
several previous works already suggested applying this method for document clustering, gene expression data analysis, spectral analysis and more.
most such methods are character-based, and thus have the potential to automatically capture non-word features of a document, such as punctuation, word-stems, and features spanning more than one word.
integration of phonetic and graphic features in poetic text categorization judgements}, journal = {poetics}, year = {1996}, volume = {23}, number = {5}, pages = {363--380}, url = {}, abstract = {
this paper introduces a new effective model for text categorization with great corpus (more or less 1 million documents).
the open and modularized system architecture makes our classifier be extendible.
text categorization must work reliably on all input, and thus must tolerate some level of these kinds of problems.
experimental evaluation on real-world data shows that the proposed approach gives good results.
the former never consider the negative features, which are quite valuable, while the latter cannot ensure the optimal combination of the two kinds of features especially on imbalanced data.
{morgan kaufmann publishers, san francisco, us}, url = {http://www.research.att.com/~lewis/papers/lewis94e.ps}, abstract = {uncertainty sampling methods iteratively request class labels for training instances whose classes are uncertain despite the previous labeled instances.
autoslog is a dictionary construction system that has been shown to substantially reduce the time required for knowledge engineering by learning extraction patterns automatically.
finally ssfcm results in improved performance and faster execution time as more weight is given to training documents.}, } @inproceedings{bennett02, author = {paul n. bennett and susan t. dumais and eric horvitz}, title = {probabilistic combination of text classifiers using reliability indicators: models and results},
the method harnesses reliability indicators-variables that provide signals about the performance of classifiers in different situations.
hierarchical classifi- cation is a more efficient method - instead of a single classifier, we use a set of classifiers distributed over a class taxonomy, one for each internal node.
we conduct an empirical study on several document classification tasks which confirms the value of our methods in large scale semi-supervised settings."
in assigning 124 documents to 9 categories, there were 97 cases of agreement with professional indexers.
these knowledge-based categorization methods are more powerful and accurate than statistical techniques.
we present experimental results showing that employing our active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings.}, } @inproceedings{tong92, author = {richard tong and adam winkler and pamela gage}, title = {classification trees for document routing: a report on the trec experiment},
many corpora, such as internet directories, digital libraries, and patent databases are manually organized into topic hierarchies, also called taxonomies.
a user study compared our new category interface with the typical ranked list interface of search results.
using multinomial naive bayes and regularized logistic regression as classifiers, our experiments show both great potential and actual merits of explicitly combining positive and negative features in a nearly optimal fashion according to the imbalanced data.}, } @inproceedings{zhou00, author = {shuigeng zhou and ye fan and jiangtao hua and fang yu and yunfa hu}, title = {
such a system will have to be adaptive to the user changing interest.
experimental results show that the improvements to active learning require less than two-thirds as many labeled training examples as previous qbc approaches, and that the combination of em and active learning requires only slightly more than half as many labeled training examples to achieve the same accuracy as either the improved active learning or em alone.}, } @inproceedings{mccallum98b, author = {andrew k. mccallum and ronald rosenfeld and tom m. mitchell and andrew y. ng}, title =
again, as in [14], the addition of nlp capabilities also suggested a different application of existing methods in revised forms.
in our approach, first, layered lsi spaces are built for a better representation of the hierarchically structured domain knowledge, in order to emphasize the specific semantics and term space in each layer of the domain knowledge.
experiments using a number of e-mail documents generated by different authors on a set of topics gave promising results for both aggregated and multi-topic author categorisation.}, } @inproceedings{dhillon02, author = {inderjit dhillon and subramanyam mallela and rahul kumar}, title = {enhanced word clustering for hierarchical text classification}, booktitle = {proceedings of kdd-02, 8th acm international conference on knowledge discovery and data mining}, publisher = {acm press, new york, us}, editor = {}, year = {2002}, address = {edmonton, ca}, pages = {191--200}, url = {}, abstract = {
after defining our notion of ``text mining'', we focus on the differences between text and data mining and describe in some more detail the unique technologies that are key to successful text mining.}, } @article{doyle65, author = {lauren b. doyle}, title = {is automatic classification a reasonable application of statistical analysis of text?}, journal = {journal of the acm}, volume = {12}, number = {4}, year = {1965}, pages = {473--489}, url = {http://www.acm.org/pubs/articles/journals/jacm/1965-12-4/p473-doyle/p473-doyle.pdf}, abstract = {
experimental evidence indicates that k-nnfp is superior to k-nn in terms of classification accuracy in the presence of irrelevant features in many real world domains.}, } @inproceedings{yi00, author = {jeonghee yi and neel sundaresan}, title = {
comparing the accuracy of our method with other techniques, we observe significant dependency of the results on the data set.
our approach is to analyze the document cluster map to find centroids of some super-clusters.
raising high-degree overlapped character bigrams into trigrams for dimensionality reduction in chinese text categorization}, booktitle = {proceedings of cicling-04, 5th international conference on computational linguistics and intelligent text processing}, year = {2004}, editor = {alexander f. gelbukh}, publisher = {springer verlag, heidelberg, de}, address = {seoul, ko}, note = {published in the ``lecture notes in computer science'' series, number 2945}, pages = {584--595}, url = {}, abstract = {}, } @inproceedings{yamazaki97, author = {takefumi yamazaki and ido dagan}, title = {mistake-driven learning with thesaurus for text categorization}, booktitle = {proceedings of nlprs-97, the natural language processing pacific rim symposium}, editor = {}, publisher = {}, address = {phuket, th}, pages = {369--374}, year = {1997}, url = {ftp://www.links.nectec.or.th/pub/nlprs/paper/dana4r.ps.gz}, abstract = {
similar to indices for relational data, taxonomies make search and access more efficient.
based on two chinese corpora, a series of controlled experiments evaluated their learning capabilities and efficiency in mining text classification knowledge.
we describe the design and implementation of our system, stressing how to exploit standard, efficient relational operations like sorts and joins.
maintaining catalogues manually is becoming increasingly difficult due to the sheer amount of material on the web, and therefore it will be soon necessary to resort to techniques for automatic classification of documents.
its main feature is to bind the searching space so that optimal parameters can be selected quickly.
we also present results suggesting that traditional term clustering methods are unlikely to provide significantly improved text representations.
in this paper we propose instead that learning from the training data should also affect phase (ii), i.e.\ that information on the membership of training documents to categories be used to determine term weights.
k-nnfp is similar to k-nn except it finds the nearest neighbors according to each feature separately.
published in the ``lecture notes in computer science'' series, number 3238}, url = {http://www-ai.upb.de/aisearch/ki04-frame.pdf}, abstract = {genre classification means to discriminate between documents by means of their form, their style, or their targeted audience.
we show that the results outperform competing methods.
this task is an intermediate process in many natural language processing tasks like machine translation or multilingual information retrieval.
our results show that svm, knn and llsf significantly outperform nnet and nb when the number of positive training instances per category are small (less than ten), and that all the methods perform comparably when the categories are sufficiently common (over 300 instances).}, } @article{yang99a, author = {yiming yang}, title = {an evaluation of statistical approaches to text categorization}, journal = {information retrieval}, year = {1999}, pages = {69--90}, volume = {1}, number = {1/2}, url = {http://www.cs.cmu.edu/~yiming/papers.yy/irj99.ps}, abstract = {
this analysis also revealed, for example, that information gain and chi-squared have correlated failures, and so they work poorly together.
initial experimental results demonstrate that this approach can produce accurate recommendations.}, } @inproceedings{moschitti03, author = {alessandro moschitti}, title = {a study on optimal parameter tuning for rocchio text classifier}, booktitle = {proceedings of ecir-03, 25th
using ig thresholding with a k-nearest neighbor classifier on the reuters corpus, removal of up to 98\% removal of unique terms actually yielded an improved classification accuracy (measured by average precision).
we first show that adaboost significantly outperforms another highly effective text filtering algorithm.
in empirical tests, it consistently showed more than 10 points f-measure improvement for each of four reuters categories tested." } @inproceedings{sindhwani:2006:lss, author = "sindhwani, vikas and keerthi, s. sathiya", title =
in this research, our experiment dealt with the classification of news wire articles using category profiles.
classifier of pagetypesearch classifies web pages into the document types by comparing their pages with typical structural characteristics of the types.
finally, we measure the accuracy achieved with all words and all hm pairs combined, which turns out to be only marginally above the baseline.
the experiments were conducted on the standard reuters data set.
rakesh agrawal and paul e. stolorz and gregory piatetsky-shapiro}, publisher = {aaai press, menlo park, us}, year = {1998}, address = {new york, us}, pages = {169--173}, url = {http://www.research.whizbang.com/~wcohen/postscript/kdd-98.ps}, abstract = {whirl is an extension of relational databases that can perform ``soft joins'' based on the similarity of textual identifiers; these soft joins extend the traditional operation of joining tables based on the equivalence of atomic values.
these performance measures often assume independence between categories and do not consider documents misclassified into categories that are similar or not far from the correct categories in the category tree.
moreover, the procedure of defining analysis-level markers can be followed in order to extract useful stylistic information using existing text processing tools.}, } @inproceedings{stamatatos00a, author = {efstathios stamatatos and nikos fakotakis and george kokkinakis}, title = {
the classifiers provide heuristics to the crawler thus biasing it towards certain portions of the web graph.
using as testing ground a part of the wall street journal corpus, we show that the most frequent words of the british national corpus, representing the most frequent words of the written english language, are more reliable discriminators of text genre in comparison to the most frequent words of the training corpus.
given corpora of documents and a training set of examples of classified documents, the technique locates a minimal set of co-ordinate keywords to distinguish between classes of documents, reducing the dimensionality of the keyword vectors.
besides, we generate several knowledge base instead of one knowledge base for the classification of new object, hoping that the combination of answers of the multiple knowledge bases result in better performance.
we show that our method is especially useful for classification tasks involving a large number of categories where co-training doesn't perform very well by itself and when combined with ecoc, outperforms several other algorithms that combine labeled and unlabeled data for text classification in terms of accuracy, precision-recall tradeoff, and efficiency.}, } @inproceedings{ghani02, author = {rayid ghani}, title = {
our results do not show a dominant algorithm nor method for making algorithms cost-sensitive, but are the best reported on the test collection used, and approach real-world hand-crafted classifiers accuracy.}, } @inproceedings{goodman90, author = {marc goodman}, title = {{\sc prism}: a case-based telex classifier}, booktitle = {proceedings of iaai-90, 2nd conference on innovative applications of artificial intelligence}, publisher = {aaai press, menlo park, us}, editor = {alain rappaport and reid smith}, year = {1990}, address = {}, pages = {25--37}, url = {}, abstract = {}, } @article{gray71, author = {w. a. gray and a. j. harley}, title = {computer-assisted indexing}, journal = {information storage and retrieval}, year = {1971}, volume = {7}, number = {4}, pages = {167--174}, url = {}, abstract = {}, } @inproceedings{guo04, author = {gongde guo and hui wang and david a. bell and yaxin bi and kieran greer}, title = {an knn model-based approach and its application in text categorization}, booktitle = {
we focus upon the routing of case law summaries to various secondary law volumes in which they should be cited.
to index successfully in the defense documentation center's environment, an automated system must chose single words or phrases (dependent upon context) rapidly and economically.
automatic text classification is necessary to store documents like that.
we use a novel stop word identification method to automatically generate domain-specific stoplists which are much larger than a conventional domain-independent stoplist.
in this paper we are interested in a more general formulation where documents are organized as page sequences, as naturally occurring in digital libraries of scanned books and magazines.
based on the lpt-model, we focus on learning patterns within a relatively simple pattern language.
{basel, ch}, url = {http://airone.fub.it:8080/projects/pakm96.ps}, abstract = {with the development and diffusion of the internet worldwide connection, a large amount of information can be delivered to the users.
a belief networks-based generative model for structured documents.
we study the use of support vector machines (svms) in classifying email as spam or nonspam by comparing it to three other classification algorithms: ripper, rocchio, and boosting decision trees.
we experimented with pre-classified samples from {{\sc yahoo!}}\ and the us patent database.
we then show that a quantum leap in performance is achieved when we further modify the algorithms to better address some of the specific characteristics of the domain.
"the performance of search engines crucially depends on their ability to capture the meaning of a query most likely intended by the user.
here, the challenges are: a) obtain a high accuracy classification model; b) consume low computational time for both model training and operation; and c) occupy low storage space.
we give here a new, information theoretical interpretation of term strength, review some of its uses in focusing the processing of documents for information retrieval and describe new results obtained in document categorization.}, } @inproceedings{yang97, author = {yiming yang and jan o. pedersen}, title = {a comparative study on feature selection in text categorization}, booktitle = {proceedings of icml-97, 14th international conference on machine learning}, editor = {douglas h. fisher}, year = {1997}, address = {nashville, us}, pages = {412--420}, publisher = {morgan kaufmann publishers, san francisco, us}, url = {http://www.cs.cmu.edu/~yiming/papers.yy/ml97.ps}, abstract = {
we propose a model for basing classification of multimedia on broad, non-topical features, and show how information on targeted nearby pieces of text can be used to effectively classify photographs on a first such feature, distinguishing between indoor and outdoor images.
experimental results show that this variant provides an order of magnitude further improvement in training efficiency.
this dissertation introduces a new theoretical model for text classification systems, including systems for document retrieval, automated indexing, electronic mail filtering, and similar tasks.
the effectiveness of a classifier that uses supervised learning was analyzed in terms of its accuracy and ultimately its influence on filtering.
these results are compared to an existing operational process using boolean queries manually constructed by domain experts.
the paper in hand presents results from a user study on web genre usefulness as well as results from the construction of a genre classifier using discriminant analysis, neural network learning, and support vector machines.
a central problem in information retrieval is the automated classification of text documents.
genre or style, on the other hand, is a different and important property of text, and automatic text genre classification is becoming important for classification and retrieval purposes as well as for some natural language processing research.
an implementation of the algorithm is described which, starting with a small set of hand-labeled instances, improves its results automatically via unsupervised training.
our results with the english newswire collection show a very large gain in performance as compared to published benchmarks, while our initial results with the german newswires appear very promising.
this can aid if/ir systems that rely on the acquisition of large numbers of term weights or other measures of relevance.
we propose to exploit the natural hierarchy of topics, or taxonomy, that many corpora, such as internet directories, digital libraries, and patent databases enjoy.
the former hints a generative model of news articles, and the latter provides data enriched environments to perform red.
using the reuters collection, we show that adaptive resampling techniques can improve decision-tree performance and that relatively small, pooled local dictionaries are effective.
if the occurrence of a single word determines whether an article belongs to a category or not (and it often does) any compression scheme will likely fail to classify the article correctly.
using a representation of the content of web documents that captures these two characteristics and (2) using more effective classifiers.
{information processing and management}, year = {2006}, volume = {42}, number = {1}, pages = {155--165}, abstract = {most previous works of feature selection emphasized only the reduction of high dimensionality of the feature space.
when the data streams are produced in a changing environment the filtering has to adapt too in order to remain effective.
{morgan kaufmann publishers, san francisco, us}, pages = {200--209}, url = {http://www-ai.cs.uni-dortmund.de/dokumente/joachims_99c.ps.gz}, abstract = {this paper introduces transductive support vector machines (tsvms) for text classification.
requirements of any such system include speed and minimal end-user effort.
the authors' initial work with this algorithm has demonstrated that probabilistic structures can be automatically acquired from a training set of documents with respect to a single target concept, or a set of related concepts.
these knowledge-based categorization methods are more powerful and accurate than statistical techniques.
the categorization, which consists in assigning an international code of disease (icd) to the medical document under examination, is based on well-known information retrieval techniques.
expnet is used for relevance ranking of candidate categories of an arbitrary text in the case of text categorization, and for relevance ranking of documents via categories in the case of text retrieval.
the user then selects/labels some words from the ranked list for each class.
the kind of application that the text categorization shell, tcs, can produce is characterized.
the first set of results applies rankboost to a text representation produced using modern term weighting methods.
we report on experiences with the reuters newswire benchmark, the us patent database, and web document samples from {{\sc yahoo!}}\.}, } @inproceedings{wei01, author = {chih-ping wei and yuan-xin dong}, title = {a mining-based category evolution approach to managing online document categories},
they may not perform effectively in adaptive text filtering which is a more realistic problem.
the strategy is generic and takes advantage of training errors to successively refine the classification model of a base classifier.
finally, the generative model may be used to derive a fisher kernel expressing similarity between documents.}, } @article{gentili01, author = {g.l. gentili and mauro marinilli and alessandro micarelli and filippo sciarrone}, title = {
in this paper a system for analysis and automatic indexing of imaged documents for high-volume applications is described.
the system accurately classifies porn sites from 8 european languages.
we then systematically study the key factors in the can model that can influence the classification performance, and analyze the strengths and weaknesses of the model.} } @article{makkonen:2004:sst, author = {juha makkonen and helena ahonen-myka and marko salmenkivi}, title = {simple semantics in topic detection and tracking}, journal = {information retrieval},
our experiments in a transductive classification setting indicate that accuracy can be significantly improved by modeling relational dependencies.
the system employs several knowledge sources including a letter database, word frequency statistics for german, lists of message type specific words, morphological knowledge as well as the underlying document structure.
existing techniques for such "distributional clustering" of words are agglomerative in nature and result in (i) sub-optimal word clusters and (ii) high computational cost.
existing techniques for such "distributional clustering" of words are agglomerative in nature and result in (i) sub-optimal word clusters and (ii) high computational cost.
by emphasizing the category discrimination capability of features, the paper firstly puts forward a new weighting scheme tf*idf*ig.
we use image features such as percentages of text and non-text (graphics, images, tables, and rulings) content regions, column structures, relative point sizes of fonts, density of content area, and statistics of features of connected components which can be derived without class knowledge.
our results show simple windows are best for all test collections tested in these experiments.
in the framework of multi-domain text-to-speech synthesis it is essential to (i) design a hierarchically structured database for allowing several domains in the same speech corpus and (ii) include a text classification module that, at run time, assigns the input sentences to a domain or set of domains from the database.
k-nn is one of the most popular document categorization methods because it shows relatively good performance in spite of its simplicity.
there is strong empirical and theoretic evidence that combination of retrieval methods can improve performance.
"when search results against digital libraries and web resources have limited metadata, augmenting them with meaningful and stable category information can enable better overviews and support user exploration.
it not only approaches and sometimes exceeds svm accuracy, but also beats svm running time by orders of magnitude.
we describe an automatic system that starts with a small sample of the corpus in which topics have been assigned by hand, and then updates the database with new documents as the corpus grows, assigning topics to these new documents with high speed and accuracy.
the input space was gradually increased by using mutual information (mi) filtering and part-of-speech (pos) filtering, which determine the portion of words that are appropriate for learning from the information-theoretic and the linguistic perspectives, respectively.
our contribution is to propose robust statistical models and a relaxation labeling technique for better classification by exploiting link information in a small neighborhood around documents.
the fixed number of regular words from each class will be used as a feature vectors together with the reduced principal components from the pca.
previous reports indicate that human-engineered rule-based systems, requiring many man-years of developmental efforts, have been successfully built to ``read'' documents and assign topics to them.
document feature characteristics, derived from the training document set, capture some inherent properties of a particular category.
the derived models, inequality me models, in effect have regularized estimation with l1 norm penalties of bounded parameters.
an implementation of the algorithm is described which, starting with a small set of hand-labeled instances, improves its results automatically via unsupervised training.
text categorization presents unique challenges due to the large number of attributes present in the data set, large number of training samples, attribute dependency, and multi-modality of categories.
because the analyses of data are generally so expensive, most parts in databases remains as raw, unanalyzed primary data.
in this paper, we present pva, an adaptive personal view information agent system for tracking, learning and managing user interests in internet documents.
in two experiments, the research method of information integration theory was employed in order to test two hypotheses relating to the radical conventionalist and traditional positions on the role of specific formal textual features in the categorization of poetic texts.
this system worked very well for language classification, achieving in one test a 99.8\% correct classification rate on usenet newsgroup articles written in different languages.
previously, documents have been classified according to their contents manually.
it is shown to be more robust with respect to the training set size and to improve the performance both for ranking and classification, specially for classes with few training examples.}, } @inproceedings{denoyer03, author = {ludovic denoyer and patrick gallinari}, title = {
our results show that naive bayes is a weak choice for guiding a topical crawler when compared with support vector machine or neural network.
{843--873}, year = {2001}, volume = {15}, number = {9}, url = {}, abstract = {the volume of electronically stored information increases exponentially as the state of the art progresses.
our feature selection method strives to reduce redundancy between features while maintaining information gain in selecting appropriate features for text categorization.
using {wordnet} to complement training information in text categorization}, booktitle = {proceedings of ranlp-97, 2nd international conference on recent advances in natural language processing}, publisher = {}, editor = {ruslan milkov and nicolas nicolov and nilokai nikolov}, address = {tzigov chark, bl}, pages = {}, year = {1997}, url = {http://xxx.unizar.es/ps/cmp-lg/9709007}, abstract = {
the scheme for automatic text classification proposed in the paper, is based on document indexing, where a document is represented as a list of keywords.
we present experimental results showing that employing our active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings.}, } @inproceedings{tong92, author = {richard tong and adam winkler and pamela gage}, title = {classification trees for document routing: a report on the trec experiment}, booktitle = {proceedings of trec-1, 1st
this means that two (or several) different techniques are used to optimize the performances even if a single algorithm may have more chances to operate the right choices.
the experimental results suggest that the bigrams can substantially raise the quality of feature sets, showing increases in the break-even points and f1 measures.
in this study, we proposed a mining-based category evolution (mice) technique to adjust document categories based on existing categories and their associated documents.
to evaluate classifiers in our multipath framework, we define a new hierarchical loss function, the h-loss, capturing the intuition that whenever a classification mistake is made on a node of the taxonomy, then no loss should be charged for any additional mistake occurring in the subtree of that node.
the svm approach as represented by schoelkopf was superior to all the methods except the neural network one, where it was, although occasionally worse, essentially comparable.
we show that machine-generated decision rules appear comparable to human performance, while using the identical rule-based representation.
extensive experimentation on representative classifiers, rocchio and svm, as well as a careful analysis of the literature have been carried out to study how some nlp techniques used for indexing impact tc.
here we show how the classification accuracy of foil on this task can be improved by discovering additional regularities on the test set pages that must be classified.
these feature vectors are then used as the input to the neural networks for classification.
its main originality is its ability to simultaneously take into account the structural and the content information present in a structured document, and also to cope with different types of content (text, image, etc).
since the single layers of self-organizing maps represent different aspects of the document collection at different levels of detail, the neural network shows the document collection in a form comparable to an atlas where the user may easily select the most appropriate degree of granularity depending on the actual focus of interest during the exploration of the document collection.}, } @inproceedings{meyer04, author = {
traditionally, the categories are arranged in hierarchical manner to achieve effective searching and indexing, as well as easy comprehension for humans.
however, centralized classification approaches often are limited due to constraints on knowledge and computing resources.
such systems typically have to cope with sets of rectors of many tens of thousands of dimensions.
support vector machines (svms) excel in binary classification, but the elegant theory behind large-margin hyperplane cannot be easily extended to multi-class text classification.
using this method, we achieved high performance in text categorization both with small number and large numbers of labeled data.}, } @inproceedings{tan01, author = {
address = {basel, ch}, url = {http://airone.fub.it:8080/projects/pakm96.ps}, abstract = {with the development and diffusion of the internet worldwide connection, a large amount of information can be delivered to the users.
we show that for problems plagued with numerous redundant features the performance of c4.5 is significantly superior to that of svm, while aggressive feature selection allows svm to beat c4.5 by a narrow margin.}, } @inproceedings{galavotti00, author = {luigi galavotti and fabrizio sebastiani and maria simi}, title = {experiments on the use of feature selection and negative evidence in automated text categorization}, booktitle = {proceedings of ecdl-00, 4th european conference on research and advanced technology for digital libraries}, editor = {jos{\'e} l. borbinha and thomas baker}, publisher = {springer verlag, heidelberg, de}, note = {
previous reports indicate that human-engineered rule-based systems, requiring many man-years of developmental efforts, have been successfully built to ``read'' documents and assign topics to them.
k-nnfp is similar to k-nn except it finds the nearest neighbors according to each feature separately.
the best variant, which we call lazyboosting, is tested on the largest sense-tagged corpus available containing 192,800 examples of the 191 most frequent and ambiguous english words.
"support vector machines (svms) have been very successful in text classification.
our experiments are conducted in the text categorization domain, which is characterized by a large number of features, many of which are irrelevant.
the paper describes the novel technique of categorization by context, which instead extracts useful information for classifying a document from the context where a url referring to it appears.
for illustration we give a brief description of the content-based personal intelligent agent named personal webwatcher that uses text-learning for user customized web browsing.}, } @inproceedings{mladenic99a, author = {dunja mladeni{\'{c}} and marko grobelnik}, title = {feature selection for unbalanced class distribution and naive bayes}, booktitle = {proceedings of icml-99, 16th international conference on machine learning}, editor = {ivan bratko and saso dzeroski}, year = {1999}, address = {bled, sl}, pages =
with the former, we provide an enhanced document representation that incorporates the structural and heterogeneous nature of web documents.
{139--145}, url = {http://www.acm.org/pubs/articles/proceedings/dl/313238/p139-lim/p139-lim.pdf}, abstract = {automatic categorization of multimedia documents is an important function for a digital library system.
the concept learning model emphasizes the role manual and automated feature selection and classifier formation in text classification.
{association for computational linguistics, morristown, us}, editor = {claire cardie and ralph weischedel}, year = {1997}, address = {providence, us}, pages = {55--63}, url = {http://l2r.cs.uiuc.edu/~danr/papers/categ.ps.gz}, abstract = {learning problems in the text processing domain often map the text to a space whose dimensions are the measured features of the text, e.g., its words.
the experiments of the categorization of news articles show that the proposed schemes of text categorization outperform the schemes with crisp sets.}, } @incollection{jo99a, author = {taeho c. jo}, title = {news article classification based on categorical points from keywords in backdata}, booktitle = {computational intelligence for modelling, control and automation}, editor = {
our experiments show that our system has low overhead and achieves high classification ac-curacy across a variety of databases.}, } @inproceedings{ittner95, author = {david j. ittner and lewis, david d. and david d. ahn}, title = {
text classification by a neural network}, booktitle = {proceedings of the 23rd annual summer computer simulation conference}, editor = {}, publisher = {}, address = {baltimore, us}, pages = {313--318}, year = {1991}, url = {}, abstract = {when banks process their free-form telex traffic, the first task is the classification of the telexes.
the combination of text classifiers using reliability indicators}, journal = {information retrieval}, number = {1}, volume = {8}, pages = {67--100}, year = {2005}, url = {http://www.kluweronline.com/issn/1386-4564}, abstract = {the intuition that different text classifiers behave in qualitatively different ways has long motivated attempts to build a better metaclassifier via some combination of classifiers.
to remedy the drawback, we employ concept-based thesauri in the categorization.
in a batch mode, the programs to accomplish this indexing would require no more than fifteen minutes of cpu time per week.}, } @article{klingbiel73a, author = {paul h. klingbiel}, title = {a technique for machine-aided indexing}, journal = {information storage and retrieval}, year = {1973}, volume = {9}, number = {9}, pages = {477--494}, url = {}, abstract = {subject indexing of text can, in principle, be accomplished in many ways.
we describe a method for improving the classification of short text strings using a combination of labeled training data plus a secondary corpus of unlabeled but related longer documents.
taken as a whole, the set of web pages lacks a unifying structure and shows far more authoring style and content variation than that seen in traditional text-document collections.
this tight packaging of word pairs could bring in some semantic value.
in this paper we study the use of a semi-supervised agglomerative hierarchical clustering (ssahc) algorithm to text categorization, which consists of assigning text documents to predefined categories.
= {an extended version appears as~\cite{craven00}}, url = {http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/www/wwkb/overview-aaai98.ps.gz}, abstract = {the world wide web is a vast source of information accessible to computers, but understandable only to humans.
in this paper, we show how an operator-based view of rule induction enables the easy integration of a thesaurus as background knowledge.
we also observe that despite similar performances, different topical crawlers cover subspaces on the web with low overlap.} } @article{peng:2004:anb, author = {fuchun peng and dale schuurmans and shaojun wang}, title = {
first, we train the linear svm on a subset of training data and retain only those features that correspond to highly weighted components (in absolute value sense) of the normal to the resulting hyperplane that separates positive and negative examples.
little words can make a big difference for text classification},
furthermore, based on this approach, we build an interactive red system, hiscovery, which provides additional functions to present events, photo story and chronicle."
in addition, the training time and scaling are also important concerns.
in this paper we present a learning system for information filtering and selective information dissemination.
improving text categorization using the importance of sentences}, journal = {information processing and management}, year = {2004}, volume = {40}, number = {1}, pages = {65--79}, url = {}, abstract = {}, } @article{ko04a, author
"we present a principled methodology for filtering news stories by formal measures of information novelty, and show how the techniques can be used to custom-tailor newsfeeds based on information that a user has already reviewed.
context, as it applies to document similarity, can be accommodated by a well-defined procedure.
the authors' approach has been to use statistics in the knowledge acquisition component of a linguistic pattern-based categorization system, using statistical methods, for example, to associate words with industries and identify phrases that information about businesses or products.
analysis and empirical evidence suggest that the evaluation results on some versions of reuters were significantly affected by the inclusion of a large portion of unlabelled documents, making those results difficult to interpret and leading to considerable confusions in the literature.
term selection according to this criterium is performed by the elimination of noisy terms on a class-by-class basis, rather than by selecting the most significant ones.
after training, the incoming news articles are classified based on their similarity to the existing newsgroup categories.
when a natural split does not exist, co-training algorithms that manufacture a feature split may outperform algorithms not using a split.
the automatic indexing system {air/phys}.
for indirect comparisons, knn, llsf and word were used as baselines, since they were evaluated on all versions of reuters that exclude the unlabelled documents.
experimental data is presented showing widrow-hoff and eg to be more effective than the widely used rocchio algorithm on several categorization and routing tasks.}, } @misc{lewis97a, author = {lewis, david d.}, title = {reuters-21578 text categorization test collection.
we have empirically demonstrated that rule-based methods like ours result in high classification accuracy when the categories to which texts are to be assigned are relatively specific ones and when the texts tend to be short.
in this paper we propose instead that learning from training data should also affect phase (ii), i.e.\ that information on the membership of training documents to categories be used to determine term weights.
although such learning models succeed in exploiting relational knowledge, they are highly demanding in terms of labeled examples, because the number of categories is related to the dimension of the corresponding hierarchy.
the algorithm checks the context surrounding the target noun against that of previously observed instances and chooses the sense for which the most evidence is found, where evidence consists of a set of orthographic, syntactic, and lexical features.
using k-nn, naive bayes and centroid-based classifiers, the experimental results show that the multi-dimensional-based and hierarchical-based classification performs better than the flat-based classifications.}, } @inproceedings{thompson01, author = {paul thompson}, title = {automatic categorization of case law}, booktitle = {proceedings of icail-01, 8th international conference on artificial intelligence and law}, editor = {}, year = {2001}, address = {st.\ louis, us}, pages = {70--77}, publisher = {acm press, new york, us}, url = {http://doi.acm.org/10.1145/383535.383543}, abstract =
in the hierarchical case, a model is learned to distinguish a second-level category from other categories within the same top level.
{12}, pages = {1269--1277}, url = {http://www.math.unipd.it/~fabseb60/publications/jasist03.pdf}, abstract = {\emph{survey coding} is the task of assigning a symbolic code from a predefined set of such codes to the answer given in response to an open-ended question in a questionnaire (aka \emph{survey}).
these are based on ranking text sequences by their cross-entropy calculated using a fixed order character-based markov model adapted from the ppm text compression algorithm.
the architecture, modules, and practical results are described.}, } @article{manevitz01, author = {larry m. manevitz and malik yousef}, title = {one-class {svms} for document classification}, journal = {journal of machine learning research}, volume =
stanford, us}, pages = {895--902}, publisher = {morgan kaufmann publishers, san francisco, us}, url = {http://www.cs.cmu.edu/~sean/papers/icml2000.ps}, abstract = {machine learning typically involves discovering regularities in a training set, then applying these learned regularities to classify objects in a test set.
the biological literature presents a difficult challenge to information processing in its complexity, diversity, and in its sheer volume.
we introduce a novel hybrid system specifically designed for multi-page text documents.
we use a training set of texts with expert assigned categories to construct a network which approximately reflects the conditional probabilities of categories given a text.
a stochastic decision list is an ordered sequence of if-then-else rules, and our method can be viewed as a rule-based method for text classification having advantages of readability and refinability of acquired knowledge.
the svm approach as represented by schoelkopf was superior to all the methods except the neural network one, where it was, although occasionally worse, essentially comparable.
we present a system for searching and classifying u.s. patent documents, based on inquery.
we improve a high-accuracy maximum entropy classifier by combining an ensemble of classifiers with neural network voting.
using the same representation of documents, charade offers better performance than earlier reported experiments with decision trees on the same corpus.
pisa, it}, year = {2003}, pages = {420--435}, url = {http://link.springer.de/link/service/series/0558/papers/2633/26330420.pdf}, abstract = {current trend in operational text categorization is the designing of fast classification tools.
a hierarchical classification is obtained by evaluating the trained node classifiers in a top-down fashion.
the categorization algorithm used is a supervised learning procedure that uses a linear classifier based on the category levels.
however, in carefully controlled experiments using syntactic phrases produced by church's stochastic bracketer, in conjunction with reciprocal nearest neighbor clustering, term clustering was found to produce essentially no improvement in the properties of the phrasal representation.
the efficiency, effectiveness, and noise tolerance of this search strategy were confirmed to be better than those of a full search, a category based search, and a cluster based search with nonprobabilistic clustering.}, } @inproceedings{iwayama95a, author = {makoto iwayama and takenobu tokunaga}, title = {hierarchical bayesian clustering for automatic text classification}, booktitle = {proceedings of ijcai-95, 14th international joint conference on artificial intelligence}, editor =
in some applications, there might be human knowledge available that, in principle, could compensate for the lack of data.
this disser- tation demonstrates that supervised learning algorithms that use a small number of labeled examples and many inexpensive unlabeled examples can create high-accuracy text classifiers.
categorizing documents with the aid of knowledge-based features leverages information that cannot be deduced from the documents alone.
empirical results indicate that these classifiers are comparable with the best text classification systems.
only relatively recently did detailed studies on the impact of various document representations step into the spotlight, showing that there may be statistically significant differences in classifier performance even among variations of the classical bag-of-words model.
class labels can also be assigned based on known document types, or can be defined by the user.
in our method, independent components of document vectors are extracted using ica and concatenated with the original vectors.
in many settings, we also have the option of using pool-based active learning.
the corresponding classifier parameters are learned by optimizing an overall objective function of interest.
we found that lr performs strongly and robustly in optimizing t11su (a trec utility function) while rocchio is better for optimizing ctrk (the tdt tracking cost), a high-recall oriented objective function.
generally, filtering systems calculate the similarity between the profile and each incoming document, and retrieve documents with similarity higher than a threshold.
the experimental evaluation demonstrates that the wpcm method provides acceptable classification accuracy with the sports news datasets.}, } @inproceedings{sevillano04, author = {sevillano dominguez, xavier and alias pujol, francesc and socoro carrie, joan c.}, title = {ica-based hierarchical text classification for multi-domain text-to-speech synthesis}, booktitle = {proceedings of icassp-04, proceedings of the 29th ieee international conference on acoustics, speech, and signal processing}, editor = {}, publisher = {ieee computer society press, los alamitos, us}, address = {
the effectiveness of a classifier that uses supervised learning was analyzed in terms of its accuracy and ultimately its influence on filtering.
we define parameters of categories that make it possible to acquire numerous datasets with desired properties, which in turn allow better control over categorization experiments.
as the amount of data stored in storage media is increased exponentially, it becomes necessary to store documents according to their category, to access them easily.
because of efficiency, the latter is more suitable for text data such as web documents.
the stepwise feature selection in the decision tree algorithm is particularly effective in dealing with the large feature sets common in text categorization.
however, as documents accumulate, such categories may not capture a document's characteristics correctly.
by providing a formal analysis of the computational complexity of each classification method, followed by an investigation on the usage of different classifiers in a hierarchical setting of categorization, we show how the scalability of a method depends on the topology of the hierarchy and the category distributions.
"we demonstrate the value of using context in a new-information detection system that achieved the highest precision scores at the text retrieval conference's novelty track in 2004.
benchmark experiments showed that their predictive performance were roughly comparable, especially on clean and well organized data sets.
for example, we have previously shown how foil, a relational learner, can learn to classify web pages by discovering training set regularities in the words occurring on target pages, and on other pages related by hyperlinks.
it is demonstrated that the probabilistic corpus model which emerges from the automatic or unsupervised hierarchical organisation of a document collection can be further exploited to create a kernel which boosts the performance of state-of-the-art support vector machine document classifiers.
these distributions are useful to increase classification accuracy by exploiting information of (1) term distribution among classes, (2) term distribution within a class and (3) term distribution in the whole collection of training data.
considerable improvement in the classification accuracies of two popular classification algorithms on standard labeled data-sets with and without artificially introduced noise, as well as in the presence and absence of unlabeled data, indicates that this may be a promising method to reduce the burden of manual labeling."
a boosting machine learning approach is applied to classifying web chinese documents that share a topic hierarchy.
our experiments demonstrate that this new approach is able to learn more accurate classifiers than either of its constituent methods alone.}, } @inproceedings{craven98, author = {mark craven and dan dipasquo and dayne freitag and andrew k. mccallum and tom m. mitchell and kamal nigam and se{\'{a}}n slattery}, title = {learning to extract symbolic knowledge from the world wide web}, booktitle = {proceedings of aaai-98, 15th conference of the american association for artificial intelligence}, publisher = {aaai press, menlo park, us}, year = {1998}, pages = {509--516}, address =
in this paper, we present a method for automatic genre classification that is based on statistically selected features obtained from both subject-classified and genre classified training data.
by observing that people who search the web with the same queries often click on different, but related documents together, we draw implicit links between web pages that are clicked after the same queries.
experiments on the newsgroups and the reuters-21578 dataset indicate improved performance of the proposed classifier in comparison to other state-of-the-art methods on datasets with a small number of positive examples.}, } @article{tsay04, author = {jyh-jong tsay and jing-doo wang}, title = {
however these assumptions are often violated in practice, and poor performance can result.
it uses the internet as source of knowledge and extends it to categorize very short (less than 5 words) documents with reasonable accuracy.
this method also has the benefit to make feature selection implicit, since useless features for the categorization problem considered get a very small weight.
computer programs scan text in a document and apply a model that assigns the document to one or more prespecified topics.
this could be due to the fact that when a word along with its adjoining word - a phrase - is considered towards building a category profile, it could be a good discriminator.
we also explore the use of different kinds of codes, namely error-correcting codes, random codes, and domain and data-specific codes and give experimental results for each of them.
our substantial experimental results show that with several dimension reduction methods that are designed particularly for clustered data, higher efficiency for both training and testing can be achieved without sacrificing prediction accuracy of text classification even when the dimension of the input space is significantly reduced.} } @article{yang:2005:act, author = {hsin-chang yang and chung-hong lee}, title = {automatic category theme identification and hierarchy generation for chinese text categorization}, journal = {journal of intelligent information systems}, year = {2005}, volume = {25}, number = {1}, pages = {47--67}, abstract = {recently research on text mining has attracted lots of attention from both industrial and academic fields.
this paper presents an alter-native approach for the use of text classification methods for super-vised learning problems with numerical-valued features in which the numerical features are converted into bag-of-words features, thereby making them directly usable by text classification methods.
we first show that adaboost significantly outperforms another highly effective text filtering algorithm.
furthermore, linear and logistic regression are compared.}, } @article{furnkranz02, author = {johannes f{\"{u}}rnkranz}, title = {hyperlink ensembles: a case study in hypertext classification}, journal = {information fusion}, year = {2002}, number = {4}, volume = {3}, pages = {299--312}, url = {}, abstract = {
by applying tdfa to the document set that belongs to a given class and a set of documents that is misclassified as belonging to that class by an existent classifier, we can obtain features that take large values in the given class but small ones in other classes, as well as features that take large values in other classes but small ones in the given class.
using this method, we achieved high performance in text categorization both with small number and large numbers of labeled data.}, } @inproceedings{tan01, author = {
these generative models do not capture all the intricacies of text; however on some domains this technique substan- tially improves classification accuracy, especially when labeled data are sparse.
we also observed that extracting meta data from related web sites was extremely useful for improving classification accuracy in some of those domains.
the learning process constructs a relationship between an index term and the words relevant and irrelevant to it, based on the positive training set and negative training set, which are sample documents indexed by the index term, and those not indexed by it, respectively.
naively using terms in neighboring documents increased the error to 38\%; our hypertext classifier reduced it to 21\%.
particular attention is turned to a classifier's underlying feature set: aside from the standard feature types we introduce new features that are based on word frequency classes and that can be computed with minimum computational effort.
sample were more dramatic: the text classifier showed a 68\% error, whereas our hypertext classifier reduced this to just 21\%.}, } @article{chakrabarti98c, author =
meta-clustering algorithm, a new extension of the recent double clustering (dc) method of slonim and tishby that exhibited impressive performance on text categorization tasks.
we present detailed experimental results using naive bayes and support vector machines on the 20 newsgroups data set and a 3-level hierarchy of html documents collected from dmoz open directory.}, } @article{dhillon03, author = {inderjit dhillon and subramanyam mallela and rahul kumar}, title = {
hbc can reconstruct the original clusters more accurately than other non-probabilistic algorithms.
{2000}, pages = {176--183}, url = {http://www.acm.org/pubs/articles/proceedings/ir/345508/p176-hoashi/p176-hoashi.pdf}, abstract = {document filtering is a task to retrieve documents relevant to a user's profile from a flow of documents.
in contrast, programs that filter text streams, software that categorizes documents, agents which alert users, and many other ir systems must make decisions without human input or supervision.
documents are represented as feature-vectors that include n-grams instead of including only single words (unigrams) as commonly used when learning on text data.
text categorization must work reliably on all input, and thus must tolerate some level of these kinds of problems.
text classification from labeled and unlabeled documents using em}, journal = {machine learning}, year = {2000}, number = {2/3}, volume = {39}, pages = {103--134}, url = {http://www.cs.cmu.edu/~knigam/papers/emcat-mlj99.ps}, abstract = {this paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents.
we also found that passages have different degrees of contribution to the main topic(s), depending on their location in the test document.}, } @inproceedings{kindermann01, author = {j{\"{o}}rg kindermann and gerhard paa{{\ss}} and edda leopold}, title = {error correcting codes with optimized kullback-leibler distances for text categorization},
we provide experimental results demonstrating that the approach can significantly improve performance, and that it does not impair it.} } @incollection{cristianini01a, author = {huma lodhi and john shawe-taylor and nello cristianini and christopher j. watkins}, title = {discrete kernels for text categorisation}, booktitle = {
in concrete, we have evaluated a range of machine learning methods for the task (c4.5, naive bayes, part, support vector machines and rocchio), made cost sensitive through several methods (threshold optimization, instance weighting, and meta-cost).
each node's vocabulary is filtered and its words assigned weights with respect to the specific category.
as an example, the proposed scheme is applied to the classification of news articles into 3 categories: politics, sports, and business.
given a user's information need, some patterns in sentences such as combinations of query words, named entities and phrases, may contain more important and relevant information than single words.
in particular, we evaluate the vector and latent semantic analysis (lsa) methods, a classifier based on support vector machines (svm) and the k-nearest neighbor variations of the vector and lsa models.
we evaluate four different measures of subject similarity, derived from the web link structure, and determine how accurate they are in predicting document categories.
instead of using a randomly selected training set, the learner has access to a pool of unlabeled instances and can request the labels for some number of them.
our example problem is automatic document categorization using machine learning, where we want to identify documents relevant for the selected category.
{proceedings of the 11th usenix security symposium}, publisher = {}, editor = {dan boneh}, year = {2002}, address = {san francisco, us}, pages = {51--59}, url = {http://www.usenix.org/publications/library/proceedings/sec02/liao.html}, abstract = {a new approach, based on the k-nearest neighbor (knn) classifier, is used to classify program behavior as normal or intrusive.
the results show that our approach outperformed the bayesian independence classifier as measured by a metric that combines precision and recall measures.}, } @inproceedings{lam98, author = {wai lam and chao y. ho}, title = {using a generalized instance set for automatic text categorization}, booktitle = {proceedings of sigir-98, 21st acm international conference on research and development in information retrieval}, editor = {w. bruce croft and alistair moffat and van rijsbergen, cornelis j. and ross wilkinson and justin zobel}, publisher = {acm press, new york, us}, year = {1998}, address = {melbourne, au}, pages = {81--89}, url = {http://www.acm.org/pubs/articles/proceedings/ir/290941/p81-lam/p81-lam.pdf}, abstract = {
this is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available.
it also can be used for creating training documents.}, } @inproceedings{ko02, author = {youngjoong ko and jinwoo park and jungyun seo}, title =
{2000}, address = {san antonio, us}, pages = {195--204}, url = {ftp://ftp.cs.utexas.edu/pub/mooney/papers/libra-dl-00.ps.gz}, abstract = {recommender systems improve access to relevant products and information by making personalized suggestions based on previous examples of a user's likes and dislikes.
using a commercial medline product based on the vector space model, these physicians searched just as effectively as more experienced searchers using boolean searching.
we show that the advantage of using supervised clustering is that it is possible to have some control over the range of subjects that one would like the categorization system to address, but with a precise mathematical definition of each category.
using ig thresholding with a k-nearest neighbor classifier on the reuters corpus, removal of up to 98\% removal of unique terms actually yielded an improved classification accuracy (measured by average precision).
overall, we show that by using a few terms, categorisation accuracy can be improved substantially: unstructured leaf level categorisation can be improved by up to 8.6\%, while top-down hierarchical categorisation accuracy can be improved by up to 12\%.
we take advantage of this fact by using a hierarchically organized neural network, built up from a number of independent self-organizing maps in order to enable the true establishment of a document taxonomy.
both svm and knn are tested and compared on the 20-newsgroups database.
several different algorithms have been applied, and support vector machines (svm) have shown very good results.
we show that foil's performance can be improved by relation selection, a first order analog of feature selection.
the system includes a unique phrase help facility, which helps users find and add phrases and terms related to those in their query.},
in addition, the training time and scaling are also important concerns.
we study to what extent ocr errors affect stylistic text classification from scanned documents.
results show that the use of metadata is almost as good as the full-text version of papers.
we also observe that despite similar performances, different topical crawlers cover subspaces on the web with low overlap.} } @article{peng:2004:anb, author = {fuchun peng and dale schuurmans and shaojun wang}, title = {
abstract = {the world wide web is a vast source of information accessible to computers, but understandable only to humans.
it is demonstrated that the performance of such a classifier is further enhanced when employing the kernel derived from an appropriate hierarchic mixture model used for partitioning a document corpus rather than the kernel associated with a at non-hierarchic mixture model.
we compare different search heuristics and pruning methods known from various symbolic rule learners on a set of representative text categorization problems.
categorization of text images into content-oriented classes would be a useful capability in a variety of document handling systems.
the personal view constructor mines user interests and maps them to a class hierarchy (i.e., personal view).
the average precision of the document type-based search is 88.9\%, while the average precision of the keyword-based search is 31.2\%.
moreover, by ranking words and phrases in the citing documents according to expected entropy loss, we are able to accurately name clusters of web pages, even with very few positive examples.
svms achieve substantial improvements over the currently best performing methods and behave robustly over a variety of different learning tasks.
the best performance was achieved by the feature selection methods based on the feature scoring measure called odds ratio that is known from information retrieval.}, } @phdthesis{mladenic98c, author = {
experienced users can make effective use of such engines for tasks that can be solved by searching for tightly constrained keywords and phrases.
hyperlinks, html tags, category labels distributed over linked documents, and meta data extracted from related web sites all provide rich information for classifying hypertext documents.
most of them are irrelevant and others introduce noise which could mislead the classifiers.
the proposed performance measures consist of category similarity measures and distance based measures that consider the contributions of misclassified documents.
we obtained some encouraging results on two-category situations, and the results on the general problem seem reasonably impressive---in one case outstanding.
however, such a search is difficult with current search services, since these services only provide keyword-based search methods that are equivalent to narrowing down the target references according to domains.
"we present a kernel-based algorithm for hierarchical text classification where the documents are allowed to belong to more than one category at a time.
then, a machine learning method may be used in this simple bidimensional space to classify the documents.
using a collection factor, based on 87 per cent human consistency from other courses, the computer appears then to index with 90 per cent accuracy in this case.
"1265--1270", abstract = "evaluating text fragments for positive and negative subjective expressions and their strength can be important in applications such as single- or multi- document summarization, document ranking, data mining, etc.
experiments show the ngd kernel on the multinomial manifold to be effective for text classification, significantly outperforming standard kernels on the ambient euclidean space."
this paper shows that the accuracy of text classifiers trained with a small number of labeled documents can be improved by augmenting this small training set with a large pool of unlabeled documents.
it supports incremental training and online application of classifiers and predictive models to streams of textual, numeric, symbolic, and hybrid data records.
this analysis also revealed, for example, that information gain and chi-squared have correlated failures, and so they work poorly together.
we also propose an iterative web unit mining (iwum) method that first finds subgraphs of web pages using some knowledge about web site structure.
in natural language tasks like text categorization, we usually have an enormous amount of unlabeled data in addition to a small amount of labeled data.
the results suggest that the spatial and the temporal similarity measures need to be improved.
we describe a method for classifying pages of sequential ocr text documents into one of several assigned categories and suggest that taking into account contextual information provided by the whole page sequence can significantly improve classification accuracy.
we use a training set of texts with expert assigned categories to construct a network which approximately reflects the conditional probabilities of categories given a text.
the measurement is called the strength of a term and is a measure of how strongly the term`s occurrences correlate with the subjects of documents in the database.
our method scales well to large data sets, with numerous categories in large hierarchies.
in this paper, we describe a comparative study on techniques of feature transformation and classification to improve the accuracy of automatic text classification.
this neural model is based on significance vectors and benefits from the presentation of document clusters.
we found that a larger reference library is not necessarily better.
we show that although whirl is designed for more general similarity-based reasoning tasks, it is competitive with mature inductive classification systems on these classification tasks.
on compression-based text classification}, booktitle = {proceedings of ecir-05, 27th european conference on information retrieval}, publisher = {springer verlag}, editor = {david e. losada and juan m. fern{'{a}}ndez-luna}, address = {santiago de compostela, es}, year = {2005}, pages = {300--314}, url = {}, abstract = {compression-based text classification methods are easy to apply, requiring virtually no preprocessing of the data.
"it is well known that links are an important source of information when dealing with web collections.
for example, in hypertext classification, the labels of linked pages are highly correlated.
after 8 cycles the computer is found to have formed 9 groups consisting of about 50 per cent of documents that were also lumped together by professional indexers on the basis of subject content.
the originality of stretch lies principally in the possibility for unskilled users to define the indexes relevant to the document domains of their interest by simply presenting visual examples and applying reliable automatic information extraction methods (document classification, flexible reading strategies) to index the documents automatically, thus creating archives as desired.
the expert literary readers were found to assign significantly higher ratings to all versions of the manipulated poems than the novice readers.}, } @inproceedings{hayes88, author = {philip j. hayes and laura e. knecht and monica j. cellio}, title = {a news story categorization system}, booktitle = {proceedings of anlp-88, 2nd conference on applied natural language processing}, publisher =
while developing simpl, we also make a detailed experimental analysis of the cache performance of svms.}, } @inproceedings{chakrabarti97, author = {soumen chakrabarti and byron e. dom and rakesh agrawal and prabhakar raghavan}, title = {using taxonomy, discriminants, and signatures for navigating in text databases}, booktitle = {proceedings of vldb-97, 23rd international conference on very large data bases}, publisher = {morgan kaufmann publishers, san francisco, us}, editor = {matthias jarke and michael j. carey and klaus r. dittrich and frederick h. lochovsky and pericles loucopoulos and manfred a. jeusfeld}, year = {1997}, address = {athens, gr}, pages = {446--455}, url = {http://www.vldb.org/conf/1997/p446.pdf}, note = {an extended version appears as~\cite{chakrabarti98c}}, abstract = {
our example problem is automatic document categorization using machine learning, where we want to identify documents relevant for the selected category.
most importantly, this regularized estimation enables the model parameters to become sparse.
this knowledge is represented using publicly available ontologies that contain hundreds of thousands of concepts, such as the open directory; these ontologies are further enriched by several orders of magnitude through controlled web crawling.
furthermore, a learning feedback technique is introduced.
text categorization: the assignment of subject descriptors to magazine articles}, journal = {information processing and management}, pages = {841--861}, year = {2000}, number = {6}, volume = {36}, url = {}, abstract = {automatic text categorization is an important research area and has a potential for many text-based applications including text routing and filtering.
{acm press, new york, us}, editor = {}, year = {2003}, address = {new orleans, us}, pages = {232--239}, url = {http://doi.acm.org/10.1145/956863.956909}, abstract = {most existing studies of text classification assume that the training data are completely labeled.
"we enhance machine learning algorithms for text categorization with generated features based on domain-specific and common-sense knowledge.
the use of a vector space classifier and training method robust to large feature sets, combined with discarding of low frequency ocr output strings are the key to our approach.}, } @inproceedings{iwayama94, author = {makoto iwayama and takenobu tokunaga}, title = {a probabilistic model for text categorization: based on a single random variable with multiple values}, booktitle = {proceedings of anlp-94, 4th conference on applied natural language processing}, publisher = {association for computational linguistics, morristown, us}, editor = {}, year = {1994}, address = {
our first set of experiments applies the c4.5 decision tree induction algorithm to this learning task.
therefore, it is very costly to assign a category for them because humans investigate their contents.
we also investigate codes with optimized kl distance between the text categories which are merged in the code-words.
our investigation leads to conclude that association rule mining is a good and promising strategy for efficient automatic text categorization.}, } @inproceedings{zelikovitz00, author = {sarah zelikovitz and haym hirsh}, title = {
consistent with previous findings, we find that feature selection based on the labeled training set has little effect.
published in the ``lecture notes in computer science'' series, number 1910}, url = {http://link.springer.de/link/service/series/0558/papers/1910/19100490.pdf}, abstract = {supervised learning algorithms usually require large amounts of training data to learn reasonably accurate classifiers.
the text categorization module described in the paper provides a front-end filtering function for the larger dr-link text retrieval system (liddy and myaeng 1993).
"we present a kernel-based algorithm for hierarchical text classification where the documents are allowed to belong to more than one category at a time.
all three algorithms achieved high precision on both test sets, with the augmented relevancy signatures algorithm and the case-based algorithm reaching 100\% precision with over 60\% recall on one set.
if a category's recall exceeds its precision, the category is too strong and its level is reduced.
the system selects the category whose profile has the smallest distance to the document's profile.
we address these problems with a system for topical information space navigation that combines the query-based and taxonomic systems.
at the end, we also share the results of a survey conducted with this year’s cup participants.
for example, in a scientific paper domain, papers are related to each other via citation, and are also related to their authors.
context, as it applies to document similarity, can be accommodated by a well-defined procedure.
the indexing strategy first automatically classifies the document, thus avoiding pre-sorting, then locates and reads the information pertaining to the specific document class.
on the use of bernoulli mixture models for text classification}, journal = {pattern recognition}, year = {2002}, volume = {35}, number = {12}, pages = {2705--2710}, url = {}, abstract = {mixture modelling of class-conditional densities is a standard pattern recognition technique.
"the performance of search engines crucially depends on their ability to capture the meaning of a query most likely intended by the user.
it is shown that the performance of such a classifier is further enhanced when employing the kernel derived from an appropriate hierarchic mixture model used for partitioning a document corpus rather than the kernel associated with a flat non-hierarchic mixture model.
given these inputs, the system learns to extract information from other pages and hyperlinks on the web.
experiments in text classification}, booktitle = {proceedings of ecir-03, 25th european conference on information retrieval}, publisher = {springer verlag}, editor = {fabrizio sebastiani}, address = {pisa, it}, year = {2003}, pages = {41--56}, url = {http://link.springer.de/link/service/series/0558/papers/2633/26330041.pdf}, abstract = {link analysis methods have become popular for information access tasks, especially information retrieval, where the link information in a document collection is used to complement the traditionally used content information.
these feature vectors are then used as the input to the neural networks for classification.
{187--194}, url = {http://dlib.computer.org/conferen/icdm/1754/pdf/17540187.pdf}, abstract = {with the rapid growth of textual information available on the internet, having a good model for classifying and managing documents automatically is undoubtly important.
"common approaches to multi-label classification learn independent classifiers for each category, and employ ranking or thresholding schemes for classification.
within this paradigm, a general inductive process automatically builds a classifier by ``learning'', from a set of previously classified documents, the characteristics of one or more categories; the advantages are a very good effectiveness, a considerable savings in terms of expert manpower, and domain independence.
in the hierarchical case, a model is learned to distinguish a second-level category from other categories within the same top level.
relevance feedback on a small portion (0.05~0.2%) of the tdt5 test documents yielded significant performance improvements, measuring up to a 54% reduction in ctrk and a 20.9% increase in t11su (with b=0.1), compared to the results of the top-performing system in tdt2004 without relevance feedback information."
we found small advantages in accuracy for hierarchical models over flat models.
our second set of results examines the behavior of rankboost when it has to learn not only a ranking function but also all aspects of term weighting from raw data.
we formulate the problem of automated survey coding as a \emph{text categorization} problem, i.e.\ as the problem of learning, by means of supervised machine learning techniques, a model of the association between answers and codes from a training set of pre-coded answers, and applying the resulting model to the classification of new answers.
in previous work, such "distributional clustering" of features has been found to achieve improvements over feature selection in terms of classification accuracy, especially at lower number of features [2, 28].
profile allows the user to update on-line his profile and to check the discrepancy between his assessment and the prediction of relevance of the system.
machine learning schemes fare better because they automatically eliminate irrelevant features and concentrate on the most discriminating ones.}, } @inproceedings{frasconi01, author = {paolo frasconi and giovanni soda and alessandro vullo}, title = {
rough set (rs) theory can be applied to reducing the dimensionality of data used in if/ir tasks, by providing a measure of the information content of datasets with respect to a given classification.
we compare their effectiveness in classifying ocr texts and the corresponding correct ascii texts in two domains: business letters and abstracts of technical reports.
first, we explain how the extraction patterns can be generated automatically using only preclassified texts as input.
mh$^kr$}, an improved boosting algorithm, and its application to text categorization.
the results are compared to 1-of-n coding (i.e.\ one svm for each text category).
however, this perspective is not appropriate in the case of many digital libraries that offer as contents scanned and optically read books or magazines.
this paper describes a learning news agent hynet which uses hybrid neural network techniques for classifying news titles as they appear on an internet newswire.
the technique for machine-aided indexing (mai) developed at the defense documentation center (ddc) is illustrated on a randomly chosen abstract.
to verify our new method, we conducted experiments on two language newsgroup data sets: one written by english and the other written by korean.
from a machine learning point of view, this study was motivated by the size of the inspected data in such applications.
we approached the task with careful consideration of the specialized terminology and paid special attention to dealing with various forms of gene synonyms, so as to exhaustively locate the occurrences of the target gene.
similar to indices for relational data, taxonomies make search and access more efficient.
it is based on a hybrid case-based architecture, where two multilayer perceptrons are integrated into a case-based reasoner.
the average precision of the document type-based search is 88.9\%, while the average precision of the keyword-based search is 31.2\%.
{}, year = {2001}, address = {darmstadt, de}, publisher = {}, pages = {24--40}, url = {http://cis.paisley.ac.uk/vino-ci0/fisher_hierarchic.ps}, abstract = {this paper demonstrates that the probabilistic corpus model which emerges from the automatic or unsupervised hierarchical organisation of a document collection can be further exploited to create a kernel which boosts the performance of state-of-the-art support vector machine document classifiers.
we extracted the words around the gene occurrences and used them to represent the gene for go domain code annotation.
our methodology makes use of a well-known corpus of transcribed and topic-labeled speech (the switchboard corpus), and involves an interesting double use of the boostexter learning algorithm.
this calls for using a feature selection method, not only to reduce the number of features but also to increase the sparsity of document vectors.
autoslog is a dictionary construction system that has been shown to substantially reduce the time required for knowledge engineering by learning extraction patterns automatically.
we show how to train these models effectively, and how to use approximate probabilistic inference over the learned model for collective classification of multiple related entities.
it is shown that the performance of such a classifier is further enhanced when employing the kernel derived from an appropriate hierarchic mixture model used for partitioning a document corpus rather than the kernel associated with a flat non-hierarchic mixture model.
even though the learning ability and computational complexity of training in support vector machines may be independent of the dimension of the feature space, reducing computational complexity is an essential issue to efficiently handle a large number of terms in practical applications of text classification.
feature generation is accomplished through contextual analysis of document text, implicitly performing word sense disambiguation.
the texts represent short web-page descriptions from the dmoz open directory web-page ontology.
experiments show that feature selection using weights from linear svms yields better classification performance than other feature weighting methods when combined with the three explored learning algorithms.
the essential formula is cue validity borrowed from cognitive psychology, and used to select from all possible single word based features, the best predictors of a given category.
experiments on two real-world spidering tasks show a three-fold improvement in spidering efficiency over traditional breadth-first search, and up to a two-fold improvement over reinforcement learning with immediate reward only.}, } @article{ribeironeto01, author = {berthier ribeiro-neto and alberto h.f. laender and luciano r. {de lima}}, title = {
our approach to text classification uses case-based reasoning to represent natural language contexts that can be used to classify texts with extremely high precision.
in comparison to the previously proposed agglomerative strategies our divisive algorithm is much faster and achieves comparable or higher classification accuracies.
show positive results on modestly sized datasets.
in this paper we propose an integration of a selforganizing map and semantic networks from wordnet for a text classification task using the new reuters news corpus.
the hierarchical structure is initially used to train different second-level classifiers.
our approach has the advantage of a very fast training phase, and the rules of the classifier generated are easy to understand and manually tuneable.
we demonstrate that, although this categorization problem is quite different from 'topical' text classification, certain categories of messages can nonetheless be detected with high precision (above 80%) and reasonable recall (above 50%) using existing text-classification learning methods.
experimental results show that the improvements to active learning require less than two-thirds as many labeled training examples as previous qbc approaches, and that the combination of em and active learning requires only slightly more than half as many labeled training examples to achieve the same accuracy as either the improved active learning or em alone.}, } @inproceedings{mccallum98b, author = {andrew k. mccallum and ronald rosenfeld and tom m. mitchell and andrew y. ng}, title = {improving text classification by shrinkage in a hierarchy of classes}, booktitle = {proceedings of icml-98, 15th international conference on machine learning}, editor = {jude w. shavlik}, year = {1998}, address = {
in earlier work we described an approach for converting numerical features into bags of tokens so that text classification methods can be applied to numerical classification problems, and showed that the resulting learning methods are competitive with traditional numerical classification methods.
with this extended model, we also have improved the well-known probabilistic classification method based on the bernoulli document generation model.
second, they are inaccurate, because of mistakes made by these indexers as well as the difficulties users have in choosing keywords for their queries, and the ambiguity a keyword may have.
using four corpora from the topic detection and tracking (tdt) forum and the text retrieval conferences (trec) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.
the same techniques can be used to determine if a document is fiction or non-fiction with approximately 98 per cent accuracy.}, } @inproceedings{kosmynin96, author = {arkadi kosmynin and ian davidson}, title = {using background contextual knowledge for documents representation}, booktitle = {proceedings of podp-96, 3rd international workshop on principles of document processing}, editor = {charles k. nicholas and derick wood}, year = {1996}, address = {palo alto, ca}, pages = {123--133}, publisher = {springer verlag, heidelberg, de}, note =
the other is that there are no training data for this classification problem.
the recently introduced information bottleneck method provides an information theoretic framework, for extracting features of one variable, that are relevant for the values of another variable.
by emphasizing the category discrimination capability of features, the paper firstly puts forward a new weighting scheme tf*idf*ig.
alexios chouchoulas and qiang shen}, title = {rough set-aided keyword reduction for text categorization}, journal = {applied artificial intelligence}, pages = {843--873}, year = {2001}, volume = {15}, number = {9}, url = {}, abstract = {the volume of electronically stored information increases exponentially as the state of the art progresses.
several different algorithms have been applied, and support vector machines (svm) have shown very good results.
with the use of suitable dimensionality reduction techniques and efficient algorithms, both llsf and expnet successfully scaled to this very large problem with a result significantly outperforming word-matching and other automatic learning methods applied to the same corpus.}, } @article{yang96b, author =
our implementation uses an inverted file to store the trained term structures of each newsgroup, and uses a list similar to the inverted file to buffer the newly arrival articles, for efficient routing and updating purposes.
we also show that whirl can be efficiently used to select from a large pool of unlabeled items those that can be classified correctly with high confidence.}, } @article{cohen99, author = {william w. cohen and yoram singer}, title = {context-sensitive learning methods for text categorization}, journal = {acm transactions on information systems}, year = {1999}, volume = {17}, number = {2}, pages = {141--173}, url = {http://www.acm.org/pubs/articles/journals/tois/1999-17-2/p141-cohen/p141-cohen.pdf}, abstract =
moreover, bwm-nbs exhibits the strong stability in categorization performance.}, } @inproceedings{xue04, author =
"we demonstrate that it is possible to perform automatic sentiment classification in the very noisy domain of customer feedback data.
the combination with content-based methods can further improve the results, but too much noise may be introduced, since the text of web pages is a much less reliable source of information.
the method harnesses reliability indicators---variables that provide a valuable signal about the performance of classifiers in different situations.
this new synthesis of neural networks, learning and information retrieval techniques allows us to scale up to a real-world task and demonstrates a lot of potential for hybrid plausibility networks for semantic text routing agents on the internet.}, } @inproceedings{wermter99a, author =
we describe a classifier of email queries, which executes text categorization by topic.
we also show that less aggressive clustering sometimes results in improved classification accuracy over classification without clustering.}, } @inproceedings{bao01, author = {yongguang bao and satoshi aoyama and xiaoyong du and kazutaka yamada and naohiro ishii}, title = {a rough set-based hybrid method to text categorization}, booktitle = {proceedings of wise-01, 2nd international conference on web information systems engineering}, editor = {m. tamer {\"o}zsu and hans-j{\"{o}}rg schek and katsumi tanaka and yanchun zhang and yahiko kambayashi}, publisher =
for these reasons, tcfp can be a useful classifier in the areas, which need a fast and high-performance text categorization task.}, } @inproceedings{ko02a, author =
a system that performs text categorization aims to assign appropriate categories from a predefined classification scheme to incoming documents.
the paper shows that the accuracy of a naive bayes text classifier can be significantly improved by taking advantage of a hierarchy of classes.
as output, the system evaluates a set of weighted hypotheses about the type of the actual letter.
when using a state-of-the-art classifier, knn, the average accuracy is 96.40\%, outperforming all the other systems evaluated on the same collection, including the traditional term-word by knn (88.52\%); sleeping-experts (82.22\%); sparse phrase by four-word sleeping-experts (86.34\%); and boolean combinations of words by ripper (87.54\%).
then, a filtering system with the neural network integrated into it was used to filter the medical documents and this performance was compared with the filtering results achieved using the baseline system.
we also show that addressing named entities preferentially is useful only in certain situations.
this collection is tailored for automating the attribution of international patent classification codes to patent applications and is made publicly available for future research work.
the model is based on the concept of ``uncertainty sampling'', a technique that allows for relevance feedback both on relevant and non relevant documents.
our key insight is that many of the data sources have their own categorization, and classification accuracy can be improved by factoring in the implicit information in these source categorizations.
using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on tdt5, trec10 and trec11.
searching for documents by their type or genre is a natural way to enhance the effectiveness of document retrieval.
in particular, we develop metrics that estimate the difficulty of a dataset by examining the host directory structure.
we show that by using large feature vectors in combination with feature reduction, we can train linear support vector machines that achieve high classification accuracy on data that present classification challenges even for a human annotator.
in addition, we investigate alternative classification and evaluation methods, and the effects that secondary features have on indoor/outdoor classification.
in brief, it tries to avoid considering features whose discrimination capability is sufficiently covered by already selected features, reducing in size the set of the features used to characterize the document set.
for the application of the air/phys system a large-scale dictionary containing more than 600000 word-descriptor relations resp.
our findings suggest that the best results occur when using the very brief descriptions of the {{\sc yahoo!}}\ categorized entries; these brief descriptions are provided either by the entries' submitters or by the {{\sc yahoo!}}\ human indexers and accompany most {{\sc yahoo!}}\-indexed entries.}, } @inproceedings{lai01, author = {kwok-yin lai and wai lam}, title = {meta-learning models for automatic textual document categorization}, booktitle = {proceedings of pakdd-01, 5th pacific-asia conferenece on knowledge discovery and data mining}, editor = {david cheung and qing li and graham williams}, year = {2001}, publisher = {springer verlag, heidelberg, de}, address = {hong kong, cn}, note = {
a stochastic decision list is an ordered sequence of if-then rules, and our method can be viewed as a rule-based method for text clsssification having advantages of readability and refinability of acquired knowledge.
in addition, we investigate alternative classification and evaluation methods, and the effects that secondary features have on indoor/outdoor classification.
the other unusual features of our research are the longevity of our agents and the fact that they undergo a continual training process; feedback from the user enables the agent to adapt to the user's long-term information requirements.}, } @inproceedings{cohen95, author = {william w. cohen}, title = {
this approach is well suited to learning in hypertext domains because its statistical component allows it to characterize text in terms of word frequencies, whereas its relational component is able to describe how neighboring documents are related to each other by hyperlinks that connect them.
however these assumptions are often violated in practice, and poor performance can result.
however, no significant gain was obtained in the digital library.
automated information filtering (if) and information retrieval (ir) systems are therefore acquiring rapidly increasing prominence.
experimental results indicate that, at the same level of vector sparsity, feature selection based on svm normals yields better classification performance than odds ratio- or information gainbased feature selection when linear svm classifiers are used.}, } @inproceedings{bruckner97, author = {t. bruckner}, title = {
the use of stw allows the terms that are distributed most differently in the positive and negative examples of the categories of interest to be weighted highest.
for example, google uses the text in citing documents (documents that link to the target document) for search.
this dissertation addresses the knowledge-engineering bottleneck for a natural language processing task called ``information extraction''.
this suggests that df thresholding, the simplest method with the lowest cost in computation, can be reliably used instead of ig or chi when the computation of these measures are too expensive.
text categorization experiments supported a number of predictions of the concept learning model about properties of phrasal representations, including dimensionality properties not previously measured for text representations.
for both data sets, boosting trees and svms had acceptable test performance in terms of accuracy and speed.
morgan kaufmann publishers, san francisco, us}, url = {http://www.robotics.stanford.edu/~stong/papers/tong_koller_ml00.ps.gz}, abstract = {support vector machines have met with significant success in numerous real-world learning tasks.
tampere, fi}, year = {2002}, pages = {137--144}, url = {http://doi.acm.org/10.1145/564376.564402}, abstract = {to improve performance in text categorization, it is important to extract distinctive features for each class.
in spite of these differences, both ripper and sleeping experts perform extremely well across a wide variety of categorization problems, generally outperforming previously applied learning methods.
the crucial question of the quality of automatic classification is treated at considerable length, and empirical data are introduced to support the hypothesis that classification quality improves as more information about each document is used for input to the classification program.
{tokyo, jp}, year = {2001}, pages = {307--314}, url = {http://www.afnlp.org/nlprs2001/pdf/0079-01.pdf}, abstract = {this paper presents a method for incorporating natural language processing into existing text categorization procedures.
we show that although whirl is designed for more general similarity-based reasoning tasks, it is competitive with mature inductive classification systems on these classification tasks.
in our experiments we demonstrate significantly superior performance both over a single classifier as well as over the use of the traditional weighted-sum voting approach.
conversely, a category's level is increased to strengthen it if its precision exceeds its recall.
the system could potentially scale up to an operational size of 10 million words of text per year - the equivalent of a dozen bibles or a third of the encyclopedia britannica.
text classification algorithms were used to automatically classify arbitrary search results into an existing category structure on-the-fly.
they allow us to construct compact feature sets with few elements, with which a satisfactory genre diversi- fication is achieved.
in this paper we are interested in a more general formulation where documents are organized as page sequences, as naturally occurring in digital libraries of scanned books and magazines.
as a first step toward automatic go annotation, we aim to assign go domain codes given a specific gene and an article in which the gene appears, which is one of the task challenges at the trec 2004 genomics track.
in three tests the percent of results categorized for five representative queries was high enough to suggest practical benefits: general web search (76-90\%), government web search (39-100\%), and the bureau of labor statistics website (48-94\%).
again, as in [14], the addition of nlp capabilities also suggested a different application of existing methods in revised forms.
class labels can also be assigned based on known document types, or can be defined by the user.
on the other hand, a discriminative measure is proposed for term selection and is combined with the plu-based likelihood ratio to determine the text category.
extensive experiments using two benchmarks and a large real-life collection are conducted.
in this paper, we present a weight adjusted k-nearest neighbor (waknn) classification that learns feature weights based on a greedy hill climbing technique.
although we usually estimate the model so that it completely satisfies the equality constraints on feature expectations with the me method, complete satisfaction leads to undesirable overfitting, especially for sparse features, since the constraints derived from a limited amount of training data are always uncertain.
thus, our method seems to be well suited for heterogeneous document collections.}, } @article{klingbiel73, author = {paul h. klingbiel}, title = {machine-aided indexing of technical literature}, journal =
an automatic document classification system using pattern recognition techniques}, booktitle = {proceedings of asis-78, 41st
we demonstrate the effectiveness of our categorization approach using two real-world document collections from the medline database.
with the increasing availability of lexical resources in electronic form (including lexical databases (ldbs), machine readable dictionaries, etc.), there is an interesting opportunity for the integration of them in learning-based atc.
we conclude that our techniques permit automatic categorisation using very large train-ing collections, vocabularies, and numbers of categories.}, } @article{shin01, author =
our approach integrates wordnet information with two training approaches through the vector space model.
in contrast, the text categorization task allows much cleaner determination of text representation properties.
our results show that overall, svms and k-nn lsa perform better than the other methods, in a statistically significant way.}, } @incollection{caropreso01, author = {maria fernanda caropreso and stan matwin and fabrizio sebastiani}, title = {a learner-independent evaluation of the usefulness of statistical phrases for automated text categorization}, year = {2001}, booktitle = {text databases and document management: theory and practice}, editor = {amita g. chin}, publisher = {idea group publishing}, address =
genre classification means to discriminate between documents by means of their form, their style, or their targeted audience.
scoring rules can further take advantage of the hierarchy by considering only second-level categories that exceed a threshold at the top level.
experimental results indicate that our method outperforms not only the method using distributions based on hard clustering, but also the method using word-based distributions and the method based on cosine-similarity.}, } @article{li98a, author = {li, yong h. and jain, anil k.}, title = {classification of text documents}, journal = {the computer journal}, year = {1998}, volume = {41}, number = {8}, pages =
the discrimination between informative keywords and functional keywords is not crisp.
however, many systems set a relatively high threshold to reduce retrieval of non-relevant documents, which results in the ignorance of many relevant documents.
for example, google uses the text in citing documents (documents that link to the target document) for search.
a relatively high degree of accuracy was achieved by the supervised method, however, classification accuracy varied across classes.
we investigate this problem by looking at the task of designing kernels for hypertext classification, where both words and links information can be exploited.
first, unlabeled data can hurt performance in domains where the generative modeling assumptions are too strongly violated.
this work provides an important insight on which measures derived from links are more appropriate to compare web documents and how these measures can be combined with content-based algorithms to improve the effectiveness of web classification.}, } @inproceedings{caldon03, author = {patrick caldon}, title =
the learning approach presented is attribute-efficient and, therefore, appropriate for domains having very large number of attributes.
for example, in email classification, it is possible to use instance representations that consider not only the text of each message, but also numerical-valued features such as the length of the message or the time of day at which it was sent.
the kind of application that the text categorization shell, tcs, can produce is characterized.
the method is quantitative analysis of the glosses of such definitions that these terms are given in on- dictionaries, and on the use of the resulting term representations semi-supervised term classification.
the text categorization (tc) is the automated assignment of text documents to predefined categories based on document contents.
in addition, we investigate alternative classification and evaluation methods, and the effect that a secondary feature can have on indoor/outdoor classification.
in our learning experiments naive bayesian classifier was used on text data.
the experimental results show that our new approaches give better results for both micro-averaged f1 and macro-averaged f1 scores.}, } @article{lehnert94, author =
the recently introduced information bottleneck method provides an information theoretic framework, for extracting features of one variable, that are relevant for the values of another variable.
examples are agents for locating information on world wide web and usenet news filtering agents.
by observing that people who search the web with the same queries often click on different, but related documents together, we draw implicit links between web pages that are clicked after the same queries.
we are also given a training corpus of documents already placed in one or more categories.
"automatic classification of data items, based on training samples, can be boosted by considering the neighborhood of data items in a graph structure (e.g., neighboring documents in a hyperlink environment or co-authors and their publications for bibliographic data entries).
such engines can build giant indices that let you quickly retrieve the set of all web pages containing a given word or string.
we do not believe our results are specific to ppm.
we demonstrate that the idc algorithm is especially advantageous when the data exhibits high attribute noise.
the experimental result on the fbis document corpus shows that the atf algorithm outperforms the pure eg (exponentiated-gradient) algorithm.}, } @inproceedings{yu99, author = {
"it is well known that web-page classification can be enhanced by using hyperlinks that provide linkages between web pages.
we learn a hierarchical classifier that is guided by a layered semantic hierarchy of answer types, and eventually classifies questions into finegrained classes.
an effective information filtering system is one that provides the exact information that fulfills user's interests with the minimum effort by the user to describe it.
the results show that filtering significantly improves the recall of the method, and that also has the effect of significantly improving the overall performance.} } @article{combarromdrm05, title =
"it is well known that web-page classification can be enhanced by using hyperlinks that provide linkages between web pages.
our experimental results on three real world data sets show that we achieve substantial improvements over standard naive bayes classification, while also achieving state of the art performance that competes with the best known methods in these cases.}, } @inproceedings{peng03a, author = {fuchun peng and dale schuurmans and shaojun wang}, title = {language and task independent text categorization with simple language models}, booktitle = {proceedings of hlt-03, 3rd human language technology conference}, publisher = {},
the experimental results show that the proposed method outperforms a direct application of a statistical learner often used for subject classification.
the levels are adjusted in order to obtain a balance between recall and precision for each category.
this model allows us to simultaneously take into account structure and content information.
the algorithms are compared on learning speed and error rate.
this hybrid approach of neural selforganization and symbolic hypernym relationships is successful to achieve good classification rates on 100,000 full-text news articles.
taken as a whole, the set of web pages lacks a unifying structure and shows far more authoring style and content variation than that seen in traditional text-document collections.
{information processing and management}, year = {2006}, volume = {42}, number = {1}, pages = {155--165}, abstract = {most previous works of feature selection emphasized only the reduction of high dimensionality of the feature space.
in our classifier, web documents are represented by n-grams (n$\leq 4$) that are easy to be extracted.
a tc system for chinese texts using words as features is implemented.
experiments on 20 newsgroups (20ng), reuters corpus volume 1 (rcv1) and open directory project (odp) data show that ocfs is consistently better than ig and chi with smaller computation time especially when the reduced dimension is extremely small."
we show accurate results on a large collection of free-form questions used in trec 10.}, } @inproceedings{li03, author = {cong li and ji-rong wen and hang li}, title =
performances of all three classifiers degrade from the reuters collection to the ohsumed collection, but decision forest remains to be superior.}, } @inproceedings{chen01, author
published in the ``lecture notes in computer science'' series, number 2997}, pages = {197--208}, url = {http://springerlink.metapress.com/openurl.asp?genre=article&issn=0302-9743&volume=2997&spage=197}, abstract = {high dimensionality of feature space is a main obstacle for text categorization (tc).
in many important text classification problems, acquiring class labels for training documents is costly, while gathering large quantities of unlabeled data is cheap.
this resulted in slightly reduced performance.
we report on computational experience using this procedure.
although both the contents and time information of news articles are helpful to red, most researches focus on the utilization of the contents of news articles.
however, the phrasal pre-processing and pattern matching methods that seem to work for categorization have the disadvantage of requiring a fair amount of knowledge-encoding by human beings.
we present experimental results showing that employing our active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings.}, note =
in contrast, the text categorization task allows much cleaner determination of text representation properties.
these experiments suggest that stopword lists and stemming algorithms may remove or conflate many words that could be used to create more effective indexing terms.}, } @inproceedings{riloff96, author = {ellen riloff}, title = {using learned extraction patterns for text classification}, booktitle = {connectionist, statistical, and symbolic approaches to learning for natural language processing}, editor = {stefan wermter and ellen riloff and gabriele scheler}, pages = {275--289}, year = {1996}, publisher = {springer verlag, heidelberg, de}, note = {
in general, based on the current recall and precision performance, as well as the detailed analysis, we show that recurrent plausibility networks hold a lot of potential for developing learning and robust newswire agents for the internet.}, } @inproceedings{wibowo02, author = {wahyu wibowo and hugh e. williams}, title = {simple and accurate feature selection for hierarchical categorisation},
published in the ``lecture notes in computer science'' series, number 2175}, editor = {floriana esposito}, year = {2001}, pages = {320--325}, address = {bari, it}, url = {http://link.springer.de/link/service/series/0558/papers/2175/21750320.pdf}, abstract = {feature selection and weighting are the primary activity of every learning algorithm for text classification.
documents are typically represented by sparse vectors under the vector space model, where each word in the vocabulary is mapped to one coordinate axis and its occurrence in the document gives rise to one nonzero component in the vector representing that document.
our algorithm has the following features: it does not need any natural language processing technique and it is robust for noisy data.
our approach uses a natural language processing task called information extraction as a basis for high-precision text classification.
this corpus has been developed by eads company in the context of a large web site filtering application.}, } @article{denoyer04, author = {ludovic denoyer and patrick gallinari}, title = {bayesian network model for semi-structured document classification}, journal = {information processing and management}, year = {2004}, volume = {40}, number = {5}, pages = {807--827}, url = {}, abstract = {}, } @article{devel01, author =
when user interests change, in pva, not only the contents, but also the structure of the user profile are modified to adapt to the changes.
neural networks perform equally well with either set of features and can take advantage of the additional information available when both feature sets are used as input.}, } @article{schutze98, author = {hinrich sch{\"{u}}tze}, title = {automatic word sense discrimination}, journal = {computational linguistics}, year =
with term-descriptor rules from the dictionary, descriptor indications are formed.
pos tagging and recognition of proper nouns received a specific experimental attention and provided significant effects on measured accuracy.}, } @inproceedings{basili01, author = {roberto basili and alessandro moschitti and maria t. pazienza}, title = {nlp-driven ir: evaluating performances over a text classification task}, booktitle = {proceeding of ijcai-01, 17th international joint conference on artificial intelligence}, editor = {bernhard nebel}, address = {
the system selects the best collections for the query.
since in both tasks text as a sequence of words is of crucial importance, propositional learners have strong limitations, although viewing learning for tc and ie as inductive logic programming (ilp) problems is obvious, most approaches rather use proprietary formalisms.
for a slim fraction of all documents (0.77\% for category coding and 1.4\% for subcategory coding), the algorithm makes assignments that are clearly incorrect.
we make available detailed, per-category experimental results, as well as corrected versions of the category assignments and taxonomy structures, via online appendices.}, } @inproceedings{lewis91, author = {lewis, david d.}, title = {data extraction as text categorization: an experiment with the {muc-3} corpus.}, booktitle = {proceedings of muc-3, 3rd message understanding conference}, editor = {}, publisher =
experiments with the muc corpus suggest that case-based text classification can achieve very high levels of precision and outperforms our previous algorithms based on relevancy signatures.}, } @article{riloff94, author = {ellen riloff and wendy lehnert}, title =
from a machine learning point of view, this study was motivated by the size of the inspected data in such applications.
however, such a search is difficult with current search services, since these services only provide keyword-based search methods that are equivalent to narrowing down the target references according to domains.
comparative analysis shows that the performances achieved are relatively close to the best tc models (e.g. support vector machines).}, } @inproceedings{moschitti04, author = {alessandro moschitti and roberto basili}, title = {complex linguistic features for text classification: a comprehensive study}, booktitle = {proceedings of ecir-04, 26th european conference on information retrieval research}, editor = {sharon mcdonald and john tait}, year = {2004}, address = {sunderland, uk}, publisher = {springer verlag, heidelberg, de}, note = {
the performance of the system using the neural network classifier was generally satisfactory and, as expected, the filtering performance varied with regard to the accuracy rates of classes.}, } @inproceedings{moulinier96, author = {isabelle moulinier and gailius ra{\u{s}}kinis and jean-gabriel ganascia}, title = {
informative keywords are the ones which reflect the contents of a document.
we modify the query-by-committee (qbc) method of active learning to use the unlabeled pool for explicitly estimating document density when selecting examples for labeling.
but in cases where many features are highly redundant with each other, we must utilize other means, for example, more complex dependence models such as bayesian network classifiers.
from this perspective, bns was the top single choice for all goals except precision, for which information gain yielded the best result most often.
we believe that database categorization can be a potentially effective technique for good database selection, especially in the internet environment, where short queries are usually submitted.
the levels are adjusted in order to obtain a balance between recall and precision for each category.
statistical methods for categorization, on the other hand, are easy to implement and require little or no human customization.
we show that optimal effectiveness occurs when using only a small proportion of the indexing terms available, and that effectiveness peaks at a higher feature set size and lower effectiveness level for a syntactic phrase indexing than for word-based indexing.
this process requires less effort than providing words with no help or manual labeling of documents.
lexical databases accumulate information on the lexical items of one or several languages.
extensive experiments reported in the paper shows that this new weighting method improves significantly the classification accuracy as measured on many categorization tasks."
a combination of all the above results in a multi-stage ned system that performs much better than baseline single-stage ned systems.}, } @inproceedings{kwok98, author = {james t. kwok}, title = {automated text categorization using support vector machine}, booktitle = {proceedings of iconip'98, 5th international conference on neural information processing}, editor = {}, year = {1998}, address = {kitakyushu, jp}, pages = {347--351}, url = {http://www.comp.hkbu.edu.hk/7ejamesk/papers/iconip98.ps.gz}, abstract = {
six nonjudgmental criteria are used in testing the hypothesis for 100 keyword lists (each list representing a document) for a series of computer runs in which the number of words per document is increased progressively from 12 to 36.
results show that the text in citing documents, when available, often has greater discriminative and descriptive power than the text in the target document itself.
the input space was gradually increased by using mutual information (mi) filtering and part-of-speech (pos) filtering, which determine the portion of words that are appropriate for learning from the information-theoretic and the linguistic perspectives, respectively.
when few examples are available, we observe that accuracy is sensitive to k and that best k tends to increase with training size.
this allows the individual models for each topic on the second level to focus on finer discriminations within the group.
to retain the information in the structure, we have developed a structured vector model, which represents a document with a structured vector, whose elements can be either terms or other structured vectors.
experiments show the ngd kernel on the multinomial manifold to be effective for text classification, significantly outperforming standard kernels on the ambient euclidean space."
expnet is used for relevance ranking of candidate categories of an arbitrary text in the case of text categorization, and for relevance ranking of documents via categories in the case of text retrieval.
about 70\% of the web-documents are assigned to their true genre; note in this connection that no genre classification benchmark for web pages has been published so far.}, } @article{mladenic03, author = {
while knn and aram yield better performances than svm on small and clean data sets, svm and aram significantly outperformed knn on noisy data.
however, in the web space, hyperlinks are usually sparse, noisy and thus in many situations can only provide limited help in classification.
bayesian independence classifiers and k-nearest-neighbor classifiers were trained to assign scores to manually-graded essays.
in addition, the evaluation results given by the kddcup 2005 organizer confirm the effectiveness of our proposed approaches.
occurring in the collection, the subset of $r'\ll r$ features that are most useful for compactly representing the meaning of the documents.
these maps were then analyzed to obtain the category themes and their structure.
improving linear classifier for chinese text categorization}, journal = {information processing and management}, year = {2004}, volume = {40}, number = {2}, pages = {223--237}, url = {}, abstract = {}, } @article{turney00, author = {peter d. turney}, title = {
the high number of features is reduced by feature subset selection and additionally by using `stop-list', pruning low-frequency features and using a short description of each document given in the hierarchy instead of using the document itself.
experiments over real-world text corpus are carried out, which validates the effectiveness and efficiency of the proposed approach.
we experiment with various types of descriptions for the {{\sc yahoo!}}\ categories and the webpages to be categorized.
in contrast to ranking systems, binary text classification systems may need to produce result sets of any size, requiring that sampling be used to estimate their effectiveness.
our method scales well to large data sets, with numerous categories in large hierarchies.
the back data stores the information about keywords: the frequency for each category, the number of documents for each category.
one problem is that it is difficult to create the labeled training documents.
the problem of automatically determining the gender of a document's author would appear to be a more subtle problem than those of categorization by topic or authorship attribution.
in addition, aram predictive accuracy and learning efficiency can be improved by incorporating a set of rules derived from the reuters category description.
we provide an approach for automatically building the implicit links between web pages using web query logs, together with a thorough comparison between the uses of implicit and explicit links in web page classification.
in some applications, there might be human knowledge available that, in principle, could compensate for the lack of data.
in contrast, software for text categorization, message filtering, textual data mining, and related tasks is less common.
our evaluation using the 1996 reuters corpus which consists of 806,791 documents shows that automatically constructing hierarchy improves classification accuracy."
the best of all connectionist architectures presented here achieves near human performance (79.1\%).
we introduce a model for learning patterns for text categorization (the lpt-model) that does not rely on an attribute-value representation of documents but represents documents essentially "as they are".
the second set of experiments applies the genex algorithm to the task.
the results show multi-agent classification can achieve promising classification results
a dynamic probabilistic model to visualise topic evolution in text streams}, journal = {journal of intelligent information systems}, year = {2002}, note = {special issue on automated text categorization}, volume = {18}, number = {2/3}, pages = {107--125}, url = {http://www.wkap.nl/article.pdf?391242}, abstract = {
when an existing document is used as an exemplar, the completeness and accuracy with which topically related documents are retrieved is comparable to that of the best existing systems.
to facilitate web object searching and organizing, in this paper, we propose a novel approach to web object indexing, by discovering its inherent structure information with existed domain knowledge.
automated information filtering (if) and information retrieval (ir) systems are therefore acquiring rapidly increasing prominence.
in contrast, for the reuters collection, we only achieve mediocre results.
we present results from text classification experiments that compare relevancy signatures, which use local linguistic context, with corresponding indexing terms that do not.
results obtained from evaluation show that the integration of wordnet clearly outperforms training approaches, and that an integrated technique can effectively address the classification of low frequency categories.}, } @inproceedings{delima98, author = {de lima, luciano r. and laender, alberto h. and ribeiro-neto, berthier a.}, title = {
the graph neighborhood is taken into consideration to exploit locality patterns while at the same time avoiding overfitting.
our experiments on the reuters-21578 benchmark show that boosting is not effective in improving the performance of the base classifiers on common categories.
the module evaluates a large incoming stream of documents to determine which documents are sufficiently similar to a profile at the broad subject level to warrant more refined representation and matching.
the method is evaluated using the umls metathesaurus as the underlying hierarchical structure, and the ohsumed test set of medline records.
specifically, the classifier provides an efficient information extraction and takes the meaning of words into consideration.
this architecture acquires its language model and dictionary adaptively and hence avoids handcoding of either.
after identifying the weakness and strength of each technique, we propose a new technique known as the generalized instance set (gis) algorithm by unifying the strengths of lnn and linear classifiers and adapting to characteristics of text categorization problems.
in assigning 124 documents to 9 categories, there were 97 cases of agreement with professional indexers.
this is achieved by the exploitation of the a priori domain knowledge available, that there are relatively homogeneous temporal segments in the data stream.
experiments on a large-scale chinese document collection with 71,674 texts show that the f1 metric of categorization performance of bwm-nbs gets to 94.9\% in the best case, which is 26.4\% higher than that of tf*idf, 19.1\% higher than that of tf*idf*ig, and 5.8\% higher than that of bwm under the same condition.
in addition, unlike other feature selection models --- which typically require different feature selection parameters for categories at different hierarchical levels --- our technique works equally well for all categories in a hierarchical structure.
we have performed experiments which results show that the ideas we describe are promising and deserve further investigation.}, } @inproceedings{gomez02a, author = {g{\'o}mez-hidalgo, jos{\'e} m.}, title = {
because of efficiency, the latter is more suitable for text data such as web documents.
we consider a family of models that take into account the fact that relevant documents may contain irrelevant passages; the originality of the model is that it does not explicitly segment documents but rather considers all possible segmentations in its final score.
the document collection is trained by a self-organizing map to form two feature maps.
"this paper shows how citation-based information and structural content (e.g., title, abstract) can be combined to improve classification of text documents into predefined categories.
furthermore we demonstrate that the hsom is able to map large text collections in a semantically meaningful way and therefore allows a ``semantic browsing'' of text databases.}, } @article{paijmans98, author = {paijmans, hans}, title = {
therefore, this method can be used in areas where low-cost text categorization is needed.
furthermore, detailed analysis of the retrieval performance on each individual test query is provided.}, } @inproceedings{lang95, author = {ken lang}, title = {{\sc newsweeder}: learning to filter netnews}, booktitle = {proceedings of icml-95, 12th international conference on machine learning}, editor = {armand prieditis and stuart j. russell}, address = {lake tahoe, us}, pages = {331--339}, year = {1995}, publisher =
we also incorporate a batch updating scheme to periodically do maintenance on the term structures of the news database after training.
a new reference collection of patent documents for training and testing automated categorization systems is established and described in detail.
the main problem is to identify what words are best suited to classify the documents in such a way as to discriminate between them.
when combined with the classification power of the svm, this method yields high performance in text categorization.
"we demonstrate that it is possible to perform automatic sentiment classification in the very noisy domain of customer feedback data.
the paper in hand presents results from a user study on web genre usefulness as well as results from the construction of a genre classifier using discriminant analysis, neural network learning, and support vector machines.
experiments on the newsgroups and the reuters-21578 dataset indicate improved performance of the proposed classifier in comparison to other state-of-the-art methods on datasets with a small number of positive examples.}, } @article{tsay04, author = {jyh-jong tsay and jing-doo wang}, title = {
documents are represented as feature-vectors that include n-grams instead of including only single words (unigrams) as commonly used when learning on text data.
the new approach is based on extracting patterns, in the form of two logical expressions, which are defined on various features (indexing terms) of the documents.
empirical results indicate that these classifiers are comparable with the best text classification systems.
simpl uses efficient sequential scans and sorts, and is comparable in speed and memory scalability to widely-used naive bayes (nb) classifiers, but it beats nb accuracy decisively.
this is still true even for the maximum entropy (me) method, whose flexible modeling capability has alleviated data sparseness more successfully than the other probabilistic models in many nlp tasks.
we show on three text categorization data sets that this approach can rescue what would otherwise be disastrously bad training situations, producing much more effective classifiers."
these are based on ranking text sequences by their cross-entropy calculated using a fixed order character-based markov model adapted from the ppm text compression algorithm.
at their core, our algorithms employ recently developed modified finite newton techniques.
{tzigov chark, bl}, pages = {}, year = {1997}, url = {http://xxx.unizar.es/ps/cmp-lg/9709007}, abstract = {automatic text categorization (tc) is a complex and useful task for many natural language applications, and is usually performed through the use of a set of manually classified documents, a training collection.
this distribution can be estimated very efficiently from the data which immediately leads to an efficient model selection algorithm.
we show that our specialization of the naive bayes classifier is considerably more accurate (7 to 29\% absolute increase in accuracy) than a standard implementation.
the idea is to identify sub-topics of the original classes which can help improve the categorization process.
we propose a theory of genres as bundles of facets, which correlate with various surface cues, and argue that genre detection based on surface cues is as successful as detection based on deeper structural properties.}, } @inproceedings{khmelev03, author = {dmitry v. khmelev and william j. teahan}, title = {a repetition based measure for verification of text collections and for text categorization}, booktitle = {proceedings of sigir-03, 26th acm international conference on research and development in information retrieval}, editor = {jamie callan and gordon cormack and charles clarke and david hawking and alan smeaton}, publisher = {acm press, new york, us}, address = {toronto, ca}, year = {2003}, pages = {104--110}, url = {http://doi.acm.org/10.1145/860435.860456}, abstract = {
the algorithm checks the context surrounding the target noun against that of previously observed instances and chooses the sense for which the most evidence is found, where evidence consists of a set of orthographic, syntactic, and lexical features.
published in the ``lecture notes in computer science'' series, number 2004}, pages = {423--436}, url = {http://link.springer.de/link/service/series/0558/papers/2004/20040423.pdf}, abstract = {a new way of representing texts written in natural language is introduced, as a conditional probability distribution at the letter level learned with a variable length markov model called adaptive context tree model.
the high number of features is reduced by taking into account the hierarchical structure and using feature subset selection based on the method known from information retrieval.
to this end, we propose a set of style markers including analysis-level measures that represent the way in which the input text has been analyzed and capture useful stylistic information without additional cost.
we demonstrate that precision and recall can be significantly improved by solving the categorization problem taking hierarchy into account.
by assuming that documents are created by a parametric generative model, expectation-maximization (em) finds local maximum a posteriori models and classifiers from all the data|labeled and unlabeled.
the experimental results show that the system using a simple classifier achieved 95.31\% accuracy.
this paper presents an alter-native approach for the use of text classification methods for super-vised learning problems with numerical-valued features in which the numerical features are converted into bag-of-words features, thereby making them directly usable by text classification methods.
this paper introduces a new effective model for text categorization with great corpus (more or less 1 million documents).
benchmark experiments showed that their predictive performance were roughly comparable, especially on clean and well organized data sets.
this algorithm alleviates the problem of local minimum in the tsvm optimization procedure while also being computationally attractive.
integrating linguistic resources in tc through wsd}, journal = {computers and the humanities}, year = {2001}, number = {2}, volume = {35}, pages =
experiments show the bin-based method is highly competitive with other current methods.
relevance feedback on a small portion (0.05~0.2%) of the tdt5 test documents yielded significant performance improvements, measuring up to a 54% reduction in ctrk and a 20.9% increase in t11su (with b=0.1), compared to the results of the top-performing system in tdt2004 without relevance feedback information."
in comparison against a recently proposed technique that appears to be the only one of the kind, we obtained up to 18.5\% of improvement in effectiveness while reducing the processing time dramatically.
we introduce appropriate cost-sensitive measures, investigating at the same time the effect of attribute-set size, training-corpus size, lemmatization, and stop lists, issues that have not been explored in previous experiments.
most of all, tcfp is about one hundred times faster than k-nn.
our experiments on a variety of text categorization tasks indicate that there is significant potential in improving classifier performance by feature reweighting, beyond that achieved via selective sampling alone (standard active learning) if we have access to an oracle that can point to the important (most predictive) features.
different transformations of input data: stemming, normalization, logtf and idf, together with dimensionality reduction, are found to have a statistically significant improving or degrading effect on classification performance measured by classical metrics -- accuracy, precision, recall, f$_1$ and f$_2$. the emphasis of the study is not on determining the best document representation which corresponds to each classifier, but rather on describing the effects of every individual transformation on classification, together with their mutual relationships.} } @inproceedings{radovanovic:2006:ibd, author = {milo\v{s} radovanovi\'c and mirjana ivanovi\'c}, title = {interactions between document representation and feature selection in text categorization}, booktitle = {proceedings of dexa-06, 17th international conference on database and expert systems applications}, year = {2006}, pages = {489--498}, series = {lecture notes in computer science}, volume = {4080}, address = {krakow, poland}, publisher =
given a user's information need, some patterns in sentences such as combinations of query words, named entities and phrases, may contain more important and relevant information than single words.
however, it significantly degrades precision when ambiguity arises, i.e., when there exist more than one candidate category to which a document can be assigned.
these results help explain why co-training algorithms are both discriminative in nature and robust to the assumptions of their embedded classifiers.}, } @phdthesis{nigam01, author = {kamal nigam}, title = {using unlabeled data to improve text classification}, school = {computer science department, carnegie mellon university}, address = {pittsburgh, us}, year = {2001}, url = {http://www-2.cs.cmu.edu/~knigam/papers/thesis-nigam.pdf}, abstract = {
improving text categorization using the importance of sentences}, journal = {information processing and management}, year = {2004}, volume = {40}, number = {1}, pages = {65--79}, url = {}, abstract = {}, } @article{ko04a, author
documents are typically represented by sparse vectors under the vector space model, where each word in the vocabulary is mapped to one coordinate axis and its occurrence in the document gives rise to one nonzero component in the vector representing that document.
text categorization is then cast as the problem of finding coordinate transformations that reflects the inherent similarity from the data.
therefore, the proposed novelty detection approach focuses on the identification of previously unseen query-related patterns in sentences.
our experimental results demonstrate that the technique of refining the training set reduces from one-third to two-thirds of the storage.
second, they permit straightforward application of sophisticated smoothing techniques from statistical language modeling, which allows one to obtain better parameter estimates than the standard laplace smoothing used in naive bayes classification.
in the framework of multi-domain text-to-speech synthesis it is essential to (i) design a hierarchically structured database for allowing several domains in the same speech corpus and (ii) include a text classification module that, at run time, assigns the input sentences to a domain or set of domains from the database.
yet, for many text classification tasks, providing labeled training documents is expensive, while unlabeled documents are readily available in large quantities.
in our literature survey, we have found that the existing hierarchical classification experiments used a variety of measures to evaluate performance.
for the first of them, a collection of articles from pc week magazine, the addition of word-pairs increases micro-averaged breakeven accuracy by more than 6\% point from a baseline accuracy (without pairs) of around 40\%.
evaluation against the reuters-21578 collection shows both techniques have levels of performance that approach benchmark methods, and the ability of one of the classifiers to produce realistic measures of confidence in its decisions is shown to be useful for prioritizing relevant documents.}, } @inproceedings{lee02c, author = {kang hyuk lee and judy kay and byeong ho kang and uwe rosebrock}, title = {
the goal of the research described here is to automatically create a computer understandable world wide knowledge base whose content mirrors that of the world wide web.
we have developed a text classifier that misclassifies only 13\% of the documents in the reuters benchmark; this is comparable to the best results ever obtained.
we describe a methodology and system (named accio) for automatically acquiring labeled datasets for text categorization from the world wide web, by capitalizing on the body of knowledge encoded in the structure of existing hierarchical directories such as the open directory.
we demonstrate the potential of these networks using an 82,339 word corpus from the reuters newswire, reaching recall and precision rates above 92\%.
these include feature sets that are constructed by latent semantic indexing, `local dictionaries' in the form of the words that score highest in frequency in positive class examples and feature sets that are constructed by relevance feedback strategies such as j.j. rocchio's (1971) feedback algorithm or genetic algorithms.
moreover, a new measure for the evaluation of system performances has been introduced in order to compare three different techniques (flat, hierarchical with proper training sets, hierarchical with hierarchical training sets).
comparisons with traditional rocchio's algorithm adapted for text categorization, as well as flat neural network classifiers are provided.
the measurement is called the strength of a term and is a measure of how strongly the term`s occurrences correlate with the subjects of documents in the database.
in our learning experiments, for each of the subproblems, naive bayesian classifier was used on text data.
our approach to classification is based on "visual similarity" of layout structure and is implemented by building a supervised classifier, given examples of each class.
this ability depends on both text representation and feature filtering.
the dimension of the feature vectors is then reduced by linear transformation, keeping the essential information.
{acm press, new york, us}, editor = {henrique paques and ling liu and david grossman}, year = {2001}, address = {atlanta, us}, pages = {105--113}, url = {http://www.stanford.edu/~krist/papers/cikm2001.pdf}, abstract = {documents are commonly categorized into hierarchies of topics, such as the ones maintained by yahoo! and the open directory project, in order to facilitate browsing and other interactive forms of information retrieval.
{hanley and belfus}, year = {1996}, address = {washington, us}, pages = {358--362}, url = {http://www.cs.cmu.edu/afs/cs/user/yiming/www/courses/bibliography/papers/scamc96.ps}, abstract = {whether or not high accuracy classification methods can be scaled to large applications is crucial for the ultimate usefulness of such methods in text categorization.
the effectiveness of the llsf mapping and the significant improvement over alternative approaches was evident in the tests.}, } @article{yang94, author = {yiming yang and christopher g. chute}, title = {
we develop a framework to incorporate unlabeled data in the error-correcting output coding (ecoc) setup by decomposing multiclass problems into multiple binary problems and then use co-training to learn the individual binary classification problems.
the rationale behind our proposal is that taking into account contextual information provided by the whole page sequence can help disambiguation and improves single page classification accuracy.
we treat a document as a set of phrases, which the learning algorithm must learn to classify as positive or negative examples of keyphrases.
{ieee computer society press, los alamitos, us}, address = {maebashi city, jp}, year = {2002}, pages = {187--194}, url = {http://dlib.computer.org/conferen/icdm/1754/pdf/17540187.pdf}, abstract = {with the rapid growth of textual information available on the internet, having a good model for classifying and managing documents automatically is undoubtly important.
experimental results show that (1) using an enhanced representation of web documents is crucial for an effective categorisation of web documents, and (2) a theoretical interpretation of the k-nearest neighbour classifier gives us improvement over the standard k-nearest neighbour classifier.}, } @inproceedings{goldberg95, author =
defined as the activity of automatically % % building, by means of machine learning techniques, automatic text % % classifiers, i.e. systems capable of assigning to a text % % document one or more thematic categories from a predefined set.
to specify a user's problem solving task, we introduce the concept of document types that directly relate to the problem solving tasks; with this approach, users can easily designate problem solving tasks.
in this paper a system for analysis and automatic indexing of imaged documents for high-volume applications is described.
the results are analyzed from multiple goal perspectives-accuracy, f-measure, precision, and recall-since each is appropriate in different situations.
the architecture relies on hidden markov models whose emissions are bag-of-words resulting from a multinomial word event model, as in the generative portion of the naive bayes classifier.
text categorization using feature projections}, booktitle = {proceedings of coling-02, the 19th international conference on computational linguistics}, year = {2002}, editor = {}, pages = {}, address = {taipei, tw}, url = {http://acl.ldc.upenn.edu/coling2002/proceedings/data/area-28/co-269.pdf}, abstract = {automatic text categorization is a problem of automatically assigning text documents to predefined categories.
even when multi-labels are sparse, the models improve subset classification error by as much as 40%."
in three tests the percent of results categorized for five representative queries was high enough to suggest practical benefits: general web search (76-90\%), government web search (39-100\%), and the bureau of labor statistics website (48-94\%).
the set of all indications from a document leading to the same descriptor is called a relevance description.
we describe a method for improving the classification of short text strings using a combination of labeled training data plus a secondary corpus of unlabeled but related longer documents.
for each category, all corresponding document texts from the training sample are concatenated to a megadocument, which is indexed using standard methods.
machine learning schemes fare better because they automatically eliminate irrelevant features and concentrate on the most discriminating ones.}, } @inproceedings{frasconi01, author = {paolo frasconi and giovanni soda and alessandro vullo}, title = {
the rule base defines what categories the application can assign to texts and contains rules that make the categorization decisions for particular texts.
therefore, the indexing system needs some fault-tolerating features.
the essential formula is cue validity borrowed from cognitive psychology, and used to select from all possible single word-based features the `best` predictors of a given category.
in an effort to deal more effectively with this large vocabulary and improve information processing, a method of focus has been developed which allows one to classify terms based on a measure of their importance in describing the content of the documents in which they occur.
first, terms (single words or phrases) are identified in the document text.
experimental results indicate that our method outperforms not only the method using distributions based on hard clustering, but also the method using word-based distributions and the method based on cosine-similarity.}, } @article{li98a, author = {li, yong h. and jain, anil k.}, title = {classification of text documents}, journal = {the computer journal}, year = {1998}, volume = {41}, number = {8}, pages = {537--546}, url = {}, abstract = {
we analyze the relative utility of document text, and the text in citing documents near the citation, for classification and description.
users can search for pants or classify patent text.
with term-descriptor rules from the dictionary, descriptor indications are formed.
they are accurate, robust, and quick to apply to test instances.
we find that our simple corrections result in a fast algorithm that is competitive with state-of-the-art text classification algorithms such as the support vector machine.}, } @inproceedings{rennie99, author = {jason rennie and andrew k. mccallum}, title = {using reinforcement learning to spider the web efficiently}, booktitle = {proceedings of icml-99, 16th international conference on machine learning}, editor = {ivan bratko and saso dzeroski}, year = {1999}, address = {bled, sl}, publisher = {morgan kaufmann publishers, san francisco, us}, pages = {335--343}, url = {http://www.watson.org/~jrennie/papers/icml99.ps.gz}, abstract = {consider the task of exploring the web in order to find pages of a particular kind or on a particular topic.
this combination is based on the notion of classifier reliability and presented gains of up to 14\% in micro-averaged f1 in the web collection.
learning}, in which information on the membership of training documents in categories is used.
despite a limited number of training examples, combining an effective feature selection with the chi(2) learning algorithm for training the text classifier results in an adequate categorization of new magazine articles.}, } @inproceedings{mooney00, author = {raymond j. mooney and loriene roy}, title = {content-based book recommending using learning for text categorization}, booktitle = {proceedings of dl-00, 5th acm conference on digital libraries}, editor = {}, publisher = {acm press, new york, us}, year = {2000}, address = {san antonio, us}, pages = {195--204}, url = {ftp://ftp.cs.utexas.edu/pub/mooney/papers/libra-dl-00.ps.gz}, abstract = {recommender systems improve access to relevant products and information by making personalized suggestions based on previous examples of a user's likes and dislikes.
the hypothesis that cdm's performance will exceed two non domain specific algorithms, bayesian classification and decision tree learners, is empirically tested.}, } @inproceedings{gomez02, author = {g{\'o}mez-hidalgo, jos{\'e} m. and de buenaga rodr{\'{\i}}guez, jos{\'e} m. and ureña l{\'o}pez, luis a. and mart{\'{\i}}n valdivia, maria t. and garc{\'{\i}}a vega, manuel}, title = {integrating lexical knowledge in learning-based text categorization}, booktitle = {proceedings of jadt-02, 6th international conference on the statistical analysis of textual data}, publisher = {}, editor = {}, address =
in an unsupervised setting, our models produced coherent clusters with a very natural interpretation, even for instance types that do not have any attributes.}, } @inproceedings{taskar02, author = {ben taskar and pieter abbeel and daphne koller}, title = {discriminative probabilistic models of relational data}, booktitle = {proceedings of uai-02, 18th conference on uncertainty in artificial intelligence}, year = {2002}, address = {edmonton, ca}, pages = {485--492}, publisher = {morgan kaufmann publishers, san francisco, us}, editor = {}, url = {}, abstract = {
in addition, a comparison of the two filtering methods clarified that pos filtering on svms consistently outperformed mi filtering, which indicates that svms cannot find irrelevant parts of speech.
the determination of categories and their hierarchical structures were most done by human experts.
we present the results of experimenting with theseus, a classifier that exploits this technique.}, } @inproceedings{avancini03, author = {henri avancini and alberto lavelli and bernardo magnini and fabrizio sebastiani and roberto zanoli}, title = {expanding domain-specific lexicons by term categorization}, year = {2003}, booktitle = {proceedings of sac-03, 18th acm symposium on applied computing}, address = {melbourne, us}, publisher =
usually, these bigrams are likely to survive for their strength of discriminating documents after the process of feature selection.
experimental comparison given on real-world data collected from web users shows that characteristics of the problem domain and machine learning algorithm should be considered when feature scoring measure is selected.
one way is to ask the user to provide them, which is difficult because the user usually can only give a few words (which are insufficient for accurate learning).
svms performed best when using binary features.
this problem can be tackled since a couple of recent learners (ripper and scar) do not require a preprocessing step.
since tcfp algorithm is very simple, its implementation and training process can be done very easily.
finally, we measure the accuracy achieved with all words and all hm pairs combined, which turns out to be only marginally above the baseline.
in a second experiment, we ignored nouns, verbs and adjectives and replaced them by grammatical tags and bigrams.
our experiments using the webkb dataset showed that iwum improves the overall classification performance and works very well on the more structured parts of a web site.}, } @inproceedings{taghva00, author = {taghva, kazem and nartker, thomas a. and julie borsack and steven lumos and allen condit and ron young}, title = {
we show that the benefit of using a first-order representation in this domain is relatively modest; in particular, the performance difference between flipper and foil and their propositional counterparts is quite small, compared to the differences between foil and flipper.
all the presented experiments are based on unrestricted text downloaded from the world wide web without any manual text preprocessing or text sampling.
in this work we reproduce these results and go further to show that when the training sample is small word clusters can yield significant improvement in classification accuracy (up to 18\%) over the performance using the words directly.}, } @inproceedings{soucy01, author = {pascal soucy and guy w. mineau}, title = {a simple feature selection method for text classification}, booktitle = {proceeding of ijcai-01, 17th international joint conference on artificial intelligence}, editor = {bernhard nebel}, address = {
we conclude that memory-based anti-spam filtering for mailing lists is practically feasible, especially when combined with additional safety nets.
experiments also show that the proportion of meaningful terms extracted from training data is relative to the classification accuracy in outside testing.}, } @inproceedings{lam01, author = {wai lam and kwok-yin lai}, title = {a meta-learning approach for text categorization}, booktitle = {proceedings of sigir-01, 24th acm international conference on research and development in information retrieval}, editor =
the concept learning model suggests that the poor statistical characteristics of a syntactic indexing phrase representation negate its dsirable semantic characteristics.
conventional methods such as decision trees have had competitive, but not optimal, predictive performance.
we describe the new representations and try to justify our hypothesis that they could improve the performance of a rule based learner.
the hypothesis that cdm's performance will exceed two non domain specific algorithms, bayesian classification and decision tree learners, is empirically tested.}, } @inproceedings{gomez02, author = {g{\'o}mez-hidalgo, jos{\'e} m. and de buenaga rodr{\'{\i}}guez, jos{\'e} m. and ureña l{\'o}pez, luis a. and mart{\'{\i}}n valdivia, maria t. and garc{\'{\i}}a vega, manuel}, title =
the importance of different features is reported.
lexical databases accumulate information on the lexical items of one or several languages.
% % % % concerning urls from which to download on-line copies of the % % papers, where possible i have included urls with unrestricted % % access (e.g. home pages of authors).
we investigate this problem by looking at the task of designing kernels for hypertext classification, where both words and links information can be exploited.
we use a novel stop word identification method to automatically generate domain-specific stoplists which are much larger than a conventional domain-independent stoplist.
the hypernym relation in wordnet supplements the neural model in classification.
finally, the effectiveness of term distributions to improve classification accuracy is explored with regard to the training set size and the number of classes.}, } @article{leung97, author = {chi-hong leung and wing-kay kan}, title = {a statistical learning approach to automatic indexing of controlled index terms}, journal = {journal of the american society for information science}, year = {1997}, number = {1}, pages = {55--67}, volume = {48}, url = {http://www3.interscience.wiley.com/cgi-bin/fulltext?id=39602&placebo=ie.pdf}, abstract = {
the accuracy of the supervised classifier was established by comparing its performance with a baseline system that uses human classification information.
in empirical tests, it consistently showed more than 10 points f-measure improvement for each of four reuters categories tested." } @inproceedings{sindhwani:2006:lss, author = "sindhwani, vikas and keerthi, s. sathiya", title =
in this work we investigate the usefulness of {\em $n$-grams} for document indexing in text categorization (tc).
we developed a user interface that organizes web search results into hierarchical categories.
we use the technique of genetic programming (gp), (koza and rice 1992), to evolve classifying agents.
using the results evaluated on the other versions of reuters which exclude the unlabelled documents, the performance of twelve methods are compared directly or indirectly.
the representations are evaluated using the ripper learning algorithm on the reuters-21578 and digitrad test corpora.
by doing so, model probability and classification accuracy come into correspondence, allowing unlabeled data to improve classification performance.
http://doi.acm.org/10.1145/585058.585079}, abstract = {categorisation of digital documents is useful for organisation and retrieval.
we show that such unlabeled background knowledge can greatly decrease error rates, particularly if the number of examples or the size of the strings in the training set is small.
the texts to be indexed are abstracts written in english.
to use traditional feature-vector- based learning methods, one could treat the presence or ab-sence of a word as a boolean feature and use these binary-valued features together with the numerical features.
as more information becomes available on-line, intelligent information retrieval will be crucial in order to navigate the information highway efficiently and effectively.
however, the complexity of natural languages and the extremely high dimensionality of the feature space of documents have made this classification problem very difficult.
we report the results of systematic experimentation of these two methods performed on the standard {\sc reuters-21578} benchmark.}, } @article{gale93, author = {
we report on experiments with different kernels for both of these implementations and with different representations of the data, including binary vectors, tf-idf representation and a modification called ``hadamard" representation.
{hanley and belfus}, year = {1996}, address = {washington, us}, pages = {358--362}, url = {http://www.cs.cmu.edu/afs/cs/user/yiming/www/courses/bibliography/papers/scamc96.ps}, abstract = {whether or not high accuracy classification methods can be scaled to large applications is crucial for the ultimate usefulness of such methods in text categorization.
in this article, we apply ml to text-database analyses and knowledge acquisitions from text databases.
this distribution can be estimated very efficiently from the data which immediately leads to an efficient model selection algorithm.
{using text classification to predict the gene knockout behaviour of {s.\ cerevisiae}}, booktitle = {proceedings of apbc-03, 1st asia-pacific bioinformatics conference}, editor = {yi-ping p. chen}, publisher =
the use of stw allows the terms that are distributed most differently in the positive and negative examples of the categories of interest to be weighted highest.
{18}, number = {3}, pages = {311--322}, url = {http://www.kluweronline.com/issn/0924-669x}, abstract = {this paper reports our comparative evaluation of three machine learning methods, namely k nearest neighbor (knn), supportvector machines (svm), and adaptive resonance associative map (aram) for chinese document categorization.
successfully managing information means being able to find relevant new information and to correctly integrate it with pre-existing knowledge.
particular attention is turned to a classifier's underlying feature set: aside from the standard feature types we introduce new features that are based on word frequency classes and that can be computed with minimum computational effort.
the multi-classifier approach helps us leverage all the relevant textual features and meta data, and appears to generalize to related classification tasks.}, } @inproceedings{amati96, author = {gianni amati and daniela d'aloisi and vittorio giannini and flavio ubaldini}, title = {an integrated system for filtering news and managing distributed data}, booktitle = {proceedings of pakm-96, 1st international conference on practical aspects of knowledge management}, editor = {}, publisher = {}, year = {1996}, pages = {}, note = {an extended version appears as~\cite{amati97b}},
= {dublin, ie}, pages = {31--40}, url = {http://www.acm.org/pubs/articles/proceedings/ir/188490/p31-hoch/p31-hoch.pdf}, abstract = {this paper presents the infoclas system applying statistical methods of information retrieval for the classification of german business letters into corresponding message types such as order, offer, enclosure, etc.
in the work presented here, the decision tree learning algorithm c4.5 is applied on a corpus of financial news articles.
moreover, the procedure of defining analysis-level markers can be followed in order to extract useful stylistic information using existing text processing tools.}, } @inproceedings{stamatatos00a, author = {efstathios stamatatos and nikos fakotakis and george kokkinakis}, title = {
a system that performs text categorization aims to assign appropriate categories from a predefined classification scheme to incoming documents.
in particular, this method is most similar to naive bayes; it generally performs at least as well as naive bayes, and sometimes better.}, } @inproceedings{sable02, author = {carl sable and kathleen mckeown and kenneth w. church}, title = {nlp found helpful (at least for one text categorization task)}, booktitle = {proceedings of emnlp-02, conference on empirical methods in natural language processing}, address = {philadelphia, us}, year = {2002}, publisher = {association for computational linguistics}, pages = {172--179}, } @inproceedings{sable99, author =
the use of bigrams to enhance text categorization}, journal = {information processing and management}, year = {2002}, volume = {38}, number = {4}, pages = {529--546}, url = {
for solving these kinds of problems, neural networks have the advantage of extracting the underlying relationships between the input data and the output classes automatically.
similarity in word space is based on second-order co-occurrence: two tokens (or contexts) of the ambiguous word are assigned to the same sense cluster if the words they co-occur with in turn occur with similar words in a training corpus.
in this work we present a comparative study of digital library citations and web links, in the context of automatic text classification.
we show how these networks can support text routing of noisy newswire titles according to different given categories.
this makes it a good candidate for a general "install-and-forget" term selection mechanism.
the author presents the core technology of teklis, the results on the filtering and routing tasks and a discussion of the insights gained through participation in the exercise.}, } @inproceedings{cai03, author = {lijuan cai and thomas hofmann}, title = {
a combination of different classifiers produced better results than any single type of classifier.
for this task, an indexing dictionary with rules for mapping terms from the text onto descriptors is required, which can be derived automatically from a set of manually indexed documents.
because text domains present much irrelevant information, effective feature reduction is essential to improve classifiers' effectiveness and efficiency.
by allowing this interactivity in the clustering process, c-evolve achieves considerably higher clustering accuracy (10 to 20\% absolute increase in our experiments) than the popular k-means and agglomerative clustering methods.}, } @inproceedings{agrawal01, author = {rakesh agrawal and ramakrishnan srikant}, title = {
our experiments clearly indicate that automatic categorization improves the retrieval performance compared with no categorization.
the use of a controlled vocabulary allows for a more consistent description of corporate documents, and promotes easier access by people across the company.
when a probabilistic text categorization is extended to a cluster-based one, the use of hbc offers better performance than the use of non-probabilistic algorithms.}, } @inproceedings{iwazume96, author = {michiaki iwazume and hideaki takeda and toyoaki nishida}, title = {ontology-based information gathering and text categorization from the internet}, booktitle = {
furthermore, we focus on the fact that document collections lend themselves naturally to a hierarchical structure defined by the subject matter of the documents.
we conclude that memory-based anti-spam filtering for mailing lists is practically feasible, especially when combined with additional safety nets.
the impact of rule insertion is most significant for categories with a small number of relevant documents.}, } @article{tan02, author =
we tested this method on both retrieval and indexing with a set of medline documents which has been used by other information retrieval systems for evaluations.
training algorithms are derived for both cases, and illustrated on real data by clustering news stories and categorising newsgroup messages.
in this paper, we suggest, for text categorization, the integration of external wordnet lexical information to supplement training data for a semi-supervised clustering algorithm which can learn from both training and test documents to classify new unseen documents.
we find that error correcting codes perform better than 1-of-n coding with increasing code length.
we show that sequential minimal optimization can be used in training wmsvm.
however, we find that ppm does not compete with the published state of the art in the use of machine learning for text categorization.
this paper presents the results of the application of an instance-based learning algorithm k-nearest neighbor method on feature projections (k-nnfp) to text categorization and compares it with k-nearest neighbor classifier (k-nn).
because they do not exploit dependencies between labels, such techniques are only well-suited to problems in which categories are independent.
text categorization is the natural consequence of such automatic category generation process.}, } @inproceedings{yang00c, author = {hsin-chang yang and chung-hong lee}, title = {automatic category structure generation and categorization of chinese text documents},
in this paper, we present a formal description of the feature quantity, as well as some illustrative examples of applying such a quantity to different types of information retrieval tasks: representative term selection and text categorization.}, } @inproceedings{aizawa01, author = {akiko aizawa}, title = {linguistic techniques to improve the performance of automatic text categorization}, booktitle = {proceedings of nlprs-01, 6th natural language processing pacific rim symposium}, editor = {}, publisher = {}, address = {tokyo, jp}, year = {2001}, pages = {307--314}, url = {http://www.afnlp.org/nlprs2001/pdf/0079-01.pdf}, abstract = {this paper presents a method for incorporating natural language processing into existing text categorization procedures.
"retrospective news event detection (red) is defined as the discovery of previously unidentified events in historical news corpus.
we discovered that misclassifications by the citation-link based classifiers are in fact difficult cases, hard to classify even for humans."
in brief, it tries to avoid considering features whose discrimination capability is sufficiently covered by already selected features, reducing in size the set of the features used to characterize the document set.
we classify movie reviews using features based upon these taxonomies combined with standard ``bag-of-words'' features, and report state-of-the-art accuracy of 90.2%.
the ddc method for subject indexing is very close to operational status for a data base which grows at the rate of two million words of text per year.}, } @inproceedings{klinkenberg00, author = {ralf klinkenberg and thorsten joachims}, title = {
we do not believe our results are specific to ppm.
reducing the dimensionality, or selecting a good subset of features, without sacrificing accuracy, is of great importance for neural networks to be successfully applied to the area.
as an example, the proposed scheme is applied to the classification of news articles into 3 categories: politics, sports, and business.
on their own, the new representations are not found to produce significant performance improvements.
in this model, borrowed from information retrieval, documents are represented as a vector where each component is associated with a particular word from the vocabulary.
the categories in such problems usually are neither conditionally independent from each other nor mutually exclusive, therefore it is not trivial to directly employ state-of-the-art classification algorithms without losing information of relation among categories.
then, a machine learning method may be used in this simple bidimensional space to classify the documents.
"2005", pages = "294--303", abstract = "a web object is defined to represent any meaningful object embedded in web pages (e.g. images, music) or pointed to by hyperlinks (e.g. downloadable files).
on both corpora the algorithms we present outperform adaptations to topic-ranking of rocchio's algorithm and the perceptron algorithm.
we show that our specialization of the naive bayes classifier is considerably more accurate (7 to 29\% absolute increase in accuracy) than a standard implementation.
but our experiments on human subjects indicate that human feedback on feature relevance can identify a sufficient proportion (65%) of the most relevant features.
the msdn corpus is collected from an online news website maintained by the min-sheng daily news, taiwan.
extensive tests of the model suggest its application as a viable and robust tool for large scale text classification and filtering, as well as a basic module for more complex scenarios.}, } @article{bayer98, author = {thomas bayer and ulrich kressel and heike mogg-schneider and ingrid renz}, title = {categorizing paper documents.
morgan kaufmann publishers, san francisco, us}, url = {}, abstract = {kernel methods like support vector machines have successfully been used for text categorization.
we present a method to detect automatically pornographic content on the web.
the graph neighborhood is taken into consideration to exploit locality patterns while at the same time avoiding overfitting.
we start from the observation that support vector machines, one of the best text categorization methods cannot scale up to handle the large document collections involved in many real word problems.
we also used the generated structure to categorize text documents.
additionally, the computation procedure can be improved to locate the sets of duplicate or plagiarised documents.
we find that term selection and our modified lsi representations lead to similar topic spotting performance, and that this performance is equal to or better than other published results on the same corpus.}, } @mastersthesis{wiener95a, author = {erik d. wiener}, title = {
in the first phase, a multilayer feed-forward neural network was trained to classify medical documents in the area of cell biology.
experimental evidence indicates that k-nnfp is superior to k-nn in terms of classification accuracy in the presence of irrelevant features in many real world domains.}, } @inproceedings{yi00, author = {jeonghee yi and neel sundaresan}, title = {
in our literature survey, we have found that the existing hierarchical classification experiments used a variety of measures to evaluate performance.
knowledge base whose content mirrors that of the world wide web.
finally, it shows how the performance of multinomial naive bayes can be improved using locally weighted learning.
the best performance was achieved by the feature selection based on a feature scoring measure known from information retrieval called odds ratio and using relatively small number of features.}, } @inproceedings{mladenic04, author = {
in contrast to this approach we use the frequencies of occurrence of the most frequent words of the entire written language.
we present results comparing the performance of boostexter and a number of other text-categorization algorithms on a variety of tasks.
such methods can provide automatic indexing and keyword assignment capabilities that are at least as accurate as human indexers in many applications.
we used precision and recall to measure the effectiveness of our classifier.
using a public corpus, we show that stacking can improve the efficiency of automatically induced anti-spam filters, and that such filters can be used in real-life applications.}, } @article{sakkis03, author = {georgios sakkis and ion androutsopoulos and georgios paliouras and vangelis karkaletsis and constantine d. spyropoulos and panagiotis stamatopoulos}, title = {
it also makes use of bayesian classification techniques to classify new documents within an existing categorization scheme.
the author describes a series of experiments that show how the extraction patterns learned by autoslog can be used for text classification.
the accuracy of modern text classification systems rivals that of trained human professionals, thanks to a combination of information retrieval (ir) technology and machine learning (ml) technology.
we present the results of experimenting with theseus, a classifier that exploits this technique.}, } @inproceedings{avancini03, author = {henri avancini and alberto lavelli and bernardo magnini and fabrizio sebastiani and roberto zanoli}, title = {expanding domain-specific lexicons by term categorization}, year = {2003}, booktitle = {proceedings of sac-03, 18th acm symposium on applied computing}, address = {melbourne, us}, publisher =
a proposed purification process can effectively reduce the dimensionality of the feature space from 50,576 terms in the word-based approach to 19,865 terms in the unknown word-based approach.
results with an algorithm extended by thesaurus knowledge are presented and interpreted.
these patterns are used to retrieve sentences, which are then determined to be novel if it is likely that a new answer is present.
the analysis predicts learning curves with a very high precision and thus contributes to a better understanding of why and when over-fitting occurs.
because they do not exploit dependencies between labels, such techniques are only well-suited to problems in which categories are independent.
a very efficient categorizer system.
the rapid growth of data in large databases, such as text databases and scientific databases, requires efficient computer methods for automating analyses of the data with the goal of acquiring knowledges or making discoveries.
above 90\% correct recognition rates have been achieved for the major categories concerned.
in this work we investigate the usefulness of $n$-grams in tc independently of any specific learning algorithm.
the algorithms are compared on learning speed and error rate.
in this model, borrowed from information retrieval, documents are represented as a vector where each component is associated with a particular word from the vocabulary.
this paper proposes topic difference factor analysis (tdfa) as a method to extract projection axes that reflect topic differences between two document sets.
next, we investigate the application of automatic categorization to text retrieval.
boosting support vector machines for text classification through parameter-free threshold relaxation}, booktitle = {proceedings of cikm-03, 12th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, editor = {}, year = {2003}, address = {new orleans, us}, pages =
a hierarchical classification is obtained by evaluating the trained node classifiers in a top-down fashion.
we propose a model for basing classification of multimedia on broad, non-topical features, and show how information on targeted nearby pieces of text can be used to effectively classify photographs on a first such feature, distinguishing between indoor and outdoor images.
{236--256}, publisher = {kluwer academic publishers}, address = {dordrecht, nl}, url = {http://www.cais.ntu.edu.sg/~sunaixin/paper/sun_hcl.pdf}, abstract = {hierarchical text classification refers to assigning text documents to the categories in a given category tree based on their content.
specifically, the classifier provides an efficient information extraction and takes the meaning of words into consideration.
we use techniques from statistical pattern recognition to efficiently separate the feature words or discriminants from the noise words at each node of the taxonomy.
additional text is provided in coded form so that the reader can more fully explore this technique and form his own opinion of the applicability and versatility of this particular procedure.
we believe this approach is effective in reducing the development time to implement classification systems involving large number of topics for the purpose of classification, message routing etc.}, } @incollection{masand94, author = {briji masand}, title = {optimising confidence of text classification by evolution of symbolic expressions}, booktitle = {advances in genetic programming}, publisher = {
comparing efficiency, knn was notably more costly in terms of time and memory than the other two methods.
we use the classification on keywords as the baseline, which we compare with the contribution of the pure hm pairs to classification accuracy, and the incremental contributions from heads and modifiers.
such categories offer a standardized and universal way for referring to or describing the nature of real world objects, activities, documents and so on, and may be used (we suggest) to semantically characterize the content of documents.
experimental results show that the high-degree biased bigrams should be eliminated from the feature set, and the s-br1 scheme is quite effective for further dimensionality reduction in chinese text categorization, after a feature selection process with a chi-cig score function.}, } @inproceedings{xue04a, author = {xue, dejun and sun, maosong}, title = {
in contrast to previous stylometric approaches, we attempt to take full advantage of existing natural language processing (nlp) tools.
this task is easy to understand, but the lack of straightforward training set, subjective user intents of queries, poor information in short queries, and high noise level make the task very challenge.
new filtering and disambiguation methods are used as pre-processing to solve the problems caused by the use of the thesaurus.
our results on two datasets of scanned journals from the making of america collection confirm the importance of using whole page sequences.
in our experiments we show that, whenever an existing pn dictionary allows the identification of 50\% of the proper nouns within a corpus, our technique allows, without additional manual effort, the successful recognition of about 90\% of the remaining 50\%.}, } @inproceedings{peters02, author = {c. peters and cornelis h. koster}, title = {uncertainty-based noise reduction and term selection in text categorization}, booktitle = {proceedings of ecir-02, 24th european colloquium on information retrieval research}, editor = {fabio crestani and mark girolami and van
our method is unique in that decision lists are automatically constructed on the basis of the principle of minimizing extended stochastic complexity (esc), and with it we are able to construct decision lists that have fewer errors in classification.
we provide background, present procedures for building metaclassifiers that take into consideration both reliability indicators and classifier outputs, and review a set of comparative studies undertaken to evaluate the methodology.}, } @inproceedings{bennett03, author = {paul n. bennett}, title = {using asymmetric distributions to improve text classifier probability estimates}, booktitle = {proceedings of sigir-03, 26th acm international conference on research and development in information retrieval}, editor = {jamie callan and gordon cormack and charles clarke and david hawking and alan smeaton}, publisher = {acm press, new york, us}, address = {toronto, ca}, year = {2003}, pages = {111--118}, url = {http://doi.acm.org/10.1145/860435.860457},
compared to a previously tested naive bayes filter, the memory-based filter performs on average better, particularly when the misclassification cost for non-spam messages is high.}, } @inproceedings{sasaki98, author = {minoru sasaki and kenji kita}, title = {automatic text categorization based on hierarchical rules}, booktitle = {proceedings of the 5th international conference on soft computing and information}, publisher =
at each node, this classifier can ignore the large number of ``noise'' words in a document.
relation selection improves foil's performance as measured by any of recall, precision, f-measure, or error rate.
the ability, of the approach to generalize, given a minimum of training data is also addressed.
nevertheless, it is shown that automated text categorization techniques can exploit combinations of simple lexical and syntactic features to infer the gender of the author of an unseen formal written document with approximately 80 per cent accuracy.
it supports incremental training and online application of classifiers and predictive models to streams of textual, numeric, symbolic, and hybrid data records.
experimental results show that this method improves the precision of k-nn up to 13.86% without compromising its recall.}
we observed that our new method made a significant improvement in all classifiers and both data sets.}, } @article{ko04, author = {youngjoong ko and jinwoo park and jungyun seo}, title = {
then, test documents are scanned and categories ranked based on the presence of vocabulary terms.
this result suggests that useful task-tracking tools could be constructed based on automatic classification into this taxonomy."
these patterns are used to retrieve sentences, which are then determined to be novel if it is likely that a new answer is present.
this property causes k-nnfp to eliminate possible adverse effects of irrelevant features on the classification accuracy.
instead of using the text on a page for deriving features that can be used for training a classifier, we suggest to use portions of texts from all pages that point to the target page.
furthermore, the performance of the refined centroid classifier implemented is comparable, if not better, to that of state-of-the-art support vector machine (svm)-based classifier, but offers a much lower computational cost."
in our reference collections, measures based on co-citation tend to perform better for pages in the web directory, with gains up to 37\% over text based classifiers, while measures based on bibliographic coupling perform better in a digital library.
the results is an original statistical classifier fed with linguistic (i.e. more complex) features and characterized by the novel feature selection and weighting model.
in addition, we carefully analyze the internal representation using cluster analysis and output representations using a new surface error technique.
it was found that categorizers consisting of the words with highest tf.idf values scored best.}, } @inproceedings{paliouras99, author = {georgios paliouras and vangelis karkaletsis and constantine d. spyropoulos}, title = {learning rules for large vocabulary word sense disambiguation}, booktitle = {proceedings of ijcai-99, 16th international joint conference on artificial intelligence}, editor = {
in this paper we describe an automated method of classifying research project descriptions: a human expert classifies a sample set of projects into a set of disjoint and pre-defined classes, and then the computer learns from this sample how to classify new projects into these classes.
we adopt a machine learning approach and the model parameters are learned from a labeled training set of representative documents.
the goal of these experiments was to automatically discover classification patterns that can be used for assignment of topics to the individual newswires.
{morgan kaufmann publishers, san francisco, us}, url = {http://www.cs.cmu.edu/~yiming/papers.yy/hypertext-icml01.ps.gz}, abstract = {hypertext poses new text classification research challenges as hyperlinks, content of linked documents, and meta data about related web sites all provide richer sources of information for hypertext classification that are not available in traditional text classification.
{published in the ``lecture notes in computer science'' series, number 2291}, pages = {353--362}, url = {http://www.cs.ucd.ie/staff/nick/home/research/download/finn-ecir2002.ps.gz}, abstract = {the world wide web is a vast repository of information, but the sheer volume makes it difficult to identify useful documents.
optimizing and estimating effectiveness is greatly aided if classifiers that explicitly estimate the probability of class membership are used.}, } @article{lewis95a, author = {lewis, david d.}, title = {
on investigating the root cause, we find that a large class of feature scoring methods suffers a pitfall: they can be blinded by a surplus of strongly predictive features for some classes, while largely ignoring features needed to discriminate difficult classes.
moreover, it is demonstrated that our proposed framework is successfully applied to another task of the genomics track, showing comparable results to the best performing system." } @inproceedings{yang:2005:raf, author =
in this paper we demonstrate this pitfall hurts performance even for a relatively uniform text classification task.
in this report, we try to prove that a previous filtering of the words used by svm in the classification can improve the overall performance.
we also propose an iterative web unit mining (iwum) method that first finds subgraphs of web pages using some knowledge about web site structure.
we also show that addressing named entities preferentially is useful only in certain situations.
an additional test submitted 250 trec queries to a search engine and successfully categorized 66\% of the top 100 using the odp and 61\% of the top 350.
this collection is tailored for automating the attribution of international patent classification codes to patent applications and is made publicly available for future research work.
when more documents are archived, new terms, new concepts and concept-drift will frequently appear.
only papers that have % % been the object of formal publication (i.e. conferences and % % journals) are to be included in the bibliography, so as to avoid % % its explosion and the inclusion of material bound to obsolescence.
moreover, a macro-averaged recall and precision was calculated: the former reported a 0.72, the latter a 0.79.
such systems typically have to cope with sets of rectors of many tens of thousands of dimensions.
twenty subjects from expert or novice literary reading experience backgrounds were, in two experiments, required to rate two parallel sets of graphically and phonetically manipulated poems.
our fully implemented and recently deployed system shows that a superior classification engine for this task can be constructed from a combination of classifiers.
this task is an intermediate process in many natural language processing tasks like machine translation or multilingual information retrieval.
finally, the obtained structure attributes are used to re-organize and index the web objects.
today, text categorization is a necessity due to the very large amount of text documents that we have to deal with daily.
results from experiments show that this filter has successfully rejected a sufficient number of non-relevant documents, resulting in an improvement of filtering performance.}, } @inproceedings{hoch94, author = {rainer hoch}, title = {using ir techniques for text classification in document analysis}, booktitle = {proceedings of sigir-94, 17th acm international conference on research and development in information retrieval}, editor = {
in order to classify a new document, the most similar megadocument determines the category to be assigned.
we focus our discussion on the ability to discriminate between authors for the case of both aggregated e-mail topics as well as across different email topics.
the fragments it looks for are determined by a set of knowledge-based rules.
relevant phrases and contexts are acquired automatically using a training corpus.
semi-automated methods were used to build a lexicon of appraising adjectives and their modifiers.
we report the performance of this approach on data sets both with and without the inclusion of the background text, and compare our work to other efforts that can incorporate unlabeled data and other background text in the classification process.}, } @inproceedings{zelikovitz02, author
we present a de-centralized approach and system implementation (named macci) for text classification using a multi-agent framework.
this study indicates that, while some document categorization algorithms could be adopted for database categorization, algorithms that take into consideration the special characteristics of databases may be more effective.
"in this paper, we use a blog corpus to demonstrate that we can often identify the author of an anonymous text even where there are many thousands of candidate authors.
empirical estimates of weights (likelihood ratios) become unstable when counts are small.
at each node, this classifier can ignore the large number of noise words in a document.
we report the results of systematic experimentation of these two methods performed on the standard {\sc reuters-21578} benchmark.}, } @article{gale93, author = {
description-oriented approaches are more flexible with respect to the underlying representations, but the definition of the feature vector is a heuristic step.
the results suggest that information extraction techniques can support high-precision text classification and, in general, using more extracted information improves performance.
experiments in the terrorism domain suggest that increasing the amount of linguistic context can improve performance.
however, a problem with iterative training techniques such as svm is that during their learning or training phase, they require the entire training collection to be held in main-memory; this is infeasible for large training collections such as dmoz or large news wire feeds.
we address this open challenge by using a combination of classifiers with different performance characteristics to effectively reduce the performance variance on average of the overall system across all classes, including those not seen before.
finally, the effectiveness of term distributions to improve classification accuracy is explored with regard to the training set size and the number of classes.}, } @article{leung97, author = {chi-hong leung and wing-kay kan}, title = {a statistical learning approach to automatic indexing of controlled index terms}, journal = {journal of the american society for information science}, year = {1997}, number = {1}, pages = {55--67}, volume = {48}, url = {http://www3.interscience.wiley.com/cgi-bin/fulltext?id=39602&placebo=ie.pdf}, abstract = {
text categorization with many redundant features: using aggressive feature selection to make {svm}s competitive with {c4.5}}, booktitle = {proceedings of icml-04, 21st international conference on machine learning}, editor = {carla e. brodley}, year = {2004}, address = {banff, ca}, pages = {}, publisher =
we found that a larger reference library is not necessarily better.
this method also has the benefit to make feature selection implicit, since useless features for the categorization problem considered get a very small weight.
though our experiments with words yielded good results, we found instances where the phrase-based approach produced more effectiveness.
for illustration we give a brief description of the content-based personal intelligent agent named personal webwatcher that uses text-learning for user customized web browsing.}, } @inproceedings{mladenic99a, author = {dunja mladeni{\'{c}} and marko grobelnik}, title = {feature selection for unbalanced class distribution and naive bayes}, booktitle = {proceedings of icml-99, 16th international conference on machine learning}, editor = {ivan bratko and saso dzeroski}, year = {1999}, address = {bled, sl}, pages = {258--267}, publisher = {morgan kaufmann publishers, san francisco, us}, url = {http://www-ai.ijs.si/dunjamladenic/papers/pww/pwwicml99final.ps.gz}, abstract = {
in this paper, we examine the effects of page evolution on genre classification of web pages.
we discuss the role of importance-weights (e.g. document frequency and redundancy), which is not yet fully understood in the light of model complexity and calculation cost, and we show that time consuming lemmatization or stemming can be avoided even when classifying a highly inflectional language like german.}, } @article{lertnattee04, author = {verayuth lertnattee and thanaruk theeramunkong}, title = {effect of term distributions on centroid-based text categorization}, journal = {information sciences}, year = {2004}, number = {1}, volume = {158}, pages = {89--115}, url = {http://dx.doi.org/10.1016/j.ins.2003.07.007}, abstract = {
text categorization is then cast as the problem of finding coordinate transformations that reflects the inherent similarity from the data.
in this paper, we present a boosting-based learning method for text filtering that uses naive bayes classifiers as a weak learner.
improved estimates of the term distributions are made by differentiation of words in the hierarchy according to their level of generality/specificity.
finally, the naive bayesian filter is compared, in terms of performance, to a filter that uses keyword patterns, and which is part of a widely used e-mail reader.}, } @article{appiani01, author = {enrico appiani and francesca cesarini and annamaria colla and massimiliano diligenti and marco gori and simone marinai and giovanni soda}, title = {automatic document classification and indexing in high-volume applications}, journal =
the result is a model for the automatic selection of parameters.
they were tested on a corpus of articles from the dutch newspaper nrc, and pre-classified into four categories.
in the first approach, the content (eg., text) plays an important role, while in the second approach, the existence of several knowledge sources (eg., several users) is required.
we present simpl, a nearly linear-time classification algorithm which mimics the strengths of svms while avoiding the training bottleneck.
furthermore, linear and logistic regression are compared.}, } @article{furnkranz02, author = {johannes f{\"{u}}rnkranz}, title =
in this work we investigate the usefulness of {\em $n$-grams} for document indexing in text categorization (tc).
hyperlinks, html tags, category labels distributed over linked documents, and meta data extracted from related web sites all provide rich information for classifying hypertext documents.
in this paper, however, we show that in the case of text classification, term-frequency transformations have a larger impact on the performance of svm than the kernel itself.
specifically, we apply the information bottleneck method to find word-clusters that preserve the information about document categories and use these clusters as features for classification.
while the decision tree based classifier outperforms the bayesian classifier when features and training size are selected optimally for both, a carefully designed naive bayesian classifier is more robust.}, } @inproceedings{diaz98, author = {d{\'{\i}}az esteban, alberto and de buenaga rodr{\'{\i}}guez, manuel and ure{\~n}a l{\'o}pez, l. alfonso and garc{\'{\i}}a vega, manuel}, title =
we also present two performance optimizations of waknn that improve the computational performance by a few orders of magnitude, but do not compromise on the classification quality.
this means that the category of potentially relevant documents for most profiles would contain at least 80\% of all documents later determined to be relevant to the profile.
second, we present the word-augmented relevancy signatures algorithm that uses lexical items to represent domain-specific role relationships instead of semantic features.
the document collection is trained by a self-organizing map to form two feature maps.
the existence, public availability, and widespread acceptance of a standard benchmark for a given information retrieval (ir) task are beneficial to research on this task, since they allow different researchers to experimentally compare their own systems by comparing the results they have obtained on this benchmark.
our analysis and empirical evaluation show substantial improvement in the accuracy of catalog integration.}, } @inproceedings{aizawa00, author = {akiko aizawa}, title = {
however, the use of a text-classification system on this is a bit more problematic - in the most straight-forward approach each number would be considered a distinct token and treated as a word.
the best f1 value of our two solutions is 9.6% higher than the best of all other participants’ solutions.
an important aspect of this dissertation is that autoslog and the text classification systems can be easily ported across domains.}, } @inproceedings{riloff95, author = {ellen riloff}, title = {
the accuracy of classification achieved with our method appears better than or comparable to those of existing rule-based methods.
the bin-based method is intended for tasks where there is insufficient training data to estimate a separate weight for each word.
furthermore, based on this approach, we build an interactive red system, hiscovery, which provides additional functions to present events, photo story and chronicle."
in particular, we demonstrate (1) how variation in document length can be tolerated by either normalizing feature weights or by using negative weights, (2) the positive effect of applying a threshold range in training, (3) alternatives in considering feature frequency, and (4) the benefits of discarding features while training.
published in the ``lecture notes in computer science'' series, number 2175}, editor = {floriana esposito}, year = {2001}, pages = {320--325}, address = {bari, it}, url = {http://link.springer.de/link/service/series/0558/papers/2175/21750320.pdf}, abstract = {feature selection and weighting are the primary activity of every learning algorithm for text classification.
the combination of evidence from a document and citing documents can improve on either information source alone.
unsolicited commercial e-mail, or "spam", floods mailboxes, causing frustration, wasting bandwidth, and exposing minors to unsuitable content.
usually, these bigrams are likely to survive for their strength of discriminating documents after the process of feature selection.
the results indicate that in each case, the refined classifiers achieve significant performance improvement over the base classifiers used.
furthermore, the online approach offers the advantage of continuous learning in the batch-adaptive text filtering task.}, } @inproceedings{chakrabarti02, author = {soumen chakrabarti and shourya roy and mahesh soundalgekar}, title = {fast and accurate text classification via multiple linear discriminant projections}, booktitle = {proceedings of vldb-02, 28th international conference on very large data bases}, publisher = {}, editor = {}, year = {2002}, address = {hong kong, cn}, pages = {658--669}, url = {http://www.vldb.org/conf/2002/s19p01.pdf}, abstract = {support vector machines (svms) have shown superb performance for text classification tasks.
the experimental results support the claim that a custom-designed algorithm (genex), incorporating specialized procedural domain knowledge, can generate better keyphrases than a general-purpose algorithm (c4.5).
we present detailed experimental results using naive bayes and support vector machines on the 20newsgroups data set and a 3-level hierarchy of html documents collected from the open directory project (www.dmoz.org).}, } @inproceedings{diao00, author = {yanlei diao and hongjun lu and dekai wu}, title = {
on-line information services generally depend on keyword indices rather than other methods of retrieval, because of the practical features of keywords for storage, dissemination, and browsing as well as for retrieval.
however, even this algorithm is aided by an initial prefiltering of features, confirming the results found by almuallim and dietterich on artificial data sets.
we argue that this evaluation measure is also very well suited for text categorization tasks.
in this paper, we introduce can models and apply them to various text classification problems.
this simplifies the creation of knowledge-based if/ir systems, speeds up their operation, and allows easy editing of the rule bases employed.
using four corpora from the topic detection and tracking (tdt) forum and the text retrieval conferences (trec) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.
as a global observation, knn, llsf and a neural network method had the best performance; except for a naive bayes approach, the other learning algorithms also performed relatively well.}, } @inproceedings{yavuz98, author = {yavuz, tuba and g{\"u}venir, h. altay}, title = {application of k-nearest neighbor on feature projections classifier to text categorization}, booktitle = {proceedings of iscis-98, 13th
this is particularly useful when labeling text is a labor-intensive job and when there is a large amount of information available about a particular problem on the world wide web.
its main originality is its ability to simultaneously take into account the structural and the content information present in a structured document, and also to cope with different types of content (text, image, etc).
they can be computed at essentially no extra cost immediately after training a single svm.
in this paper we propose simple, heuristic solutions to some of the problems with naive bayes classifiers, addressing both systemic issues as well as problems that arise because text is not actually generated according to a multinomial model.
in order to reflect the subtopic structure of a document, we propose a new passage-level or passage-based text categorization model, which segments a test document into several passages, assigns categories to each passage, and merges the passage categories to the document categories.
to retain the information in the structure, we have developed a structured vector model, which represents a document with a structured vector, whose elements can be either terms or other structured vectors.
the results confirm that our meta-model approach can exploit the advantage of its component algorithms, and demonstrate a better performance than existing algorithms.}, } @inproceedings{lam97, author = {wai lam and kon f. low and chao y. ho}, title = {using a bayesian network induction approach for text categorization}, booktitle = {proceedings of ijcai-97, 15th international joint conference on artificial intelligence}, editor = {martha e. pollack}, publisher = {morgan kaufmann publishers, san francisco, us}, year = {1997}, address = {nagoya, jp}, pages = {745--750}, url = {}, abstract = {
the use of naive bayes allows the boosting algorithm to utilize term frequency information while maintaining probabilistically accurate confidence ratio.
using as testing ground a part of the wall street journal corpus, we show that the most frequent words of the british national corpus, representing the most frequent words of the written english language, are more reliable discriminators of text genre in comparison to the most frequent words of the training corpus.
several previous works already suggested applying this method for document clustering, gene expression data analysis, spectral analysis and more.
similarly, we use a training set of queries and their related documents to obtain empirical associations between query words and indexing terms of documents, and use these associations to predict the related documents of arbitrary queries.
employing the thesaurus entails structuring categories into hierarchies, since their structure needs to be conformed to that of the thesaurus for capturing relationships between categories.
infoclas is a first step towards the understanding of documents proceeding to a classification-driven extraction of information.
morgan kaufmann publishers, san francisco, us}, url = {http://www.robotics.stanford.edu/~stong/papers/tong_koller_ml00.ps.gz}, abstract = {support vector machines have met with significant success in numerous real-world learning tasks.
in addition, we compare the algorithms on a larger collection of 1700 texts and describe an automated method for empirically deriving appropriate threshold values.
we develop an automatic text categorization approach and investigate its application to text retrieval.
this study reports the results of a series of experiments in the techniques of automatic document classifications.
experimental results confirm improved performance, breaking through the plateau previously reached in the field." } @inproceedings{soucy:2005:btw, author =
this article uses the structure that is present in the semantic space of topics in order to improve performance in text categorization: according to their meaning, topics can be grouped together into ``meta-topics'', e.g., gold, silver, and copper are all metals.
second, smoothing techniques from statistical language modeling can be used to recover better estimates than the laplace smoothing techniques usually used in naive bayes classification.
in addition, they work much better at certain tasks, such as identifying major events in texts, than at others, such as determining what sort of business or product is involved in a news event.
on the other hand, we also observe that linked pages can be more harmful than helpful when the linked neighborhoods are highly ``noisy'' and that links have to be used in a careful manner.
however, as the number of positive training data decreases, the boundary of svmc starts overfitting at some point and end up generating very poor results.
we describe the new representations and try to justify our hypothesis that they could improve the performance of a rule based learner.
furthermore, they are fully automatic, eliminating the need for manual parameter tuning.}, } @inproceedings{joachims99, author = {thorsten joachims}, title = {transductive inference for text classification
our experiments on human subjects indicate that human feedback on feature relevance can identify a sufficient proportion of the most relevant features (over 50% in our experiments).
the authors' approach has been to use statistics in the knowledge acquisition component of a linguistic pattern-based categorization system, using statistical methods, for example, to associate words with industries and identify phrases that information about businesses or products.
the first set of experiments tests a recurrent neural network for the task of library title classification.
one way is to ask the user to provide them, which is difficult because the user usually can only give a few words (which are insufficient for accurate learning).
we use telltale as our classifier; telltale uses n-grams to compute the similarity between documents.
searching for documents by their type or genre is a natural way to enhance the effectiveness of document retrieval.
research on machine learning for text categorization, already advancing at a rapid pace, could be further accelerated if better test collections were available.}, } @article{lewis04, author =
we show that, even with an average word error rate of around 50\%, the categorization performance loss with respect to the clean version of the same documents is negligible.}, url = {ftp://ftp.idiap.ch/pub/reports/2003/rr03-61.pdf}, } @inproceedings{vinokourov01, author = {alexei vinokourov and mark girolami}, title = {document classification employing the fisher kernel derived from probabilistic hierarchic corpus representations},
in order to determine whether information within a sentence has been seen in material read previously, our system integrates information about the context of the sentence with novel words and named entities within the sentence, and uses a specialized learning algorithm to tune the system parameters."
a typical example is information filtering, i.e. the adaptive classification of documents with respect to a particular user interest.
the system is based on calculating and comparing profiles of n-gram frequencies.
results show that the text in citing documents, when available, often has greater discriminative and descriptive power than the text in the target document itself.
moreover, it is demonstrated that our proposed framework is successfully applied to another task of the genomics track, showing comparable results to the best performing system." } @inproceedings{yang:2005:raf, author =
the task, in essence, is to have a computing machine read a document and on the basis of the occurrence of selected clue words decide to which of many subject categories the document in question belongs.
in addition, the way in which learning with redundancy influences categorization performance is also studied.}, } @inproceedings{moulinier97, author = {isabelle moulinier}, title = {
our experimental results indicate that the naive bayes classifier and the subspace method outperform the other two classifiers on our data sets.
for this task, an indexing dictionary with rules for mapping terms from the text onto descriptors is required, which can be derived automatically from a set of manually indexed documents.
the concept learning model suggests that the poor statistical characteristics of a syntactic indexing phrase representation negate its dsirable semantic characteristics.
furthermore, our analysis reveals the precise dependence of the rate of convergence on the eigenstructure of the data each node observes.
we propose here the use of soft clustering of words, i.e., in which a word can be assigned to several different clusters and each cluster is characterized by a specific word probability distribution.
if the occurrence of a single word determines whether an article belongs to a category or not (and it often does) any compression scheme will likely fail to classify the article correctly.
our simulation results also show the effectiveness of idc in text categorization problems.
using n-gram frequency profiles provides a simple and reliable way to categorize documents in a wide range of classification tasks.}, } @inproceedings{ceci03, author = {michelangelo ceci and donato malerba}, title = {hierarchical classification of html documents with {webclassii}}, booktitle = {
our experimental results using real netnews articles and newsgroups demonstrate (1) applying feature reduction to the training set improves the routing accuracy, efficiency, and database storage; (2) updating improves the routing accuracy; and (3) the batch technique improves the efficiency of the updating operation.}, } @inproceedings{huffman94, author = {stephen huffman and marc damashek}, title =
this process involves an activity of {\em supervised learning}, in which information on the membership of training documents in categories is used.
experimental results show that the high-degree biased bigrams should be eliminated from the feature set, and the s-br1 scheme is quite effective for further dimensionality reduction in chinese text categorization, after a feature selection process with a chi-cig score function.}, } @inproceedings{xue04a, author = {xue, dejun and sun, maosong}, title = {
sample were more dramatic: the text classifier showed a 68\% error, whereas our hypertext classifier reduced this to just 21\%.}, } @article{chakrabarti98c, author = {soumen chakrabarti and byron e. dom and rakesh agrawal and prabhakar raghavan}, title = {scalable feature selection, classification and signature generation for organizing large text databases into hierarchical topic taxonomies}, journal = {journal of very large data bases}, year = {1998}, number = {3}, volume = {7}, pages = {163--178}, url = {http://www.cs.berkeley.edu/~soumen/vldb54_3.pdf}, abstract = {we explore how to organize large text databases hierarchically by topic to aid better searching, browsing and filtering.
using the vector space model, each document is represented by its original feature vector augmented with external feature vector generated using wordnet.
a generic system for domain and language independent text categorization}, journal = {computer vision and image understanding}, year = {1998}, number = {3}, volume = {70}, pages = {299--306}, url = {http://www.idealibrary.com/links/doi/10.1006/cviu.1998.0687/pdf}, abstract = {text categorization assigns predefined categories to either electronically available texts or those resulting from document image analysis.
in addition, a comparison of the two filtering methods clarified that pos filtering on svms consistently outperformed mi filtering, which indicates that svms cannot find irrelevant parts of speech.
experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 30\%.}, } @inproceedings{nigam00a, author = {kamal nigam and rayid ghani}, title = {analyzing the applicability and effectiveness of co-training}, booktitle = {proceedings of cikm-00, 9th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, address = {mclean, us}, editor = {arvin agah and jamie callan and elke rundensteiner}, year = {2000}, pages = {86--93}, url = {http://www.cs.cmu.edu/~knigam/papers/cotrain-cikm00.pdf}, abstract = {recently there has been significant interest in supervised learning algorithms that combine labeled and unlabeled data for text learning tasks.
the categories in such problems usually are neither conditionally independent from each other nor mutually exclusive, therefore it is not trivial to directly employ state-of-the-art classification algorithms without losing information of relation among categories.
the comparison of the examined models demonstrates that techniques from information retrieval integrated into recurrent plausibility networks performed well even under noise and for different corpora.}, } @inproceedings{wermter02, author = {stefan wermter and chihli hung}, title = {
experimental results show that this method improves the precision of k-nn up to 13.86% without compromising its recall.}
the goal of the research described here is to automatically create a computer understandable knowledge base whose content mirrors that of the world wide web.
we also present results with algorithms other than co-training in this framework and show that co-training is uniquely suited to work well within ecoc.}, } @inproceedings{giorgetti03, author = {daniela giorgetti and fabrizio sebastiani}, title = {multiclass text categorization for automated survey coding}, year = {2003}, address =
in this paper we describe an automated method of classifying research project descriptions: a human expert classifies a sample set of projects into a set of disjoint and pre-defined classes, and then the computer learns from this sample how to classify new projects into these classes.
description-oriented approaches are more flexible with respect to the underlying representations, but the definition of the feature vector is a heuristic step.
therefore, it is very costly to assign a category to them because a human investigates their contents.
when applied to text classification, these learning algorithms lead to svms with excellent precision but poor recall.
first, terms (single words or phrases) are identified in the document text.
this approach is well suited to learning in hypertext domains because its statistical component allows it to characterize text in terms of word frequencies, whereas its relational component is able to describe how neighboring documents are related to each other by hyperlinks that connect them.
the word-pairs are selected automatically using a technique based on frequencies of n-grams (sequences of characters), which takes into account both the frequencies of word-pairs as well as the context in which they occur.
text categorization is useful for indexing documents for information retrieval, filtering parts for document understanding, and summarizing contents of documents of special interests.
by using the hierarchically structured subject domain and classification rules, the classifier's engine assigns an email query to the most relevant category or categories.}, } @article{zheng04, author = {zhaohui zheng and xiaoyun wu and rohini srihari}, title = {
the logic representation of sentences required by the adopted learning algorithm is obtained by detecting structure in raw text trough a parser.
we have evaluated expnet in categorization and retrieval on a document collection of the medline database, and observed a performance in recall and precision comparable to the linear least squares fit (llsf) mapping method, and significantly better than other methods tested.
error minimization is difficult in high-dimensional feature spaces because the convergence process is slow and the models are prone to overfitting.
we present the results of some experiments done on real data: two different classifications of our research projects.}, } @article{borko63, author = {harold borko and myrna bernick}, title = {automatic document classification}, journal = {journal of the association for computing machinery}, year = {1963}, volume = {10}, number = {2}, pages = {151--161}, url = {http://www.acm.org/pubs/articles/journals/jacm/1963-10-2/p151-borko/p151-borko.pdf}, } @article{borko64, author = {harold borko and myrna bernick}, title = {automatic document classification.
measuring the similarity of two documents is conducted by comparing a pair of their corresponding sub-vectors at a time.
the ecoc method scales well to large data sets with a large number of classes.
augmenting naive {b}ayes classifiers with statistical language models}, journal = {information retrieval}, publisher = {springer science}, year = {2004}, volume = {7}, number = {3-4}, pages =
in this paper, we explore correlations among categories with maximum entropy method and derive a classification algorithm for multi-labelled documents.
however, most of them are useless for document categorization because of the weakness in representing document contents.
furthermore, a learning feedback technique is introduced.
this article reports on our experiments and results on the effectiveness of different feature sets and information fusion from some combinations of them in classifying free text documents into a given number of categories.
but our experiments on human subjects indicate that human feedback on feature relevance can identify a sufficient proportion (65%) of the most relevant features.
it connects the statistical properties of text-classification tasks with the generalization performance of a svm in a quantitative way.
given corpora of documents and a training set of examples of classified documents, the technique locates a minimal set of co-ordinate keywords to distinguish between classes of documents, reducing the dimensionality of the keyword vectors.
the set of all indications from a document leading to the same descriptor is called a relevance description.
while most work on kdd has been concerned with structured databases, there has been little work on handling the huge amount of information that is available only in unstructured textual form.
in our reference collections, measures based on co-citation tend to perform better for pages in the web directory, with gains up to 37\% over text based classifiers, while measures based on bibliographic coupling perform better in a digital library.
alternative methods are also tested on these data collections for comparison.
such a knowledge base would enable much more effective retrieval of web information, and promote new uses of the web to support knowledge-based inference and problem solving.
on one of these datasets (the 20 newsgroups) the method based on word clusters significantly outperforms the word-based representation in terms of categorization accuracy or representation efficiency.
error minimization is difficult in high-dimensional feature spaces because the convergence process is slow and the models are prone to overfitting.
the rationale behind our proposal is that taking into account contextual information provided by the whole page sequence can help disambiguation and improves single page classification accuracy.
however, such systems sacrifice efficiency to boost effectiveness.
the proposed approach does not require professional librarians or that the end users have extensive training.
for this reason, the automatic construction of disambiguation rules is highly desirable.
our experiments, make use of the reuters 21578 database of documents and consist of a binary classification for each of the ten most populous categories of the reuters database.
the r-measure can be effectively computed using the suffix array data structure.
we show that the advantage of using supervised clustering is that it is possible to have some control over the range of subjects that one would like the categorization system to address, but with a precise mathematical definition of each category.
we show that machine-generated decision rules appear comparable to human performance, while using the identical rule-based representation.
all the experiments result in a significant improvement with respect to other purely statistical methods (e.g. [yang, 1999]), thus stressing the relevance of the available linguistic information.
the neural network is simulated on a vax computer with a fast learning algorithm, and is combined with some non-statistical knowledge from the feature definition system.
{87--103}, year = {2000}, url = {http://www.his.sunderland.ac.uk/ps/ir4.pdf}, abstract = {the research project agnet develops agents for neural text routing in the internet.
we also used the generated structure to categorize text documents.
additional text is provided in coded form so that the reader can more fully explore this technique and form his own opinion of the applicability and versatility of this particular procedure.
furthermore, the online approach offers the advantage of continuous learning in the batch-adaptive text filtering task.}, } @inproceedings{adami03, author = {giordano adami and paolo avesani and diego sona}, title = {
these results demonstrate that this approach can scale up to a large real-world task and show a lot of potential for text classification.}, } @inproceedings{wermter99, author = {stefan wermter and christo panchev and garen arevian}, title = {hybrid neural plausibility networks for news agents}, booktitle = {proceedings of aaai-99, 16th conference of the american association for artificial intelligence}, publisher = {aaai press, menlo park, us}, editor = {}, year = {1999}, pages = {93--98}, address = {orlando, us}, url = {http://www.his.sunderland.ac.uk/ps/aaai99.pdf}, abstract = {
from these web subgraphs, web units are constructed and classified into semantic concepts (or categories) in an iterative manner.
ts compares favorably with the other methods with up to 50\% vocabulary reduction but is not competitive at higher vocabulary reduction levels.
to do this, we use techniques from statistical pattern recognition to efficiently separate the feature words, or discriminants, from thenoise words at each node of the taxonomy.
combinations of multiple classifiers did not always improve the classification accuracy compared to the best individual classifier.
results obtained from evaluation show that the integration of wordnet can outperform approaches based only on training.}, } @article{diederich03, author = {
through our experiments on the reuters-21578 news database, we showed that aram performed reasonably well in mining categorization knowledge from sparse and high dimensional document feature space.
we report here on experiments using a committee of winnow-based learners and demonstrate that this approach can reduce the number of labeled training examples required over that used by a single winnow learner by 1-2 orders of magnitude.}, } @inproceedings{liere98, author = {ray liere and prasad tadepalli}, title = {active learning with committees: preliminary results in comparing winnow and perceptron in text categorization}, booktitle = {proceedings of conald-98, 1st
the author describes a series of experiments that show how the extraction patterns learned by autoslog can be used for text classification.
the experimental results show that modulating the structure of the user profile increases the accuracy of a personalization system.}, } @article{chen03, author = {chen l. and tokuda n. and nagai a.}, title = {a new differential lsi space-based probabilistic document classifier}, journal =
accordingly,' our approach is based on problem solving tasks.
simpl uses efficient sequential scans and sorts, and is comparable in speed and memory scalability to widely-used naive bayes (nb) classifiers, but it beats nb accuracy decisively.
despite this, we show for one data set that fax quality images can be categorized with nearly the same accuracy as the original text.
text classification tasks, like text categorization, help the users to access to the great amount of text they find in the internet and their organizations.
we present a de-centralized approach and system implementation (named macci) for text classification using a multi-agent framework.
keywords alone cannot always distinguish the relevant from the irrelevant texts and some relevant texts do not contain any reliable keywords at all.
in a different manner from topographical techniques previously utilized for static text collections, the topography is an outcome of the coherence in time of the data stream in the proposed model.
for the first of them, a collection of articles from pc week magazine, the addition of word-pairs increases micro-averaged breakeven accuracy by more than 6\% point from a baseline accuracy (without pairs) of around 40\%.
[1], and shows that some of the modifications included in twcnb may not be necessary to achieve optimum performance on some datasets.
furthermore, the effectiveness of word sense disambiguation for different parts of speech (nouns and verbs) is examined empirically.}, } @inproceedings{pang02, author = {
organizing search results allows users to focus on items in categories of interest rather than having to browse through all the results sequentially.}, } @inproceedings{chen00a, author = {hao chen and tin kam ho}, title = {evaluation of decision forests on text categorization}, booktitle = {proceedings of the 7th spie conference on document recognition and retrieval}, publisher =
although both the contents and time information of news articles are helpful to red, most researches focus on the utilization of the contents of news articles.
a generic system for domain and language independent text categorization}, journal = {computer vision and image understanding}, year = {1998}, number = {3}, volume = {70}, pages = {299--306}, url = {http://www.idealibrary.com/links/doi/10.1006/cviu.1998.0687/pdf}, abstract = {text categorization assigns predefined categories to either electronically available texts or those resulting from document image analysis.
we test this approach on a large collection of personal e-mail messages, which we make publicly available in "encrypted" form contributing towards standard benchmarks.
we compare the effectiveness of five different automatic learning algorithms for text categorization in terms of learning speed, real-time classification speed, and classification accuracy.
exploiting structural information for semi-structured document categorization}, journal = {information processing and management}, volume = {42}, number = {3}, pages =
however, we find that ppm does not compete with the published state of the art in the use of machine learning for text categorization.
since the sequential approach is much more efficient, requiring only 14\%-16\% of the comparisons used in the other approaches, we find it to be a good choice for classifying text into large hierarchical structures.} } @inproceedings{dumais98, author = {
the results of these computational experiments on a sample of 2897 text documents from the tipster collection indicate that the first approach has many advantages over the vsm approach for solving this type of text document classification problem.
with the use of suitable dimensionality reduction techniques and efficient algorithms, both llsf and expnet successfully scaled to this very large problem with a result significantly outperforming word-matching and other automatic learning methods applied to the same corpus.}, } @article{yang96b, author =
first, we explain how the extraction patterns can be generated automatically using only preclassified texts as input.
prior to text categorization, a feature generator analyzes the documents and maps them onto appropriate ontology concepts, which in turn induce a set of generated features that augment the standard bag of words.
then the system computes a profile for a particular document that is to be classified.
we also demonstrate that the retrieval performance using automatic categorization achieves the same retrieval quality as the performance using manual categorization.
comparison with manually assigned classes shows that link information enhances classification in data with sufficiently high link density, but is detrimental to performance at low link densities or if the quality of the links is degraded.
we also used the generated structure to categorize text documents.
a sequential algorithm for training text classifiers: corrigendum and additional data}, journal = {sigir forum}, year = {1995}, pages = {13--19}, volume = {29}, number = {2}, url = {http://www.research.att.com/~lewis/papers/lewis95g.ps}, abstract = {
we suggest, for text categorization, the integration of external wordnet lexical information to supplement training data for a semi-supervised clustering algorithm which (i) uses a finite design set of labeled data to (ii) help agglomerative hierarchical clustering algorithms (ahc) partition a finite set of unlabeled data and then (iii) terminates without the capacity to classify other objects.
using various data sets, their performances are investigated and compared to a standard centroid-based classifier (tdidf) and a centroid-based classifier modified with information gain.
we see genre classification as a powerful instrument to bring web-based search services closer to a user's information need.
we experimentally evaluated waknn on 52 document data sets from a variety of domains and compared its performance against several classification algorithms, such as c4.5, ripper, naive-bayesian, pebls and vsm.
based on this consideration, the authors have built a neural network classification system, which has three subsystems: a user-maintainable feature definition subsystem, a feature extraction subsystem, and a neural network subsystem.
the ability, of the approach to generalize, given a minimum of training data is also addressed.
as the amount of data stored in storage media is increased exponentially, it becomes necessary to store documents according to their category, to access them easily.
however, even the most successful techniques are defeated by many real-world applications that have a strong time-varying component.
based on the lpt-model, we focus on learning patterns within a relatively simple pattern language.
among the four dimensionality reduction techniques proposed, principal component analysis was found to be the most effective in reducing the dimensionality of the feature space.}, } @article{lam99a, author = {lam, wai and ruiz, miguel e. and srinivasan, padmini}, title = {automatic text categorization and its applications to text retrieval}, journal = {ieee transactions on knowledge and data engineering}, year = {1999}, number = {6}, volume = {11}, pages = {865--879}, url = {http://www.cs.uiowa.edu/~mruiz/papers/ieee-tkde.ps}, abstract = {
the vast majority of them represent cases that can only be fully categorized with the assistance of a human subject (because, for instance, they require specific knowledge of a given pathology).
moreover, our feature selection method sometimes produces more improvements of conventional machine learning algorithms over support vector machines which are known to give the best classification accuracy.} } @article{pant:2005:lcc, author = {gautam pant and padmini srinivasan}, title =
we start from the observation that support vector machines, one of the best text categorization methods cannot scale up to handle the large document collections involved in many real word problems.
when the complete feature set is available, the classifier learning algorithm can better relate to the suitable representation level the different complex features like linguistic ones (e.g. syntactic categories associated to words in the training material or terminological expressions).
in particular, evaluations on ocr documents are very rare.
our enhancements include using lidstone's law of succession instead of laplace's law, under-weighting long documents, and over-weighting author and subject.
while regular support vector machines (svms) try to induce a general decision function for a learning task, tsvms take into account a particular test set and try to minimize misclassifications of just those particular examples.
it requires an indexing dictionary with rules mapping terms of the respective subject field onto descriptors and inverted lists for terms occurring in a set of documents of the subject field and descriptors manually assigned to these documents.
since in both tasks text as a sequence of words is of crucial importance, propositional learners have strong limitations, although viewing learning for tc and ie as inductive logic programming (ilp) problems is obvious, most approaches rather use proprietary formalisms.
however, separate databases of short system call sequences have to be built for different programs, and learning program profiles involves time-consuming training and testing processes.
the expert literary readers were found to assign significantly higher ratings to all versions of the manipulated poems than the novice readers.}, } @inproceedings{hayes88, author = {philip j. hayes and laura e. knecht and monica j. cellio}, title = {a news story categorization system}, booktitle = {proceedings of anlp-88, 2nd conference on applied natural language processing}, publisher = {association for computational linguistics, morristown, us}, address = {austin, us}, editor = {}, year = {1988}, pages = {9--17}, url = {}, note = {
in our learning experiments naive bayesian classifier was used on text data.
this method, which we call uncertainty sampling, reduced by as much as 500-fold the amount of training data that would have to be manually classified to achieve a given level of effectiveness.}, } @article{lewis94b, author = {lewis, david d. and philip j. hayes}, title = {guest editors' introduction to the special issue on text categorization}, journal = {acm transactions on information systems}, volume = {12}, number = {3}, pages = {231}, year = {1994}, } @inproceedings{lewis94c, author = {lewis, david d. and jason catlett}, title =
grammatical errors do not exceed five per cent of the output, so human screening is satisfactorily low.
we find that even a relatively high level of errors in the ocred documents does not substantially affect stylistic classification accuracy."
automated text classification is attractive because it frees organizations from the need of manually organizing document bases, which can be too expensive, or simply infeasible given the time constraints of the application or the number of documents involved.
the results reveal that subjects are sensitive to the manipulations of graphic and phonetic information and use the same additive information integration rule in making poetic text categorization judgements.
it is important to define what constitutes good effectiveness for these autonomous systems, tune the systems to achieve the highest possible effectiveness, and estimate how the effectiveness changes as new data is processed.
"it is well known that links are an important source of information when dealing with web collections.
we thereby treat the problem of classifying documents as that of conducting statistical hypothesis testing over finite mixture models.
the result is a model for the automatic selection of parameters.
when more documents are archived, new terms, new concepts and concept-drift will frequently appear.
this is a problem in which there are large amounts of data available, but the rules for classification are not explicitly available.
these methods can greatly reduce the number of instances that an expert need label.
we review methods for analyzing novelty and then describe newsjunkie, a system that personalizes news for users by identifying the novelty of stories in the context of stories they have already reviewed.
thus, svm adapts efficiently in dynamic environments that require frequent additions to the document collection.
however, when lsi is used is conduction with statistical classification, there is a dramatic improvement in performance.} } @inproceedings{hull96, author = {david a. hull and jan o. pedersen and hinrich sch{\"u}tze}, title = {method combination for document filtering}, booktitle = {proceedings of sigir-96, 19th acm international conference on research and development in information retrieval}, editor = {hans-peter frei and donna harman and peter sch{\"{a}}uble and ross wilkinson}, publisher = {acm press, new york, us}, year = {1996}, address = {z{\"{u}}rich, ch}, pages = {279--288}, url = {ftp://parcftp.xerox.com/pub/qca/papers/sigirfiltering96.ps}, abstract = {
this paper studies the iterative double clustering (idc) meta-clustering algorithm, a new extension of the recent double clustering (dc) method of slonim and tishby that exhibited impressive performance on text categorization tasks.
classification experiments usually grab a snapshot (temporally and spatially) of the web for a corpus.
rather than performing lsi's singular value decomposition (svd) process solely on the training data, we instead use an expanded term-by-document matrix that includes both the labeled data as well as any available and relevant background text.
the experiments reported in this paper deal with the relationship between specific formal textual features, i.e. graphic and phonetic information, and the reader's literary educational background in the categorization of poetic texts.
in a second experiment, we ignored nouns, verbs and adjectives and replaced them by grammatical tags and bigrams.
unlike other machine learning techniques, it allows easy incorporation of new documents into an existing trained system.
these theoretical findings are supported by our experiments, which show that hyperbolic soms can successfully be applied to text categorization and yield results comparable to other state-of-the-art methods.
in comparison to the previously proposed agglomerative strategies our divisive algorithm achieves higher classification accuracy especially at lower number of features.
further experiments using two less orthodox categorizers are also presented which suggest that combining text categorizers can be successful, provided the essential element of 'difference' is considered.}, } @article{urena01, author = {l. alfonso ure{\~{n}}a-l{\'{o}}pez and manuel buenaga and jos{\'{e}} m. g{\'{o}}mez}, title = {
in order to classify a new document, the most similar megadocument determines the category to be assigned.
as a consequence, these algorithms cannot take full advantage of the ``weighted'' representations (consisting of vectors of continuous attributes) that are customary in information retrieval tasks, and that provide a much more significant rendition of the document's content than binary representations.
the new approach is based on extracting patterns, in the form of two logical expressions, which are defined on various features (indexing terms) of the documents.
experiments show the bin-based method is highly competitive with other current methods.
phrases, word senses and syntactic relations derived by natural language processing (nlp) techniques were observed ineffective to increase retrieval accuracy.
this technique used in the learning process modifies the relationship between an index term and its relevant and irrelevant words to improve the learning performance and, thus, the indexing performance.
moreover, a new measure for the evaluation of system performances has been introduced in order to compare three different techniques (flat, hierarchical with proper training sets, hierarchical with hierarchical training sets).
many methods can be used to categorize texts once their words are known, but ocr can garble a large proportion of words, particularly when low quality images are used.
{storer, james a. and cohn, martin}, publisher = {ieee computer society press, los alamitos, us}, year = {2000}, address = {snowbird, us}, pages = {200--209}, url = {http://dlib.computer.org/conferen/dcc/0592/pdf/05920555.pdf}, abstract = {text categorization is the assignment of natural language texts to predefined categories based on their content.
we have used the receiver operating characteristic convex hull method for the evaluation, that best suits classification problems in which target conditions are not known, as it is the case.
the average f1 value of our two submitted solutions is 94.4% higher than the average f1 value from all other submitted solutions."
a combined use of the projections on and the distances to the dlsi spaces introduced from the differential document vectors improves the adaptability of the lsi (latent semantic indexing) method by capturing unique characteristics of documents.
thus, feature reduction is often performed in order to increase the efficiency and effectiveness of the classification.
furthermore, the supervised lower dimensional space greatly improves the retrieval performance when compared to lsi.}, } @inproceedings{kawatani02, author = {
thus, the classifier has a small model size and is very fast.
this article studies aggressive word removal in text categorization to reduce the noise in free texts and to enhance the computational efficiency of categorization.
the crawling process is modeled as a parallel best-first search over a graph defined by the web.
first, undirected models do not impose the acyclicity constraint that hinders representation of many important relational dependencies in directed models.
moreover, our feature selection method sometimes produces more improvements of conventional machine learning algorithms over support vector machines which are known to give the best classification accuracy.} } @article{pant:2005:lcc, author = {gautam pant and padmini srinivasan}, title = {learning to crawl:
we discovered that the knowledge about relevance among queries and documents can be used to obtain empirical connections between query terms and the canonical concepts which are used for indexing the content of documents.
experimental results show that the choice of thresholding strategy can significantly influence the performance of knn, and that the "optimal" strategy may vary by application.
experimental comparison given on real-world data collected from web users shows that characteristics of the problem domain and machine learning algorithm should be considered when feature scoring measure is selected.
the document organization and classification performance of our ica-based hierarchical classifier are evaluated in several encouraging experiments conducted on a journalistic-style text corpus for speech synthesis in catalan.}, } @inproceedings{shanahan03, author
however, as documents accumulate, such categories may not capture a document's characteristics correctly.
our enhancements include using lidstone's law of succession instead of laplace's law, under-weighting long documents, and over-weighting author and subject.
profile allows the user to update on-line the profile and to check the discrepancy between the assessment and the prediction of relevance of the system.
in addition, centralized approaches are more vulnerable to attacks or system failures and less robust in dealing with them.
only papers that have % % been the object of formal publication (i.e. conferences and % % journals) are to be included in the bibliography, so as to avoid % % its explosion and the inclusion of material bound to obsolescence.
we used precision and recall to measure the effectiveness of our classifier.
because the sense distinctions made are coarse, the disambiguation can be accomplished without the expense of knowledge bases or inference mechanisms.
moreover, the system can then save such document organizations in user profiles which can then be used to help classify future query results by the same user.
we report on experiences with the reuters newswire benchmark, the us patent database, and web document samples from {{\sc yahoo!}}\.}, } @inproceedings{wei01, author = {chih-ping wei and yuan-xin dong}, title = {a mining-based category evolution approach to managing online document categories}, booktitle = {proceedings of hicss-01, 34th annual hawaii international conference on system sciences}, publisher =
however, the phrasal pre-processing and pattern matching methods that seem to work for categorization have the disadvantage of requiring a fair amount of knowledge-encoding by human beings.
these connections do not depend on whether there are shared terms among the queries and documents; therefore, they are especially effective for a mapping from queries to the documents where the concepts are relevant but the terms used by article authors happen to be different from the terms of database users.
text categorization algorithms usually represent documents as bags of words and consequently have to deal with huge numbers of features.
the concept learning model emphasizes the role manual and automated feature selection and classifier formation in text classification.
when training classifiers on large collections of documents, both the time and memory requirements connected with processing of these vectors may be prohibitive.
the category that contains the largest categorical points is selected as the category of a document.
the agreement between the automated grader and the final manual grade was as good as the agreement between human graders.}, } @inproceedings{larkey99, author =
we present here a transductive boosting method for text categorization in order to make use of the large amount of unlabeled data efficiently.
performance is usually, though not always, less good here, but the term weighting functions implicit in the resulting ranking functions are intriguing, and the approach could easily be adapted to mixtures of textual and nontextual data.}, } @inproceedings{jacobs92, author = {paul s. jacobs}, title = {
this word-cluster representation is computed using the recently introduced information bottleneck method, which generates a compact and efficient representation of documents.
generalization is an important ability specific to inductive learning that will predict unseen data with high accuracy based on learned concepts from training examples.
the experimental results suggest that the bigrams can substantially raise the quality of feature sets, showing increases in the break-even points and f1 measures.
we present the results of experiments with a preliminary implementation of the technique.}, } @inproceedings{attardi99, author = {giuseppe attardi and antonio gull{\'{\i}} and fabrizio sebastiani}, title = {automatic web page categorization by link and context analysis},
experimental results show that it achieves nearly perfect performance on a set of hard cases.}, } @inproceedings{chen00, author = {hao chen and susan t. dumais}, title = {bringing order to the web: automatically categorizing search results}, booktitle = {proceedings of chi-00, acm international conference on human factors in computing systems}, publisher = {acm press, new york, us}, editor = {}, year = {2000}, address = {den haag, nl}, pages = {145--152}, url = {http://www.acm.org/pubs/articles/proceedings/chi/332040/p145-chen/p145-chen.pdf}, abstract = {
using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines.
in the test of the expert network method on cacm documents, for example, an 87\% removal of unique words reduced the vocabulary of documents from 8,002 distinct words to 1,045 words, which resulted in a 63\% time saving and a 74\% memory saving in the computation of category ranking, with a 10\% precision improvement, on average, over not using word removal.
in many important text classification problems, acquiring class labels for training documents is costly, while gathering large quantities of unlabeled data is cheap.
svms achieve substantial improvements over the currently best performing methods and behave robustly over a variety of different learning tasks.
when user's interests change, pva, in not only the contents, but also in the structure of user profile, is modified to adapt to the changes.
the task, in essence, is to have a computing machine read a document and on the basis of the occurrence of selected clue words decide to which of many subject categories the document in question belongs.
instead of using the text on a page for deriving features that can be used for training a classifier, we suggest to use portions of texts from all pages that point to the target page.
pattern matching produces fairly accurate and fast categorisation over a large number of classes, while information extraction provides fine-grained classification for a reduced number of classes.
comparative analysis shows that the performances achieved are relatively close to the best tc models (e.g. support vector machines).}, } @inproceedings{moschitti04, author = {alessandro moschitti and roberto basili}, title = {complex linguistic features for text classification: a comprehensive study}, booktitle = {proceedings of ecir-04, 26th european conference on information retrieval research}, editor = {sharon mcdonald and john tait}, year = {2004}, address = {sunderland, uk}, publisher = {springer verlag, heidelberg, de}, note = {
empirical results are given on a number of dataset, showing that our feature selection method is more effective than koller and sahamis method [koller, d., & sahami, m. (1996).
{morgan kaufmann publishers, san francisco, us}, url = {}, abstract = {kernel methods like support vector machines have successfully been used for text categorization.
"it is often useful to classify email according to the intent of the sender (e.g., 'propose a meeting', 'deliver information').
however, in addition to relying on labeled training data, we improve classification accuracy by also using unlabeled data and other forms of available ``background" text in the classification process.
this filtering process is a word sense disambiguation task.
accordingly,' our approach is based on problem solving tasks.
its severe assumptions make such efficiency possible but also adversely affect the quality of its results.
the svms' results common to both filtering are that 1) the optimal number of features differed completely across categories, and 2) the average performance for all categories was best when all of the words were used.
we've applied these techniques to online banking applications to enhance automated e-mail routing.}, } @article{wermter00, author = {stefan wermter}, title = {neural network agents for learning semantic text classification}, journal = {information retrieval}, number = {2}, volume = {3}, pages = {87--103}, year = {2000}, url = {http://www.his.sunderland.ac.uk/ps/ir4.pdf}, abstract = {
experimental results show that our neuro-genetic algorithm is able to perform as well as, if not better than, the best results of neural networks to date, while using fewer input features.}, } @inproceedings{zaiane02, author = {osmar r. za{\"{\i}}ane and maria-luiza antonie}, title = {
words, contexts and senses are represented in word space, a high-dimensional real-valued space in which closeness corresponds to semantic similarity.
simulation results on both toy-data settings and an actual application on internet chat line discussion analysis is presented by way of demonstration.}, } @inproceedings{kao03, author = {anne kao and lesley quach and steve poteet and steve woods}, title = {
the results demonstrated that representative sampling offers excellent learning performance with fewer labeled documents and thus can reduce human efforts in text classification tasks.}, } @inproceedings{xue03, author = {dejun xue and maosong sun}, title = {chinese text categorization based on the binary weighting model with non-binary smoothing}, booktitle = {proceedings of ecir-03, 25th
in this paper we show how performance on new event detection (ned) can be improved by the use of text classification techniques as well as by using named entities in a new way.
the architecture, modules, and practical results are described.}, } @article{manevitz01, author = {larry m. manevitz and malik yousef}, title = {one-class {svms} for document classification}, journal = {journal of machine learning research}, volume = {2}, month = {december}, pages = {139--154}, year = {2001}, url = {http://www.ai.mit.edu/projects/jmlr/papers/volume2/manevitz01a/manevitz01a.pdf}, abstract = {
this risk is found to be most severe when little data is available.
our results show that the identification of hypertext regularities in the data and the selection of appropriate representations for hypertext in particular domains are crucial, but seldom obvious, in real-world problems.
we present the results of some experiments done on real data: two different classifications of our research projects.}, } @article{borko63, author = {harold borko and myrna bernick}, title = {automatic document classification}, journal = {journal of the association for computing machinery}, year = {1963}, volume = {10}, number = {2}, pages = {151--161}, url =
we show how to do this for binary text classification systems, emphasizing that different goals for the system lead to different optimal behaviors.
similarly, we use a training set of queries and their related documents to obtain empirical associations between query words and indexing terms of documents, and use these associations to predict the related documents of arbitrary queries.
our empirical results show that the proposed approach, text categorization using feature projections (tcfp), outperforms k-nn, rocchio, and naive bayes.
it uses the internet as source of knowledge and extends it to categorize very short (less than 5 words) documents with reasonable accuracy.
using a public corpus, we show that stacking can improve the efficiency of automatically induced anti-spam filters, and that such filters can be used in real-life applications.}, } @article{sakkis03, author = {georgios sakkis and ion androutsopoulos and georgios paliouras and vangelis karkaletsis and constantine d. spyropoulos and panagiotis stamatopoulos}, title = {a memory-based approach to anti-spam filtering for mailing lists}, journal = {information retrieval}, publisher =
we compare their effectiveness in classifying ocr texts and the corresponding correct ascii texts in two domains: business letters and abstracts of technical reports.
in this paper, we present a boosting-based learning method for text filtering that uses naive bayes classifiers as a weak learner.
in this paper, we present a formal description of the feature quantity, as well as some illustrative examples of applying such a quantity to different types of information retrieval tasks: representative term selection and text categorization.}, } @inproceedings{aizawa01, author = {akiko aizawa}, title = {linguistic techniques to improve the performance of automatic text categorization}, booktitle = {proceedings of nlprs-01, 6th natural language processing pacific rim symposium}, editor = {}, publisher = {}, address =
experiments in text classification}, booktitle = {proceedings of ecir-03, 25th european conference on information retrieval}, publisher = {springer verlag}, editor = {fabrizio sebastiani}, address = {pisa, it}, year = {2003}, pages = {41--56}, url = {http://link.springer.de/link/service/series/0558/papers/2633/26330041.pdf}, abstract = {link analysis methods have become popular for information access tasks, especially information retrieval, where the link information in a document collection is used to complement the traditionally used content information.
we describe a method for classifying pages of sequential ocr text documents into one of several assigned categories and suggest that taking into account contextual information provided by the whole page sequence can significantly improve classification accuracy.
in practice, the assumption is too restrictive since a web page itself may not always correspond to a concept instance of some semantic concept (or category) given to the classification task.
these results reach most of the state-of-the-art techniques of machine learning applied to text categorization, demonstrating that this new weighting scheme does perform well on this particular task.}, } @inproceedings{dinunzio04, author = {giorgio m. {di nunzio}}, title = {
as the results, the micro averaged f1 measure for reuters-21578 improved from 83.69 to 87.27\%.}, } @article{kehagias03, title = {a comparison of word- and sense-based text categorization using several classification algorithms}, author = {athanasios kehagias and vassilios petridis and vassilis g. kaburlasos and pavlina fragkou}, journal = {journal of intelligent information systems}, year = {2003}, volume = {21}, number = {3}, pages =
comparing the accuracy of our method with other techniques, we observe significant dependency of the results on the data set.
keywords alone cannot always distinguish the relevant from the irrelevant texts and some relevant texts do not contain any reliable keywords at all.
we discuss the role of structural information for classification and describe experiments on a small collection of class labeled structured documents.
in this paper, we systematically compare combination strategies in the context of document filtering, using queries from the tipster reference corpus.
consistent with previous findings, we find that feature selection based on the labeled training set has little effect.
this model gives new representations of both news articles and news events.
with large number of categories organized as a tree, hierarchical text classification helps users to find information more quickly and accurately.
preliminary experiments with 1998 darpa bsm audit data show that the knn classifier can effectively detect intrusive attacks and achieve a low false positive rate.}, } @article{liddy94, author = {elizabeth d. liddy and woojin paik and edmund s. yu}, title = {
the texts represent short web-page descriptions from the dmoz open directory web-page ontology.
this can be thought of as automatic feature selection, which is expected to improve generalization performance further.
we show that a good hierarchical machine learning-based categoriser can be developed using small numbers of features from pre-categorised training documents.
pos tagging and recognition of proper nouns received a specific experimental attention and provided significant effects on measured accuracy.}, } @inproceedings{basili01, author = {roberto basili and alessandro moschitti and maria t. pazienza}, title = {nlp-driven ir: evaluating performances over a text classification task}, booktitle =
the documents are retrieved by considering both the predicted relevance and its value as a training observation.
we also observe that the deviation formula and discrimination formula using document frequency ratios also work as expected.
however, once a misclassification occurs at a high level class, it may result in a class that is far apart from the correct one.
generally, filtering systems calculate the similarity between the profile and each incoming document, and retrieve documents with similarity higher than a threshold.
using four subsets of the reuters text categorization test collection and a full-text test collection of which documents are varying from tens of kilobytes to hundreds, we evaluate the proposed model, especially the effectiveness of various passage types and the importance of passage location in category merging.
the method harnesses reliability indicators-variables that provide signals about the performance of classifiers in different situations.
we therefore propose the category-similarity measures and distance-based measures to consider the degree of misclassification in measuring the classification performance.
document feature characteristics, derived from the training document set, capture some inherent category-specific properties of a particular category.
it is called gaussian weighting and it is a supervised learning algorithm that, during the training phase, estimates two very simple and easily computable statistics which are: the presence \emphp, how much a term \emph{t} is present in a category \emph{c}; the expressiveness \emphe, how much \emph{t} is present outside \emph{c} in the rest of the domain.
furthermore, our analysis reveals the precise dependence of the rate of convergence on the eigenstructure of the data each node observes.
however, compression-based classification methods have drawbacks (such as slow running time), and not all such methods are equally effective.
parameter tuning through cross-validation becomes very difficult when the validation set contains no or only a few examples of the classes in the evaluation set.
in this paper, we study such a problem of performing text classification without labeled negative data tc-won).
our solution obtained creativity and precision runner-up awards at the competition.
the results of this experiment were subsequently used to create a new large medical test collection, which was used in experiments with the smart retrieval system to obtain baseline performance data as well as compare smart with the other searchers.}, } @inproceedings{hoashi00, author = {keiichiro hoashi and kazunori matsumoto and naomi inoue and kazuo hashimoto}, title = {document filtering methods using non-relevant information profile}, booktitle = {proceedings of sigir-00, 23rd acm international conference on research and development in information retrieval}, editor = {nicholas j. belkin and peter ingwersen and mun-kew leong}, publisher = {acm press, new york, us}, address = {athens, gr}, year =
the document collection was trained by a self-organizing map to form two feature maps.
the classifiers and regression equations were then applied to a new set of essays.
we also present a new interactive clustering algorithm, c-evolve, for topic discovery.
we provide experimental results demonstrating that the approach can significantly improve performance, and that it does not impair it.} } @incollection{cristianini01a, author = {huma lodhi and john shawe-taylor and nello cristianini and christopher j. watkins}, title = {discrete kernels for text categorisation}, booktitle = {
in our system, the user navigates through the query response not as a flat unstructured list, but embedded in the familiar taxonomy, and annotated with document signatures computed dynamically with respect to where the user is located at any time.
using a database of 20,569 documents, we verify that the algorithm attains levels of average precision in the 70-80\% range for category coding and in the 60-70\% range for subcategory coding.
the algorithm uses the information gain metric, combined with various frequency thresholds.
in this paper the suitability of different document representations for automatic document classification is compared, investigating a whole range of representations between bag-of-words and bag-of-phrases.
athena satisfies these requirements through linear-time classification and clustering engines which are applied interactively to speed the development of accurate models.
we carry out experiments over two different corpora and find that the proposed measures perform better than the existing ones."
the derived models, inequality me models, in effect have regularized estimation with l1 norm penalties of bounded parameters.
our working hypothesis is that it is often easier to classify a hypertext page using information provided on pages that point to it instead of using information that is provided on the page itself.
the analysis predicts learning curves with a very high precision and thus contributes to a better understanding of why and when over-fitting occurs.
rcut is most natural for online response but is too coarse-grained for global or local optimization.
using a collection factor, based on 87 per cent human consistency from other courses, the computer appears then to index with 90 per cent accuracy in this case.
hyperlinks pose new problems not addressed in the extensive text classification literature.
this article studies aggressive word removal in text categorization to reduce the noise in free texts and to enhance the computational efficiency of categorization.
typical text classifiers learn from example texts that are manually categorized.
{saarbr{\"{u}}cken, de}, url = {http://nlp3.korea.ac.kr/proceeding/coling2000/coling/ps/066.ps}, abstract = {the goal of text categorization is to classify documents into a certain number of pre-defined categories.
the system also worked reasonably well for classifying articles from a number of different computer-oriented newsgroups according to subject, achieving as high as an 80\% correct classification rate.
thus, our method seems to be well suited for heterogeneous document collections.}, } @article{klingbiel73, author = {paul h. klingbiel}, title = {machine-aided indexing of technical literature}, journal = {information storage and retrieval}, year = {1973}, volume = {9}, number = {2}, pages = {79--84}, url = {}, abstract = {
it analyzes the particular properties of learning with text data and identifies why svms are appropriate for this task.
this can be considered as the effective combination of documents with no topic or class labels (unlabeled data), labeled documents, and prior domain knowledge (in the form of the known hierarchic structure), in providing enhanced document classification performance.}, } @article{vinokourov02, author = {
without any computation-intensive resampling, the new estimators are computationally much more efficient than cross-validation or bootstrapping.
experimental results on three real world data sets from usenet, yahoo, and corporate web pages show improved performance, with a reduction in error up to 29\% over the traditional flat classifier.}, } @inproceedings{mccallum98c, author = {andrew mccallum and k. nigam}, title = {
it is shown that foil usually forms classifiers with lower error rates and higher rates of precision and recall with a relational encoding than with a propositional encoding.
using k-nn, naive bayes and centroid-based classifiers, the experimental results show that the multi-dimensional-based and hierarchical-based classification performs better than the flat-based classifications.}, } @inproceedings{thompson01, author = {paul thompson}, title = {automatic categorization of case law},
using insights gained from examining the way humans make fast decisions when classifying text documents, two new text classification algorithms are developed based on sequential sampling processes.
this paper studies the ability of symbolic learning algorithms to perform a text categorization task.
using a machine learning approach, we build classifiers that accept an audio file of conversational human speech as input, and output an estimate of the topic being discussed.
the paper describes how despite this fact the inner product can be efficiently evaluated by a dynamic programming technique.
besides, we generate several knowledge base instead of one knowledge base for the classification of new object, hoping that the combination of answers of the multiple knowledge bases result in better performance.
the mcnemar test shows that in most categories the increases are very significant.
in order to accomplish this testing, we employ the em algorithm which helps efficiently estimate parameters in a finite mixture model.
first, they relax some of the independence assumptions of naive bayes—allowing a local markov chain dependence in the observed variables—while still permitting efficient inference and learning.
however, the use of a text-classification system on this is a bit more problematic - in the most straight-forward approach each number would be considered a distinct token and treated as a word.
collaboration with friends of the earth allows us to test our ideas in a non-academic context involving high volumes of documents.
the main problem is to identify what words are best suited to classify the documents in such a way as to discriminate between them.
it employs a meta-learning phase using document feature characteristics.
the texts to be indexed are abstracts written in english.
subjective human evaluation of the keyphrases generated by extractor suggests that about 80\% of the keyphrases are acceptable to human readers.
infoclas is a first step towards the understanding of documents proceeding to a classification-driven extraction of information.
the experiments show substantial improvements over inductive methods, especially for small training sets, cutting the number of labeled training examples down to a 20th on some tasks.
the computer knows only the number of categories.
in this paper, however, we show that in the case of text classification, term-frequency transformations have a larger impact on the performance of svm than the kernel itself.
thus, they do not fully make use of the weight information provided by standard term weighting methods.
this is achieved by the exploitation of the a priori domain knowledge available, that there are relatively homogeneous temporal segments in the data stream.
we also show that whirl can be efficiently used to select from a large pool of unlabeled items those that can be classified correctly with high confidence.}, } @article{cohen99, author = {william w. cohen and yoram singer}, title = {context-sensitive learning methods for text categorization}, journal = {acm transactions on information systems}, year = {1999}, volume = {17}, number = {2}, pages = {141--173}, url = {http://www.acm.org/pubs/articles/journals/tois/1999-17-2/p141-cohen/p141-cohen.pdf}, abstract = {two recently implemented machine-learning algorithms, ripper and sleeping-experts for phrases, are evaluated on a number of large text categorization problems.
moreover, the performance of these greedy methods may be deteriorated when the reserved data dimension is extremely low.
it is shown that foil usually forms classifiers with lower error rates and higher rates of precision and recall with a relational encoding than with a propositional encoding.
the co-training setting applies to datasets that have a natural separation of their features into two disjoint sets.
the results reveal that subjects are sensitive to the manipulations of graphic and phonetic information and use the same additive information integration rule in making poetic text categorization judgements.
we then show that a quantum leap in performance is achieved when we further modify the algorithms to better address some of the specific characteristics of the domain.
then the final output of the pca is combined with the feature vectors from the class-profile which contains the most regular words in each class.
to do this, we use techniques from statistical pattern recognition to efficiently separate the feature words, or discriminants, from thenoise words at each node of the taxonomy.
as a practical matter, we also explain how the text classification system can be easily ported across domains.}, } @phdthesis{riloff94a, author = {ellen riloff}, title = {information extraction as a basis for portable text classification systems}, school = {department of computer science, university of massachusetts}, address = {
performance can be significantly improved by using active learning to select high-quality initializations, and by using alternatives to em that avoid low-probability local maxima.}, } @inproceedings{nigam98, author = {kamal nigam and andrew k. mccallum and sebastian thrun and tom m. mitchell}, title = {
we present a set of small-scale but reasonable experiments in text genre detection, author identification, and author verification tasks and show that the proposed method performs better than the most popular distributional lexical measures, i.e., functions of vocabulary richness and frequencies of occurrence of the most frequent words.
a boosting machine learning approach is applied to classifying web chinese documents that share a topic hierarchy.
we demonstrate the potential of these networks using an 82,339 word corpus from the reuters newswire, reaching recall and precision rates above 92\%.
here we show how the classification accuracy of foil on this task can be improved by discovering additional regularities on the test set pages that must be classified.
our system can be used in any application that requires fast and easily adaptable text categorization in terms of stylistically homogeneous categories.
this algorithm alleviates the problem of local minimum in the tsvm optimization procedure while also being computationally attractive.
we define parameters of categories that make it possible to acquire numerous datasets with desired properties, which in turn allow better control over categorization experiments.
in this paper we describe extensive experiments for semantic text routing based on classified library titles and newswire titles.
the indexing dictionary can be derived automatically from a set of manually indexed documents.
categorizing web documents in hierarchical catalogues}, booktitle = {proceedings of ecir-01, 23rd european colloquium on information retrieval research}, editor = {}, year = {2001}, address = {darmstadt, de}, publisher = {}, pages = {}, url = {http://ls6-www.informatik.uni-dortmund.de/bib/fulltext/ir/frommholz:01a.pdf}, abstract = {automatic categorization of web documents (e.g. html documents)
we are also given a training corpus of documents already placed in one or more categories.
when a natural split does not exist, co-training algorithms that manufacture a feature split may outperform algorithms not using a split.
it of applications, ranging from tracking usersõ products or about political candidates as expressed forums, to customer relationship management.
the specifics of our classifier is that it allows accurate categorization of short messages containing only a few words.
in addition, the evaluation results given by the kddcup 2005 organizer confirm the effectiveness of our proposed approaches.
our simulation results also show the effectiveness of idc in text categorization problems.
using the same representation of documents, charade offers better performance than earlier reported experiments with decision trees on the same corpus.
we employ machine learning techniques to create dynamic document categorizations based on the full-text of articles that are retrieved in response to users' queries.
experiments over real-world text corpus are carried out, which validates the effectiveness and efficiency of the proposed approach.
this allows the individual models for each topic on the second level to focus on finer discriminations within the group.
a major challenge in indexing unstructured hypertext databases is to automatically extract meta-data that enables structured searching using topic taxonomies, circumvents keyword ambiguity and improves the quality of searching and profile-based routing and filtering.
empirical estimates of weights (likelihood ratios) become unstable when counts are small.
after 8 cycles the computer is found to have formed 9 groups consisting of about 50 per cent of documents that were also lumped together by professional indexers on the basis of subject content.
textual information is processed by two methods of analysis: a natural language analysis followed by a statistical analysis.
our experiments show that this method significantly outperforms the combination of single label approach."
despite its simplicity, results of experiments on web pages and tv closed captions demonstrate high classification accuracy.
in addition, we compare the algorithms on a larger collection of 1700 texts and describe an automated method for empirically deriving appropriate threshold values.
we conclude that even the most careful term selection cannot overcome the differences in document frequency between phrases and words, and propose the use of term clustering to make phrases more cooperative.}, } @article{krier02, author = {marc krier and francesco zacc{\`a}}, title = {automatic categorization applications at the european patent office}, journal = {world patent information}, year = {2002}, volume =
the results obtained by ripper surpass those of the operational process.}, } @inproceedings{tong00, author = {simon tong and daphne koller}, title = {support vector machine active learning with applications to text classification}, booktitle = {proceedings of icml-00, 17th international conference on machine learning}, editor = {pat langley}, year = {2000}, address = {stanford, us}, pages = {999--1006}, publisher =
it is found that decision forest outperforms both c4.5 and knn in all cases, and that category dependent term selection yields better accuracies.
this paper reports on a system that uses natural language text processing to derive keywords from free text news stories, separate these keywords into segments, and automatically build a segmented database.
using the intra- and extra-document statistics, both a simple posteriori calculation on a small example and an experiment on a large reuters-21578 database demonstrate the advantage of the dlsi space-based probabilistic classifier over the lsi space-based classifier in classification performance.}, } @inproceedings{chen04, author = {wenliang chen and jingbo zhu and honglin wu and yao tianshun}, title = {automatic learning features using bootstrapping for text categorization}, booktitle = {proceedings of cicling-04, 5th international conference on computational linguistics and intelligent text processing}, year = {2004}, editor = {alexander f. gelbukh}, publisher = {springer verlag, heidelberg, de}, address = {seoul, ko}, note = {
we demonstrate that the classifiers perform 10-15\% better than relevance feedback via rocchio expansion for the trec-2 and trec-3 routing tasks.
the simple method of conducting hypothesis testing over word-based distributions in categories suffers from the data sparseness problem.
but in cases where many features are highly redundant with each other, we must utilize other means, for example, more complex dependence models such as bayesian network classifiers.
when an existing document is used as an exemplar, the completeness and accuracy with which topically related documents are retrieved is comparable to that of the best existing systems.
we also observe that dimensionality reduction techniques eliminate a large number of ocr errors and improve categorization results.}, } @inproceedings{taira01, author =
comparisons with traditional rocchio's algorithm adapted for text categorization, as well as flat neural network classifiers are provided.
while knn and aram yield better performances than svm on small and clean data sets, svm and aram significantly outperformed knn on noisy data.
we can say that the usage of machine learning techniques on text databases (usually referred to as text-learning) is an important part of the content-based approach.
parameter tuning through cross-validation becomes very difficult when the validation set contains no or only a few examples of the classes in the evaluation set.
moreover, a macro-averaged recall and precision was calculated: the former reported a 0.72, the latter a 0.79.
in reality, however, many information retrieval problems can be more accurately described as learning a binary classifier from a set of incompletely labeled examples, where we typically have a small number of labeled positive examples and a very large number of unlabeled examples.
although much research has been done on text categorization, this algorithm is novel in that it is unsupervised, i.e., it does not require pre-labeled training examples, and it can assign multiple category labels to documents.
when choosing optimal pairs of metrics for each of the four performance goals, bns is consistently a member of the pair---e.g., for greatest recall, the pair bns + f1-measure yielded the best performance on the greatest number of tasks by a considerable margin.}, } @inproceedings{forman04, author = {
we built these profiles by selecting feature words and phrases from the training documents.
to select the proper number of , we use assures the degree of our disappointment in any differences between the true distribution over inputs and the learner's prediction.
the approach of using a separate category tree represents an extension of the standard relevance list, and provides a way to refine the search on need, offering the user a non-imposing, but potentially powerful tool for locating needed information quickly and efficiently.
for the hierarchical approach, we found the same accuracy using a sequential boolean decision rule and a multiplicative decision rule.
in particular, whirl generally achieves lower generalization error than c4.5, ripper, and several nearest-neighbor methods.
when the complete feature set is available, the classifier learning algorithm can better relate to the suitable representation level the different complex features like linguistic ones (e.g. syntactic categories associated to words in the training material or terminological expressions).
the model is based on the concept of `uncertainty sampling', a technique that allows for relevance feedback both on relevant and nonrelevant documents.
in previous work, such "distributional clustering" of features has been found to achieve improvements over feature selection in terms of classification accuracy, especially at lower number of features [2, 28].
an object-oriented design allows easy addition of new preprocessors, machine learning algorithms, and classifier types.}, } @article{li02, author
w. bruce croft and alistair moffat and van rijsbergen, cornelis j. and ross wilkinson and justin zobel}, publisher = {acm press, new york, us}, year = {1998}, address = {melbourne, au}, pages = {90--95}, url = {http://cobar.cs.umass.edu/pubfiles/ir-121.ps}, abstract = {several standard text-categorization techniques were applied to the problem of automated essay grading.
{581--586}, url = {http://dlib.computer.org/conferen/ijcnn/0619/pdf/06193581.pdf}, abstract = {one important task for text data mining is automatic text categorization, which assigns a text document to some predefined category according to their correlations.
the article documents the author's participation in the filtering and routing tasks of trec-6 with the commercial filtering system teklis.
the effectiveness of the llsf mapping and the significant improvement over alternative approaches was evident in the tests.}, } @article{yang94, author = {yiming yang and christopher g. chute}, title = {
reprinted in karen sparck jones and peter willett (eds.), ``readings in information retrieval'', morgan kaufmann, san francisco, us, 1997, pp.\ 513--517.}, url = {http://www.acm.org/pubs/articles/proceedings/ir/62437/p333-biebricher/p333-biebricher.pdf}, abstract = {since october 1985, the automatic indexing system air/phys has been used in the input production of the physics data base of the fachinformationszentrum karlsruhe/west germany.
this study reports the results of a series of experiments in the techniques of automatic document classifications.
to facilitate web object searching and organizing, in this paper, we propose a novel approach to web object indexing, by discovering its inherent structure information with existed domain knowledge.
linear support vector machines (svms) are particularly promising because they are very accurate, quick to train, and quick to evaluate.} } @inproceedings{elyaniv01, author = {
since tcfp algorithm is very simple, its implementation and training process can be done very easily.
to demonstrate the language independent and task independent nature of these classifiers, we present experimental results on several text classification problems—authorship attribution, text genre classification, and topic detection—in several languages—greek, english, japanese and chinese.
we also report results of making actual use of the selected $n$-grams in the context of a linear classifier induced by means of the rocchio method.}, } @inproceedings{carreras01, author = {xavier carreras and llu\'{\i}s m\'arquez}, title = {
this margin widened in tasks with high class skew, which is rampant in text classification problems and is particularly challenging for induction algorithms.
neural networks allow us to model higher-order interaction between document terms and to simultaneously predict multiple topics using shared hidden features.
{morgan kaufmann publishers, san francisco, us}, url = {http://robotics.stanford.edu/users/sahami/papers-dir/ml96-mcmm.ps}, abstract = {the paper introduces the use of the multiple cause mixture model for automatic text category assignment.
we adopt a machine learning approach and the model parameters are learned from a labeled training set of representative documents.
we show that our method is especially useful for classification tasks involving a large number of categories where co-training doesn't perform very well by itself and when combined with ecoc, outperforms several other algorithms that combine labeled and unlabeled data for text classification in terms of accuracy, precision-recall tradeoff, and efficiency.}, } @inproceedings{ghani02, author = {rayid ghani}, title = {
then, test documents are scanned and categories ranked based on the presence of vocabulary terms.
text categorization experiments demonstrates the ability of this representation to catch information about the semantic content of the text.}, } @inproceedings{viechnicki98, author = {peter viechnicki}, title = {
in this paper, we describe a novel text classifier that can effectively cope with structured documents.
the results is an original statistical classifier fed with linguistic (i.e. more complex) features and characterized by the novel feature selection and weighting model.
the system does not perform a complete semantic or syntactic analyses of the input stories.
we test this approach on a large collection of personal e-mail messages, which we make publicly available in "encrypted" form contributing towards standard benchmarks.
we focus on the robustness of these methods in dealing with a skewed category distribution, and their performance as function of the training-set category frequency.
we then systematically study the key factors in the can model that can influence the classification performance, and analyze the strengths and weaknesses of the model.} } @article{makkonen:2004:sst, author = {juha makkonen and helena ahonen-myka and marko salmenkivi}, title = {simple semantics in topic detection and tracking}, journal = {information retrieval}, publisher =
the combination of these techniques significantly influences the overall performance of text categorization.
text categorization: a symbolic approach}, booktitle = {proceedings of sdair-96, 5th annual symposium on document analysis and information retrieval}, publisher = {}, editor = {}, address = {las vegas, us}, year = {1996}, pages = {87--99}, url = {http://www-poleia.lip6.fr/~moulinie/sdair.ps.gz}, abstract = {recent research in machine learning has been concerned with scaling-up to large data sets.
link information alone is able to obtain gains of up to 46 points in f1, when compared to a traditional content-based classifier.
the use of bigrams to enhance text categorization}, journal = {information processing and management}, year = {2002}, volume = {38}, number = {4}, pages = {529--546}, url = {http://www.serve.com/cmtan/meng/ig_m.pdf}, abstract = {
in comparison to the previously proposed agglomerative strategies our divisive algorithm achieves higher classification accuracy especially at lower number of features.
"we enhance machine learning algorithms for text categorization with generated features based on domain-specific and common-sense knowledge.
however, it significantly degrades precision when ambiguity arises, i.e., when there exist more than one candidate category to which a document can be assigned.
we are particularly interested in domain transfer: how well the learned classifiers generalize from the training corpus to a new document corpus.
the process generates, for each $c_{i}$ in a set $c=\{c_{1},\ldots,c_{m}\}$ of domains, a lexicon $l^{i}_{1}$, bootstrapping from an initial lexicon $l^{i}_{0}$ and a set of documents $\theta$ given as input.
these results are compared to an existing operational process using boolean queries manually constructed by domain experts.
profile filters the netnews and uses a scale of 11 predefined values of relevance.
our approach is to analyze the document cluster map to find centroids of some super-clusters.
three dictionaries produced by autoslog for different domains performed well in the author`s text classification experiments.}, } @incollection{riloff99, author = {ellen riloff and jeffrey lorenzen}, title = {extraction-based text categorization: generating domain-specific role relationships}, booktitle = {natural language information retrieval}, editor = {tomek strzalkowski}, year = {1999}, pages = {167--196}, publisher =
an extended set of e-mail document features including structural characteristics and linguistic patterns were derived and, together with a support vector machine learning algorithm, were used for mining the e-mail content.
intuitively, one would like each of the two kernels to contribute information that is not available to the other.
the dimension of the feature vectors is then reduced by linear transformation, keeping the essential information.
comparison with manually assigned classes shows that link information enhances classification in data with sufficiently high link density, but is detrimental to performance at low link densities or if the quality of the links is degraded.
initial experimental results demonstrate that this approach can produce accurate recommendations.}, } @inproceedings{moschitti03, author = {alessandro moschitti}, title = {a study on optimal parameter tuning for rocchio text classifier}, booktitle = {proceedings of ecir-03, 25th
this is expensive, requires a degree of sophistication about linguistics and classification, and makes it difficult to use combinations of weak predictors.
the user interface helps users search in fields without requiring the knowledge of inquery query operators.
within this paradigm, a general inductive process automatically builds a classifier by ``learning'', from a set of previously classified documents, the characteristics of one or more categories; the advantages are a very good effectiveness, a considerable savings in terms of expert manpower, and domain independence.
it uses fisher's linear discriminant, a classical tool from statistical pattern recognition, to project training instances to a carefully selected low-dimensional subspace before inducing a decision tree on the projected instances.
considerable improvement in the classification accuracies of two popular classification algorithms on standard labeled data-sets with and without artificially introduced noise, as well as in the presence and absence of unlabeled data, indicates that this may be a promising method to reduce the burden of manual labeling."
a tc system for chinese texts using words as features is implemented.
this reduced feature space is then used to train a classifier over a larger training set because more documents now fit into the same amount of memory.
we found that it and other existing methods failed to produce good results on an industrial text classification problem.
training algorithms are derived for both cases, and illustrated on real data by clustering news stories and categorising newsgroup messages.
empirical results indicate that our approach outperforms the best published results on this reuters collection.
we describe a method for classifying news stories using memory based reasoning (mbr) a k-nearest neighbor method), that does not require manual topic definitions.
the determination of category themes and their hierarchical structures were most done by human experts.
in the domain of terrorism, autoslog created a dictionary using a training corpus and five person-hours of effort that achieved 98\% of the performance of a hand-crafted dictionary that took approximately 1500 person-hours to build.
these results demonstrate that this approach can scale up to a large real-world task and show a lot of potential for text classification.}, } @inproceedings{wermter99, author = {stefan wermter and christo panchev and garen arevian}, title = {hybrid neural plausibility networks for news agents}, booktitle = {proceedings of aaai-99, 16th conference of the american association for artificial intelligence}, publisher =
document classification is characterized by the large number of attributes involved in the objects (documents).
above 90\% correct recognition rates have been achieved for the major categories concerned.
rtcut, a new method combining the strength of category ranking and scoring, outperforms both pcut and rcut significantly.}, } @article{yang02, author = {yiming yang and se{\'{a}}n slattery and rayid ghani}, title = {a study of approaches to hypertext categorization}, journal = {journal of intelligent information systems}, year = {2002}, note = {special issue on automated text categorization}, volume = {18}, number = {2/3}, pages = {219--241}, url = {http://www.wkap.nl/article.pdf?391248}, abstract = {
these results help explain why co-training algorithms are both discriminative in nature and robust to the assumptions of their embedded classifiers.}, } @phdthesis{nigam01, author = {kamal nigam}, title = {using unlabeled data to improve text classification}, school =
however, the exponential growth in the volume of on-line textual information makes it nearly impossible to maintain such taxonomic organization for large, fast-changing corpora by hand.
visual keywords can be constructed automatically from samples of visual data through supervised/unsupervised learning.
results show that for hierarchical techniques it is better to use hierarchical training sets.}, } @inproceedings{cerny83, author = {barbara a. cerny and anna okseniuk and j. dennis lawrence}, title = {
we augment naive bayes models with statistical n-gram language models to address short-comings of the standard naive bayes text classifier.
the results reveal that a new feature selection metric we call 'bi-normal separation' (bns), outperformed the others by a substantial margin in most situations.
we also carefully analyze the case of those documents whose categorization is not in accordance with the one provided by the human specialists.
maintaining catalogues manually is becoming increasingly difficult, due to the sheer amount of material on the web; it is thus becoming necessary to resort to techniques for the automatic classification of documents.
extensive experiments using two benchmarks and a large real-life collection are conducted.
to save the storage space and computation time in text categorization, efficient and effective algorithms for reducing the data before analysis are highly desired.
we propose to solve a text categorization task using a new metric between documents, based on a priori semantic knowledge about words.
we found that lr performs strongly and robustly in optimizing t11su (a trec utility function) while rocchio is better for optimizing ctrk (the tdt tracking cost), a high-recall oriented objective function.
the test results show the hybrid method is better than the previous rough set-based approach.}, } @inproceedings{basili00, author = {roberto basili and alessandro moschitti and maria t. pazienza}, title = {language-sensitive text classification}, booktitle = {proceedings of riao-00, 6th international conference ``recherche d'information assistee par ordinateur''}, editor = {}, address = {paris, france}, year = {2000}, pages = {331--343}, url = {}, abstract =
most previous studies found that the majority of these features are relevant for classification, and that the performance of text categorization with support vector machines peaks when no feature selection is performed.
we describe a text categorization task and an experiment using documents from the reuters and ohsumed collections.
many methods can be used to categorize texts once their words are known, but ocr can garble a large proportion of words, particularly when low quality images are used.
we present a generative bayesian model for the modeling of structured (e.g. xml) documents.
the former never consider the negative features, which are quite valuable, while the latter cannot ensure the optimal combination of the two kinds of features especially on imbalanced data.
however, this perspective is not appropriate in the case of many digital libraries that offer as contents scanned and optically read books or magazines.
we show that the accuracy of a naive bayes classifier over text classification tasks can be significantly improved by taking advantage of the error-correcting properties of the code.
this is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available.
the experimental results show that the system using a simple classifier achieved 95.31\% accuracy.
in particular, boosting methods such as adaboost have shown good performance applied to real text data.
experimental results show that the methods are a significant improvement over previously used methods in a number of areas.
however, the algorithm is much more efficient (because the learner does not have to be invoiced at all) and thus solves model selection problems with as many as a thousand relevant attributes and 12000 examples.}, } @inproceedings{schneider03, author = {{karl-michael} schneider}, year = {2003}, title = {a comparison of event models for naive bayes anti-spam e-mail filtering}, pages = {}, address = {}, editor = {}, booktitle = {proceedings of eacl-03, 11th conference of the european chapter of the association for computational linguistics}, url = {http://www.phil.uni-passau.de/linguistik/mitarbeiter/schneider/pub/eacl2003.pdf}, abstract = {}, } @inproceedings{schutze95, author = {hinrich sch{\"{u}}tze and david a. hull and jan o. pedersen}, title = {
the article documents the author's participation in the filtering and routing tasks of trec-6 with the commercial filtering system teklis.
the proxy logs the user's activities and extracts the user's interests without user intervention.
using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on tdt5, trec10 and trec11.
the rule base defines what categories the application can assign to texts and contains rules that make the categorization decisions for particular texts.
to demonstrate the language independent and task independent nature of these classifiers, we present experimental results on several text classification problemsauthorship attribution, text genre classification, and topic detectionin several languagesgreek, english, japanese and chinese.
we have tried our representation scheme for automatic document categorisation on the reuters' test set of documents.
their only potential drawback is their training time and memory requirement.
this system worked very well for language classification, achieving in one test a 99.8\% correct classification rate on usenet newsgroup articles written in different languages.
therefore, an accurate classifier is an essential component of a hypertext database.
in natural language tasks like text categorization, we usually have an enormous amount of unlabeled data in addition to a small amount of labeled data.
a direct computation of this feature vector would involve a prohibitive amount of computation even for modest values of k, since the dimension of the feature space grows exponentially with k.
document feature characteristics, derived from the training document set, capture some inherent properties of a particular category.
the approach of using a separate category tree represents an extension of the standard relevance list, and provides a way to refine the search on need, offering the user a non-imposing, but potentially powerful tool for locating needed information quickly and efficiently.
the input nodes of the network are words in the training texts, the nodes on the intermediate level are the training texts, and the output nodes are categories.
rough set (rs) theory can be applied to reducing the dimensionality of data used in if/ir tasks, by providing a measure of the information content of datasets with respect to a given classification.
using multinomial naive bayes and regularized logistic regression as classifiers, our experiments show both great potential and actual merits of explicitly combining positive and negative features in a nearly optimal fashion according to the imbalanced data.}, } @inproceedings{zhou00, author = {shuigeng zhou and ye fan and jiangtao hua and fang yu and yunfa hu}, title = {hierachically classifying chinese web documents without dictionary support and segmentation procedure}, booktitle = {proceedings of waim-00, 1st international conference on web-age information management}, publisher = {springer verlag, heidelberg, de}, editor = {hongjun lu and aoying zhou}, note = {
thus, svm adapts efficiently in dynamic environments that require frequent additions to the document collection.
experiments on two real-world spidering tasks show a three-fold improvement in spidering efficiency over traditional breadth-first search, and up to a two-fold improvement over reinforcement learning with immediate reward only.}, } @article{ribeironeto01, author = {berthier ribeiro-neto and alberto h.f. laender and luciano r. {de lima}}, title = {
{843--848}, url = {}, abstract = {a language-independent means of gauging topical similarity in unrestricted text is described.
we show that foil's performance can be improved by relation selection, a first order analog of feature selection.
furthermore, by combining these methods, we significantly reduced the variance in performance of our event tracking system over different data collections, suggesting a robust solution for parameter optimization.}, } @inproceedings{yang00a, author = {yiming yang and thomas ault and thomas pierce}, title = {
experiments on a real-world data set show a reduction in classification error by up to 66\% over the traditional naive bayes classifier.
{santiago de compostela, es}, year = {2005}, pages = {300--314}, url = {}, abstract = {compression-based text classification methods are easy to apply, requiring virtually no preprocessing of the data.
in addition, we find that some types of appraisal appear to be more significant for sentiment classification than others."
scoring rules can further take advantage of the hierarchy by considering only second-level categories that exceed a threshold at the top level.
our experimental comparison of eleven feature scoring measures show that considering domain and algorithm characteristics significantly improves the results of classification.}, } @article{moens00, author = {marie-francine moens and jos dumortier}, title = {
we suggest, for text categorization, the integration of external wordnet lexical information to supplement training data for a semi-supervised clustering algorithm which (i) uses a finite design set of labeled data to (ii) help agglomerative hierarchical clustering algorithms (ahc) partition a finite set of unlabeled data and then (iii) terminates without the capacity to classify other objects.
the relevance values are interpreted as subjective probabilities and hence are mapped into the real interval [0; 1].
a proposed purification process can effectively reduce the dimensionality of the feature space from 50,576 terms in the word-based approach to 19,865 terms in the unknown word-based approach.
this paper describes a learning news agent hynet which uses hybrid neural network techniques for classifying news titles as they appear on an internet newswire.
for the comparison, we run a series of experiments using a digital library of computer science papers and a web directory.
even when multi-labels are sparse, the models improve subset classification error by as much as 40%."
the experimental results show that the proposed methods of incorporating prior knowledge is effective." } @inproceedings{pekar:2004:cwp, author =
it outperforms existing systems by keeping most of their interesting properties (i.e. easy implementation, low complexity and high scalability).
the process generates, for each $c_{i}$ in a set $c=\{c_{1},\ldots,c_{m}\}$ of domains, a lexicon $l^{i}_{1}$, bootstrapping from an initial lexicon $l^{i}_{0}$ and a set of documents $\theta$ given as input.
we employ machine learning techniques to create dynamic document categorizations based on the full-text of articles that are retrieved in response to users' queries.
in this study, we proposed a mining-based category evolution (mice) technique to adjust document categories based on existing categories and their associated documents.
the method maintains a window on the training data.
previous researches on advanced representations for document retrieval have shown that statistical state-of-the-art models are not improved by a variety of different linguistic representations.
the number of common keywords between keywords from the document itself and representative keywords from back data classifies documents.
adaptive information filtering using evolutionary computation}, journal = {information sciences}, year = {2000}, volume = {122}, number = {2/4}, pages = {121--140}, url = {http://www.elsevier.nl/gej-ng/10/23/143/56/27/27/article.pdf}, abstract = {information filtering is concerned with filtering data streams in such a way as to leave only pertinent data (information) to be perused.
we can say that the usage of machine learning techniques on text databases (usually referred to as text-learning) is an important part of the content-based approach.
our analyses show that when the positive training data is not too under-sampled, svmc significantly outperforms other methods because svmc basically exploits the natural "gap" between positive and negative documents in the feature space, which eventually corresponds to improving the generalization performance.
in addition, we find that some types of appraisal appear to be more significant for sentiment classification than others."
we find that on average, labeling a feature takes much less time than labeling a document.
we experimentally evaluated waknn on 52 document data sets from a variety of domains and compared its performance against several classification algorithms, such as c4.5, ripper, naive-bayesian, pebls and vsm.
however, the complexity of natural languages and the extremely high dimensionality of the feature space of documents have made this classification problem very difficult.
we present a generative bayesian model for the modeling of structured (e.g. xml) documents.
classification of documents allow the automatic distribution or archiving of letters and is also an excellent starting point for higher-level document analysis.}, } @article{hoyle73, author = {w.g. hoyle}, title = {automatic indexing and generation of classification by algorithm}, journal = {information storage and retrieval}, year = {1973}, volume = {9}, number = {4}, pages = {233--242}, url = {}, abstract = {a system of automatic indexing based on bayes' theorem is described briefly.
they can be computed at essentially no extra cost immediately after training a single svm.
the document organization and classification performance of our ica-based hierarchical classifier are evaluated in several encouraging experiments conducted on a journalistic-style text corpus for speech synthesis in catalan.}, } @inproceedings{shanahan03, author = {james g. shanahan and norbert roma}, title = {
additionally, the computation procedure can be improved to locate the sets of duplicate or plagiarised documents.
first, they relax some of the independence assumptions of naive bayesallowing a local markov chain dependence in the observed variableswhile still permitting efficient inference and learning.
support vector machines (svms) excel in binary classification, but the elegant theory behind large-margin hyperplane cannot be easily extended to multi-class text classification.
owing to the use of context-sensitive features, the classifier is very accurate.
in this article, we apply ml to text-database analyses and knowledge acquisitions from text databases.
given seeker, the text retrieval system, we achieved these results in about two person-months.
automated text classification is attractive because it frees organizations from the need of manually organizing document bases, which can be too expensive, or simply infeasible given the time constraints of the application or the number of documents involved.
the algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents.
for example, in email classification, it is possible to use instance representations that consider not only the text of each message, but also numerical-valued features such as the length of the message or the time of day at which it was sent.
the technique of periodical updates improves the routing accuracy ranging from 20\% to 100\% but incurs runtime overhead.
in this paper we describe experiments that investigate the effects of ocr errors on text categorization.
a major knowledge-engineering bottleneck for information extraction systems is the process of constructing an appropriate dictionary of extraction patterns.
rtcut, a new method combining the strength of category ranking and scoring, outperforms both pcut and rcut significantly.}, } @article{yang02, author = {yiming yang and se{\'{a}}n slattery and rayid ghani}, title = {
experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 33\%.}, } @inproceedings{oh00, author = {hyo-jung oh and sung hyon myaeng and mann-ho lee}, title = {a practical hypertext categorization method using links and incrementally available class information}, booktitle = {
the strategy is generic and takes advantage of training errors to successively refine the classification model of a base classifier.
in this paper we show how performance on new event detection (ned) can be improved by the use of text classification techniques as well as by using named entities in a new way.
we also show that features found to be useful in one corpus do not transfer well to other corpora with different genres." } @article{vogel:2005:kddcup2005, author =
moreover, the system can then save such document organizations in user profiles which can then be used to help classify future query results by the same user.
our novel classifier can take advantage of the information in the structure of document that conventional, purely term-based classifiers ignore.
the layout of a document contains a significant amount of information that can be used to classify it by type in the absence of domain-specific models.
in addition, nlp systems can increase the information contained in keyword fields by separating keywords into segments, or distinct fields that capture certain discriminating content or relations among keywords.
in this paper, we present a hierarchical text classifier based on independent component analysis (ica), which is capable of (i) organizing the contents of the corpus in a hierarchical manner and (ii) classifying the texts to be synthesized according to the learned structure.
all of these methods showed significant improvement (up to 71\% reduction in weighted error rates) over the performance of the original knn algorithm on tdt benchmark collections, making knn among the top-performing systems in the recent tdt3 official evaluation.
furthermore, the use of shape coding is particularly advantageous over ocr in the processing of page images of poor quality.
it produces inferior results because it is insensitive to subtle differences between articles that belong to a category and those that do not.
our novel classifier can take advantage of the information in the structure of document that conventional, purely term-based classifiers ignore.
we use information from a pre-existing taxonomy in order to supervise the creation of a set of related clusters, though with some freedom in defining and creating the classes.
our experiments show that the lower dimensional spaces computed by our algorithm consistently improve the performance of traditional algorithms such as c4.5, k-nearest-neighbor, and
pcut copes better with rare categories and exhibits a smoother trade-off in recall versus precision, but is not suitable for online decision making.
mh$^kr$} is based on the idea to build, at every iteration of the learning phase, not a single classifier but a sub-committee of the $k$ classifiers which, at that iteration, look the most promising.
a machine-aided indexing (mai) system is described that indexes one million words of text per hour of cpu time.
this study benchmarks the performance of twelve feature selection metrics across 229 text classification problems drawn from reuters, ohsumed, trec, etc.
a combination of different classifiers produced better results than any single type of classifier.
in general, based on the current recall and precision performance, as well as the detailed analysis, we show that recurrent plausibility networks hold a lot of potential for developing learning and robust newswire agents for the internet.}, } @inproceedings{wibowo02, author = {wahyu wibowo and hugh e. williams}, title = {simple and accurate feature selection for hierarchical categorisation}, booktitle = {proceedings of the 2002 acm symposium on document engineering}, publisher = {acm press, new york, us}, editor = {}, year = {2002}, address = {mclean, us}, pages = {111--118}, url = {http://doi.acm.org/10.1145/585058.585079}, abstract = {categorisation of digital documents is useful for organisation and retrieval.
we show that it can effectively select an appropriate window size in a robust way.}, } @inproceedings{knorz82, author = {knorz, gerhard}, title = {a decision theory approach to optimal automated indexing}, booktitle = {proceedings of sigir-82, 5th acm international conference on research and development in information retrieval}, year =
the motivation is that there are statistical problems associated with natural language text when it is applied as input to existing machine learning algorithms (too much noise, too many features, skewed distribution).
on-line information services generally depend on keyword indices rather than other methods of retrieval, because of the practical features of keywords for storage, dissemination, and browsing as well as for retrieval.
evaluating the performance of a two-level implementation on the reuters-22173 testbed of newswire articles shows the most significant improvement for rare classes.}, } @article{weiss99, author = {sholom m. weiss and chidanand apt\'{e} and fred j. damerau and david e. johnson and frank j. oles and thilo goetz and thomas hampp}, title = {maximizing text-mining performance}, journal = {ieee intelligent systems}, year = {1999}, number = {4}, volume = {14}, pages =
for text categorization (tc) are available fewer and less definitive studies on the use of advanced document representations as it is a relatively new research area (compared to document retrieval).
finally, the generative model may be used to derive a fisher kernel expressing similarity between documents.}, } @article{gentili01, author = {g.l. gentili and mauro marinilli and alessandro micarelli and filippo sciarrone}, title = {
hence, the creation of new directories or the modification of existing ones require strong investments.
we show that judicious use of a hierarchy can significantly improve both the speed and effectiveness of the categorization process.
the results show that the use of the hierarchical structure improves text categorization performance significantly.}, } @inproceedings{ruiz99a, author = {miguel e. ruiz and padmini srinivasan}, title = {
{4}, pages = {719--736}, url = {http://www.jucs.org/jucs_4_9/categorisation_by_context}, abstract = {assistance in retrieving of documents on the world wide web is provided either by search engines, through keyword based queries, or by catalogues, which organise documents into hierarchical collections.
experiments with the purely theoretical approach and with several heuristic variations show that heuristic assumptions may yield significant improvements.}, } @article{fuhr94, author = {norbert fuhr and ulrich pfeifer}, title = {probabilistic information retrieval as combination of abstraction inductive learning and probabilistic assumptions}, journal = {acm transactions on information systems}, year = {1994}, number = {1}, volume = {12}, pages = {92--115}, url = {http://ls6-www.informatik.uni-dortmund.de/bib/fulltext/ir/fuhr_pfeifer:94.ps.gz},
we review methods for analyzing novelty and then describe newsjunkie, a system that personalizes news for users by identifying the novelty of stories in the context of stories they have already reviewed.
using the intra- and extra-document statistics, both a simple posteriori calculation on a small example and an experiment on a large reuters-21578 database demonstrate the advantage of the dlsi space-based probabilistic classifier over the lsi space-based classifier in classification performance.}, } @inproceedings{chen04, author = {wenliang chen and jingbo zhu and honglin wu and yao tianshun}, title = {automatic learning features using bootstrapping for text categorization}, booktitle = {proceedings of cicling-04, 5th international conference on computational linguistics and intelligent text processing}, year = {2004}, editor = {alexander f. gelbukh}, publisher = {springer verlag, heidelberg, de}, address = {seoul, ko}, note = {
the simple tfidf classifier is chosen to train sample data and to classify other new data.
performance is usually, though not always, less good here, but the term weighting functions implicit in the resulting ranking functions are intriguing, and the approach could easily be adapted to mixtures of textual and nontextual data.}, } @inproceedings{jacobs92, author = {paul s. jacobs}, title = {
we show that it can effectively select an appropriate window size in a robust way.}, } @inproceedings{knorz82, author = {knorz, gerhard}, title = {a decision theory approach to optimal automated indexing}, booktitle = {proceedings of sigir-82, 5th acm international conference on research and development in information retrieval}, year = {1982}, editor = {gerard salton and hans-jochen schneider}, pages = {174--193}, address = {berlin, de}, publisher = {springer verlag, heidelberg, de}, note = {published in the ``lecture notes in computer science'' series, number 146}, url = {}, abstract = {}, } @inproceedings{ko00, author = {youngjoong ko and jungyun seo}, title = {automatic text categorization by unsupervised learning},
a theoretical analysis and experiments show that the new method can effectively estimate the performance of svm text classifiers in an efficient way.}, } @inproceedings{joachims01b, author = {thorsten joachims and nello cristianini and john shawe-taylor}, title = {composite kernels for hypertext categorisation}, booktitle = {proceedings of icml-01, 18th international conference on machine learning}, editor = {carla brodley and andrea danyluk}, address = {williams college, us}, year = {2001}, pages = {250--257}, publisher =
a machine-aided indexing (mai) system is described that indexes one million words of text per hour of cpu time.
existing classification schemes which ignore the hierarchical structure and treat the topics as separate classes are often inadequate in text classification where the there is a large number of classes and a huge number of relevant features needed to distinguish between them.
a system called autoslog is presented which automatically constructs dictionaries for information extraction, given an appropriate training corpus.
for solving these kinds of problems, neural networks have the advantage of extracting the underlying relationships between the input data and the output classes automatically.
in particular, we show that in our environment, ocr errors have no effect on categorization when we use a classifier based on the naive bayes model.
to our knowledge, this work is the first to report performance results with the entire new reuters corpus.}, } @article{craven00, author = {mark craven and dan dipasquo and dayne freitag and andrew k. mccallum and tom m. mitchell and kamal nigam and se{\'{a}}n slattery}, title = {
though our experiments with words yielded good results, we found instances where the phrase-based approach produced more effectiveness.
= {}, editor = {}, year = {1994}, address = {las vegas, us}, pages = {161--175}, url = {http://www.nonlineardynamics.com/trenkle/papers/sdair-94-bc.ps.gz}, abstract = {text categorization is a fundamental task in doc-ument processing, allowing the automated handling of enormous streams of documents in electronic form.
when few examples are available, we observe that accuracy is sensitive to k and that best k tends to increase with training size.
the architecture relies on hidden markov models whose emissions are bag-of-words resulting from a multinomial word event model, as in the generative portion of the naive bayes classifier.
our approach has practical advantages for problem solving by introducing the viewpoint of tasks to achieve higher performance.}, } @inproceedings{mccallum98, author = {andrew k. mccallum and kamal nigam}, title = {
experimental results confirm the predictions of the theory in the hypertext domain.}, } @inproceedings{joachims01c, author = {thorsten joachims}, title = {a statistical learning model of text classification with support vector machines}, booktitle = {proceedings of sigir-01, 24th acm international conference on research and development in information retrieval},
we develop an automatic text categorization approach and investigate its application to text retrieval.
this paper studies how link information can be used to improve classification results for web collections.
we found that pages in some genres change rarely if at all and can be used in present-day research experiments without requiring an updated version.
in our classifier, web documents are represented by n-grams (n$\leq 4$) that are easy to be extracted.
the input nodes of the network are words in the training texts, the nodes on the intermediate level are the training texts, and the output nodes are categories.
{2000}, number = {6}, volume = {36}, url = {}, abstract = {automatic text categorization is an important research area and has a potential for many text-based applications including text routing and filtering.
stretch offers ease of use and application programming and the ability to dynamically adapt to new types of documents.
the technique for machine-aided indexing (mai) developed at the defense documentation center (ddc) is illustrated on a randomly chosen abstract.
the rapid expansion of multimedia digital collections brings to the fore the need for classifying not only text documents but their embedded non-textual parts as well.
examples are agents for locating information on world wide web and usenet news filtering agents.
a series of experiments indicates that the use of senses does not result in any significant categorization improvement.}, } @inproceedings{kessler97, author = {brett kessler and geoff nunberg and hinrich sch{\"{u}}tze}, title = {automatic detection of text genre}, booktitle = {proceedings of acl-97, 35th annual meeting of the association for computational linguistics}, publisher = {morgan kaufmann publishers, san francisco, us}, editor =
in this work we investigate the usefulness of $n$-grams in tc independently of any specific learning algorithm.
this combination is based on the notion of classifier reliability and presented gains of up to 14\% in micro-averaged f1 in the web collection.
extending whirl with background knowledge for improved text classification}, journal = {information retrieval}, volume = {10}, number = {1}, pages = {35--67}, year = {2007}, month = {january} } @article{serrano:2007:eld, author = {
we describe here an n-gram-based approach to text categorization that is tolerant of textual errors.
this paper shows that the accuracy of text classifiers trained with a small number of labeled documents can be improved by augmenting this small training set with a large pool of unlabeled documents.
we show that our method is especially useful for text classification tasks involving a large number of categories and outperforms other semi-supervised learning techniques such as em and co-training.
experiments show that the models outperform their single-label counterparts on standard text corpora.
text categorization based on {k}-nearest neighbor approach for web site classification}, journal = {information processing and management}, year = {2003}, volume = {39}, number = {1}, pages = {25--44}, url = {}, abstract = {}, } @inproceedings{kwon99, author =
when applied to text classification, these learning algorithms lead to svms with excellent precision but poor recall.
experiments with the muc corpus suggest that case-based text classification can achieve very high levels of precision and outperforms our previous algorithms based on relevancy signatures.}, } @article{riloff94, author = {ellen riloff and wendy lehnert}, title = {information extraction as a basis for high-precision text classification}, journal = {acm transactions on information systems}, year = {1994}, number = {3}, volume = {12}, pages = {296--333}, url = {http://www.cs.utah.edu/~riloff/psfiles/single-acm.ps}, abstract = {
the bases of the cdm are research results about the way that humans learn categories and concepts vis-a-vis contrasting concepts.
comparisons with traditional rocchio's algorithm adapted for text categorization, as well as flat neural network classifiers are provided.
for these reasons, tcfp can be a useful classifier in the areas, which need a fast and high-performance text categorization task.}, } @inproceedings{ko02a, author =
this knowledge is represented using publicly available ontologies that contain hundreds of thousands of concepts, such as the open directory; these ontologies are further enriched by several orders of magnitude through controlled web crawling.
the pattern extraction is aimed at providing descriptions (in the form of two logical expressions) of the two classes of positive and negative examples.
the word-pairs are selected automatically using a technique based on frequencies of n-grams (sequences of characters), which takes into account both the frequencies of word-pairs as well as the context in which they occur.
this article describes an approach to tc based on the integration of a training collection (reuters-21578) and a lexical database (wordnet 1.6) as knowledge sources.
both svm and knn are tested and compared on the 20-newsgroups database.
these metrics are shown to be good predictors of categorization accuracy that can be achieved on a dataset, and serve as efficient heuristics for generating datasets subject to user's requirements.
in this paper, we show how inverted indexes can be used for scalable training in categorisation, and propose novel heuristics for a fast, accurate, and memory efficient approach.
although such learning models succeed in exploiting relational knowledge, they are highly demanding in terms of labeled examples, because the number of categories is related to the dimension of the corresponding hierarchy.
{seattle, us}, pages = {256--263}, url = {http://www.cs.cmu.edu/~yiming/papers.yy/sigir95.ps}, abstract = {the paper studies noise reduction for computational efficiency improvements in a statistical learning method for text categorization, the linear least squares fit (llsf) mapping.
our technique also adapts gracefully to the fraction of neighboring documents having known topics.
document classification is characterized by the large number of attributes involved in the objects (documents).
one way to achieve this aim is by applying machine learning techniques to training data containing the various senses of the ambiguous words.
the model is evaluated on the reuters test collection and compared to the multinomial naive bayes model.
the importance of text mining stems from the availability of huge volumes of text databases holding a wealth of valuable information that needs to be mined.
the method combines information derived from n-grams (consecutive sequences of n characters) with a simple vector-space technique that makes sorting, categorization, and retrieval feasible in a large multilingual collection of documents.
experimental comparisons of the performance of the kernel compared with a standard word feature space kernel (joachims, 1998) show positive results on modestly sized datasets.
classification of documents allow the automatic distribution or archiving of letters and is also an excellent starting point for higher-level document analysis.}, } @article{hoyle73, author = {w.g. hoyle}, title = {automatic indexing and generation of classification by algorithm}, journal =
{843--848}, url = {}, abstract = {a language-independent means of gauging topical similarity in unrestricted text is described.
it not only approaches and sometimes exceeds svm accuracy, but also beats svm running time by orders of magnitude.
the links between nodes are computed based on statistics of the word distribution and the category distribution over the training set.
based on this understanding, we present solutions inspired by round-robin scheduling that avoid this pitfall, without resorting to costly wrapper methods.
using the same representation of categories, experiments show a significant improvement when the above mentioned method is used.
preliminary experiments with 1998 darpa bsm audit data show that the knn classifier can effectively detect intrusive attacks and achieve a low false positive rate.}, } @article{liddy94, author = {elizabeth d. liddy and woojin paik and edmund s. yu}, title = {
the paper demonstrates good performance of context-group discrimination for a sample of natural and artificial ambiguous words.}, } @mastersthesis{scott98, author =
in order to verify our methods, we test a large body of tagged japanese newspaper articles created by rwcp.
these algorithms make extremely fast decisions, because they need to examine only a small number of words in each text document.
support vector machines provide the best accuracy on test data.}, } @article{skarmeta00, author =
textual information is processed by two methods of analysis: a natural language analysis followed by a statistical analysis.
the vast majority of them represent cases that can only be fully categorized with the assistance of a human subject (because, for instance, they require specific knowledge of a given pathology).
this can be considered as the effective combination of documents with no topic or class labels (unlabeled data), labeled documents, and prior domain knowledge (in the form of the known hierarchic structure), in providing enhanced document classification performance.}, } @inproceedings{vinot03, author = {
the world wide web is a vast repository of information, but the sheer volume makes it difficult to identify useful documents.
with the knn classifier, the frequencies of system calls are used to describe the program behavior.
the investigation includes different attribute and distance-weighting schemes, and studies on the effect of the neighborhood size, the size of the attribute set, and the size of the training corpus.
surprisingly, this unsupervised procedure can be competitive with a (supervised) svm trained with a small training set.
we also analyse the relationships of news headlines and their contents of the new reuters corpus by a series of experiments.
we argue that this evaluation measure is also very well suited for text categorization tasks.
a text is represented as a set of cases and we classify a text as relevant if any of its cases is deemed to be relevant.
owing to the use of context-sensitive features, the classifier is very accurate.
the feature sets are based on the ``latent semantics'' of a reference library - a collection of documents adequately representing the desired concepts.
they allow us to construct compact feature sets with few elements, with which a satisfactory genre diversi- fication is achieved.
{59--72}, url = {http://www.cs.uiowa.edu/~mruiz/papers/sigcr97/sigcrfinal2.html}, abstract = {this paper presents the results obtained from a series of experiments in automatic text categorization of medline articles.
a problem in the use of both algorithms is that they require documents to be represented by binary vectors, indicating presence or absence of the terms in the document.
the links between nodes are computed based on statistics of the word distribution and the category distribution over the training set.
previous researches have investigated the use of $n$-grams (or some variant of them) in the context of specific learning algorithms, and thus have not obtained general answers on their usefulness for tc.
in addition, more than 80\% of automatically extracted terms are meaningful.
we extracted the words around the gene occurrences and used them to represent the gene for go domain code annotation.
in our experiments we compare the effectiveness of the svm -based feature selection with that of more traditional feature selection methods, such as odds ratio and information gain, in achieving the desired tradeoff between the vector sparsity and the classification performance.
upon close examination of the algorithm, we concluded that the algorithm is most successful in correctly classifying more positive documents, but may cause more negative documents to be classified incorrectly.}, } @inproceedings{taskar01, author = {benjamin taskar and eran segal and daphne koller}, title = {probabilistic classification and clustering in relational data}, booktitle = {
abis minimizes user's effort in selecting the huge amount of available documents.
we provide experimental results on a webpage classification task, showing that accuracy can be significantly improved by modeling relational dependencies.}, } @article{tauritz00, author = {daniel r. tauritz and joost n. kok and ida g. sprinkhuizen-kuyper}, title = {
our evaluation using the 1996 reuters corpus which consists of 806,791 documents shows that automatically constructing hierarchy improves classification accuracy."
they use credible knowledge resources, including a us government organizational hierarchy, a thematic hierarchy from the open directory project (odp) web directory, and personal browse histories, to add valuable metadata to search results.
in this paper we study the use of a semi-supervised agglomerative hierarchical clustering (ssahc) algorithm to text categorization, which consists of assigning text documents to predefined categories.
however the existing clustering, techniques are agglomerative in nature and result in (i) suboptimal word clusters and (ii) high computational cost.
experimental results indicate that the mfom classifier gives improved f1 and enhanced robustness over the conventional one.
our technique also adapts gracefully to the fraction of neighboring documents having known topics.
we see genre classification as a powerful instrument to bring web-based search services closer to a user's information need.
overall, we show that by using a few terms, categorisation accuracy can be improved substantially: unstructured leaf level categorisation can be improved by up to 8.6\%, while top-down hierarchical categorisation accuracy can be improved by up to 12\%.
in previous research, lsi has produced a small improvement in retrieval performance.
we use the classification on keywords as the baseline, which we compare with the contribution of the pure hm pairs to classification accuracy, and the incremental contributions from heads and modifiers.
we show that the results outperform competing methods.
in many cases, users would like to search for information of a certain 'object', rather than a web page containing the query terms.
these search engines are, however, unsuited for a wide range of equally important tasks.
the indexing dictionary can be derived automatically from a set of manually indexed documents.
we provide background, present procedures for building metaclassifiers that take into consideration both reliability indicators and classifier outputs, and review a set of comparative studies undertaken to evaluate the methodology.}, } @inproceedings{bickel04, author = {steffen bickel and tobias scheffer}, title =
however, when lsi is used is conduction with statistical classification, there is a dramatic improvement in performance.} } @inproceedings{hull96, author = {david a. hull and jan o. pedersen and hinrich sch{\"u}tze}, title = {method combination for document filtering}, booktitle = {proceedings of sigir-96, 19th acm international conference on research and development in information retrieval}, editor = {hans-peter frei and donna harman and peter sch{\"{a}}uble and ross wilkinson}, publisher =
in addition, we investigate alternative classification and evaluation methods, and the effect that a secondary feature can have on indoor/outdoor classification.
when user's interests change, pva, in not only the contents, but also in the structure of user profile, is modified to adapt to the changes.
experimental results indicate that, at the same level of vector sparsity, feature selection based on svm normals yields better classification performance than odds ratio- or information gainbased feature
categorical points to each category are computed by summing the frequency of each keyword from back data, or the number of documents from it.
however, it does show that tfidf conversion and document length normalization are important.
the results obtained by ripper surpass those of the operational process.}, } @inproceedings{tong00, author = {simon tong and daphne koller}, title = {support vector machine active learning with applications to text classification}, booktitle = {proceedings of icml-00, 17th international conference on machine learning}, editor = {pat langley}, year = {2000}, address = {stanford, us}, pages = {999--1006}, publisher =
in the test of the expert network method on cacm documents, for example, an 87\% removal of unique words reduced the vocabulary of documents from 8,002 distinct words to 1,045 words, which resulted in a 63\% time saving and a 74\% memory saving in the computation of category ranking, with a 10\% precision improvement, on average, over not using word removal.
integrating linguistic resources in tc through wsd}, journal = {computers and the humanities}, year = {2001}, number = {2}, volume = {35}, pages = {215--230}, url = {http://www.wkap.nl/article.pdf?266250}, abstract = {information access methods must be improved to overcome the information overload that most professionals face nowadays.
therefore, this method can be used in areas where low-cost text categorization is needed.
experiments with simulated concept drift scenarios based on real-world text data compare the new method with other window management approaches.
we carry out experiments over two different corpora and find that the proposed measures perform better than the existing ones."
one problem is that it is difficult to create the labeled training documents.
"we demonstrate the value of using context in a new-information detection system that achieved the highest precision scores at the text retrieval conference's novelty track in 2004.
we evaluate the algorithms on the reuters-21578 corpus and the new corpus released by reuters in 2000.
our experiments demonstrate that this new approach is able to learn more accurate classifiers than either of its constituent methods alone.}, } @inproceedings{craven98, author = {mark craven and dan dipasquo and dayne freitag and andrew k. mccallum and tom m. mitchell and kamal nigam and se{\'{a}}n slattery}, title = {learning to extract symbolic knowledge from the world wide web}, booktitle = {proceedings of aaai-98, 15th conference of the american association for artificial intelligence}, publisher = {aaai press, menlo park, us}, year = {1998}, pages = {509--516}, address = {madison, us}, note
in this categorization process recall is considered more important than precision.
our approach to wsd is also based on the integration of two linguistic resources: a training collection (semcor and reuters-21578) and a lexical database (wordnet 1.6).}, } @inproceedings{vert01, author = {jean-philippe vert}, title = {
we use support vector machine (svm) classifiers, which have been shown to be efficient and effective for classification, but not previously explored in the context of hierarchical classification.
improving performance of text categorization by combining filtering and support vector machines}, journal = {journal of the american society for information science and technology}, year = {2004}, volume = {55}, number =
furthermore, the online approach offers the advantage of continuous learning in the batch-adaptive text filtering task.}, } @inproceedings{chakrabarti02, author = {soumen chakrabarti and shourya roy and mahesh soundalgekar}, title = {fast and accurate text classification via multiple linear discriminant projections}, booktitle = {proceedings of vldb-02, 28th international conference on very large data bases}, publisher = {}, editor = {}, year = {2002}, address = {hong kong, cn}, pages = {658--669}, url = {http://www.vldb.org/conf/2002/s19p01.pdf}, abstract = {support vector machines (svms) have shown superb performance for text classification tasks.
experimental results show that (1) using an enhanced representation of web documents is crucial for an effective categorisation of web documents, and (2) a theoretical interpretation of the k-nearest neighbour classifier gives us improvement over the standard k-nearest neighbour classifier.}, } @inproceedings{goldberg95, author = {goldberg, jeffrey l.}, title = {cdm: an approach to learning in text categorization}, booktitle = {proceedings of ictai-95, 7th international conference on tools with artificial intelligence}, publisher = {ieee computer society press, los alamitos, us}, editor = {}, address = {herndon, us}, year = {1995}, pages = {258--265}, url = {}, note = {an extended version appears as~\cite{goldberg96}}, abstract = {
with the knn classifier, the frequencies of system calls are used to describe the program behavior.
this set of relevant features varies widely throughout the hierarchy, so that, while the overall relevant feature set may be large, each classifier only examines a small subset.
previous research on automated text categorization has mixed machine learning and knowledge engineering methods, making it difficult to draw conclusions about the performance of particular methods.
application of feature subset selection techniques improves the performance.
the algorithm is automatic and unsupervised in both training and application: senses are induced from a corpus without labeled training instances or other external knowledge sources.
in this paper, we focus on using user assisted text classification in conjunction with a web portal, multiple document management systems and an ontology, to provide a powerful solution for organizing information about a company's technology.
a relatively high degree of accuracy was achieved by the supervised method, however, classification accuracy varied across classes.
results on a real-world text datasets show that these learners may substantially benefit from using a large amount of unlabeled documents in addition to some labeled documents.}, } @inproceedings{larkey96, author = {leah s. larkey and w. bruce croft}, title =
in the first approach, the content (eg., text) plays an important role, while in the second approach, the existence of several knowledge sources (eg., several users) is required.
we experimentally evaluate the quality of the lower dimensional spaces both in the context of document categorization and improvements in retrieval performance on a variety of different document collections.
profile allows the user to update on-line his profile and to check the discrepancy between his assessment and the prediction of relevance of the system.
because text domains present much irrelevant information, effective feature reduction is essential to improve classifiers' effectiveness and efficiency.
{morgan kaufmann publishers, san francisco, us}, url = {http://www.cs.cmu.edu/~rayid/mypapers/ecoc-icml.ps}, abstract = {we report the results of a study on topic spotting in conversational speech.
{209--228}, url = {http://trec.nist.gov/pubs/trec1/papers/17.txt}, abstract = {describes an approach to document routing on the trec corpus that employs a technique for the automatic construction of classification trees.
non-textual information is processed by a symbolic learning technique.
it requires an indexing dictionary with rules mapping terms of the respective subject field onto descriptors and inverted lists for terms occurring in a set of documents of the subject field and descriptors manually assigned to these documents.
we conclude by examining factors that make the sentiment classification problem more challenging.}, } @article{park04, author = {seong-bae park and byoung-tak zhang}, title = {co-trained support vector machines for large scale unstructured document classification using unlabeled data and syntactic information}, journal =
the algorithm's predictive accuracy is competitive with other recently introduced hierarchical multi-category or multilabel classification learning algorithms."
we use a simple perceptron to optimize the relative emphasis of each semantic class in the tracking and detection decisions.
we benchmark several widely used supervised learning methods on rcv1-v2, illustrating the collection's properties, suggesting new directions for research, and providing baseline results for future studies.
such a scheme has several potential advantages because it does not require any pre-processing of the input text.
drawing on interviews with reuters personnel and access to reuters documentation, we describe the coding policy and quality control procedures used in producing the rcv1 data, the intended semantics of the hierarchical category taxonomies, and the corrections necessary to remove errorful data.
the model is based on the concept of ``uncertainty sampling'', a technique that allows for relevance feedback both on relevant and non relevant documents.
using synthetically generated data we empirically demonstrate that whenever the dc procedure is successful in recovering some of the structure hidden in the data, the extended idc procedure can incrementally compute a dramatically better classification, with minor additional computational resources.
{pittsburgh, us}, year = {2001}, url = {http://www-2.cs.cmu.edu/~knigam/papers/thesis-nigam.pdf}, abstract = {one key difficulty with text classification learning algorithms is that they require many hand-labeled examples to learn accurately.
in our system, the user navigates through the query response not as a flat unstructured list, but embedded in the familiar taxonomy, and annotated with document signatures computed dynamically with respect to where the user is located at any time.
compared to a previously tested naive bayes filter, the memory-based filter performs on average better, particularly when the misclassification cost for non-spam messages is high.}, } @inproceedings{sasaki98, author = {minoru sasaki and kenji kita}, title =
in two experiments, the research method of information integration theory was employed in order to test two hypotheses relating to the radical conventionalist and traditional positions on the role of specific formal textual features in the categorization of poetic texts.
using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines.
however, the effect of boosting for rare categories varies across classifiers: for svms and decision trees, we achieved a 13-17\% performance improvement in macro-averaged f1 measure, but did not obtain substantial improvement for the other two classifiers.
we use a simple perceptron to optimize the relative emphasis of each semantic class in the tracking and detection decisions.
this study indicates that, while some document categorization algorithms could be adopted for database categorization, algorithms that take into consideration the special characteristics of databases may be more effective.
link information alone is able to obtain gains of up to 46 points in f1, when compared to a traditional content-based classifier.
we evaluate the inequality me models on text categorization datasets, and demonstrate their advantages over standard me estimation, similarly motivated gaussian map estimation of me models, and support vector machines (svms), which are one of the state-of-the-art methods for text categorization.} } @article{kim:2005:drt, author = {
we identify document genre is an important factor in retrieving useful documents and focus on the novel document genre dimension of subjectivity.
the average f1 value of our two submitted solutions is 94.4% higher than the average f1 value from all other submitted solutions."
it is built on top of a text-categorization paradigm where text articles are annotated with keywords organized in a hierarchical structure.
such a knowledge base would enable much more effective retrieval of web information, and promote new uses of the web to support knowledge-based inference and problem solving.
the subsequences are weighted by an exponentially decaying factor of their full length in the text, hence emphasising those occurrences that are close to contiguous.
experimental results show that our system can achieve satisfactory performance, which is comparable with other traditional classifiers.}, } @inproceedings{zhou03, author = {shuigeng zhou and tok wang ling and jihong guan and jiangtao hu and aoying zhou}, title = {fast text classification: a training-corpus pruning based approach}, booktitle = {proceedings of dasfaa-03, 8th ieee international conference on database advanced systems for advanced application}, editor = {}, publisher = {ieee computer society press, los alamitos, us}, year = {2003}, address = {kyoto, jp}, pages = {127--136}, url = {}, abstract = {}, } @inproceedings{zu03, author = {guowei zu and wataru ohyama and tetsushi wakabayashi and fumitaka kimura}, title = {accuracy improvement of automatic text classification based on feature transformation}, booktitle = {proceedings of doceng-03, acm symposium on document engineering}, publisher = {acm press, new york, us},
however, in carefully controlled experiments using syntactic phrases produced by church's stochastic bracketer, in conjunction with reciprocal nearest neighbor clustering, term clustering was found to produce essentially no improvement in the properties of the phrasal representation.
we describe a classifier of email queries, which executes text categorization by topic.
surprisingly, this unsupervised procedure can be competitive with a (supervised) svm trained with a small training set.
our second set of results examines the behavior of rankboost when it has to learn not only a ranking function but also all aspects of term weighting from raw data.
preliminary results show improved accuracy, as well as reduced cost, resulting from these automated techniques.}, } @inproceedings{rennie03, author = {jason rennie and lawrence shih and jaime teevan and david karger}, title = {
we do so by applying feature selection to the pool of all $k$-grams ($k\leq n$), and checking how many $n$-grams score high enough to be selected in the top $\sigma$ $k$-grams.
while regular support vector machines (svms) try to induce a general decision function for a learning task, tsvms take into account a particular test set and try to minimize misclassifications of just those particular examples.
however, compression-based classification methods have drawbacks (such as slow running time), and not all such methods are equally effective.
since it does not require complicated parameterization, it is simpler to use and more robust than comparable heuristics.
our analyses show that when the positive training data is not too under-sampled, svmc significantly outperforms other methods because svmc basically exploits the natural "gap" between positive and negative documents in the feature space, which eventually corresponds to improving the generalization performance.
further experiments using two less orthodox categorizers are also presented which suggest that combining text categorizers can be successful, provided the essential element of 'difference' is considered.}, } @article{urena01, author =
experimental results show that our system can effectively and efficiently classify chinese web documents.}, } @inproceedings{zhou02, author = {shuigeng zhou and jihong guan}, title = {an approach to improve text classification efficiency}, booktitle = {proceedings of adbis-02, 6th east-european conference on advances in databases and information systems}, publisher = {springer verlag, heidelberg, de}, editor = {yannis manolopoulos and pavol n{\'a}vrat}, year = {2002}, address =
we find that adding the words in the linked neighborhood to the page having those links (both inlinks and outlinks) were helpful for all our classifiers on one data set, but more harmful than helpful for two out of the three classifiers on the remaining datasets.
as a by-product, we can compute for each document a set of terms that occur significantly more often in it than in the classes to which it belongs.
we also incorporate a batch updating scheme to periodically do maintenance on the term structures of the news database after training.
semi-automated methods were used to build a lexicon of appraising adjectives and their modifiers.
ts compares favorably with the other methods with up to 50\% vocabulary reduction but is not competitive at higher vocabulary reduction levels.
the baseline method combines the terms of all the articles of each newsgroup in the training set to represent the newsgroups as single vectors.
this task arises in the construction of search engines and web knowledge bases.
it uses fisher's linear discriminant, a classical tool from statistical pattern recognition, to project training instances to a carefully selected low-dimensional subspace before inducing a decision tree on the projected instances.
boosting to correct the inductive bias for text classification}, booktitle = {proceedings of cikm-02, 11th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, editor = {}, year = {2002}, address = {mclean, us}, pages =
to accomplish this task, each substantive word in a text is first categorized using a feature set based on the semantic subject field codes (sfcs) assigned to individual word senses in a machine-readable dictionary.
experimental results show that modulating the structure of user profile does increase the accuracy of personalization systems.}, } @article{chen02, author = {
we find that error correcting codes perform better than 1-of-n coding with increasing code length.
second, they are inaccurate, because of mistakes made by these indexers as well as the difficulties users have in choosing keywords for their queries, and the ambiguity a keyword may have.
given seeker, the text retrieval system, we achieved these results in about two person-months.
the second set of experiments applies the genex algorithm to the task.
the svms' results common to both filtering are that 1) the optimal number of features differed completely across categories, and 2) the average performance for all categories was best when all of the words were used.
the test results show the hybrid method is better than the previous rough set-based approach.}, } @inproceedings{basili00, author = {roberto basili and alessandro moschitti and maria t. pazienza}, title = {language-sensitive text classification}, booktitle = {proceedings of riao-00, 6th international conference ``recherche d'information assistee par ordinateur''}, editor = {}, address = {paris, france}, year = {2000}, pages = {331--343}, url = {},
computer programs scan text in a document and apply a model that assigns the document to one or more prespecified topics.
a problem in the use of both algorithms is that they require documents to be represented by binary vectors, indicating presence or absence of the terms in the document.
author detection with svms on full word forms was remarkably robust even if the author wrote about different topics.}, } @inproceedings{dinunzio03, author = {giorgio m. {di nunzio} and alessandro micarelli}, title = {does a new simple gaussian weighting approach perform
finally, we show empirically that this categorization system utilizing a machine-derived taxonomy performs as well as a manual categorization process, but at a far lower cost.}, } @inproceedings{agrawal00, author = {rakesh agrawal and roberto j. bayardo and ramakrishnan srikant}, title = {{\sc athena}: mining-based interactive management of text databases}, booktitle = {proceedings of edbt-00, 7th international conference on extending database technulogy},
we use information from a pre-existing taxonomy in order to supervise the creation of a set of related clusters, though with some freedom in defining and creating the classes.
again, boosting compares favourably to the other benchmark algorithms.}, } @article{fall03, author = {c. j. fall and a. t{\"o}rcsv{\'a}ri and k. benzineb and g. karetka}, title = {automated categorization in the {international patent classification}}, journal = {sigir forum}, year = {2003}, pages = {10--25}, volume = {37}, number = {1}, url = {http://www.acm.org/sigir/forum/s2003/cjf_manuscript_sigir.pdf}, abstract = {a new reference collection of patent documents for training and testing automated categorization systems is established and described in detail.
technology from machine learning (ml) will offer efficient tools for the intelligent analyses of the data using generalization ability.
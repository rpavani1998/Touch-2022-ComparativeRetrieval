it is unlikely that any real-world  database would allow you to cleanly separate four important classes into two  subgroups in this way.
although we can make recommendations as to which splitting rule is best suited  to which type of problem, it is good practice to always use several splitting  rules and compare the results.
variations on twoing an important variation  of the twoing rule is power-modified twoing, which places an even heavier  weight on splitting the data in a node into two equal-sized partitions.
with multiple classes, you might care how the errors are distributed across classes.
the answer is simple.
unfortunately, previous examinations of  splitting rule performance, the ones that found no differences, did not look at  data-mining problems with large data sets where obtaining a good answer is  genuinely difficult.
while imperfect, it is still an unusually accurate tree.
as you work with  different types of data and problems, you will begin to learn which splitting  rules typically work best for specific problem types.
you should experiment with several different  splitting rules and should expect different results from each.
between  two trees with equal overall error rates, you might prefer a tree that performs  better on a particular class or classes.
twoing and entropy splitting rules the philosophy of twoing is far different than that of  gini.
in this example of vehicle choice, the twoing rule was the winner; in other data sets, the winner might be power-modified twoing, gini, or yet another rule.
in the first run, using cart's default criterion, the gini rule grew an 89-node tree with a relative error of 0.919.
if the purpose of a decision tree is  to yield insight into a causal process or into the structure of a database,  splitting rules of similar accuracy can yield trees that vary greatly in their  usefulness for interpreting and understanding the data.
this paper explores the key differences between three important splitting criteria: gini, twoing and entropy, for three- and greater-level classification trees, and suggests how to choose the right one for a particular problem type.
if the purpose of a decision tree is to yield insight into a causal process or into the structure of a database, splitting rules of similar accuracy can yield trees that vary greatly in their usefulness for interpreting and understanding the data.
so what is gini trying to do?
for target variables with four to nine levels, twoing has a good chance of being the best splitting rule.
for a two-level dependent variable that can be predicted with a relative error of less than 0.50, the gini splitting rule is typically best.
the diagram below shows the best possible split the twoing rule could find.
once the first split is made, gini continues attempting to split  the data that require further segmentation, i.e., the right child node that  contains classes b, c and d. using the same strategy, gini attempts to pull out  all the class b records, separating them from the other classes in the node.
a data-mining project concerning consumer  choice from a set of 28 vehicles is a typical example of the substantial  differences that can be observed across alternative splitting rules.
take the  last rule of thumb, for example.
in such circumstances, the difference between gini and twoing could easily be the difference between success and failure (e.g., in the case of fraud detection).
so, why haven't the differences been noticed?
classification and regression trees, pacific grove: wadsworth, 1984.
because each rule represents a different philosophy as to the  purpose of the decision tree, each grows a different style of tree.
rather than initially pulling out a single class
of course, such a separation may not be possible using the available data, but, if it is, the gini opts for that split.
as you work with different types of data and problems, you will begin to learn which splitting rules typically work best for specific problem types.
nevertheless, they  represent such a consistent set of empirical findings that we expect them to  continue to hold in other domains and data sets.
it will always favor working on the largest or, if you use costs or weights, the most "important" class in a node.
although the second tree also has quite a high error rate in percentage of  error terms, it is a dramatic improvement over the first.
gini, twoing, and entropy
it will always favor working on the largest  or, if you use costs or weights, the most "important" class in a  node.
further reading: breiman, l., j. friedman, r. olshen and c. stone.
when you are trying to detect fraud, identify borrowers who will declare bankruptcy in the next 12 months, target a direct mail campaign, or tackle other real-world business problems that do not admit of 90+ percent accuracy rates (with currently available data), the splitting rule you choose could materially affect the accuracy and value of your decision tree.
further, even when different splitting rules yield similarly accurate classifiers, the differences between them may still matter.
contrary to popular opinion in  data mining circles, our experience indicates that splitting criteria do  matter; in fact, the difference between using the right rule and the wrong rule  could add up to millions of dollars of lost opportunity.
breiman, l. some properties of splitting criteria, statistics department, university of california, berkeley. 1992.
nevertheless, they represent such a consistent set of empirical findings that we expect them to continue to hold in other domains and data sets.
for example, with four classes, a, b, c, and d, representing 40, 30, 20, and 10 percent of the data, respectively, the gini rule would immediately attempt to pull out the class a records into one node.
unfortunately, previous examinations of splitting rule performance, the ones that found no differences, did not look at data-mining problems with large data sets where obtaining a good answer is genuinely difficult.
when you are trying to detect fraud, identify  borrowers who will declare bankruptcy in the next 12 months, target a direct  mail campaign, or tackle other real-world business problems that do not admit  of 90+ percent accuracy rates (with currently available data), the splitting  rule you choose could materially affect the accuracy and value of your decision  tree.
there will also be times when the value of a decision tree is determined by how rich the best nodes are in certain values of the target variable, and the overall accuracy of the tree is irrelevant.
a data-mining project concerning consumer choice from a set of 28 vehicles is a typical example of the substantial differences that can be observed across alternative splitting rules.
rather than initially pulling out a single class, twoing first segments the classes into two groups, attempting to find groups that together add up to 50 percent of the data.
general rules of thumb the following rules of thumb are based on our experience in the  telecommunications, banking, and market research arenas, and may not apply  literally to other subject matters or even other data sets.
the second run, using cart's twoing rule, grew a tree with 50 nodes with a relative error of 0.876.
therefore, we cannot expect to grow trees like the one above routinely; however, gini will try to come as close as possible to this ideal.
if they were, no one  would ever receive an unwelcome direct mail piece and bank losses on bad debts  would be zero.
reviewing the tree sequences below, one can see that the twoing rule makes a better first split and continues to pull ahead of gini on progressively larger trees.
contrary to popular opinion in data mining circles, our experience indicates that splitting criteria do matter; in fact, the difference between using the right rule and the wrong rule could add up to millions of dollars of lost opportunity.
the entropy rule,  which is very similar to twoing in practice, strives for similar splits.
the best known rules for binary recursive partitioning are gini, twoing, and entropy.
while we have found numerous examples where judicious choice of a splitting rule reduces the error rate of a classification tree by five to ten percent, the differences go beyond accuracy.
the diagram  below shows the best possible gini split for these data.
variations on twoing an important variation of the twoing rule is power-modified twoing, which places an even heavier weight on splitting the data in a node into two equal-sized partitions.
the diagram below shows the best possible split the twoing rule  could find.
the best known rules for binary recursive partitioning are gini, twoing,  and entropy.
gini attempts to separate classes by focusing on one class at a time.
the entropy rule, which is very similar to twoing in practice, strives for similar splits.
when your golf ball is one inch from the cup, which club or even which end you use is not important because you will be able to sink the ball in one stroke.
for  higher-level categorical dependent variables with 10 or more levels, twoing or  power- modified twoing is often considerably more accurate than gini.
with multiple  classes, you might care how the errors are distributed across classes.
reviewing  the tree sequences below, one can see that the twoing rule makes a better first  split and continues to pull ahead of gini on progressively larger trees.
do decision-tree splitting criteria matter?
nevertheless, you should never rely on a single rule alone; experimentation is always wise.
because each rule represents a different philosophy as to the purpose of the decision tree, each grows a different style of tree.
given that there is no one best splitting rule for all problem types or all purposes, it is important that the decision-tree tool you employ offer several proven splitting rules with distinct styles and operating characteristics.
when data sets are  small and highly-accurate trees can be generated easily, the particular  splitting rule does not matter.
for a two-level dependent variable that can be predicted with a relative error of 0.80 or higher, power-modified twoing tends to perform best.
when  the perfect splits illustrated above are available, twoing and power-modified  twoing will select the same split.
when the perfect splits illustrated above are available, twoing and power-modified twoing will select the same split.
the second run, using cart's twoing rule,  grew a tree with 50 nodes with a relative error of 0.876.
even when the overall accuracy of two trees grown by different splitting rules is identical, the usefulness of the trees for revealing data structure can be quite different.
gini then tackles the last heterogeneous node, striving to separate class c  from class d. if the gini rule is successful, the final tree would contain four  "pure" child nodes: a pure decision tree such as the above is attainable only in very  rare circumstances; in most real world applications, database fields that  clearly partition class from class are not available.
gini is the default rule in cart precisely because it is so often the best splitting rule.
while this approach might seem short sighted, gini performance is frequently so good that you should always experiment with it to see how well it does.
when only imperfect partitions are  available, power-modified twoing is more likely to generate a near 50-50  partition of the data than is simple twoing.
twoing then searches for a split to separate the two subgroups.
cart tree sequence for 28 level target variable using gini # of terminal
once the first split is made, gini continues attempting to split the data that require further segmentation, i.e., the right child node that contains classes b, c and d. using the same strategy, gini attempts to pull out all the class b records, separating them from the other classes in the node.
for target variables with four to  nine levels, twoing has a good chance of being the best splitting rule.
cart tree sequence for 28 level target variable using gini# of terminal cart tree sequence for 28 level target variable using twoing# of terminal which splitting rule you choose when growing a decision tree does matter; sometimes the differences between rules will be modest and at other times profound.
this paper  explores the key differences between three important splitting criteria:   gini, twoing and entropy, for three- and greater-level classification trees,  and suggests how to choose the right one for a particular problem type.
twoing then searches for a split to separate the two  subgroups.
gini attempts to separate classes  by focusing on one class at a time.
for higher-level categorical dependent variables with 10 or more levels, twoing or power- modified twoing is often considerably more accurate than gini.
when data sets are small and highly-accurate trees can be generated easily, the particular splitting rule does not matter.
the list  of variables in this exercise was quite limited; in particular, the trees  needed to be grown without reference to important drivers of choice such as  income or price, so the level of accuracy attainable was expected to be low.
however, splits that approach this ideal might be possible, and these are the splits that twoing seeks to find.
the list of variables in this exercise was quite limited; in particular, the trees needed to be grown without reference to important drivers of choice such as income or price, so the level of accuracy attainable was expected to be low.
nevertheless, you should  never rely on a single rule alone; experimentation is always wise.
when your golf ball is one inch from the cup,  which club or even which end you use is not important because you will be  able to sink the ball in one stroke.
although the second tree also has quite a high error rate in percentage of error terms, it is a dramatic improvement over the first.
in this example of  vehicle choice, the twoing rule was the winner; in other data sets, the winner  might be power-modified twoing, gini, or yet another rule.
of course, such a separation may not be possible using  the available data, but, if it is, the gini opts for that split.
therefore, we cannot expect to grow trees like the one above  routinely; however, gini will try to come as close as possible to this ideal.
while this approach might seem short sighted, gini performance is  frequently so good that you should always experiment with it to see how well it  does.
for a two-level  dependent variable that can be predicted with a relative error of less than  0.50, the gini splitting rule is typically best.
, twoing first segments  the classes into two groups, attempting to find groups that together add up to  50 percent of the data.
between two trees with equal overall error rates, you might prefer a tree that performs better on a particular class or classes.
it is unlikely that any real-world database would allow you to cleanly separate four important classes into two subgroups in this way.
the following rules of thumb are based on our experience in the telecommunications, banking, and market research arenas, and may not apply literally to other subject matters or even other data sets.
further, even when different splitting rules yield similarly accurate  classifiers, the differences between them may still matter.
february 16, 2012 - february 18, 2012 orlando, fl, booth 12 march 05, 2012 - march 09, 2012 san  francisco, ca booth 224 statistical learning and data mining iii march 15, 2012 - march 16,  2012 palo alto, ca, booth tba informs or april 15, 2012 - april 17, 2012 huntington beach, ca  software workshop april 15, 1-2:45pm booth 6 jsm july 28, 2012 - august 02, 2012 san diego, ca, booth tba view full calendar do  splitting rules really matter? introduction do decision-tree splitting criteria matter?
gini then tackles the last heterogeneous node, striving to separate class c from class d. if the gini rule is successful, the final tree would contain four "pure" child nodes: a pure decision tree such as the above is attainable only in very rare circumstances; in most real world applications, database fields that clearly partition class from class are not available.
gini is the default rule in cart precisely because it is so often the  best splitting rule.
however, splits that approach this ideal might be  possible, and these are the splits that twoing seeks to find.
twoing and entropy splitting rules the philosophy of twoing is far different than that of gini.
for a two-level dependent  variable that can be predicted with a relative error of 0.80 or higher,  power-modified twoing tends to perform best.
in  the first run, using cart's default criterion, the gini rule grew an 89-node  tree with a relative error of 0.919.
you should experiment with several different splitting rules and should expect different results from each.
gini splitting rule gini looks for the largest class in your database (e.g., class a) and strives to isolate it from all other classes.
the diagram below shows the best possible gini split for these data.
if they were, no one would ever receive an unwelcome direct mail piece and bank losses on bad debts would be zero.
again, this is an ideal split.
a hypothetical example of a more realistic decision tree grown by gini is displayed below.
for example, with four classes,  a, b, c, and d, representing 40, 30, 20, and 10 percent of the data,  respectively, the gini rule would immediately attempt to pull out the class a  records into one node.
a  hypothetical example of a more realistic decision tree grown by gini is  displayed below.
so, why  haven't the differences been noticed?
take the last rule of thumb, for example.
when only imperfect partitions are available, power-modified twoing is more likely to generate a near 50-50 partition of the data than is simple twoing.
although we can make recommendations as to which splitting rule is best suited to which type of problem, it is good practice to always use several splitting rules and compare the results.
gini splitting rule gini looks for the largest class in your database (e.g., class a) and  strives to isolate it from all other classes.
the assignment of application samples to  sets proceeds in a hierarchical manner, i.e., each sample is assigned to the  first set where it "fits" (where the definition of the application  sample set would include the respective sample).
individual data points are represented in two-dimensional space (see below),  where axes represent the variables (x on the horizontal axis and y on the vertical axis).
to the right of  this point, presumably, we find only "factorial scree" –  "scree" is the geological term referring to the debris that  collects on the lower part of a rocky slope.
statistica enterprise/qc offers a high-performance database (or an optimized interface to existing databases), real-time monitoring and alarm notification for the production floor, a comprehensive set of analytical tools for engineers, sophisticated reporting features for management, six sigma reporting options, and much more.
for additional information, see alsoquality control charts; assignable causes and actions.
see also, data reduction.
this product offers a highly economical alternative to multimillion dollar investments in new or upgraded equipment (hardware).
n*m3/[(n-1)*(n-2)*3] where m3 is equal to: (xi-meanx)3 3 is the standard deviation (sigma) raised to the third power n is the valid number of cases.
the sequence of values from each  selected variable will be represented by consecutive areas stacked on one  another in this type of graph.
for example, a scalable network allows the network administrator to add many additional nodes without the need to redesign the basic system.
shuffle, back propagation (in neural networks).
thestatistica pi connector utilizes the pi user access control and security model, allows for interactive browsing of tags, and takes advantages of dedicated pi functionality for interpolation and snapshot data.
the student's t distribution has density function (for = 1, 2, ...): where is the degrees of freedom (gamma) is the gamma function is the constant pi (3.14...)
refer to kish (1965) for a detailed discussion of the advantages and characteristics of probability samples andepsem samples.
thus, the stiffness parameter determines the degree to which the fitted curve depends on local configurations of the analyzed values.
this is equivalent to one minus the explained variance of the model.
statsoft's statistica variance estimation and precisionoffers a comprehensive set of techniques for analyzing data from experiments that include both fixed and random effects using reml (restricted maximum likelihood estimation).
for example, you may have a data set that contains information about who from among a list of customers targeted for a special promotion responded to that offer.
statistica integrated with the pi system is being used for streamlined and automated analyses for applications such as process analytical technology (pat) in fda-regulated industries, advanced process control (apc) systems in chemical and petrochemical industries, and advisory systems for process optimization and compliance in the energy utility industry.
with statistica variance estimation and precision, you can obtain estimates of variance components and use them to make precision statements while at the same time comparing fixed effects in the presence of multiple sources of variation.
for more information, seesurvival analysis.
when applying (in data or text mining) algorithms for derivingassociation rules of the general form if body then head (e.g., if (car=porsche and age<20) then (risk=high and insurance=high)), the support value is computed as the joint probability (relative frequency of co-occurrence) of thebody and head of each association rule.
statistica enterprise provides an efficient interface to enterprise-wide data repositories and a means for collaborative work as well as all the statistical functionality available in any or allstatistica products.
sequential/stacked plots, 2d - step area.
statistical significance (p-value).
smoothing techniques can be used in two different situations.
the iterative gradient-descent training algorithms (back propagation, quasi-newton, conjugate gradient descent, levenberg-marquardt, quick propagation, delta-bar-delta, and kohonen) all attempt to reduce the training error on each epoch.
these three points would define a  triangle; in more than two dimensions, the "figure" produced by these  points is called asimplex.
this contour plot presents a 2-dimensional projection of the spline-smoothed surface fit to the data (see3d sequential surface plot.
will "fit" inside the lower and upper specification limits for the  process.
the goal of the analyze phase is to identify the root cause(s) of quality problems, and to confirm those causes using the appropriate data analysis tools.
for discussion of the generalized linear model and the link functions it  uses, see thegeneralized linear models topic.
your goal would be to find market segments, i.e., groups of observations that are relatively similar to each other on certain variables; once identified, you could then determine how best to reach one or more clusters by providing certain goods or services you think may have some special utility or appeal to individuals in that segment (cluster).
scatterplot, 2d - regular.
the main problem with  spurious correlations is that we typically do not know what the  "hidden" agent is.
some software programs implement an extension to the test described by royston (1982), which allows it to be applied to large samples (with up to 5000 observations).
statistica sequence, association and link analysis (sal).
multistream is well suited to help pharmaceutical manufacturers and power generation facilities leverage the data collected into their existing specialized process data bases for multivariate and predictive process control, for actionable advisory systems.
split selection for  classification trees refers to the process of selecting the splits on the  predictor variables that are used to predict membership in the classes of  the dependent variable for the cases or objects in the analysis.
signal detection theory (sdt) is an application of statistical decision theory used to detect a signal embedded in noise.
for example, stemming will ensure that both "travel" and "traveled" will be recognized by the program as the same word.
the space plots specific layout may facilitate exploratory  examination of specific types of three-dimensional data.
each input variable is treated in turn as if it were "unavailable" (hunter, 2000).
if you split the distribution in half at its mean (or median), then the distribution of values would be a "mirror image" about this central point.
if the variables are not related, then the points form an irregular "cloud" (see the categorized scatterplot below for examples of both types of data sets).
scatterplot smoothers.
it features a selection of completely integrated, and automated, ready to deploy "as is" (but also easily customizable) specific data mining solutions for a wide variety of business applications.
a sigma restricted model uses the sigma-restricted coding to represent effects for categorical predictor variables in general linear models and generalized linear models.
the two coordinates (x and y) that determine the location of each point correspond to its specific values on the two variables.
in the illustration shown above the area outside the upper specification limit (greater than usl) is defined as one million opportunities to produce defects.
the scatterplot visualizes a relation (correlation)  between two variablesx and y (e.g., weight and height).
smoothing techniques can be used in two different  situations.
in this type of graph, the sequences of values of variables selected in the first list will be represented by consecutivestep areas stacked on one another while the sequences of values of variables selected in the second list will be represented by consecutivestep lines stacked on one another (over the step area representing the last variable from the first list).
the define phase is concerned with the definition of project goals and boundaries, and the identification of issues that need to be addressed to achieve the higher sigma level.
we then produce line charts of the variability in those samples, and consider their closeness to target specifications.
however, in practice, we usually ignore the lower tail of the normal curve because (1) in many cases, the process "naturally" has one-sided specification limits (e.g., very low delay times are not really a defect, only very long times; very few customer complaints are not a problem, only very many, etc.), and (2) when a 6 * sigma process has been achieved, the area under the normal curve below the lower specification limit is negligible.
here the outcome variable of interest is not (and perhaps cannot be) directly observed.
the scree test involves finding the place where the smooth decrease  of eigenvalues appears to level off to the right of the plot.
this important transformation will bring all values (regardless of their distributions and original units of measurement) to compatible units from a distribution with a mean of 0 and a standard deviation of 1.
we can specify a target error level, for the training subset, the selection subset, or both.
randomly assigning cases to the training and verification sets, so that these are (as far as possible) statistically unbiased.
for more information on procedures for determining the optimal number of  factors to retain, see the section onreviewing the results of a principal  components analysis in factor analysis and how many dimensions to specify in  multi-dimensional scaling.
statistica automated neural networks.
shuffle data (in neural networks).
the original application of this type of plot was in the  context ofspectral analysis in order to investigate the behavior of  non-stationary time series.
sequential/stacked plots.
the 6 * sigma process shifted upwards by 1.5 * sigma will only produce 3.4 defects (i.e., "parts" or "cases" greater than the upper specification limit) per one million opportunities.
stratified random sampling.
a six sigma process is one that can be expected to produce only 3.4 defects per one million opportunities.
an unsmoothed surface (no smoothing function is applied) is drawn through the points in the 3d scatterplot.
if a number of models are studied, it is often possible to identify key variables that are always of high sensitivity, others that are always of low sensitivity, and "ambiguous" variables that change ratings and probably carry mutually redundant information.
in this type of graph, the  sequences of values of variables selected in the first list will be represented  by consecutive areas stacked on one another while the sequences of values of  variables selected in the second list will be represented by consecutivelines stacked on one another (over the area representing the last variable from  the first list).
instead, at each iteration the function will be evaluated at m+1 points in the m dimensional parameter space.
hence, if the user-specified sets encompass all valid samples,  the default all-samples set will actually become empty (since all samples will  be assigned to one of the user-defined sets).
the general approach to quality control charting  is straightforward: we extract samples of a certain size from the ongoing  production process.
we can use a sql statement in  order to specify the desired tables, fields, rows, etc. to return as data.
ei is the error for the ith case hi is the leverage for the ith case p is the number of coefficients in the model and i =
spectral plots have clear advantages over the regular 3d scatterplots  when we are interested in examining how a relationship between two  variables changes across the levels of a third variable, as is shown in the  next illustration.
stub-and-banner tables are essentially two-way tables, except that two lists of categorical variables (instead of just two individual variables) are crosstabulated.
the names of ally-variables from the two lists will be included in the  legend followed either by the letter (l) or (r), denoting the left-y and right-y axis, respectively.
stopping rule (in classification trees).
for example, the standardized effect in a 2 samplet -test is the difference between the two means, divided by the standard  deviation, i.e., es = (µ1 - µ2)/s
however, in biomedical research multiple censoring often exists, for example, when patients are discharged from a hospital after  different amounts (times) of treatment, and the researcher knows that the  patient survived up to those (differential) points of censoring.
each computes predicted classifications for a crossvalidation sample,  from which overall goodness-of-fit statistics (e.g., misclassification rates)  can be computed.
for more information, see power analysis.
in vector terminology, this is the dot product of the weight vector with the input vector, plus a bias value.
in time series analysis, a stationary  series has a constant mean, variance, and autocorrelation through time (i.e.,  seasonal dependencies have been removed viadifferencing).
in other words, assuming that in the population there was no relation between those variables whatsoever, and we were repeating experiments like ours one after another, we could expect that approximately in every 20 replications of the experiment there would be one in which the relation between the variables in question would be equal or stronger than in ours.
this standardized measure of  effect size is used in the analysis of variance to characterize the overall  level of population effects, and is very similar to thermsse.
this is specially designed for use in generalized regression networks, and should not be employed elsewhere.
one option offered bystructural equation modeling is to precede the gauss-newton procedure with a few iterations utilizing the "method of steepest descent."
this statistic is used to evaluate the statistical  significance of parameter estimates computed viamaximum likelihood methods.
these types of charts are sometimes also referred to as shewhart control charts (named after w. a. shewhart who is generally credited as being the first to introduce these methods; see shewhart, 1931).
survivorship function.
visually, as a result of smoothing, a jagged line pattern should be transformed into a smooth curve.
sensitivity analysis can be used purely for informative purposes, or  to perform input pruning.
for example, if we were to take a simple random sample with a sampling fraction of 1/10,000 from a population of 1,000,000 cases, each case would have a 1/10,000 probability of being selected into the sample, which will consist of approximately 1/10,000 * 1,000,000 = 100 observations.
six sigma methodology and management strategies provide an overall framework for organizing company wide quality control efforts.
skewness (this term was first used by pearson, 1895) measures the  deviation of the distribution from symmetry.
in particular, a maximum number of epochs must always be specified.
for example, the standardized effect in a 2 samplet -test is the difference between the two means, divided by the standard deviation, i.e., es = (µ1 - µ2)/s
the points representing the  proportions of the component variables (x, y, and z) in a ternary  graph are plotted in a 2-dimensional display for each level of thegrouping  variable (or user-defined subset of data).
textbookstatistics  glossarys sampling fraction.
inserting a few steepest descent iterations may help in situations where the iterative routine "gets lost" after only a few iterations.
standard error of the mean.
a particular model might depend wholly on one, wholly on the other, or on some arbitrary combination of them.
the semi-partial or part correlation is  similar to thepartial correlation statistic.
stacked generalization.
the forward sweeping transformation for a column k can be summarized in the following four steps (where the e's refer to the elements of a symmetric matrix): the reverse sweeping operation reverses the changes effected by these transformations.
we must be  cautious in the conclusions we draw about the importance of variables.
scheffe's test is considered to be one of the most conservative post hoc tests (for a detailed discussion of different post hoc tests, see winer, michels, & brown (1991).
the algorithm will stop when a number of generations pass during which the error is always the given amount worse than the best it ever achieved.
in this type of graph, the sequence of values  from each selected variable is stacked on one another.
individual data points are represented by point markers in two-dimensional space, where axes represent the variables.
other methods for combining the prediction from multiple models or methods (e.g., from multiple datasets used for learning) areboosting and bagging (voting ).
an ongoing process that at some point was centered will shift over time.
presenting training cases  in a random order on eachepoch to prevent various undesirable effects that can  otherwise occur (such as oscillation and convergence to local minima).
for more examples of how scatterplot data helps identify the patterns of  relations between variables, seeoutliers and brushing.
thus, the semi-partial or part correlation is a  better indicator of the "practical relevance" of a predictor, because  it is scaled to (i.e., relative to) the total variability in the dependent  (response) variable.
a different point marker and color is used for each of the multipley-variables and referenced in the legend so that individual plots representing different variables can be discriminated in the graph.
statsoft's statistica text miner is an optional extension ofstatistica data miner.
neural networks  based on the topological properties of the human brain, also known as kohonen  networks (kohonen, 1982; fausett, 1994,; haykin, 1994; patterson, 1996).
see also,data rotation (in 3d space) in graphical techniques .
in general, random sampling is the process of randomly selecting observations from a population, to create a subsample that "represents" the observations in that population (see kish, 1965; see alsoprobability sampling, simple random sampling, epsem samples; see also representative sample for a brief exploration of this often misunderstood notion).
the c code generator is an add-on.
if the experiment is terminated at a particular point in time, then a single point of censoring exists, and the data set is said to be single-censored.
stepwise regression.
the idea can best be summarized with the following graphs.
[(xi-x-bar)2/n-1]1/2 xbar   is the sample mean n         is the sample size.
the range of the stiffness parameters is 0 < s < 1.
the goal of the measure phase is to gather information about the current situation, to obtain baseline data on current process performance, and to identify problem areas.
if height is a suppressor variable, then it will suppress, or control for, irrelevant variance (i.e., variance that is shared with the predictor and not the criterion), thus increasing the partial correlation.
the term "supervised" learning is usually applied to cases in which a particular classification is already observed and recorded in a training sample, and you want to build a model to predict those classifications (in a new testing sample).
statistica text miner.
consider an example experiment where we start with 100 light bulbs, and terminate the experiment after a certain amount of time.
the standard error of the mean (first used by yule, 1897) is the theoretical standard deviation of all sample means of size n drawn from a population and depends on both the population variance (sigma) and the sample size (n) as indicated below: = (2/n)1/2 where 2 is the population variance and n is the sample size.
statistica enterprise/qc.
for an overview of stepwise regression and model fit criteria seegeneral stepwise regression or multiple regression; for nonlinear stepwise and best subset regression, see generalized linear models .
the purpose of the classification analysis would be to build a model to predict who (from a different list of new potential customers) is likely to respond to the same (or a similar) offer in the future.
stationary series (in time series).
(for example, inneural networks, the stopping conditions include the maximum number ofepochs, target error performance and the minimum error improvement thresholds.
there are situations in which censoring can occur at different times (multiple censoring), or only at a particular point in time (single censoring).
the goal of the improve phase is to implement solutions that address the problems (root causes) identified during the previous (analyze) phase.
neural networks based on the topological properties of the human brain, also known as kohonen networks (kohonen, 1982; fausett, 1994,; haykin, 1994; patterson, 1996).
a sigma restricted model uses the  sigma-restricted coding to represent effects for categorical predictor  variables in general linear models and generalized linear models.
see also,descriptive statistics overview.
statsoft's statistica advanced linear/nonlinear models offers a wide array of the most advanced linear and nonlinear modeling tools on the market, supports continuous and categorical predictors, interactions, hierarchical models; automatic model selection facilities; also, includes variance components, time series, and many other methods; all analyses include extensive, interactive graphical support and built-in complete visual basic scripting.
statsoft's statistica document management system (sdms) is a scalable solution for flexible, productivity-enhancing management of local or web-based document repositories (fda/iso compliant).
unlike the regular scatterplot in which one variable is represented by the horizontal axis and one by the vertical axis, the multiple scatterplot consists of multiple plots and represents multiple correlations: one variable (x) is represented by the horizontal axis, and several variables (y's) are plotted against the vertical axis.
the data mining solutions are driven by powerful procedures from five modules, which can also be used interactively and/or used to build, test, and deploy new solutions: statistica data warehouse.
the error-based stopping conditions can also be specified independently for the error on the training set and the error on the selection set (if any).
six sigma calculators will compute the number of defects per million  opportunities (dpmo) as well as the yield, expressed as the percent of the area  under the normal curve that falls below the upper specification limit (in the  illustration above).
in this type of ternary graph, the triangular coordinate systems are used to plot four (or more) variables (the componentsx, y, and z, and the responses v1, v2, etc.) in three dimensions (ternary 3d scatterplots or surface plots).
in unsupervised learning, the situation is different.
statsoft's statistica enterprise/qc is designed for local and global enterprise quality control and improvement applications including six sigma.
by gathering information about the various stages of the process and performing statistical analysis on that information, thespc engineer is able to take necessary action (often preventive) to ensure that the overall process stays in-control and to allow the product to meet all desired specifications.
an unsmoothed surface (no smoothing function is  applied)
thus, each set is defined by a set of computation samples (from which various statistics are computed, e.g.,sigma, means, etc.) and a set of application samples (to which the respective statistics, etc. are applied).
split selection (for classification trees).
for a discussion of statistical significance, see elementary  concepts.
see also,logistic function and hyperbolic tangent  function.
the animation above shows various tail areas (p-values) for a student's t distribution with 15 degrees of freedom.
it often identifies variables that can be safely ignored  in subsequent analyses, and key variables that must always be retained.
we specify a maximum number of epochs for these iterative algorithms.
smoothing techniques for3d bivariate histograms allow us to fit surfaces to 3d representations of bivariate frequency data.
scree plot, scree test.
typically, we begin with a list ofn observations that comprises the entire population from which we wish to extract a simple random sample (e.g., a list of registered voters); we can then generatek random case numbers (without replacement) in the range from 1 ton, and select the respective cases into the final sample (with a sampling fraction or known selection probability of k/n).
detailed discussions of the spearmanr statistic, its power and efficiency can be found in gibbons (1985), hays (1981), mcnemar (1969), siegel (1956), siegel and castellan (1988), kendall (1948), olds (1949), or hotelling and pabst (1936).
there may be interdependent variables that are useful only if included as a  set.
spc involves monitoring processes, identifying problem areas, recommending methods to reduce variation and verifying that they work, optimizing the process, assessing the reliability of parts, and other analytic operations.
if the population  proportion is, and the sample size is n, the standard error of the proportion  when sampling from an infinite population is sp = (p(1-p)/n)**1/2 for more information, see power analysis.
the process of determining the "right-sized" classification tree is described in thecomputational methods section of classification trees.
similarly, a variable that encodes relatively unimportant information, but is the only variable to do so, may have higher sensitivity than any number of variables that mutually encode more important information.
sigma restricted model.
the sequence of values from each selected variable will be represented by consecutive areas stacked on one another in this type of graph.
see also, mclain, 1974.
fitting functions to scatterplot data helps identify the patterns of  relations between variables (see example below).
for example, in two dimensions (i.e., when there are two parameters to be estimated), the program will evaluate the function at three points around the current optimum.
the two coordinates (x and y) that determine the location of each point correspond to its  specific values on the two variables.
six sigma is a well-structured, data-driven methodology for eliminating defects, waste, or quality control problems of all kinds in manufacturing, service delivery, management, and other business activities.
standard error of the proportion.
if the ratio is one or lower, making the variable "unavailable"  either has no effect on the performance of the network, or actually enhances  it.
there is a third variable (the initial size of the fire) that influences both the amount of losses and the number of firemen.
if applied to the input data, standardization also makes the results of a variety of statistical techniques entirely independent of the ranges of values or the units of measurements (see the discussion of these issues inelementary concepts, basic statistics, multiple regression, factor analysis, and others).
we must be cautious in the conclusions we draw about the importance of variables.
for example, in two dimensions (i.e., when there  are two parameters to be estimated), the program will evaluate the function at  three points around the current optimum.
input variables are not, in general, independent; that is, there are interdependencies between variables.
cattell (1966) proposed that thisscree plot can be used to graphically determine the optimal number of factors to retain.
this type of graph offers a distinctive means of representing  3d scatterplot data through the use of a separate x-y plane positioned at a user-selectable level of the verticalz-axis (which  "sticks up" through the middle of the plane).
the sample estimate of the  population standard deviation is computed as: s =
however, it must be deployed with some care, for reasons that are explained below.
the short run quality control chart , for short production runs, plots transformations of the observations of variables or attributes for multiple parts, each of which constitutes a distinct "run," on the same chart.
while this test  is not as accurate as explicit likelihood-ratio test statistics based on the  ratio of the likelihoods of the model that includes the parameter of interest,  over the likelihood of the model that does not, its computation is usually much  faster.
these are organized into the categories of activities that make up the six  sigma effort: define (d), measure (m), analyze (a), improve (i), control (c); or dmaic for short.
similarly, a variable that encodes relatively  unimportant information, but is the only variable to do so, may have higher  sensitivity than any number of variables that mutually encode more important  information.
experience has shown that combining the predictions from multiple methods often yields more accurate predictions than can be derived from any one method (e.g., see witten and frank, 2000).
the more  sensitive the network is to a particular input, the greater the deterioration  we can expect, and therefore the greater the ratio.
see also, exploratory data analysis and data mining techniques and  smoothing bivariate distributions.
however, in cases when we know where to look, we can usepartial correlations that control for (i.e., partial out) the influence of specified variables.
typically, we begin with a  list ofn observations that comprises the entire population from  which we wish to extract a simple random sample (e.g., a list of  registered voters); we can then generatek random case numbers  (without replacement) in the range from 1 ton, and select the  respective cases into the final sample (with a sampling fraction or known  selection probability of k/n).
to define the sensitivity of a particular variable,v, we first run the network on a set of test cases, and accumulate the network error.
radial units have equal output values lying on hyperspheres in pattern space.
in that case, even if the process mean shifts by 1.5 * sigma in one  direction (e.g., to +1.5 sigma in the direction of the upper specification  limit), then the process will still produce very few defects.
then sensitivity analysis produces an arbitrary relative  sensitivity to them.
motorola, in their implementation of six sigma strategies, determined  that it is reasonable to assume that a process will shift over time by  approximately 1.5 * sigma (see, for example, harry and schroeder, 2000).
in the illustration shown above the area  outside the upper specification limit (greater than usl) is defined as one  million opportunities to produce defects.
sensitivity analysis can be used purely for informative purposes, or to perform input pruning.
hence, if the user-specified sets encompass all valid samples, the default all-samples set will actually become empty (since all samples will be assigned to one of the user-defined sets).
sql (structured query language) enables us to query an outside  data source about the data it contains.
software (e.g., a data base management system, such as ms sql server or oracle) that can be expanded to meet future requirements without the need to restructure its operation (e.g., split data into smaller segments) to avoid a degradation of its performance.
the probability of rejecting a false statistical null hypothesis.
given the hierarchical nature of classification trees, these splits are selected one at time, starting with the split at the root node, and continuing with splits of resulting child nodes until splitting stops, and the child nodes that have not been split become terminal nodes.
see also, post hoc comparisons.
the sequence of values from each selected variable will be represented by consecutive step areas stacked on one another in this type of graph.
(ei/s)/(1-i)1/2 where ei is the error for the ith case
the shapiro-wilk w test  is the preferred test of normality because of its good power properties as  compared to a wide range of alternative tests (shapiro, wilk, & chen,  1968).
this type of graph offers a distinctive means of representing 3d scatterplot data through the use of a separate x-y plane positioned at a user-selectable level of the verticalz-axis (which "sticks up" through the middle of the plane).
supervised and unsupervised learning.
your predictors areheight and weight of the runner.
however, the interdependence between variables  means that no scheme of single ratings per variable can ever reflect the  subtlety of the true situation.
sensitivity analysis rates variables  according to the deterioration in modeling performance that occurs if that  variable is no longer available to the model.
the goal of the control phase is to evaluate and monitor the results of the previous phase (improve).
see models for data mining.
to prevent this option from aborting the run prematurely, specify a longer window.
statistica monitoring and alerting server (mas).statsoft's statistica monitoring and alerting server (mas) is a system that enables users to automate the continual monitoring of hundreds or thousands of critical process and product parameters.
given that we have effectively removed some information that presumably the network uses (i.e. one of its input variables), we would reasonably expect some deterioration in error to occur.
suppressor variable.
smoothing techniques for3d bivariate histograms allow us to  fit surfaces to 3d representations of bivariate frequency data.
if a number of models are  studied, it is often possible to identify key variables that are always of high  sensitivity, others that are always of low sensitivity, and  "ambiguous" variables that change ratings and probably carry mutually  redundant information.
the eigenvalues for successive factors can be displayed in a simple line plot.
see also,data reduction.
split selection for classification trees refers to the process of selecting the splits on the predictor variables that are used to predict membership in the classes of the dependent variable for the cases or objects in the analysis.
the sweeping operator is used extensively ingeneral linear models, multiple regression, and similar techniques.
moreover, if either is eliminated the model may compensate adequately because the other still provides the key information.
each input variable is treated in turn  as if it were "unavailable" (hunter, 2000).
the number of non-defects can be considered the yield of the  process.
sequential surface plot, 3d.
instead,  at each iteration the function will be evaluated at m+1 points in the m  dimensional parameter space.
shapiro-wilk w test.
simple random sampling is a type of  probability sampling where observations are randomly selected from a population  with a known probability orsampling fraction.
six sigma calculators will compute the number of defects per million opportunities (dpmo) as well as the yield, expressed as the percent of the area under the normal curve that falls below the upper specification limit (in the illustration above).
the survivorship function (commonly denoted as r(t)) is the complement to the cumulative distribution function (i.e., r(t)=1-f(t)); the survivorship function is also referred to as the reliability or survival function (since it describes the probability of not failing or of surviving until a certain timet; e.g., see lee, 1992).
successive values of each series are plotted along thex-axis, with each successive series represented along the y -axis.
= ö(s2/n) s2 is the sample variance n is the sample size.
the sweeping transformation of matrices is commonly used to efficiently perform stepwise multiple regression (see dempster, 1969, jennrich, 1977) or similar analyses; a modified version of this transformation is also used to compute theg2 generalized inverse.
while in the everyday language, the term "standardization" means converting to a common standard or making something conform to a standard (i.e., its meaning is similar to the term normalization in data analysis), in statistics, this term has a very specific meaning and refers to the transformation of data by subtracting each value from some reference value (typically a sample mean) and diving it by the standard deviation (typically a sample sd).
a lower s.d. ratio indicates a better prediction.
the 6 * sigma  process shifted upwards by 1.5 * sigma will only produce 3.4 defects (i.e.,  "parts" or "cases" greater than the upper specification  limit) per one million opportunities.
since the population variance is typically unknown, the best estimate for the standard error of the mean is then calculated as: = (s2/n)1/2 where s2 is the sample variance (our best estimate of the population variance) and n is the sample size.
the term six sigma derives from the goal to achieve a process variation, so  that ± 6 * sigma (the estimate of the population standard deviation)
the basic measure of sensitivity is the ratio of the error with missing value substitution to the original error.
thus, each set is defined by a set of computation samples (from which  various statistics are computed, e.g.,sigma, means, etc.) and a set of  application samples (to which the respective statistics, etc. are applied).
the ongoing monitoring is an automated and efficient method for: statistica multistream.
refer to kish (1965) for a detailed discussion  of the advantages and characteristics of probability samples andepsem samples.
simple random sampling is a type of probability sampling where observations are randomly selected from a population with a known probability orsampling fraction.
see, neural networks.
it is therefore the preferred method for evaluating the statistical significance of parameter estimates in stepwise or best-subset model building methods.
statistical process control (spc).the term statistical process control (spc) is typically used in context of manufacturing processes (although it may also pertain to services and other activities), and it denotes statistical methods used to monitor and improve the quality of the respective operations.
if the population proportion is, and the sample size is n, the standard error of the proportion when sampling from an infinite population is sp = (p(1-p)/n)**1/2 for more information, see power analysis.
the concept of the six  sigma process is important insix sigma quality improvement programs.
the shapiro-wilk w test is the preferred test of normality because of its good power properties as compared to a wide range of alternative tests (shapiro, wilk, & chen, 1968).
in this sequential plot, a spline-smoothed  surface is fit to each data point.
standard residual value.
see also, quality control and process analysis.
however, due to the  independent scaling used for the two list of variables, it can facilitate  comparisons between variables with values in different ranges.
the advantage of spectral plots over regular 3d scatterplots is well-illustrated in the comparison of the two displays of the same data set shown below.
the short run quality control chart , for short  production runs, plots transformations of the observations of variables or  attributes for multiple parts, each of which constitutes a distinct  "run," on the same chart.
the goal of the measure phase is to gather information about the  current situation, to obtain baseline data on current process performance, and  to identify problem areas.
successive values of each series are plotted along the x -axis, with each successive series represented along they-axis.
of course, in many cases any  "outcomes" (e.g., parts) that are produced that fall below the  specification limit can be equally defective.
studentized residuals.
instead, only the first derivative information in the gradient is used.
the spectral plot makes it easier to see that the relationship betweenpressure and yield changes from an "inverted u" to a "u".
n*m3/[(n-1)*(n-2)*3] where m3     is equal to: (xi-meanx)3 3      is the standard deviation (sigma) raised to the third  power n        is the valid number of  cases.
spc uses such basic statistical quality control methods as quality control charts (sheward, pareto, and others), capability analysis, gage repeatability/reproducibility analysis, and reliability analysis.
during an iterative process (e.g., fitting, searching, training), the conditions that must be true for the process to stop.
a general approach to estimating the parameters of the signal detection model is via the use of thegeneralized linear model.
in this type of graph, the  sequences of values of variables selected in the first list will be represented  by consecutivestep areas stacked on one another while the sequences of  values of variables selected in the second list will be represented by  consecutivestep lines stacked on one another (over the step area  representing the last variable from the first list).
the sample estimate of the population standard deviation is computed as: s =
dot product units have equal output values along hyperplanes in pattern space.
in a regression problem, the ratio of the prediction error  standard deviation to the original output data standard deviation.
survival analysis (exploratory and hypothesis testing) techniques include descriptive methods for estimating the distribution of survival times from a sample, methods for comparing survival in two or more groups, and techniques for fitting linear or non-linear regression models to survival data.
performs a normalized exponential (i.e. the outputs  add up to 1).
the regular scatterplot visualizes a  relation between two variablesx and y ( e.g., weight and height).
however, while the squared partial correlation between a predictorx1 and a response variable y can be interpreted as the proportion of (unique) variance accounted for byx1, in the presence of other predictorsx2, ... , xk, relative to the residual or unexplained variance that cannot be accounted for byx2, ... , xk, the squared semi-partial or part correlation is the proportion of (unique) variance accounted for by the predictorx1, relative to the total variance ofy.
it  is also sometimes called theefficient score statistic.
a model-building technique that finds subsets of predictor variables that most adequately predict responses on a dependent variable by linear (or nonlinear) regression, given the specified criteria for adequacy of model fit.
signal detection theory (sdt).
there are several methods available for unsupervised learning, including principal components and classification analysis, factor analysis, multidimensional scaling, correspondence analysis, neural networks, self-organizing feature maps (sofm, kohonen networks);particularly powerful algorithms for pattern recognition and clustering are theem and k -means clustering algorithms.
an ongoing process that at some point was centered will shift over  time.
how does sensitivity analysis work?
in that case, we may want to  consider the lower tail of the respective (shifted) normal distribution as well.
some software programs implement an extension to the test described by  royston (1982), which allows it to be applied to large samples (with up to 5000  observations).
also assume that weight andheight are correlated.
suppose your data mining project includes tree classifiers, such as c&rt and chaid, linear discriminant analysis (e.g., see gda), and neural networks.
the standard error (this term was first used by yule, 1897) is the standard deviation of a mean and is computed as: std.err.
statistica powersolutions.
see also, 3d scatterplot - custom ternary graph, data reduction and data rotation (in 3d space).
hence, most standard six sigma calculators will be based on a 1.5 * sigma shift.
specify a negative improvement threshold if you want to stop training only when a significant deterioration in the error is detected.
the points representing the proportions of the component variables (x, y, and z) in a ternary graph are plotted in a 2-dimensional display for each level of thegrouping variable (or user-defined subset of data).
the goal of the control phase is to evaluate and monitor the  results of the previous phase (improve).
a specialized activation function for one-of-n encoded  classification networks.
statistica quality control charts.
the concept of stacking (short for  stacked generalization) applies to the area ofpredictive data mining, to  combine the predictions from multiple models.
for additional information, see alsoquality control charts; assignable  causes and actions.
in stacking, the predictions from different classifiers are used as input into ameta-learner, which attempts to combine the predictions to create a final best predicted classification.
motorola, in their implementation of six sigma strategies, determined that it is reasonable to assume that a process will shift over time by approximately 1.5 * sigma (see, for example, harry and schroeder, 2000).
large values of the parameter produce smoother curves that adequately represent the overall pattern in the data set at the expense of local details.
a scatterplot for the x-variable and each of the selected y-variables will be plotted, but the variables entered into the first list (calledleft-y) will be plotted against theleft-y axis, whereas the variables entered into the second list (calledright-y) will be plotted against the right-y axis.
becausestatistica’s flexible data import options, the methods available instatistica text miner can also be useful for processing other unstructured input (e.g., image files imported as data matrices, etc.).
sometimes error improvement may slow down for a while or even rise temporarily (particularly if the shuffle option is used with back propagation, or non-zero noise is specified, as these both introduce an element of noise into the training process).
for example, you may have a database of customers with various demographic indicators and variables potentially relevant to future purchasing behavior.
sequential/stacked plots, 2d - column.
cases in the two groups would be assigned values of 1 or -1, respectively, on  the coded predictor variable, so that if the regression coefficient for the  variable is positive, the group coded as 1 on the predictor variable will have  a higher predicted value (i.e., a higher group mean) on the dependent variable,  and if the regression coefficient is negative, the group coded as -1 on the  predictor variable will have a higher predicted value on the dependent  variable.
signal detection theory (sdt) is an  application of statistical decision theory used to detect a signal embedded in  noise.
in this type of graph, the sequences of values of variables selected in the first list will be represented by consecutive areas stacked on one another while the sequences of values of variables selected in the second list will be represented by consecutivelines stacked on one another (over the area representing the last variable from the first list).
see alsoenterprise-wide systems.
this is the standard deviation of the distribution of the sample proportion over repeated samples.
this statistic is used to evaluate the statistical significance of parameter estimates computed viamaximum likelihood methods.
six sigma methodology and management strategies provide an overall  framework for organizing company wide quality control efforts.
statistica data warehouse consists of a suite of powerful, flexible component applications, including: statistica document management system (sdms).
the function that controls the weight is determined by thestiffness parameter, which can be modified.
in addition to standardized residuals several methods (includingstudentized residuals, studentized deleted residuals, dffits, and standardized dffits) are available for detecting outlying values (observations with extreme values on the set of predictor variables or the dependent variable).
they attempt to perform classification by measuring the distance of normalized cases from exemplar points in pattern space (the exemplars being stored by the units).
for discussion of the generalized linear model and the link functions it uses, see thegeneralized linear models topic.
symmetrical distribution.
see also, 3d scatterplot - custom ternary graph, data reduction and data  rotation (in 3d space).
the general approach to quality control charting is straightforward: we extract samples of a certain size from the ongoing production process.
scatterplot, 2d - frequency.
for example,  a scalable network allows the network administrator to add many additional  nodes without the need to redesign the basic system.
there may be interdependent variables that are useful only if included as a set.
it expects one incoming weight to equal +1, one to equal -1, and the others to equal zero.
the sequence of values from each selected variable will be represented by consecutive segments of vertical columns stacked on one another in this type of graph.
a general approach to estimating the parameters of the signal detection  model is via the use of thegeneralized linear model.
see also,mahalanobis distance, deleted residual and cook’s distance.
in this type of graph, individual values of one or more series of data are represented along thex-axis as a series of "spikes" (point symbols with lines descending to the base plane).
the control limits computed for  those transformed values can then be applied to determine if the production  process is in control, to monitor continuing production, and to establish  procedures for continuous quality improvement.
of course, the computation samples and application samples can be (and often are) not the same.
this type of task calls for an unsupervised learning algorithm, because learning (fitting of models) in this case cannot be guided by previously known classifications.
now, assume that height is not correlated withtime, but weight is.
the statistical significance of a result is an estimated measure of the degree to which it is "true" (in the sense of "representative of the population").
consider, for example, the case where two input variables encode the same information (they might even be copies of the same variable).
once sensitivities have been calculated for all variables, they may be ranked in order.
the stopping rule for a classification tree refers to the criteria that are used for determining the "right-sized" classification tree, that is, a classification tree with an appropriate number of splits and optimal predictive accuracy.
however, it must be deployed with some care, for reasons that are explained  below.
see also,correlation, partial correlation, basic  statistics, multiple regression, structural equation modeling (sepath).
thus, no more than the number of factors to the left of this point should be retained.
see alsoenterprise-wide  systems.
moreover, if either is eliminated the model may compensate  adequately because the other still provides the key information.
however, the interdependence between variables means that no scheme of single ratings per variable can ever reflect the subtlety of the true situation.
this is a standard graphical tool widely used in statistical quality control.
a suppressor variable (in multiple regression ) has zero (or close to zero) correlation with the criterion but is correlated with one or more of the predictor variables, and therefore, it will suppress irrelevant variance of independent variables.
this contour plot presents a 2-dimensional  projection of the spline-smoothed surface fit to the data (see3d sequential  surface plot.
this transformation has a wide variety of applications because it makes the distributions of values easy to compare across variables and/or subsets.
the conditions are cumulative; i.e., if several stopping conditions are specified, training ceases when any one of them is satisfied.
the term stemming refers to the reduction of words to their roots so that, for example, different grammatical forms or declinations of verbs are identified and indexed (counted) as the same word.
specifically, training may be stopped when: the error drops below a given level; the error fails to improve by a given amount over a given number of epochs.
an s-shaped curve, with a near-linear central response and saturating limits.
sensitivity analysis can give important insights into the usefulness of  individual variables.
the sequence of values from each  selected variable will be represented by consecutive lines stacked on one  another in this type of graph.
dot product units perform a weighted sum of their inputs, minus the threshold value.
given that we have effectively removed some information that presumably the  network uses (i.e. one of its input variables), we would reasonably expect some  deterioration in error to occur.
an s-shaped curve, with a near-linear central response  and saturating limits.
a set of points in the feature space that determines the boundary between objects of different class memberships.
radial units calculate the square of the distance between the two points inn dimensional space (where n is the number of inputs) represented by the input pattern vector and the unit's weight vector.
an efficient algorithm for optimizing a  linear model.
the lower the coefficient, the more the shape of the curve is influenced by individual data points (i.e., the curve "bends" more to accommodate individual values and subsets of values).
a sensitivity analysis indicates  which input variables are considered most important by that particular neural  network.
for a discussion of statistical significance, see elementary concepts.
[(xi-µ)2/n]1/2 where µ is the population mean n is the population size.
a curve is fitted to the xy coordinate data using the  bicubic spline smoothing procedure.
important components of effective, modern spc systems are real-time access to data and facilities to document and respond to incoming qc data on-line, efficient central qcdata warehousing, and groupware facilities allowing qc engineers to share data and reports (see also,enterprise spc).
while monitoring an ongoing process, it often becomes necessary to adjust the center line values or control limits, as those values are being refined over time.
a sensitivity analysis indicates which input variables are considered most important by that particular neural network.
in combination with thecross entropy error function, allows multilayer perceptron networks to be modified for class probability estimation (bishop, 1995; bridle, 1990).
for more information on procedures for determining the optimal number of factors to retain, see the section onreviewing the results of a principal components analysis in factor analysis and how many dimensions to specify in multi-dimensional scaling.
for  information onsql syntax, consult an sql manual.
randomly assigning cases to the training  and verification sets, so that these are (as far as possible) statistically  unbiased.
spectral plots have clear advantages over the regular 3d scatterplots when we are interested in examining how a relationship between two variables changes across the levels of a third variable, as is shown in the next illustration.
sequential/stacked plots, 2d - step.
the transformations rescale the variable values of interest such that they are of comparable magnitudes across the different short production runs (or parts).
more technically, the value of thep-value represents a decreasing index of the reliability of a result.
the double-y scatterplot can be used to compare images of several  correlations by overlaying them in a single graph.
sequential/stacked plots, 2d - mixed line.
the formula for studentized deleted residuals is given by sdresidi = dresidi/ s(i) for dresid = ei/(1-i ) and where s(i) =
this is a standard graphical tool widely used in  statistical quality control.
see also, exploratory data analysis and data mining techniques and smoothing bivariate distributions.
statistica process optimization integrates all quality control charts, process capability analyses, experimental design procedures, and six sigma methods with a comprehensive library of cutting-edge techniques for exploratory and predictive data mining.
a specialized activation function for one-of-n encoded classification networks.
this is equivalent to one minus the  explained variance of the model.
see also, data mining techniques.
in the steepest descent approach, values of the parameter vector q on each iteration are obtained as k+1 =
it is particularly recommended to monitor the selection error for minimum improvement, as this helps to prevent over-learning.
six sigma methodology is based on the combination of well-established statistical quality control techniques, simple and advanced data analysis methods, and the systematic training of all personnel at every level in the organization involved in the activity or process targeted bysix sigma.
for more information on process control systems, see the asqc/aiag'sfundamental statistical process control reference manual (1991).
like the, partial correlation, it  is a measure of the correlation between two variables that remains after  controlling for (i.e., "partialling" out) the effects of one or more  other predictor variables.
we then run the network again using the  same cases, but this time replacing the observed values ofv with the value  estimated by the missing value procedure, and again accumulate the network  error.
nonetheless, in practice it is extremely useful.
of  course, the computation samples and application samples can be (and often are)  not the same.
single and multiple censoring.
in so doing, it assigns a single rating value to each variable.
while this test is not as accurate as explicit likelihood-ratio test statistics based on the ratio of the likelihoods of the model that includes the parameter of interest, over the likelihood of the model that does not, its computation is usually much faster.
specifically, thep-value represents the probability of error that is involved in accepting our observed result as valid, that is, as "representative of the population."
otherwise, if we were to draw a simple random sample for the analysis (with 1% of responders), then practically all model building techniques would likely predict a simple "no-response" for all cases and would be (trivially) correct in 99% of the cases.
for example, decarlo  (1998) shows howsignal detection models based on different underlying  distributions can easily be considered by using the generalized linear model  with differentlink functions.
there are situations in which censoring can  occur at different times (multiple censoring), or only at a particular  point in time (single censoring).
the transformations rescale the variable  values of interest such that they are of comparable magnitudes across the  different short production runs (or parts).
scatterplot, 3d. 3d scatterplots visualize a relationship between  three or more variables, representing thex, y, and one or more z (vertical) coordinates of each point in 3-dimensional space (see graph below).
in this type of graph, individual values of one or more  series of data are represented along thex-axis as a series of  "spikes" (point symbols with lines descending to the base plane).
general linear models
six  sigma methodology is based on the combination of well-established statistical  quality control techniques, simple and advanced data analysis methods, and the  systematic training of all personnel at every level in the organization  involved in the activity or process targeted bysix sigma.
instead, we want to detect some "structure" or clusters in the data that may not be trivially observable.
the shapiro-wilk w test is used in testing for  normality.
for reviews ofsix sigma strategies, refer to harry and schroeder (2000), or pyzdek (2001).
statsoft's statistica quality control charts offers fully customizable (e.g., callable from other environments), easy and quick to use, versatile charts with a selection of automation options and user-interface shortcuts to simplify routine work (a comprehensive tool for six sigma methods).
note that each sample must be uniquely assigned to one application set; in other words, each sample has control limits based on statistics (e.g.,sigma ) computed for one particular set.
hi is the leverage for the ith case
a surface is fitted to the xyz coordinate data using the bicubic spline smoothing procedure.
this hierarchical search always begins at the last set that the user specified, and not with the all-samples set.
for example, thep-value of .05 (i.e.,1/20) indicates that there is a 5% probability that the relation between the variables found in our sample is a "fluke."
see also correlation, spurious correlations, partial correlation, basic statistics, multiple regression, general linear models, general stepwise regression, structural equation modeling (sepath).
however, if height is included in the model, then an additional 14% of the variability of time is accounted for even though height is not correlated with time (see below): rt.hw**2 = 0.5**2/(1 - 0.6**2) =
the number of non-defects can be considered the yield of the process.
hence,  most standard six sigma calculators will be based on a 1.5 * sigma shift.
scatterplot, 3d. 3d scatterplots visualize a relationship between three or more variables, representing thex, y, and one or more z (vertical) coordinates of each point in 3-dimensional space (see graph below).
each series to be plotted is spaced along they-axis.
statsoft's statistica data warehouse is the ultimate high-performance, scalable system for intelligent management of unlimited amounts of data, distributed across locations worldwide.
thus, every 3d  histogram can be turned into a smoothed surface providing a sensitive method  for revealing non-salient overall patterns of data and/or identifying patterns  to use in developing quantitative models of the investigated phenomenon.
in so doing, it assigns a single  rating value to each variable.
it may therefore rate the variables as of low sensitivity, even though they might encode key information.
the standard deviation  of a population of values is computed as: =
for information onsql syntax, consult an sql manual.
if the rms falls below this level, training ceases.
the goal of the analyze phase is to identify the root cause(s) of  quality problems, and to confirm those causes using the appropriate data  analysis tools.
it is therefore the preferred method for evaluating the statistical  significance of parameter estimates in stepwise or best-subset model building  methods.
statsoft's statistica enterprise is an integrated multi-user software system designed for general purpose data analysis and business intelligence applications in research, marketing, finance, and other industries.
see also correlation, spurious correlations, partial correlation, basic  statistics, multiple regression, general linear models, general stepwise regression, structural equation modeling (sepath).
stub and banner tables (banner tables).
see also, descriptive statistics overview.
however, while the squared partial correlation  between a predictorx1 and a response variable y can be  interpreted as the proportion of (unique) variance accounted for byx1,  in the presence of other predictorsx2, ... , xk, relative to the  residual or unexplained variance that cannot be accounted for byx2, ... , xk, the squared semi-partial or part correlation is the proportion of  (unique) variance accounted for by the predictorx1, relative to the  total variance ofy.
this is the standard deviation of the  distribution of the sample proportion over repeated samples.
frequency scatterplots display the frequencies of overlapping points between two variables in order to visually represent data point weight or other measurable characteristics of individual data points.
surface plot (from raw data).
for example, there is a correlation between the total amount of losses in a fire and the number of firemen that were putting out the fire; however, what this correlation does not indicate is that if we call fewer firemen, we would lower the losses.
see also,data  reduction.
this post hoc test can be used to determine the significant differences between group means in an analysis of variance setting.
sql (structured query language) enables us to query an outside data source about the data it contains.
so, for example, the predicted classifications from the tree classifiers, linear model, and the neural network classifier(s) can be used as input variables into a neural network meta-classifier, which will attempt to "learn" from the data how to combine the predictions from the different models to yield maximum classification accuracy.
in time series analysis, the general purpose of smoothing techniques  is to "bring out" the major patterns or trends in a time series,  while de-emphasizing minor fluctuations (random noise).
the main problem with spurious correlations is that we typically do not know what the "hidden" agent is.
statistica advanced linear/nonlinear models.
for reviews ofsix sigma  strategies, refer to harry and schroeder (2000), or pyzdek (2001).
sensitivity analysis rates variables according to the deterioration in modeling performance that occurs if that variable is no longer available to the model.
the standard deviation of a population of values is computed as: =
in 2d scatterplots, various smoothing methods are  available to fit a function through the points to best represent (summarize)  the relationship between the variables.
note that each sample must be uniquely assigned to one application set; in  other words, each sample has control limits based on statistics (e.g.,sigma ) computed for one particular set.
these types of charts are sometimes  also referred to as shewhart control charts (named after w. a. shewhart who is  generally credited as being the first to introduce these methods; see shewhart,  1931).
one-sided vs. two-sided limits.
to define the sensitivity of a  particular variable,v, we first run the network on a set of test cases, and  accumulate the network error.
for example, suppose we expressed the area above the upper specification limit in terms of one million opportunities to produce defects.
scatterplot, 2d - double-y. this type of scatterplot can be considered to  be a combination of twomultiple scatterplots for one x-variable and two different sets (lists) ofy-variables.
the define phase is concerned with the definition of project goals  and boundaries, and the identification of issues that need to be addressed to  achieve the higher sigma level.
statistica data miner.
it is recommended to assign variables to axes such that the variable that is most likely to discriminate between patterns of relation among the other two is specified as z. see also, data rotation (in 3d space) in the graphical techniques topic.
over-sampling particular strata to over-represent rare events.
the sequence of values from each  selected variable will be represented by consecutive segments of vertical  columns stacked on one another in this type of graph.
other methods for combining the prediction from multiple models or methods  (e.g., from multiple datasets used for learning)
it is the square  root of the sum of squared standardized effects divided by the number of  effects.
stopping conditions (in neural networks).
the advantage of spectral plots over regular 3d scatterplots  is well-illustrated in the comparison of the two displays of the same data set  shown below.
this is the standardized residual value (observed  minus predicted divided by the square root of the residual mean square).
statsoft's statistica pi connector is an optionalstatistica add-on component that allows for direct integration to data stored in the pi data historian.
these methods  have recently become very popular, due to numerous success stories from major  us-based as well as international corporations.
the multiple scatterplot is used to compare images of several correlations by overlaying them in a single graph that uses one common set of scales (e.g., to reveal the underlying structure of factors or dimensions in discriminant function analysis).
singular value decomposition.
specifies that the rms error on the training subset, the selection subset, or both must improve by at least this amount, or training will cease (if the window parameter is non-zero).
when initial values for the parameters are far from the ultimate minimum, the approximate hessian used in the gauss-newton procedure may fail to yield a proper step direction during iteration.
seemultiple regression, neural networks.
experience has shown that combining the predictions from  multiple methods often yields more accurate predictions than can be derived  from any one method (e.g., see witten and frank, 2000).
in 2d scatterplots, various smoothing methods are available to fit a function through the points to best represent (summarize) the relationship between the variables.
these methods have recently become very popular, due to numerous success stories from major us-based as well as international corporations.
they are not used in any other layers of any standard network architecture.
if we "control" for this variable (e.g., consider only fires of a fixed size), the correlation will either disappear or perhaps even change its sign.
dot product units are used in multilayer perceptron and linear networks, and in the final layers ofradial basis function, pnn, and grnn networks.
the eigenvalues for successive factors can be  displayed in a simple line plot.
on the horizontal axes, we can plot the  frequency of the spectrum against consecutive time intervals, and indicate on  the z-axis the spectral densities at each interval (see for example, shumway,  1988, page 82).
an important pre-processing step before indexing input documents fortext mining is the stemming of words.
scatterplot, 2d - multiple.
square root of the signal to noise ratio (f).
this is the standardized residual value (observed minus predicted divided by the square root of the residual mean square).
it is the square root of the sum of squared standardized effects divided by the number of effects.
fitting functions to scatterplot data helps identify the patterns of relations between variables (see example below).
standardized effect (es).
we then produce line charts of the variability in  those samples, and consider their closeness to target specifications.
the concept of stacking (short for stacked generalization) applies to the area ofpredictive data mining, to combine the predictions from multiple models.
see also,categorical predictor variables, design matrix; or general linear models.
the illustration shown above focuses on the number of defects that a  process produces.
the two coordinates (x and y) that determine the location of  each point correspond to its specific values on the two variables.
statsoft's statistica advanced linear/nonlinear models offers a wide array of the  most advanced linear and nonlinear modeling tools on the market, supports  continuous and categorical predictors, interactions, hierarchical models;  automatic model selection facilities; also, includes variance components, time  series, and many other methods; all analyses include extensive, interactive  graphical support and built-in complete visual basic scripting.
in other words, the lower triangle of the square matrix is a "mirror image" of the upper triangle with 1's on the diagonal (see below).|1 2 3 4| synaptic functions (in neural networks).
see also,logistic function and hyperbolic tangent function.
statistica enterprise server.
short run control charts.
if the two variables are strongly related, then the data points form a systematic shape (e.g., a straight line or a clear curve).
this standardized measure of effect size is used in the analysis of variance to characterize the overall level of population effects, and is very similar to thermsse.
for example, decarlo (1998) shows howsignal detection models based on different underlying distributions can easily be considered by using the generalized linear model with differentlink functions.
see also,elementary concepts.
the split selection process is described  in thecomputational methods section of classification trees.
sum-squared error function.
a matrix is symmetric if the transpose of the matrix is itself (i.e., a = a').
scatterplot, 3d - raw data.
the regular scatterplot visualizes a relation between two variablesx and y ( e.g., weight and height).
the test is based  on the behavior of the log-likelihood function at the point where the  respective parameter estimate is equal to0.0 (zero); specifically, it  uses the derivative (slope) of the log-likelihood function evaluated at the  null hypothesis value of the parameter (parameter =0.0).
spearman r. spearman r can be thought of as the regular pearson product-moment correlation coefficient (pearsonr); that is, in terms of the proportion of variability accounted for, except that spearmanr is computed from ranks.
creators of statistica data analysis  software and services welcome, register  |  login search the electronic statistics textbook statsoft.com
in many areas of research, thep-value of .05 is customarily treated as a "border-line acceptable" error level.
if the entire set is included in a model, they can be accorded significant  sensitivity, but this does not reveal the interdependency.
data sets with censored observations can be analyzed via survival analysis orweibull and reliability/failure time analysis.
visually, as a result  of smoothing, a jagged line pattern should be transformed into a smooth curve.
minimum improvement.
the test is based on the behavior of the log-likelihood function at the point where the respective parameter estimate is equal to0.0 (zero); specifically, it uses the derivative (slope) of the log-likelihood function evaluated at the null hypothesis value of the parameter (parameter =0.0).
stacking (stacked generalization).
+ hi see also, dffits, studentized residuals, and studentized deleted residuals.
successive values of each series are plotted  along thex-axis, with each successive series represented along the y -axis.
if the entire set is included in a model, they can be accorded significant sensitivity, but this does not reveal the interdependency.
the sequence of values from each  selected variable will be represented by consecutive step areas stacked on one  another in this type of graph.
sdt is used in psychophysical studies of detection, recognition, and discrimination, and in other areas such as medical research, weather forecasting, survey research, and marketing research.
software (e.g., a data base management system,  such as ms sql server or oracle) that can be expanded to meet future  requirements without the need to restructure its operation (e.g., split data  into smaller segments) to avoid a degradation of its performance.
statsoft's statistica powersolutions is a solution package aimed for use at power generation companies to optimize power plant performance, increase efficiency, and reduce emissions.
simple random sampling (srs).
a classification method based on the maximum margin hyperplane.
however, in cases when we know where to look, we  can usepartial correlations that control for (i.e., partial out) the influence  of specified variables.
repetitions of a particular analytic or computational operation or procedure.
if the experiment is terminated at a particular point in time,  then a single point of censoring exists, and the data set is said to be single-censored.
semi-partial (or part) correlation.
you may want to review the methods discussed in general classification and regression trees (gc&rt), general chaid models (gchaid), discriminant function analysis and general discriminant analysis (gda) ,marsplines (multivariate adaptive regression splines), and neural networks to learn about different techniques that can be used to build or fit models to data where the outcome variable of interest (e.g., customer did or did not respond to an offer) was observed.
[(xi-µ)2/n]1/2 µ     is the population mean n     is the population size.
the split selection process is described in thecomputational methods section of classification trees.
the standard error (this term was first used by  yule, 1897) is the standard deviation of a mean and is computed as: std.err.
ö(s2/n) where s2 is the sample variance n is the sample size.
see also, post hoc  comparisons.
cattell (1966) proposed that thisscree plot can be used to graphically determine the optimal number of factors to  retain.
statsoft's statistica sequence, association and link analysis (sal) is designed to address the needs of clients in retailing, banking, insurance, etc., industries by implementing the fastest known highly scalable algorithm with the ability to drive association and sequence rules in one single analysis.
to illustrate the sigma-restricted coding, suppose that a categorical predictor variable calledgender has two levels (i.e., male and female).
scatterplot, 2d - double-y. this type of scatterplot can be considered to be a combination of twomultiple scatterplots for one x-variable and two different sets (lists) ofy-variables.
frequency scatterplots display the frequencies  of overlapping points between two variables in order to visually represent data  point weight or other measurable characteristics of individual data points.
however, due to the independent scaling used for the two list of variables, it can facilitate comparisons between variables with values in different ranges.
standardized dffits.
in this type of ternary graph, the  triangular coordinate systems are used to plot four (or more) variables (the  componentsx, y, and z, and the responses v1, v2, etc.) in  three dimensions (ternary 3d scatterplots or surface plots).
the goal of the improve phase is to implement solutions that  address the problems (root causes) identified during the previous (analyze)  phase.
scheffe's  test is considered to be one of the most conservative post hoc tests (for a  detailed discussion of different post hoc tests, see winer, michels, &  brown (1991).
since the population variance is typically unknown, the best estimate for  the standard error of the mean is then calculated as: = (s2/n)1/2 s2    is the sample variance (our best estimate of  the population variance) and n    is the sample size.
this important transformation will bring all  values (regardless of their distributions and original units of measurement) to  compatible units from a distribution with a mean of 0 and a standard deviation  of 1.
in simple terms, what this means is that the hessian is not used to help find the direction for the next step.
for more information, please refer to pedhazur, 1982.
the space plots specific layout may facilitate exploratory examination of specific types of three-dimensional data.
the original application of this type of plot was in the context ofspectral analysis in order to investigate the behavior of non-stationary time series.
however, also specialized experimental methods (doe) and other advanced statistical techniques are often part of globalspc systems.
this post hoc test can be used to determine the significant  differences between group means in an analysis of variance setting.
sequential contour plot, 3d.
statistica multivariate statistical process control (mspc).statsoft's statistica multivariate statistical process control (mspc) is a complete solution for multivariate statistical process control, deployed within a scalable, secure analytics software platform.
in this type of graph, the sequence of values from each selected variable is stacked on one another.
for example, you are trying to predict the times of runners in a 40 meter dash.
worse, if only part of the interdependent set is included, their sensitivity will be zero, as they carry no discernable information.
to the right of this point, presumably, we find only "factorial scree" – "scree" is the geological term referring to the debris that collects on the lower part of a rocky slope.
as mentioned above, spearmanr assumes that the  variables under consideration were measured on at least anordinal (rank  order) scale; that is, the individual observations (cases) can be ranked into  two ordered series.
the sequence of values from each selected variable will be represented by consecutive lines stacked on one another in this type of graph.
instead of discarding such observations from the data analysis all together (i.e., unnecessarily loose potentially useful information) survival analysis techniques can accommodate censored observations, and "use" them in statistical significance testing and model fitting.
a surface is fitted to the xyz coordinate data using  the bicubic spline smoothing procedure.
is drawn through the points in the 3d scatterplot.
these methods are called supervised learning algorithms because the learning (fitting of models) is "guided" or "supervised" by the observed classifications recorded in the data file.
the "height" of each spike is determined by the respective value of each series.
statistica multistream is a complete enterprise system built on a robust, advanced client-server (and fully web-enabled) architecture, offers central administration and management of deployment of models, as well as cutting edge root-cause analysis and predictive data mining technology, and its analytics are seamlessly integrated with a built-in document management system.
if we "control" for this  variable (e.g., consider only fires of a fixed size), the correlation will  either disappear or perhaps even change its sign.
this transformation has a wide variety of applications because it makes the  distributions of values easy to compare across variables and/or subsets.
by default the window is zero, which means that the minimum improvement stopping condition is not used at all.
unlike the regular scatterplot in which one  variable is represented by the horizontal axis and one by the vertical axis,  the multiple scatterplot consists of multiple plots and represents multiple  correlations: one variable (x) is represented by the horizontal axis,  and several variables (y's) are plotted against the vertical axis.
here, the responses (v1, v2, etc.) associated with the proportions of the component variables (x, y, and z) in a ternary graph are plotted as the heights of the points.
this is another measure of impact of the respective  case on the regression equation.
the semi-partial or part correlation is similar to thepartial correlation statistic.
to reiterate, we may want to estimatesigma from a set of samples that are known to be in control (the computation set), and use that estimate for establishing control limits for all remaining and new samples (the application set).
this hierarchical search  always begins at the last set that the user specified, and not with the  all-samples set.
suppose your data mining project includes tree classifiers, such as c&rt  and  chaid, linear discriminant analysis (e.g., see gda), and neural  networks.
then sensitivity analysis produces an arbitrary relative sensitivity to them.
it often identifies variables that can be safely ignored in subsequent analyses, and key variables that must always be retained.
1/(c-p-1)1/2 * ((c-p)s2/1-hi) -
the scree test involves finding the place where the smooth decrease of eigenvalues appears to level off to the right of the plot.
these are organized into the categories of activities that make up the six sigma effort: define (d), measure (m), analyze (a), improve (i), control (c); or dmaic for short.
the term six sigma derives from the goal to achieve a process variation, so that ± 6 * sigma (the estimate of the population standard deviation) will "fit" inside the lower and upper specification limits for the process.
sofms (self-organizing feature maps; kohonen networks).
in probability sampling, the sampling fraction is the  (known) probability with which cases in the population are selected into the  sample.
individual data points are represented by point markers in two-dimensional  space, where axes represent the variables.
performs a normalized exponential (i.e. the outputs add up to 1).
if  applied to the input data, standardization also makes the results of a variety  of statistical techniques entirely independent of the ranges of values or the  units of measurements (see the discussion of these issues inelementary concepts, basic statistics, multiple regression, factor analysis, and others).
a nonlinear estimation algorithm that does not rely on  the computation or estimation of the derivatives of theloss function.
for more examples of how scatterplot data helps identify the patterns of relations between variables, seeoutliers and brushing.
if the skewness is clearly  different from 0, then that distribution isasymmetrical, while normal  distributions are perfectlysymmetrical.
see  also,mahalanobis distance, deleted residual and cook’s distance.
a lower s.d.  ratio indicates a better prediction.
the idea  can best be summarized with the following graphs.
in the stub-and-banner table, one list will be tabulated in the columns (horizontally) and the second list will be tabulated in the rows (vertically) of the scrollsheet.
also, we may want to  compute the control limits and center line values from aset of samples that are known to be in control, and apply those values to all subsequent  samples.
in stratified sampling, we usually apply specific (identical or different) sampling fractions to different groups (strata) in the population to draw the sample.
we can use a sql statement in order to specify the desired tables, fields, rows, etc. to return as data.
support value (association rules).
a six sigma process is one that can be expected to  produce only 3.4 defects per one million opportunities.
statsoft's statistica base offers a comprehensive set of essential statistics in a user-friendly package with flexible output management and web enablement features; it also includes all statistica graphics tools and a comprehensive visual basic development environment.
statistica pi connector.
[(xi-x-bar)2/n-1]1/2 where xbar is the sample mean n is the sample size.
see also,correlation, partial correlation, basic statistics, multiple regression, structural equation modeling (sepath).
typical survival analysis methods include life table, survival distribution, and kaplan-meier survival function estimation, and additional techniques for comparing the survival in two or more groups.
the window factor is the number of epochs across which the error must fail to improve by the specified amount, before the algorithm is deemed to have slowed down too much and is stopped.
only after identifying certain clusters can you begin to assign labels, for example, based on subsequent research (e.g., after identifying one group of customers as "young risk takers").
for example, in catalog retailing the response rate to particular catalog offers can be below 1%, and when analyzing historical data (from prior campaigns) to build a model for targeting potential customers more successfully, it is desirable to over-sample past respondents (i.e., the "rare" respondents who ordered from the catalog); we can then apply the various model building techniques for classification (seedata mining) to a sample consisting of approximately 50% responders and 50% non-responders.
this coding strategy is aptly called the sigma-restricted  parameterization, because the values used to represent group membership (1 and  -1) sum to zero.
the post-synaptic value is the +1 input divided by the -1 input.
finally,survival analysis includes the use of regression models for estimating the relationship of (multiple) continuous variables to survival times.
an error function composed by squaring the difference between sets of target and actual values, and adding these together.
here, the  responses (v1, v2, etc.) associated with the proportions of the  component variables (x, y, and z) in a ternary graph are plotted  as the heights of the points.
data sets with censored observations can be analyzed via survival analysis  orweibull and reliability/failure time analysis.
this sequential plot fits a spline-smoothed surface to each data point.
to reiterate, we may want to estimatesigma from a set  of samples that are known to be in control (the computation set), and use that  estimate for establishing control limits for all remaining and new samples (the  application set).
statistica multistream was designed for process industries in general.
the standard deviation (this term was first used by  pearson, 1894) is a commonly-used measure of variation.
an efficient algorithm for optimizing a linear model.
the standard error of the mean (first used by  yule, 1897) is the theoretical standard deviation of all sample means of size n drawn from a population and depends on both the population variance  (sigma) and the sample size (n) as indicated below: = (2/n)1/2 where 2   is the population variance and n      is the sample size.
in time series analysis, a stationary series has a constant mean, variance, and autocorrelation through time (i.e., seasonal dependencies have been removed viadifferencing).
scatterplot, 3d - ternary graph.
radial units are used in the second layer of kohonen, radial basis function, clustering, and probabilistic and generalized regression networks.
an important distinction in machine learning, and also applicable to data mining, is that between supervised and unsupervised learning algorithms.
in summary, sensitivity analysis does not rate the "usefulness"  of variables in modeling in a reliable or absolute manner.
sensitivity analysis (in neural networks).
cases in the two groups would be assigned values of 1 or -1, respectively, on the coded predictor variable, so that if the regression coefficient for the variable is positive, the group coded as 1 on the predictor variable will have a higher predicted value (i.e., a higher group mean) on the dependent variable, and if the regression coefficient is negative, the group coded as -1 on the predictor variable will have a higher predicted value on the dependent variable.
based on more than 20 years of experience in applying advanced data driven, predictive data mining/optimization technologies for process optimization in various industries, statistica powersolutions enables power plants to get the most out of their existing equipment and control systems by leveraging all data collected at their sites to identify opportunities for improvement, even for older designs such as coal-fired cyclone furnaces (as well as wall-fired or t-fired designs).
the scatterplot visualizes a relation (correlation) between two variablesx and y (e.g., weight and height).
an alternative statistic is thewald statistic.
for example in theneural network time series analysis, the number of consecutive time steps from which input variable values should be drawn to be fed into the neural network input units.
spearman r. spearman r can be thought of as the regular pearson  product-moment correlation coefficient (pearsonr); that is, in terms of  the proportion of variability accounted for, except that spearmanr is  computed from ranks.
input variables are not, in general, independent; that is, there are  interdependencies between variables.
we then run the network again using the same cases, but this time replacing the observed values ofv with the value estimated by the missing value procedure, and again accumulate the network error.
see also, pseudo-inverse.
spurious correlations.
in addition to standardized residuals several methods (including studentized residuals,studentized deleted residuals, dffits, andstandardized dffits) are available for detecting outlying values (observations with extreme values on the set of predictor variables or the dependent variable).
in summary, sensitivity analysis does not rate the "usefulness" of variables in modeling in a reliable or absolute manner.
six sigma is a well-structured, data-driven methodology  for eliminating defects, waste, or quality control problems of all kinds in  manufacturing, service delivery, management, and other business activities.
the double-y scatterplot can be used to compare images of several correlations by overlaying them in a single graph.
successive values of each series are plotted along the x-axis, with each successive series represented along the y-axis.
sequential/stacked plots, 2d - area.
in this sequential plot, a spline-smoothed surface is fit to each data point.
it may  therefore rate the variables as of low sensitivity, even though they might  encode key information.
while monitoring an ongoing  process, it often becomes necessary to adjust the center line values or control  limits, as those values are being refined over time.
it is particularly useful when  the types of models included in the project are very different.
if the w statistic is significant, then the hypothesis that the respective distribution is normal should be rejected.
student's t distribution.
for more information, see thestub and banner tables section ofbasic statistics.
so, for example, the predicted classifications  from the tree classifiers, linear model, and the neural network classifier(s)  can be used as input variables into a neural network meta-classifier, which  will attempt to "learn" from the data how to combine the predictions  from the different models to yield maximum classification accuracy.
it is recommended to  assign variables to axes such that the variable that is most likely to  discriminate between patterns of relation among the other two is specified as z. see also, data rotation (in 3d space) in the graphical techniques topic.
see also,loss function.
one component graph is produced for  each level of the grouping variable (or user-defined subset of data) and all  the component graphs are arranged in one display to allow for comparisons  between the subsets of data (categories).
stopping conditions.
if a trend emerges in those lines, or if samples fall outside pre-specified limits, then the process is declared to be out of control and the operator will take action to find the cause of the problem.
if the variables are not related, then the points form an  irregular "cloud" (see the categorized scatterplot below for examples  of both types of data sets).
in that case, even if the process mean shifts by 1.5 * sigma in one direction (e.g., to +1.5 sigma in the direction of the upper specification limit), then the process will still produce very few defects.
in time series analysis, the general purpose of smoothing techniques is to "bring out" the major patterns or trends in a time series, while de-emphasizing minor fluctuations (random noise).
for example, if we were to take a simple random sample with a sampling  fraction of 1/10,000 from a population of 1,000,000 cases, each case would have  a 1/10,000 probability of being selected into the sample, which will consist of  approximately 1/10,000 * 1,000,000 = 100 observations.
the control limits computed for those transformed values can then be applied to determine if the production process is in control, to monitor continuing production, and to establish procedures for continuous quality improvement.
a statistical effect expressed in convenient standardized units.
however, we can also define stopping conditions that may cause training to determine earlier.
like the, partial correlation, it is a measure of the correlation between two variables that remains after controlling for (i.e., "partialling" out) the effects of one or more other predictor variables.
the concept of the six sigma process is important insix sigma quality improvement programs.
height, w - weight, rth = 0.0, rtw = 0.5, and rhw = 0.6.
statsoft's statistica multistream is a solution package for identifying and implementing effective strategies for advanced multivariate process monitoring and control.
altering original variable values (according to a specific function or an algorithm) into a range that meet particular criteria (e.g., positive numbers, fractions, numbers less than 10e12, numbers with a large relative variance).
the formula forstandardized dffits is sdfiti = dffiti/(si(i)1/2) where hi is the leverage for the ith case and i = 1/n
for more information, seepower analysis.
in probability sampling, the sampling fraction is the (known) probability with which cases in the population are selected into the sample.
for example, there is a correlation  between the total amount of losses in a fire and the number of firemen that  were putting out the fire; however, what this correlation does not indicate is  that if we call fewer firemen, we would lower the losses.
scalable software systems.
scatterplot, 2d - categorized ternary graph.
weight in this instance accounts for 25% (rtw**2 = 0.5**2) of the variability of time.
if the two variables are strongly  related, then the data points form a systematic shape (e.g., a straight line or  a clear curve).
for example, suppose we expressed the area above the upper specification  limit in terms of one million opportunities to produce defects.
in that case, we may want to consider the lower tail of the respective (shifted) normal distribution as well.
a curve is fitted to the xy coordinate data using the bicubic spline smoothing procedure.
the program features a large selection of text retrieval, pre-processing, and analytic and interpretive mining procedures for unstructured text data (including web pages), with numerous options for converting text into numeric information (for mapping, clustering, predictive data mining, etc.), language-specific stemming algorithms.
there is a missing value substitution procedure, which is used to allow predictions to be made in the absence of values for one or more inputs.
statsoft's statistica data miner contains the most comprehensive selection of data mining solutions on the market, with an icon-based, extremely easy-to-use user interface.
statsoft's statistica enterprise server is the ultimate enterprise system that offers full web enablement, including the ability to runstatistica interactively or in batch from a web browser on any computer (including linux, unix), offload time consuming tasks to the servers (using distributed processing), use multi-tier client-server architecture, and manage projects over the web (supporting multithreading and distributed/parallel processing that scales to multiple server computers).
areboosting and bagging (voting ).
there is a  third variable (the initial size of the fire) that influences both the amount  of losses and the number of firemen.
sets of samples in quality control charts.
see also, type i and ii censoring and left and right censoring.
studentized deleted residuals.
the standard deviation (this term was first used by pearson, 1894) is a commonly-used measure of variation.
consider, for example, the case where two input variables encode the same  information (they might even be copies of the same variable).
in stacking, the predictions from different classifiers are used as input  into ameta-learner, which attempts to combine the predictions to create a final  best predicted classification.
the  "height" of each spike is determined by the respective value of each  series.
the sequence of values from each  selected variable will be represented by consecutive step lines stacked on one  another in this type of graph.
sequential/stacked plots, 2d - lines.
given the  hierarchical nature of classification trees, these splits are selected one at  time, starting with the split at the root node, and continuing with splits of  resulting child nodes until splitting stops, and the child nodes that have  not been split become terminal nodes.
detailed discussions of the spearmanr statistic, its  power and efficiency can be found in gibbons (1985), hays (1981), mcnemar  (1969), siegel (1956), siegel and castellan (1988), kendall (1948), olds  (1949), or hotelling and pabst (1936).
each computes predicted classifications for a crossvalidation sample, from which overall goodness-of-fit statistics (e.g., misclassification rates) can be computed.
a statistical effect expressed in convenient  standardized units.
statsoft's statistica process optimization is a powerful software solution designed to monitor processes and identify and anticipate problems related to quality control and improvement with unmatched sensitivity and effectiveness.
however, in biomedical research multiple censoring often exists, for example, when patients are discharged from a hospital after different amounts (times) of treatment, and the researcher knows that the patient survived up to those (differential) points of censoring.
the program represents a stand-alone module that can be used for both model building and deployment.
the spectral plot makes it easier to see that the relationship  betweenpressure and yield changes from an "inverted u"  to a "u".
+ hi see also, dffits, studentized residuals, and studentized  deleted residuals.
steepest descent iterations.
however, in practice, we usually ignore the lower tail of the normal  curve because (1) in many cases, the process "naturally" has  one-sided specification limits (e.g., very low delay times are not really a  defect, only very long times; very few customer complaints are not a problem,  only very many, etc.), and (2) when a 6 * sigma process has been achieved, the  area under the normal curve below the lower specification limit is negligible.
statistica variance estimation and precision.
there is a missing value  substitution procedure, which is used to allow predictions to be made in the  absence of values for one or more inputs.
this is another measure of impact of the respective case on the regression equation.
a scatterplot for the x-variable and each of the selected y-variables will be plotted, but  the variables entered into the first list (calledleft-y) will be plotted  against theleft-y axis, whereas the variables entered into the second  list (calledright-y) will be plotted against the right-y axis.
also, we may want to compute the control limits and center line values from aset of samples that are known to be in control, and apply those values to all subsequent samples.
the sequence of values from each selected variable will be represented by consecutive step lines stacked on one another in this type of graph.
sensitivity analysis can give important insights into the usefulness of individual variables.
for example, in a 1-way anova, with j groups, f is calculated as for more information, see power analysis.
if the skewness is clearly different from 0, then that distribution isasymmetrical, while normal distributions are perfectlysymmetrical.
the more sensitive the network is to a particular input, the greater the deterioration we can expect, and therefore the greater the ratio.
these three points would define a triangle; in more than two dimensions, the "figure" produced by these points is called asimplex.
it is also sometimes called theefficient score statistic.
this can be viewed as ridding the analysis of noise.
for additional information see survival analysis or the weibull and reliability/failure time analysis section of process analysis.
in a regression problem, the ratio of the prediction error standard deviation to the original output data standard deviation.
if the ratio is one or lower, making the variable "unavailable" either has no effect on the performance of the network, or actually enhances it.
of course, in many cases any "outcomes" (e.g., parts) that are produced that fall below the specification limit can be equally defective.
support vector machine (svm) support vector machine (svm)
correlations that are due mostly to the influences of one or more "other" variables.
this coding strategy is aptly called the sigma-restricted parameterization, because the values used to represent group membership (1 and -1) sum to zero.
altering original variable values (according to a specific  function or an algorithm) into a range that meet particular criteria (e.g.,  positive numbers, fractions, numbers less than 10e12, numbers with a large  relative variance).
the formula forstudentized residuals is sresi =
in somepredictive data mining applications, it is often necessary to apply stratified sampling to systematically over-sample (apply a greater sampling fraction) to particular "rare events" of interest.
on the horizontal axes, we can plot the frequency of the spectrum against consecutive time intervals, and indicate on the z-axis the spectral densities at each interval (see for example, shumway, 1988, page 82).
consider an example experiment where  we start with 100 light bulbs, and terminate the experiment after a certain  amount of time.
if a  trend emerges in those lines, or if samples fall outside pre-specified limits,  then the process is declared to be out of control and the operator will take  action to find the cause of the problem.
thus, every 3d histogram can be turned into a smoothed surface providing a sensitive method for revealing non-salient overall patterns of data and/or identifying patterns to use in developing quantitative models of the investigated phenomenon.
see alsokolmogorov-smirnov test and lilliefors test.
the names of ally-variables from the two lists will be included in the legend followed either by the letter (l) or (r), denoting the left-y and right-y axis, respectively.
stiffness parameter (in fitting options).
once sensitivities have been calculated for all variables, they may be  ranked in order.
all tools instatistica data miner can be quickly and effortlessly leveraged to analyze and "drill into" results generated viastatistica sal.
a nonlinear estimation algorithm that does not rely on the computation or estimation of the derivatives of theloss function.
statsoft's statistica automated neural networks (sann) contains a comprehensive array of statistics, charting options, network architectures, and training algorithms; c and pmml (predictive model markup language) code generators.
they attempt to perform classification by dividing pattern space into sections using intersecting hyperplanes.
the basic measure of sensitivity is the ratio  of the error with missing value substitution to the original error.
to illustrate  the sigma-restricted coding, suppose that a categorical predictor variable  calledgender has two levels (i.e., male and female).
see also, type i and ii  censoring and left and right censoring.
thus, no more than the number of  factors to the left of this point should be retained.
sdt is used in psychophysical studies of detection, recognition, and  discrimination, and in other areas such as medical research, weather  forecasting, survey research, and marketing research.
an example of a  non-scalable architecture is the dos directory structure (adding files will  eventually require splitting them into subdirectories).
statistica enterprise.
thus, the semi-partial or part correlation is a better indicator of the "practical relevance" of a predictor, because it is scaled to (i.e., relative to) the total variability in the dependent (response) variable.
in combination with thecross entropy error function, allows  multilayer perceptron networks to be modified for class probability estimation  (bishop, 1995; bridle, 1990).
it is particularly useful when the types of models included in the project are very different.
the shapiro-wilk w test is used in testing for normality.
for more information see hocking (1996) and ryan (1997).
sequential/stacked plots, 2d - mixed step.
statistica process optimization.
a  different point marker and color is used for each of the multipley-variables and referenced in the legend so that individual plots representing  different variables can be discriminated in the graph.
presenting training cases in a random order on eachepoch to prevent various undesirable effects that can otherwise occur (such as oscillation and convergence to local minima).
if the w statistic is significant, then the hypothesis that the  respective distribution is normal should be rejected.
an example of a non-scalable architecture is the dos directory structure (adding files will eventually require splitting them into subdirectories).
a defining characteristic of survival time data is that they usually include so-calledcensored observations, e.g., observations that "survived" to a certain point in time, and then dropped out from the study (e.g., patients who are discharged from a hospital).
for more information, see manning and schütze (2002).
for more details, seegeneral linear models.
shewhart control charts.
individual data points are represented in two-dimensional space (see below), where axes represent the variables (x on the horizontal axis and y on the vertical axis).
the higher thep- value, the less we can believe that the observed relation between variables in the sample is a reliable indicator of the relation between the respective variables in the population.
the multiple scatterplot is used to compare images of several  correlations by overlaying them in a single graph that uses one common set of  scales (e.g., to reveal the underlying structure of factors or dimensions in discriminant function analysis).
the illustration shown above focuses on the number of defects that a process produces.
a particular  model might depend wholly on one, wholly on the other, or on some arbitrary  combination of them.
the assignment of application samples to sets proceeds in a hierarchical manner, i.e., each sample is assigned to the first set where it "fits" (where the definition of the application sample set would include the respective sample).
skewness (this term was first used by pearson, 1895) measures the deviation of the distribution from symmetry.
one component graph is produced for each level of the grouping variable (or user-defined subset of data) and all the component graphs are arranged in one display to allow for comparisons between the subsets of data (categories).
as mentioned above, spearmanr assumes that the variables under consideration were measured on at least anordinal (rank order) scale; that is, the individual observations (cases) can be ranked into two ordered series.
the squared distance is multiplied by the threshold (which is, therefore, actually a deviation value in radial units) to produce the post synaptic value of the unit (which is then passed to the unit's activation function).
worse, if only part  of the interdependent set is included, their sensitivity will be zero, as they  carry no discernable information.
while in the everyday language, the term  "standardization" means converting to a common standard or making  something conform to a standard (i.e., its meaning is similar to the term normalization in data analysis), in statistics, this term has a very specific  meaning and refers to the transformation of data by subtracting each value from  some reference value (typically a sample mean) and diving it by the standard  deviation (typically a sample sd).
correlations that are due mostly to the influences  of one or more "other" variables.
in this case, the program may iterate into a region of the parameter space from which recovery (i.e., successful iteration to the true minimum point) is not possible.
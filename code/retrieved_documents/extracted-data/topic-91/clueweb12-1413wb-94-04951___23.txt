breiman (1997) showed that this algorithm is a form of gradient optimization in function space with the goal of minimizing the objective function the quantity yih(xi) is called the margin , because it is the amount by whichxi is correctly classified.
dietterich and bakiri (1995) report that this technique  improves the performance of both decision-tree and backpropagation learning  algorithms on a variety of difficult classification problems.
finally, the representational problem arises when the hypothesis space does not contain any hypotheses that are good approximations to the true functionf .
a learning algorithm that suffers from the statistical problem is said to have highvariance.
this tree can be used to  classify a new data point as follows.
ho (1998)  introduced the random subspace method for growing collections  of decision trees (“decision forests”).
the 32 networks were based on eight different subsets of the 119 available input features and four different network sizes.
if the base learning algorithm produces less expressive hypotheses than  decision trees, then the adaboost method is recommended.
many experiments have  employed so-called decision stumps, which are decision trees with only one  internal node.
these and other studies are  summarized in dietterich (1997) .
the primary exception are data sets in which there is a high level of mislabeled training data points.
for example, breiman (1996)  introduced the bagging ( bootstrap aggregating) method, which works as follows.
experimentally, adaboost has been  shown to be very effective at increasing the margins on the training data  points; this result suggests that adaboost will make few errors on new data  points.
in order to learn complex functions with decision stumps, it is important to exploit adaboost’s ability to directly construct an additive model.
dietterich and bakiri (1995) report that this technique improves the performance of both decision-tree and backpropagation learning algorithms on a variety of difficult classification problems.
figure 1  shows a decision  tree that explains the data points given above.
the best understood form of statistical learning is known assupervised learning (see learning and statistical inference).
experimental measurements of bias and variance have  confirmed this.
ensemble learning algorithms take a different approach.
there are three ways in which this analysis has been criticized.
available: http://citeseer.nj.nec.com/breiman97arcing.html .
recent experiments suggest that breiman’s  combination of bagging and the random subspace method is the method of choice  for decision trees: it gives excellent accuracy and works well even when there  is substantial noise in the training data.
for example, in a project to identify volcanoes on venus, cherkauer (1996) trained an ensemble of 32 neural networks.
in all of the methods described above, each hypothesis hk in the ensemble is constructed independently of the others by manipulating the inputs, the outputs, or the features, or by injecting randomness.
consequently, ensembles of decision tree classifiers perform much better than individual decision trees.
another closely related learning algorithm is the hierarchical mixture-of-experts method (seemodular and hierarchical learning systems).
for example, breiman (1996) introduced the bagging ( bootstrap aggregating) method, which works as follows.
the resulting weighted error isr = ∑id(xi)yih k(xi), where hk(xi) is the label predicted by hypothesishk.
dietterich (2000) showed that randomized trees gave significantly improved performance on 14 out of 33 benchmark tasks (and no change on the remaining 19 tasks).
dietterich (2000) showed that randomized trees gave significantly improved
there is a risk that the chosen hypothesis will not  predict future data points well.
the goal is to  construct a weighted sum of hypotheses such thath(xi) =
if not, we follow the right branch to they =
the nearest neighbor algorithm does not satisfy this constraint, because it merely identifies the training data pointxi nearest to the new point x and outputs the corresponding valueyi as the prediction for h(x ), regardless of howyi is encoded.
the quantity yih(xi) is called the margin , because it is the amount by whichxi is correctly classified.
the algorithm operates as follows.
in addition to the ensemble methods described here, there are other nonensemble learning algorithms that are similar.
the majority of research into ensemble methods has focused on constructing ensembles of decision trees.
for a good discussion of decision trees, see the books by quinlan (1993) and breiman et al. (1984) .
res., 2:263–286. freund, y., and schapire, r. e., 1996, experiments with a new boosting algorithm, inprocedings of the 13th international conference on machine learning, san francisco: morgan kaufmann, pp.
methods for coordinated construction of ensembles in all of the methods described above, each hypothesis hk in the  ensemble is constructed independently of the others by manipulating the inputs,  the outputs, or the features, or by injecting randomness.
if so, we follow the left branch to another
the input data can then be relabeled so that any of the original classes in setak are given the derived label −1 and the original classes in setbk are given the derived label +1.
road map: learning in artificial networks related reading: modular and hierarchical learning systems ♢ radial basis function networks bauer, e., and kohavi, r., 1999, an empirical comparison of voting classification algorithms: bagging, boosting, and variants,machine learn. ,
in addition, the base learning algorithm  must be sensitive to the encoding of the output values.
then new learning problems can be constructed by randomly partitioning thec classes into two subsets, ak and bk.
this relabeled data is then given to the learning algorithm, which constructs a classifierhk.
a simple vote of all of these equally good  classifiers can reduce this risk.
one line of explanation is based on the margin analysis developed by vapnik (1995) and extended by schapire et al. (1998) .
why ensemble methods work learning algorithms that output only a single hypothesis suffer from three  problems that can be partly overcome by ensemble methods: the statistical  problem, the computational problem, and the representation problem.
dietterich, t. g., 2000, an experimental comparison of three methods for  constructing ensembles of decision trees: bagging, boosting, and randomization, machine learn., 40:139–158.
freund and schapire (1996) showed improved performance on 22 benchmark problems, equal performance on one problem, and worse performance on four problems.
when thel classifiers are applied to classify a new pointx, their predictions are combined into a k -bit binary string.
to determine  which hypothesish is best, a learning algorithm can measure how well h matches f on the training data points, and it can also assess how  consistenth is with any available prior knowledge about the problem.
finally, the representational problem arises when the hypothesis space does  not contain any hypotheses that are good approximations to the true functionf .
breiman, l., 2001, random forests, machine learn., 45:5–32.
if hk(x) = −1, then each class inak receives a vote.
hence, methods like bagging that rely on instability do not produce diverse ensembles.
the computational problem arises when the learning algorithm cannot  guarantee finding the best hypothesis within the hypothesis space.
in addition to decision trees, there are many other representations for  hypotheses that have been studied, includingperceptrons, adalines, and  backpropagation (q.v.), radial basis function networks (q.v.), gaussian  processes (q.v.), graphical models, helmholtz machines, and support vector  machines (q.v.).
hence, by taking a weighted vote of hypotheses, the learning algorithm may be able to form a more accurate approximation tof.
this differs from adaboost and other additive ensembles, where the weights are determined once during training and then held constant thereafter.
experimentally, adaboost has been shown to be very effective at increasing the margins on the training data points; this result suggests that adaboost will make few errors on new data points.
an algorithm that exhibits the computational problem is sometimes described has havingcomputational variance.
let dk(xi) be the weight on data pointxi during iteration k of the algorithm.
the classification decision of the combined  classifierh is +1 if h(x) ≥ 0 and −1 otherwise.
in  addition, because the internal nodes of the tree test only a single variable,  this creates axis-parallel rectangular decision regions that can have high  bias.
if so, then we  follow the left (“yes”) branch to they =
this method chooses a random subset of the features at each node of the tree, and constrains the tree-growing algorithm to choose its splitting rule from among this subset.
performance on 14 out of 33 benchmark tasks (and no change on the remaining 19  tasks).
supervised learning can be applied to many problems,  including handwriting recognition, medical diagnosis, and part-of-speech  tagging in language processing.
an equivalent way of thinking about this method is that each class j is encoded as ak-bit codeword cj, where bit k is 1 if j ∈ bk and 0 otherwise.
it works by incrementally adding one hypothesis at a time to an ensemble.
friedman, j. h., hastie, t., and tibshirani, r., 2000, additive logistic  regression: a statistical view of boosting,ann. statist.,  28:337–407. ◆ hastie, t. j., and tibshirani, r. j., 1990, generalized additive models , london: chapman and hall.
the best experimental results have been obtained with very large  decision trees and neural networks.
+1, then each class inbk receives a vote.
showed  improved performance on 22 benchmark problems, equal performance on one  problem, and worse performance on four problems.
in most experimental studies (  freund and schapire, 1996 ;  bauer and  kohavi, 1999;  dietterich, 2000 ), adaboost (and algorithms based on it) gives  the best performance on the vast majority of data sets.
a simple vote of all of these equally good classifiers can reduce this risk.
available: http://citeseer.nj.nec.com/breiman97arcing.html .
in addition, the base learning algorithm must be sensitive to the encoding of the output values.
in this setting, each data point consists of a vector of features (denotedx) and a class label y, and it is assumed that there is some underlying functionf such that y = f( x) for each training data point (x, y).
−1  “leaf,” which predicts thatk will be silent.
starting at the so-called root (i.e., top) of the tree, we first check whetherx2 =
if  the margin is positive, then the sign ofh(xi) agrees with  the sign ofyi.
first, the bound is not tight, so it may be hiding the real explanation for adaboost’s success.
however, because the output coding can create difficult  two-class learning problems, it is important that the base learner be very  expressive.
learning describes many different activities, ranging from concept learning (q.v.) to reinforcement learning (q.v.).
for example, any method for  constructing a classifier as a weighted sum of basis functions (see, e.g., radial basis function networks) can be viewed as an additive ensemble where  each individual basis function forms one of the hypotheses.
experimental evidence has shown that ensemble methods are often much more accurate than any single hypothesis.
in all cases, these algorithms find one best hypothesis h and output it as the “solution” to the learning problem.
we will discuss each of these two approaches in turn.
+1 if k is pronounced and −1 if k is silent, and where “_” denotes positions beyond the ends of the word.
a third way to force diversity is to manipulate the output labels of the  training data.
these heuristics (such as gradient descent) can get stuck in local minima and hence fail to find the best hypothesis.
in most experimental studies ( freund and schapire, 1996 ; bauer and kohavi, 1999; dietterich, 2000 ), adaboost (and algorithms based on it) gives the best performance on the vast majority of data sets.
in neural network and decision tree algorithms, for example, the task of finding the hypothesis that best fits the training data is computationally intractable, so heuristic methods must be employed.
freund, y., and schapire, r. e., 1997, a decision-theoretic generalization  of on-line learning and an application to boosting,j. comput.
1/m, where m is the number of data  points.
◆ schapire, r. e., freund, y., bartlett, p., and lee, w. s., 1998, boosting  the margin: a new explanation for the effectiveness of voting methods,ann.
the computational problem arises when the learning algorithm cannot guarantee finding the best hypothesis within the hypothesis space.
+1 leaf, where the tree indicates thatk should be pronounced.
then each of these words can be represented by the following data points:x 1 x 2 x 3 x 4 y e s _ _
statisti., 26:1651–1686. vapnik, v., 1995, the nature of statistical learning theory, new york: springer-verlag.
hence, methods like bagging that rely on instability do not produce  diverse ensembles.
then they consider expanding the tree by replacing one of the leaves by a test of a second feature (in this case, the right leaf was replaced with a test ofx3).
this work shows that the error of an ensemble on new data points is bounded by the fraction of training data points for which the margin is less than some quantity θ > 0 plus a term that grows as ignoring constant factors and some log terms.
figure 1 shows a decision tree that explains the data points given above.
the second approach to designing ensembles is to construct the hypotheses  in a coupled fashion so that the weighted vote of the hypotheses gives a good  fit to the data.
by repeating this process k times (generating different subsets ak and bk), an ensemble  ofk classifiers h1, … , hk is obtained.
if not, we  follow the right (“no”) branch to another test: isx3 = n ?
after each of the k classifiers has voted, the class with the highest number of votes is selected as the prediction of the ensemble.
decision tree learning algorithms are known to  suffer from high variance, because they make a cascade of choices (of which  variable and value to test at each internal node in the decision tree) such  that one incorrect choice has an impact on all subsequent decisions.
this creates a resampled data set in which some data points appear multiple times and other data points do not appear at all.
the best understood form of  statistical learning is known assupervised learning (see learning and  statistical inference).
dietterich and bakiri (1995) describe a technique called error-correcting output coding.
this view suggests developing algorithms that choose the component models and the weights so that the weighted sum fits the data well.
as with the statistical problem, a weighted combination of several different local minima can reduce the risk of choosing the wrong local minimum to output.
suppose that the number of classes,c, is large.
one of the most efficient and widely applied learning algorithms searches the hypothesis space consisting of decision trees.
decision tree algorithms can be randomized by adding randomness to the process of choosing which feature and threshold to split on.
for example, in a  project to identify volcanoes on venus, cherkauer (1996)  trained an ensemble  of 32 neural networks.
then an unweighted vote of the hypotheses determines the final classification of a data point.
the classification decision of the combined classifierh is +1 if h(x) ≥ 0 and −1 otherwise.
this view suggests developing algorithms that choose  the component models and the weights so that the weighted sum fits the data  well.
the ensemble’s prediction is the classj whose codewordcj is closest (measured by the number of bits that agree) to the k-bit output string.
in addition, because the internal nodes of the tree test only a single variable, this creates axis-parallel rectangular decision regions that can have high bias.
◆ hastie, t. j., and tibshirani, r. j., 1990, generalized additive models , london: chapman and hall.
given a set ofm training data points, bagging chooses in each iteration a set of data points of sizem by sampling uniformly with replacement from the original data points.
second, even when adaboost is applied to large  decision trees and neural networks, it is observed to work very well even  though these representations have high vc-dimension.
machine intell., 20:832–844.
another closely related learning algorithm is the hierarchical  mixture-of-experts method (seemodular and hierarchical learning systems).
let dk(xi) be the  weight on data pointxi during iteration k of the  algorithm.
by repeating this process k times (generating different subsets ak and bk), an ensemble ofk classifiers h1, … , hk is obtained.
«« previous  next »» terms of use |
if not, we follow the right (“no”) branch to another test: isx3 = n ?
learning algorithms that output only a single hypothesis suffer from three problems that can be partly overcome by ensemble methods: the statistical problem, the computational problem, and the representation problem.
the goal of the learning algorithm is to find a good approximationh to f that can be applied to assign labels to newx values.
there is a risk that the chosen hypothesis will not predict future data points well.
suppose  we define a vector of features that consists of the two letters prior to thek and the two letters that follow the k.
the kth learned classifier  attempts to predict bitk of these codewords (a prediction of −1 is  treated as a binary value of 0).
and a learning algorithm that suffers from the representational problem is said to have high bias.
if such an ensemble of hypotheses can be constructed, it is easy  to see that it will be more accurate than any of its component classifiers,  because the disagreements will cancel out.
intuitively, this formula says that if the ensemble learning algorithm can  achieve a large “margin of safety” on each training data point  while using only a weighted sum of simple classifiers,then the resulting voted  classifier is likely to be very accurate.
the resulting  weighted error isr = ∑id(xi)yih k(xi), where hk(xi) is the label  predicted by hypothesishk.
to determine which hypothesish is best, a learning algorithm can measure how well h matches f on the training data points, and it can also assess how consistenth is with any available prior knowledge about the problem.
the resulting ensemble classifier was significantly more accurate than any of the individual neural networks.
the primary exception  are data sets in which there is a high level of mislabeled training data  points.
current research is exploring ways of integrating error-correcting output codes directly into the adaboost algorithm.
◆ ho, t. k., 1998, the random subspace method for constructing decision  forests,ieee trans.
methods for designing good error-correcting codes can be applied to choose the codewordscj (or, equivalently, subsets ak and bk).
the weight assigned to this hypothesis is computed by to compute the weights for the next iteration, the weight of training data pointi is set to where zk is chosen to make dk+1 sum to 1.
both of these learn a single linear discrimination rule.
an algorithm that exhibits the computational problem  is sometimes described has havingcomputational variance.
the resulting ensemble classifier was  significantly more accurate than any of the individual neural networks.
one of the most efficient and widely applied learning algorithms searches  the hypothesis space consisting of decision trees.
a second way to force diversity is to provide a different subset of the  input features in each call to the learning algorithm.
the goal is to construct a weighted sum of hypotheses such thath(xi) = ∑kwkhk(xi) has the same sign as yi , the correct label ofxi.
vapnik, v., 1995, the nature of statistical learning theory, new  york: springer-verlag.
friedman, hastie, and tibshirani (2000) expand on breiman’s analysis from a statistical perspective.
+1 if k is pronounced and −1 if k is  silent, and where “_” denotes positions beyond the ends of the word.
because the generalization ability of a single feedforward neural network  is usually very good, neural networks benefit less from ensemble methods.
◆ ho, t. k., 1998, the random subspace method for constructing decision forests,ieee trans.
similar recommendations apply to ensembles constructed using the naive bayes and fisher’s linear discriminant algorithms.
the  algorithms are very stable, which means that even substantial (random) changes  to the training data do not cause the learned discrimination rule to change  very much.
a fourth way of generating accurate and diverse ensembles is to inject randomness into the learning algorithm.
the answer is to have eachhk classify x.
◆ dietterich, t. g., and bakiri, g., 1995, solving multiclass learning  problems via error-correcting output codes,j. artif.
in neural  network and decision tree algorithms, for example, the task of finding the  hypothesis that best fits the training data is computationally intractable,
breiman (1997)  showed that this algorithm is a form of gradient  optimization in function space with the goal of minimizing the objective  function
the first approach is to construct each hypothesis independently in such a way that the resulting set of hypotheses is accurate and diverse, that is, each individual hypothesis has a reasonably low error rate for making new predictions and yet the hypotheses disagree with each other in many of their predictions.
the input data  can then be relabeled so that any of the original classes in setak are  given the derived label −1 and the original classes in setbk are  given the derived label +1.
in a hierarchical mixture, individual hypotheses are combined by a gating network that decides, based on the features of the data point, what weights should be employed.
breiman (2001) combines bagging with the random subspace method to grow random decision forests that give excellent performance.
review of ensemble algorithms ensemble learning algorithms work by running a base learning algorithm  multiple times, and forming a vote out of the resulting hypotheses.
rather than finding  one best hypothesis to explain the data, they construct aset of  hypotheses (sometimes called acommittee or ensemble) and then  have those hypotheses “vote” in some fashion to predict the label  of new data points.
the majority of research into ensemble methods has focused on constructing  ensembles of decision trees.
this relabeled data is then given to the learning  algorithm, which constructs a classifierhk.
breiman, l., 1996, bagging predictors, machine learn., 24:123–140. ◆ breiman, l., 1997, arcing the edge, technical report 486, department of statistics, university of california, berkeley.
in statistics,  such ensembles are known asgeneralized additive models (  hastie and  tibshirani, 1990).
ordinary machine learning algorithms work by searching through a space of possible functions, calledhypotheses, to find the one function, h , that is the best approximation to the unknown functionf.
in addition to decision trees, there are many other representations for hypotheses that have been studied, includingperceptrons, adalines, and backpropagation (q.v.), radial basis function networks (q.v.), gaussian processes (q.v.), graphical models, helmholtz machines, and support vector machines (q.v.).
the first approach is to construct each hypothesis independently in such a  way that the resulting set of hypotheses is accurate and diverse, that is, each  individual hypothesis has a reasonably low error rate for making new  predictions and yet the hypotheses disagree with each other in many of their  predictions.
these and other studies are summarized in dietterich (1997) .
if the base learning algorithm produces less expressive hypotheses than decision trees, then the adaboost method is recommended.
−1 “leaf,” which predicts thatk will be silent.
a contrasting view of an ensemble is that it is an additive model, that is, it predicts the class of a new data point by taking a weighted sum of a set of component models.
in this approach, the choice of one component hypothesis influences the choice of other hypotheses and of the weights assigned to them.
∑kwkhk(xi) has the same sign as yi , the correct label ofxi.
then they consider expanding  the tree by replacing one of the leaves by a test of a second feature (in this  case, the right leaf was replaced with a test ofx3).
ordinary machine learning algorithms work by searching through a space of  possible functions, calledhypotheses, to find the one function, h , that is the best approximation to the unknown functionf.
this tree can be used to classify a new data point as follows.
quinlan, j. r., 1993, c4.5: programs for empirical learning, san  francisco: morgan kaufmann.
privacy  policy | contact | faq © 2010
if the learning algorithm isunstable—that is, if small changes in the training data lead to large changes in the resulting hypothesis— then bagging will produce a diverse ensemble of hypotheses.
if so, then we follow the left (“yes”) branch to they =
a third way to force diversity is to manipulate the output labels of the training data.
the 32 networks were based on eight different subsets of  the 119 available input features and four different network sizes.
because the generalization ability of a single feedforward neural network is usually very good, neural networks benefit less from ensemble methods.
in some cases, a weighted sum of hypotheses expands the space of functions that can be represented.
each new hypothesis is constructed by a learning algorithm that seeks to minimize the classification error on aweighted training data set.
the adaboost algorithm, introduced by freund and schapire (1996 , 1997) , is an extremely effective method for constructing an additive model.
in this formula, m is  the number of training data points, andd is a measure of the expressive  power of the hypothesis space from which the individual classifiers are drawn,  known as the vc-dimension.
if hk(x) =  +1, then each class inbk receives a vote.
the function h is  called aclassifier, because it assigns class labels y to input  data pointsx.
dietterich, t. g., 2000, an experimental comparison of three methods for constructing ensembles of decision trees: bagging, boosting, and randomization, machine learn., 40:139–158.
consider the words desk, think, and hook,  where thek is pronounced, and the words back, quack, and knave, where the k is silent (in back and quack, we  will suppose that thec is responsible for the k sound).
the weight assigned to this hypothesis is  computed by to compute the weights for the next iteration, the weight of training data  pointi is set to where zk is chosen to make dk+1 sum to 1.
breiman (2001)   combines bagging with the random subspace method to grow random decision  forests that give excellent performance.
one line of explanation is based on the margin analysis developed by vapnik  (1995) and extended by  schapire et al. (1998) .
hence, ensemble methods can reduce both the bias and the variance of learning algorithms.
freund, y., and schapire, r. e., 1996, experiments with a new boosting  algorithm, inprocedings of the 13th international conference on machine  learning, san francisco: morgan kaufmann, pp.
various heuristics  are applied to choose which test to include in each iteration and when to stop  growing the tree.
this method chooses a  random subset of the features at each node of the tree, and constrains the  tree-growing algorithm to choose its splitting rule from among this subset.
each new  hypothesis is constructed by a learning algorithm that seeks to minimize the  classification error on aweighted training data set.
quinlan, j. r., 1993, c4.5: programs for empirical learning, san francisco: morgan kaufmann.
when thel classifiers are applied to  classify a new pointx, their predictions are combined into a k -bit binary string.
current research is focusing on methods  for extending adaboost to work in high noise settings.
for example, the backpropagation algorithm can be run many times, starting each time from a different random setting of the weights.
it works  by incrementally adding one hypothesis at a time to an ensemble.
second, even when adaboost is applied to large decision trees and neural networks, it is observed to work very well even though these representations have high vc-dimension.
the  answer is to have eachhk classify x.
this work shows that the error  of an ensemble on new data points is bounded by the fraction of training data  points for which the margin is less than some quantity θ > 0 plus a  term that grows as ignoring constant factors and some log terms.
◆ dietterich, t. g., and bakiri, g., 1995, solving multiclass learning problems via error-correcting output codes,j. artif.
rather than finding one best hypothesis to explain the data, they construct aset of hypotheses (sometimes called acommittee or ensemble) and then have those hypotheses “vote” in some fashion to predict the label of new data points.
this approach directly addresses the representational problem  discussed above.
intuitively, this formula says that if the ensemble learning algorithm can achieve a large “margin of safety” on each training data point while using only a weighted sum of simple classifiers,then the resulting voted classifier is likely to be very accurate.
a decision tree for pronouncing the  letterk.
minimizing j causes the margin to be maximized.
recent experiments suggest that breiman’s combination of bagging and the random subspace method is the method of choice for decision trees: it gives excellent accuracy and works well even when there is substantial noise in the training data.
a second way to force diversity is to provide a different subset of the input features in each call to the learning algorithm.
if not, the feature x3 is tested to see if it is the letter n. k is pronounced only if x2 is not c and x3 is notn.
a decision tree learning algorithm searches the space of such trees by  first considering trees that test only one feature (in this casex2 was  chosen) and making an immediate classification.
road map: learning in artificial networks related reading: modular and hierarchical learning systems  ♢ radial basis function networks references bauer, e., and kohavi, r., 1999, an empirical comparison of voting  classification algorithms: bagging, boosting, and variants,machine learn. , 36:105–139.
suppose we define a vector of features that consists of the two letters prior to thek and the two letters that follow the k.
current research is exploring ways of  integrating error-correcting output codes directly into the adaboost algorithm.
many experiments have employed so-called decision stumps, which are decision trees with only one internal node.
dietterich and bakiri (1995)  describe a technique called  error-correcting output coding.
for example, the backpropagation  algorithm can be run many times, starting each time from a different random  setting of the weights.
the input  feature subsets were selected (by hand) to group together features that were  basedon different image processing operations (such as principal component  analysis and the fast fourier transform).
suppose that the number of classes,c, is  large.
this usually gives better results than bagging and other accuracy/diversity methods.
ensemble learning algorithms work by running a base learning algorithm multiple times, and forming a vote out of the resulting hypotheses.
the adaboost algorithm, introduced by  freund and schapire (1996 ,  1997) ,  is an extremely effective method for constructing an additive model.
in this formula, m is the number of training data points, andd is a measure of the expressive power of the hypothesis space from which the individual classifiers are drawn, known as the vc-dimension.
then each of these words can  be represented by the following data points: x 1 x 2 x 3 x 4 y e  s  _  _
however, because the output coding can create difficult two-class learning problems, it is important that the base learner be very expressive.
for multiclass problems, the error-correcting output coding algorithm can produce good ensembles.
there are two main approaches to designing ensemble learning algorithms.
1/m, where m is the number of data points.
and a learning  algorithm that suffers from the representational problem is said to have high bias.
a decision tree learning algorithm searches the space of such trees by first considering trees that test only one feature (in this casex2 was chosen) and making an immediate classification.
a fourth way of generating accurate and diverse ensembles is to inject  randomness into the learning algorithm.
related nonensemble learning methods
breiman, l., friedman, j. h., olshen, r. a., and stone, c. j., 1984, classification and regression trees, monterey, ca: wadsworth and  brooks.
the value of θ can be chosen to minimize the value of this expression.
after each of the k classifiers has voted, the class with the highest number of votes is selected  as the prediction of the ensemble.
methods for designing good error-correcting codes  can be applied to choose the codewordscj (or, equivalently, subsets ak and bk).
in some cases, a weighted sum of hypotheses expands the space of  functions that can be represented.
the goal of the  learning algorithm is to find a good approximationh to f that can  be applied to assign labels to newx values.
the algorithms are very stable, which means that even substantial (random) changes to the training data do not cause the learned discrimination rule to change very much.
statisti., 26:1651–1686.
for a good discussion of decision trees, see the books by  quinlan (1993) and  breiman et al. (1984) .
experimental measurements of bias and variance have confirmed this.
similar recommendations apply to ensembles  constructed using the naive bayes and fisher’s linear discriminant  algorithms.
freund and schapire (1996)
so  heuristic methods must be employed.
various heuristics are applied to choose which test to include in each iteration and when to stop growing the tree.
consider the words desk, think, and hook, where thek is pronounced, and the words back, quack, and knave, where the k is silent (in back and quack, we will suppose that thec is responsible for the k sound).
given a set ofm training data points, bagging chooses in each iteration  a set of data points of sizem by sampling uniformly with replacement  from the original data points.
adaboost is probably the best method to apply, but favorable results have been obtained just by training several networks from different random starting weight values, and bagging is also quite effective.
more precisely, an ensemble method constructs a set of  hypotheses {h1, … , hk}, chooses a set of weights {w 1, … , wk}, and constructs the “voted” classifier h (x) = w1h1(x) + · · · + wkhk(x).
in a  hierarchical mixture, individual hypotheses are combined by a gating network  that decides, based on the features of the data point, what weights should be  employed.
decision tree learning algorithms are known to suffer from high variance, because they make a cascade of choices (of which variable and value to test at each internal node in the decision tree) such that one incorrect choice has an impact on all subsequent decisions.
the ensemble’s prediction is the classj whose  codewordcj is closest (measured by the number of bits that agree) to the k-bit output string.
in iterationk, the underlying learning algorithm constructs hypothesishk to minimize the weighted training error.
the statistical problem arises when the learning algorithm is searching a space of hypotheses that is too large for the amount of available training data.
the statistical problem arises when the learning algorithm is searching a  space of hypotheses that is too large for the amount of available training  data.
◆ cherkauer, k. j., 1996, human expert-level performance on a scientific  image analysis task by a system using combined artificial neural networks, in working notes of the aaai workshop on integrating multiple learned models (p. chan, ed.), menlo park, ca: aaai press, pp.
methods for independently constructing ensembles one way to force a learning algorithm to construct multiple hypotheses is  to run the algorithm several times and provide it with somewhat different  training data in each run.
in iterationk, the underlying learning algorithm constructs  hypothesishk to minimize the weighted training error.
experimental evidence has shown that ensemble methods are often much more  accurate than any single hypothesis.
the value of θ can be chosen to minimize the  value of this expression.
then an unweighted  vote of the hypotheses determines the final classification of a data point.
the input feature subsets were selected (by hand) to group together features that were basedon different image processing operations (such as principal component analysis and the fast fourier transform).
◆ schapire, r. e., freund, y., bartlett, p., and lee, w. s., 1998, boosting the margin: a new explanation for the effectiveness of voting methods,ann.
introduction introduction learning describes many different activities, ranging from concept  learning (q.v.) to reinforcement learning (q.v.).
the exact reasons for adaboost’s success are not fully understood.
third, it is possible to design algorithms that are more effective than adaboost at increasing the margin on the training data, but these algorithms exhibit worse performance than adaboost when applied to classify new data points.
first, the  bound is not tight, so it may be hiding the real explanation for  adaboost’s success.
if  not, we follow the right branch to they =
she  reports improved performance on 16 benchmark data sets.
+1 leaf, where the tree  indicates thatk should be pronounced.
as  with the statistical problem, a weighted combination of several different local  minima can reduce the risk of choosing the wrong local minimum to output.
such ensembles can overcome both the  statistical and computational problems discussed above.
she reports improved performance on 16 benchmark data sets.
then new learning problems can be constructed by randomly partitioning  thec classes into two subsets, ak and bk.
the best experimental results have been obtained with very large decision trees and neural networks.
in addition to the ensemble methods described here, there are other  nonensemble learning algorithms that are similar.
dietterich, t. g., 1997, machine learning research: four current directions, ai magazine, 18:97–136.
in order to learn complex functions with decision stumps, it is  important to exploit adaboost’s ability to directly construct an additive  model.
starting at the so-called root (i.e.,  top) of the tree, we first check whetherx2 =
this differs from adaboost and other additive ensembles, where the  weights are determined once during training and then held constant thereafter.
the nearest neighbor  algorithm does not satisfy this constraint, because it merely identifies the  training data pointxi nearest to the new point x and  outputs the corresponding valueyi as the prediction for h(x ), regardless of howyi is encoded.
supervised learning can be applied to many problems, including handwriting recognition, medical diagnosis, and part-of-speech tagging in language processing.
there are  two main approaches to designing ensemble learning algorithms.
the second approach to designing ensembles is to construct the hypotheses in a coupled fashion so that the weighted vote of the hypotheses gives a good fit to the data.
first, feature x2 is tested to see if it is the letter c.
consequently, ensembles of decision tree classifiers perform much better  than individual decision trees.
this approach directly addresses the representational problem discussed above.
ho (1998) introduced the random subspace method for growing collections of decision trees (“decision forests”).
adaboost is probably the best method to apply, but favorable results have been  obtained just by training several networks from different random starting  weight values, and bagging is also quite effective.
breiman, l., 1996, bagging predictors, machine learn.,  24:123–140. ◆ breiman, l., 1997, arcing the edge, technical report 486, department  of statistics, university of california, berkeley.
in this approach, the choice of one component hypothesis influences the  choice of other hypotheses and of the weights assigned to them.
friedman, j. h., hastie, t., and tibshirani, r., 2000, additive logistic regression: a statistical view of boosting,ann.
in statistics, such ensembles are known asgeneralized additive models ( hastie and tibshirani, 1990).
in such cases, adaboost will put very high weights on the noisy data  points and learn very poor classifiers.
a decision tree for pronouncing the letterk.
breiman, l., friedman, j. h., olshen, r. a., and stone, c. j., 1984, classification and regression trees, monterey, ca: wadsworth and brooks. ◆ cherkauer, k. j., 1996, human expert-level performance on a scientific image analysis task by a system using combined artificial neural networks, in working notes of the aaai workshop on integrating multiple learned models (p. chan, ed.), menlo park, ca: aaai press, pp.
initially, all training data pointsi are given a weight d 1(xi) =
decision tree algorithms can be randomized by adding  randomness to the process of choosing which feature and threshold to split on.
freund, y., and schapire, r. e., 1997, a decision-theoretic generalization of on-line learning and an application to boosting,j. comput.
friedman, hastie, and tibshirani (2000) expand on breiman’s analysis from  a statistical perspective.
in such cases, there may be several different hypotheses that all give the same accuracy on the training data, and the learning algorithm must choose one of these to output.
the function h is called aclassifier, because it assigns class labels y to input data pointsx.
third, it is possible to  design algorithms that are more effective than adaboost at increasing the  margin on the training data, but these algorithms exhibit worse performance  than adaboost when applied to classify new data points.
in such cases, there may be several different hypotheses that all give  the same accuracy on the training data, and the learning algorithm must choose  one of these to output.
a contrasting view of an ensemble is that it is an additive model,  that is, it predicts the class of a new data point by taking a weighted sum of  a set of component models.
if the margin is positive, then the sign ofh(xi) agrees with the sign ofyi.
statist., 28:337–407.
the kth learned classifier attempts to predict bitk of these codewords (a prediction of −1 is treated as a binary value of 0).
these heuristics (such as gradient descent)  can get stuck in local minima and hence fail to find the best hypothesis.
as an example, consider the problem of learning to pronounce the letter k in english.
more precisely, an ensemble method constructs a set of hypotheses {h1, … , hk}, chooses a set of weights {w 1, … , wk}, and constructs the “voted” classifier h (x) = w1h1(x) + · · · + wkhk(x).
this usually gives better results than bagging and other  accuracy/diversity methods.
if not, the feature x3 is tested to see if it is the letter n. k is pronounced only if x2 is not c and x3  is notn.
if hk(x) =  −1, then each class inak receives a vote.
for multiclass problems, the error-correcting output coding algorithm can  produce good ensembles.
current research is focusing on methods for extending adaboost to work in high noise settings.
in such cases, adaboost will put very high weights on the noisy data points and learn very poor classifiers.
if the learning algorithm isunstable—that is, if small changes in  the training data lead to large changes in the resulting hypothesis— then  bagging will produce a diverse ensemble of hypotheses.
hence, by taking a weighted vote of  hypotheses, the learning algorithm may be able to form a more accurate  approximation tof.
this creates a resampled data set in which some  data points appear multiple times and other data points do not appear at all.
a learning algorithm that suffers from the statistical problem is said to  have highvariance.
hence, ensemble methods can reduce both the bias and the variance of  learning algorithms.
if such an ensemble of hypotheses can be constructed, it is easy to see that it will be more accurate than any of its component classifiers, because the disagreements will cancel out.
for example, any method for constructing a classifier as a weighted sum of basis functions (see, e.g., radial basis function networks) can be viewed as an additive ensemble where each individual basis function forms one of the hypotheses.
in this setting, each data point consists of a vector  of features (denotedx) and a class label y, and it is assumed  that there is some underlying functionf such that y = f( x) for each training data point (x, y).
now, given a new data point x, how should it be classified?
such ensembles can overcome both the statistical and computational problems discussed above.
one way to force a learning algorithm to construct multiple hypotheses is to run the algorithm several times and provide it with somewhat different training data in each run.
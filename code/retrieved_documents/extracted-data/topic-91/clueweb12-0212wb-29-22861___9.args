while this has created sophisticated classification algorithms, many do not cope with increasing data set sizes.
the idea is that, for each base image classifier in the ensemble, a random image transformation is generated and applied to all of the images in the labeled training set.
the two-tiered algorithm described shows good scalability due to the randomized nature of the first step and the availability of efficient propositional clustering algorithms for the second step.} } @mastersthesis{evans07:_clust_class, author =
in this setting, the effectiveness of an algorithm cannot simply be assessed by accuracy alone.
in this paper we apply modifications to the standard llgc algorithm to improve efficiency to a point where we can handle datasets with hundreds of thousands of training data.
our experiments demonstrate that both methods are faster, find smaller subsets and can even increase the classification accuracy.
thus labeling is a two-step process: identify constituent phrases that are arguments to a predicate, then label those arguments with appropriate thematic roles.
moreover, instead of assigning semantic roles one at a time, an algorithm is proposed to assign all labels simultaneously; leveraging dependencies between roles and eliminating the problem of duplicate assignment.
abstract = { prediction intervals for class probabilities are of interest in machine learning because they can quantify the uncertainty about the class probability estimate for a test instance.
machine learning offers promise of a solution, but the field mainly focusses on achieving high accuracy when data supply is limited.
to find patterns in these datasets it would be useful to be able to apply modern methods of classification such as support vector machines.
the frequency and scale of data collection means that there are now many large datasets being generated.
scaling up semi-supervised learning: an efficient and effective llgc variant}, booktitle = {proc 11th pacific-asia conference on knowledge discovery and data mining}, series = {nanjing, china}, year = {2007}, pages = {236-247}, http = { http://dx.doi.org/10.1007/978-3-540-71701-0_25}, publisher = {springer}, abstract = {domains like text classification can easily supply large amounts of unlabeled data, but labeling itself is expensive.
it observes that the only valid arguments to a predicate are unembedded constituent phrases that do not overlap that predicate.
machine learning offers promise of a solution, but the field mainly focusses on achieving high accuracy when data supply is limited.
so fast, in fact, that the main problem is making sense of it all.
the resulting tree will be the same, just how it is built is different.
second, the search for each candidate argument is exponential with respect to the number of words in the sentence.
to measure improvement, a comprehensive framework for evaluating the performance of data stream algorithms is developed.
the results indicate that a voting ensemble of support vector machines, random forests, and boosted decision trees provide the best performance with auc values of up to 0.92 and equal error rate accuracies of up to 85.7\% in stratified 10-fold cross validation experiments on the graz02 complex image dataset.}, pdf = { http://www.cs.waikato.ac.nz/~ml/publications/2007/mayociras2007.pdf} } @inproceedings{mayo07:_random_convol_ensem, author = {michael mayo}, title = {random convolution ensembles}, booktitle = {advances in multimedia information processing - pcm 2007, 8th pacific rim conference on multimedia, lecture notes in computer science 4810}, publisher = {springer}, editor =
the modifications are priming of the unlabeled data, and most importantly, sparsification of the similarity matrix.
biological sequence data is, on the one hand, highly structured.
the study on numeric attributes demonstrates that sacrificing accuracy for space at the local level often results in improved global accuracy.
second, tree prediction strategy is investigated to evaluate the utility of various methods.
finally, the possibility of improving accuracy using ensemble methods is explored.
unfortunately these methods are computationally expensive, quadratic in the number of data points in fact, so cannot be applied directly.
experimental results are provided as evidence to show that a combination of the proposed argument identification and multi-argument classification algorithms outperforms all existing systems that use the same syntactic information.} } @inproceedings{kibriya07:_empir_compar_exact_neares_neigh_algor, author = {ashraf m. kibriya and eibe frank}, title = {
results show that the clustered datasets are on average fifty percent smaller than the original datasets without loss of classification accuracy which is significantly better than random selection.
the frequency and scale of data collection means that there are now many large datasets being generated.
the two-tiered algorithm described shows good scalability due to the randomized nature of the first step and the availability of efficient propositional clustering algorithms for the second step.} } @mastersthesis{evans07:_clust_class, author =
the resulting tree will be the same, just how it is built is different.
existing systems for semantic role labeling use machine learning methods to assign roles one-at-a-time to candidate arguments.
abstract = { modern information technology allows information to be collected at a far greater rate than ever before.
their ingenuity stems from updating sufficient statistics, only addressing growth when decisions can be made that are guaranteed to be almost identical to those that would be made by conventional batch learning methods.
it observes that the only valid arguments to a predicate are unembedded constituent phrases that do not overlap that predicate.
some empirical evidence is provided that suggest this racing algorithms method performs considerably better than an existing method based on so-called skeletal characterization of the respective implication.
thus labeling is a two-step process: identify constituent phrases that are arguments to a predicate, then label those arguments with appropriate thematic roles.
the experimental results provide meaningful comparisons of accuracy and processing speeds between different modifications of the hoeffding tree algorithm under various memory limits.
some empirical evidence is provided that suggest this racing algorithms method performs considerably better than an existing method based on so-called skeletal characterization of the respective implication.
first, more than one candidate can be assigned the same role, which is undesirable.
biological sequence data is, on the one hand, highly structured.
semi- supervised learning tries to exploit this abundance of unlabeled training data to improve classification.
{hoeffding trees are state-of-the-art for processing high-speed data streams.
they also show that there is no free lunch, for each dataset it is important to choose a clustering method carefully.} } @mastersthesis{kibriya07:_fast_algor_neares_neigh_searc, author =
the base classifiers are then learned using features extracted from these randomly transformed versions of the training data, and the result is a highly diverse ensemble of image classifiers.
the results indicate that a voting ensemble of support vector machines, random forests, and boosted decision trees provide the best performance with auc values of up to 0.92 and equal error rate accuracies of up to 85.7\% in stratified 10-fold cross validation experiments on the graz02 complex image dataset.}, pdf = { http://www.cs.waikato.ac.nz/~ml/publications/2007/mayociras2007.pdf} } @inproceedings{mayo07:_random_convol_ensem, author = {michael mayo}, title = {random convolution ensembles}, booktitle = {advances in multimedia information processing - pcm 2007, 8th pacific rim conference on multimedia, lecture notes in computer science 4810}, publisher = {springer}, editor =
our experiments have shown that both methods work well with high-dimensional data.} } @mastersthesis{shi07:_best_decis_tree_learn, author =
existing systems for semantic role labeling use machine learning methods to assign roles one-at-a-time to candidate arguments.
the runtime of heuristic and random searches are better but the problem still persists when dealing with high-dimensional datasets.
moreover, instead of assigning semantic roles one at a time, an algorithm is proposed to assign all labels simultaneously; leveraging dependencies between roles and eliminating the problem of duplicate assignment.
the idea is that, for each base image classifier in the ensemble, a random image transformation is generated and applied to all of the images in the labeled training set.
semi- supervised learning tries to exploit this abundance of unlabeled training data to improve classification.
{hoeffding trees are state-of-the-art for processing high-speed data streams.
our experiments have shown that both methods work well with high-dimensional data.} } @mastersthesis{shi07:_best_decis_tree_learn, author =
clusters generated without class information usually agree well with the true class labels of cluster members, i.e. class distributions inside clusters generally differ significantly from the global class distributions.
in this paper we apply modifications to the standard llgc algorithm to improve efficiency to a point where we can handle datasets with hundreds of thousands of training data.
the idea is that all likely class probability values of the test instance are included, with a pre-specified confidence level, in the calculated prediction interval.
third, single-role assignment cannot take advantage of dependencies known to exist between semantic roles of predicate arguments, such as their relative juxtaposition.
in order to simulate continuous operation, classes of synthetic data are generated providing an evaluation on a large scale.
the prediction strategy shown to perform best adaptively chooses between standard majority class and naive bayes prediction in the leaves.
abstract = { prediction intervals for class probabilities are of interest in machine learning because they can quantify the uncertainty about the class probability estimate for a test instance.
on the other hand there are large amounts of unlabelled data.
our experiments demonstrate that both methods are faster, find smaller subsets and can even increase the classification accuracy.
the base classifiers are then learned using features extracted from these randomly transformed versions of the training data, and the result is a highly diverse ensemble of image classifiers.
this is because the performance of the classifier and the cost of classification are sensitive to the choice of the features used to construct the classifier.
second, tree prediction strategy is investigated to evaluate the utility of various methods.
furthermore, unlike previous methods, the method is able to handle more than five variables.} } @phdthesis{lin07:_syntax, author =
so fast, in fact, that the main problem is making sense of it all.
furthermore, unlike previous methods, the method is able to handle more than five variables.} } @phdthesis{lin07:_syntax, author =
experimental results are provided as evidence to show that a combination of the proposed argument identification and multi-argument classification algorithms outperforms all existing systems that use the same syntactic information.} } @inproceedings{kibriya07:_empir_compar_exact_neares_neigh_algor, author = {ashraf m. kibriya and eibe frank}, title = {
the idea is that all likely class probability values of the test instance are included, with a pre-specified confidence level, in the calculated prediction interval.
clusters generated without class information usually agree well with the true class labels of cluster members, i.e. class distributions inside clusters generally differ significantly from the global class distributions.
the study on numeric attributes demonstrates that sacrificing accuracy for space at the local level often results in improved global accuracy.
while this has created sophisticated classification algorithms, many do not cope with increasing data set sizes.
finally, the possibility of improving accuracy using ensemble methods is explored.
abstract = { modern information technology allows information to be collected at a far greater rate than ever before.
third, single-role assignment cannot take advantage of dependencies known to exist between semantic roles of predicate arguments, such as their relative juxtaposition.
the experimental results provide meaningful comparisons of accuracy and processing speeds between different modifications of the hoeffding tree algorithm under various memory limits.
they also show that there is no free lunch, for each dataset it is important to choose a clustering method carefully.} } @mastersthesis{kibriya07:_fast_algor_neares_neigh_searc, author = {ashraf masood kibriya}, title = {fast algorithms for nearest neighbour search}, school = {department of computer science, university of waikato}, year = 2007, http = {http://hdl.handle.net/10289/2463}, abstract = {
the modifications are priming of the unlabeled data, and most importantly, sparsification of the similarity matrix.
second, the search for each candidate argument is exponential with respect to the number of words in the sentence.
when the data set sizes get to a point where they could be considered to represent a continuous supply, or data stream, then incremental classification algorithms are required.
this is because the performance of the classifier and the cost of classification are sensitive to the choice of the features used to construct the classifier.
to measure improvement, a comprehensive framework for evaluating the performance of data stream algorithms is developed.
effective classifiers for detecting objects}, booktitle = {proc. of the fourth international conference on computational intelligence, robotics, and autonomous systems (ciras '07)}, year = 2007, abstract = {several state-of-the-art machine learning classifiers are compared for the purposes of object detection in complex images, using global image features derived from the ohta color space and local binary patterns.
the prediction strategy shown to perform best adaptively chooses between standard majority class and naive bayes prediction in the leaves.
in order to simulate continuous operation, classes of synthetic data are generated providing an evaluation on a large scale.
to find patterns in these datasets it would be useful to be able to apply modern methods of classification such as support vector machines.
the ensemble method investigation shows that combining trees can be worthwhile, but only when sufficient memory is available, and improvement is less likely than in traditional machine learning.
results show that the clustered datasets are on average fifty percent smaller than the original datasets without loss of classification accuracy which is significantly better than random selection.
when the data set sizes get to a point where they could be considered to represent a continuous supply, or data stream, then incremental classification algorithms are required.
and fourth, execution times for existing algorithm are excessive, making them unsuitable for real-time use.
in this setting, the effectiveness of an algorithm cannot simply be assessed by accuracy alone.
the runtime of heuristic and random searches are better but the problem still persists when dealing with high-dimensional datasets.
first, more than one candidate can be assigned the same role, which is undesirable.
of the fourth international conference on computational intelligence, robotics, and autonomous systems (ciras '07)}, year = 2007, abstract = {several state-of-the-art machine learning classifiers are compared for the purposes of object detection in complex images, using global image features derived from the ohta color space and local binary patterns.
within the framework memory size is fixed in order to simulate realistic application scenarios.
scaling up semi-supervised learning: an efficient and effective llgc variant}, booktitle = {proc 11th pacific-asia conference on knowledge discovery and data mining}, series = {nanjing, china}, year = {2007}, pages = {236-247}, http = { http://dx.doi.org/10.1007/978-3-540-71701-0_25}, publisher = {springer}, abstract = {domains like text classification can easily supply large amounts of unlabeled data, but labeling itself is expensive.
within the framework memory size is fixed in order to simulate realistic application scenarios.
their ingenuity stems from updating sufficient statistics, only addressing growth when decisions can be made that are guaranteed to be almost identical to those that would be made by conventional batch learning methods.
unfortunately these methods are computationally expensive, quadratic in the number of data points in fact, so cannot be applied directly.
the ensemble method investigation shows that combining trees can be worthwhile, but only when sufficient memory is available, and improvement is less likely than in traditional machine learning.
and fourth, execution times for existing algorithm are excessive, making them unsuitable for real-time use.
on the other hand there are large amounts of unlabelled data.
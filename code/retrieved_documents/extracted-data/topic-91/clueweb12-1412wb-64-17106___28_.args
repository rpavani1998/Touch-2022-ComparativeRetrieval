as the training algorithm proceeds and the number of trees in the ensemble is increased, a larger number of the training samples are classified correctly and with increasing confidence, thereby those samples receive smaller weights on the subsequent iterations.one of the key properties of the constructed decision tree algorithms is that it is possible to compute importance (relative decisive power) of each variable.a weak classifier is only required to be better than chance, and thus can be very simple and computationally inexpensive.for example, if users want to detect some rare anomaly occurrence, the training base will likely contain much more normal cases than anomalies, so a very good classification performance will be achieved just by considering every case as normal.normally, this procedure is only applied to standalone decision trees, while tree ensembles usually build small enough trees and use their own protection schemes against overfitting.it will increase the size of the input/output layer, but will speedup the training algorithm convergence and at the same time enable "fuzzy" values of such variables, i.e. a tuple of probabilities instead of a fixed value.boosting is a powerful learning concept, which provide a solution to supervised classification learning task.the algorithm caches all the training samples, and it predicts the response for a new sample by analyzing a certain number (k) of the nearest neighbors of the sample (using voting, calculating weighted sum etc.)surrogate splits are needed to handle missing measurements and for variable importance estimation.as the algorithms have different set of features (like ability to handle missing measurements, or categorical input variables etc.), there is a little common ground between the classes.the trained model can be used further for prediction, just like any other classifier.many of them smartly combined, however, result in a strong classifier, which often outperforms most 'monolithic' strong classifiers such as svms and neural networks.k nearest neighbors the algorithm caches all the training samples, and it predicts the response for a new sample by analyzing a certain number (k) of the nearest neighbors of the sample (using voting, calculating weighted sum etc.)so a robust computation scheme could be to start with the harder constraints on the covariation matrices and then use the estimated parameters as an input for a less constrained optimization problem (often a diagonal covariation matrix is already a good enough approximation).that leads to compact, and more resistant to the training data noise, but a bit less accurate decision tree.
the reduction is achieved, however, by compressing certain data structures as they are constructed; when information is required, relevant parts are temporarily restored.)
rule selection the algorithm for refining an initial collection of rules has been altered, with the result that release 1.20 sometimes produces smaller rulesets.
- new unix options - options are now provided to ignore any .costs files and to set the random number seed (so that runs with sampling etc.
the cross-reference window itself now indicates whether cases are misclassified.
release 1.14 new data type timestamps are read and written in the form yyyy-mm-dd hh:mm:ss using a 24-hour clock.
census income data (income, 199,523 cases, obtained from uci kdd archive).
- the main window can be clicked on top of the output window.
public source code output from the sample program is easier to read, and cases are identified by their label attribute (if this is defined).
for linux uses who have installed a recent version of wine, the new release includes an optional gui with many features of the see5 user interface.
non-interactively from a ms-dos command window.
previous releases often showed low confidence for ruleset predictions even when those predictions were quite accurate.
the cross-reference window itself now indicates whether cases are misclassified.
release 1.11 - new data types - dates are input and output in the form yyyy/mm/dd and can be used with implicitly defined attributes to determine, for instance, the number of days between two dates or the day of the week on which a date falls.
previous releases often showed low confidence for ruleset predictions even when those predictions were quite accurate.
(thecross-reference facility in particular provides information that is not available from the command-line version.)
the difference is particularly noticeable when the leaves involved have very little supporting data.
sleep income forest results for 1.18 are shown in blue.
a label attribute does not affect classification in any way, but its value is displayed where possible with information about the case such as error messages, cross-referencing results etc.
for most applications this should not affect the final classifier; in some cases, the tree or ruleset will be larger or smaller, but predictive accuracy should be similar.
- enhanced multi-threading - release 2.04 will now use up to four processors and so will run faster on the new quad-core cpus and computers with two dual-core processors.
- rule selection - the algorithm for refining an initial collection of rules has been altered, with the result that release 1.20 sometimes produces smaller rulesets.
- minor tweak - boosting has been improved somewhat for large data files (more than four thousand records).
this invokes a novel method of cross-referencing classifiers and the data from which they were constructed.
attributes found to be irrelevant or harmful to predictive performance are disregarded ("winnowed") and only the remaining attributes are used to construct decision trees or rulesets.
the first line of the names file can now contain the name of another attribute instead of the list of classes separated by commas.
earlier releases names file the format of the names file has been extended to allow you to select any discrete-valued attribute as the class.
release 1.11 new data types dates are input and output in the form yyyy/mm/dd and can be used with implicitly defined attributes to determine, for instance, the number of days between two dates or the day of the week on which a date falls.
for most applications this should not affect the final classifier; in some cases, the tree or ruleset will be larger or smaller, but predictive accuracy should be similar.
muti-class applications that specify a costs file should now observe lower average misclassification costs for unseen cases, especially when rulesets are generated.
every case in this monitoring application is described by 6 numeric-valued attributes and belongs to one of six classes.
releases 1.17 and 1.18 generate rulesets with similar predictive accuracies, but notice how much faster 1.18 is -- on the largest dataset it isnearly five times as fast as 1.17.
a new button on the classifier construction dialog box allows any .costs file to be ignored.
for example, the time to generate a ruleset for one dataset with 750,000 cases was reduced by more than 10%.
release 1.06 cross-reference facility (see5) this has been extended to allow classifiers to be related to cases in .test and .cases files in addition to the .data file.
(for the technically minded, the results window no longer uses the rich edit control.)
the windows gui has also been improved slightly and the public code has a new option governing output format.
release 2.06 - new algorithm for softening thresholds - see5/c5.0 decision trees have an option to soften threshold tests for continuous attributes; values near the threshold cause both the low and high branches to be evaluated and combined probabilistically.
- speed improvements - both c5.0 and see5 are now faster, particularly for large datasets.
- simplified output for rules - when the "rules" option is selected, the output now omits information about decision trees.
- cross-validations with misclassification costs - for applications with a .costs file, the unix cross-validation script now reports average misclassification costs.
rulesets are constructed from half the data and tested on the remaining 52,954 cases to estimate their true error rate.
- better use of cost information - while the treatment of costs for two-class problems remains much the same, the handling of cost information for applications with three or more classes has been extensively revised.
this shows, for each attribute, the percentage of training cases for which the value of that attribute is both known and also used in classifying the case.
- faster boosting - boosting is speedier for large datasets, especially where the number of boosting trials exceeds 20 or so.
faster subsetting of discrete attributes when the subsetting option is invoked, the values of discrete attributes are collected into subsets.
warning: the new code will not work correctly with releases earlier than 1.10.
- result window (windows version) - the system menu for this window has been extended with the option switch to wordpad.
the reduction is achieved, however, by compressing certain data structures as they are constructed; when information is required, relevant parts are temporarily restored.)
attributes excluded/included the attributes that may be used in constructing a classifier can now be specified in the.names file.
- cross-reference facility (windows version) - see5 now has a new button that looks like a large `x'.
faster boosting boosting is speedier for large datasets, especially where the number of boosting trials exceeds 20 or so.
the dialog box in the windows version has been changed to accommodate larger values of theminimum cases option.
in release 2.05, an attribute is "used" to classify a case when it referenced by one or more conditions of anapplicable rule (i.e., a rule whose conditions are all satisfied by a case).
- changes to the see5 gui - there have been several improvements in line with suggestions made by users (and please keep them coming!): - a new edit menu brings up the .names or .costs file in wordpad, making it easier to change these files.
the most serious bug in the initial release concerned the use of rulesets together with a costs file -- the system could sometimes associate an incorrect class with a rule, leading to abnormally high error rates.
the dialog box in the windows version has been changed to accommodate larger values of theminimum cases option.
alternative ordering for rulesets a new option allows rulesets to be ordered by utility, from most important to least important for classification accuracy.
if there are more than twenty classes, when a confusion matrix would be too large, release 2.04 records the number of false positives and false negatives for each class.
this facility can help to identify problems in the training data, and can be very useful for understanding complex classifiers (such as boosted trees or rulesets).
however, the option +d that preserves detailed outputs now saves one file for each cross-validation rather than one file for each c5.0 run.
release 1.14 - new data type - timestamps are read and written in the form yyyy-mm-dd hh:mm:ss using a 24-hour clock.
- linux gui - for linux uses who have installed a recent version of wine, the new release includes an optional gui with many features of the see5 user interface.
see5/c5.0 now uses a modified test selection strategy when the training data contains thousands of cases or more.
releases 1.17 and 1.18 generate rulesets with similar predictive accuracies, but notice how much faster 1.18 is -- on the largest dataset it isnearly five times as fast as 1.17.
for the continuous target attribute in a .cases file caused problems.
on the well-known census dataset, for example, release 1.17 produces a 159-leaf decision tree that is slightly more accurate than the 211-leaf tree output by release 1.16.
this bug could cause see5/c5.0 to crash, or to give incomplete results with the cross-reference facility.
this affects the output from the public code to read and interpret see5/c5.0 ruleset classifiers, and also impacts results with boosted rulesets.
saving cross-referencing results (see5) see5's cross-referencing facility is a powerful tool for finding the cases covered by particular components of classifiers, and parts of classifiers relevant to particular cases.
- progress report file - the unix progress report file is now filestem .tmp, allowing two or more users to process copies of the same application in different directories.
the attribute winnowing option attempts to identify unhelpful attributes and exclude them from classifiers.
changes to .tree and .rules files up to release 1.11 decision tree and ruleset models have been stored as binary files.
the first line of the names file can now contain the name of another attribute instead of the list of classes separated by commas.
there were also problems when the winnowing option was used at the same time as the minimum cases option was set to 1.
- adaptation to microsoft bug-fix (network versions only) - to improve security, microsoft windows updates have disabled a feature that is used by clients to read on-line help, as documentedhere.
from this release they have been changed to ascii files, so that models generated on one machine type may be deployed on machines of another type.
warning: the new code will not work correctly with releases earlier than 1.10.
- the process for selecting rules to form the final ruleset has been revised, with the result that rulesets are generally a bit smaller without loss of predictive accuracy.
a bug in the ruleset interpreter was fixed.
for example, a test that separates one class from another in a small subset of the training cases might be genuinely interesting; on the other hand, if hundreds of alternatives have been tried, it is more likely that a similar separation can be found even when none of the tests is helpful for prediction.
in release 2.05, an attribute is "used" to classify a case when it referenced by one or more conditions of anapplicable rule (i.e., a rule whose conditions are all satisfied by a case).
click on a training case to see the parts of the classifiers relevant to that case.
(this bug only trivially affected the output of see5/c5.0 and had no noticeable impact on the trees' predictive accuracy.)
- other changes and bug fixes - there have been minor modifications to the way soft thresholds for decision trees are found.
click on a training case to see the parts of the classifiers relevant to that case.
this can noticeably reduce the time taken to process very large datasets.
this definition proved unsatisfactory because it depended on the order in which rule conditions were checked, and also because many attributes ended up having usage figures around 100%.
progress report file the unix progress report file is now filestem .tmp, allowing two or more users to process copies of the same application in different directories.
this could (very rarely) lead to different classifiers when the training data were reordered, or when see5/c5.0 was run on computers with multiple cpus.
however, the option+d that preserves detailed outputs now saves one file for each cross-validation rather than one file for each c5.0 run.
the information in the cross-referencing window at any point in time can now be saved as a text file.
this affects the output from the public code to read and interpret see5/c5.0 ruleset classifiers, and also impacts results with boosted rulesets.
as an added convenience, classifiers constructed using the sampling option are now automatically evaluated on a disjoint set of test cases.
- many-valued discrete attributes - see5/c5.0's handling of discrete attributes with numerous values is faster, and thesubsetting option uses less memory.
as an added convenience, classifiers constructed using the sampling option are now automatically evaluated on a disjoint set of test cases.
release 1.16 attribute winnowing when the number of attributes is large (in the hundreds, say) it becomes harder to distinguish predictive information from chance coincidences.
this application has seven classes (possible types of forest cover), and the cases are described in terms of 12 numeric and two multi-valued discrete attributes.
changes to results window (see5 only) viewing large results files should now be much faster; some minor format changes will also be apparent.
- confidence of ruleset predictions - calculation of the confidence of a ruleset prediction has been altered so that it more accurately reflects the ruleset's performance on unseen data.
in a customer retention application, for example, the importance of a case describing a customer might depend on the size of the customer's account.
the classifier construction settings last used with an application are stored and are reset whenever that application is selected again.
boosting is no longer disabled when this occurs, but a warning message is still printed.
- saving cross-referencing results (see5) - see5's cross-referencing facility is a powerful tool for finding the cases covered by particular components of classifiers, and parts of classifiers relevant to particular cases.
the use of costs with rulesets or boosted classifiers, however, can sometimes produce quite high error rates.
more efficient memory use for high-dimensional applications memory allocation has been improved for applications with thousands of attributes.
even without the winnowing option, the classifiers produced by release 1.16 may differ from those generated by earlier releases.
a new button on the toolbar allows the previous output to be redisplayed.
- improved error messages - problems with application files (.names, .data, .test, .costs etc) can be corrected more easily because the error message includes the line number of the file in question.
adaptation to microsoft bug-fix (network versions only) to improve security, microsoft windows updates have disabled a feature that is used by clients to read on-line help, as documentedhere.
the most serious bug in the initial release concerned the use of rulesets together with a costs file -- the system could sometimes associate an incorrect class with a rule, leading to abnormally high error rates.
these concern the display of implicitly-defined discrete attributes when the value is unknown, and the possible change of classifier settings when the "cross-reference" or "making predictions" windows are invoked immediately after a cross-validation.
a timestamp is rounded to the nearest minute and implicitly defined attributes can be used to compute functions of timestamps such as the number of minutes between two of them.
minor tweak boosting has been improved somewhat for large data files (more than four thousand records).
(thecross-reference facility in particular provides information that is not available from the command-line version.)
release 2.01 multi-threading the core of see5/c5.0 has been rewritten so that it can take advantage of computers with dual processors or intel pcs with hyper-threading technology.
the classifier construction settings last used with an application are stored and are reset whenever that application is selected again.
winnowing improved winnowing (pre-filtering the attributes) is now faster and somewhat more conservative.
the source code that facilitates such deployment has also changed substantially.
as a result, you may observe changes in the rulesets generated from your data.
release 2.02 - faster rule utility ordering - this option could be quite slow when large numbers of rules are involved.
for example, release 1.19 uses 143mb during processing of theforest application, less than half the 296mb needed by release 1.18.
release 2.04 attribute usage a new summary highlights the usage of attributes that appear in a classifier.
this enables a fixed percentage of the cases in adata file to be used for training.
furthermore, the remaining attributes that will be used to build classifiers are now ranked by importance, with an estimate of how predictive accuracy or misclassification cost would increase if individual attributes were removed.
- more efficient memory use for high-dimensional applications - memory allocation has been improved for applications with thousands of attributes.
the source code that facilitates such deployment has also changed substantially.
boosting is now noticeably faster and more resistant to noise in the data.
the problem could arise in applications with many attributes and a large number of missing values.
- false positive/false negative breakdown - see5/c5.0 currently shows a confusion matrix only when the number of classes does not exceed twenty.
muti-class applications that specify a costs file should now observe lower average misclassification costs for unseen cases, especially when rulesets are generated.
(the list of classes is still ok, so you don't have to change existing names files.)
warning: the new code will not work correctly with releases before 1.10, nor will release 1.10 work with the old source code.
the main window can be clicked on top of the output window.
in previous releases, use of the cross-reference facility in the see5 could cause problems when there were errors in thecases file.
furthermore, rules can be grouped into a number of bands, so that it is possible to see how the most important x% of rules perform on training and test data.
this option could be quite slow when large numbers of rules are involved.
- bug fix: pruning - a minor bug that could affect pruning of decision trees has been corrected.
- bug fix - in previous releases, use of the cross-reference facility in the see5 could cause problems when there were errors in the cases file.
this definition proved unsatisfactory because it depended on the order in which rule conditions were checked, and also because many attributes ended up having usage figures around 100%.
- a new button on the classifier construction dialog box allows any .costs file to be ignored.
- rules are found more quickly (quite a lot more quickly in some applications).
forest cover type data (forest, 581,012 cases, also from uci kdd archive).
boosting is no longer disabled when this occurs, but a warning message is still printed.
- census income data (income, 199,523 cases, obtained from uci kdd archive).
speed is the most obvious improvement, but the rulesets themselves are sometimes smaller.
- in the see5 gui, the "use classifier" button for interactive interpretation would not work with ruleset classifiers.
bug fix (windows only): interactive interpreter previous releases could sometimes give incorrect results for boosted classifiers.
- public source code - output from the sample program is easier to read, and cases are identified by their label attribute (if this is defined).
for example, the following graphs compare the performance of 1.18 to the previous release (1.17) on three large datasets: - sleep stage scoring data (sleep, 105,908 cases, obtained frommlc++).
release 1.08 - new attribute type label - in some applications, each case has an identifying code or serial number; this information can be recorded in alabel attribute.
this option can lead to noticeably better predictive performance and is now recommended for applications with many continuous attributes.
this bug could cause see5/c5.0 to crash, or to give incomplete results with the cross-reference facility.
if there are more than twenty classes, when a confusion matrix would be too large, release 2.04 records the number of false positives and false negatives for each class.
improved error messages problems with application files (.names, .data, .test, .costs etc) can be corrected more easily because the error message includes the line number of the file in question.
the sampling option introduced in release 1.07 allows random train/test splits of an application's data to be generated automatically.
changes to the see5 gui there have been several improvements in line with suggestions made by users (and please keep them coming!): a new edit menu brings up the .names or .costs file in wordpad, making it easier to change these files.
the difference is particularly noticeable when the leaves involved have very little supporting data.
predictive performance are disregarded ("winnowed") and only the remaining attributes are used to construct decision trees or rulesets.
one noticeable consequence is that decision trees tend to be both smaller and more accurate when there are discrete attributes with many values.
this facility can help to identify problems in the training data, and can be very useful for understanding complex classifiers (such as boosted trees or rulesets).
this option invokes the wordpad editor on the output, allowing it to be printed, edited etc.
the releases perform similarly for 25,000 training cases, but 1.17's advantage increases with size -- at around 200,000 cases, 1.17 is almost twice as fast as 1.16.
false positive/false negative breakdown see5/c5.0 currently shows a confusion matrix only when the number of classes does not exceed twenty.
this mechanism is intended to make classification models more compact without impairing their predictive accuracy.
on the well-known census dataset, for example, release 1.17 produces a 159-leaf decision tree that is slightly more accurate than the 211-leaf tree output by release 1.16.
the problem could arise in applications with many attributes and a large number of missing values.
speed improvements both c5.0 and see5 are now faster, particularly for large datasets.
new unix options options are now provided to ignore any .costs files and to set the random number seed (so that runs with sampling etc.
furthermore, rules can be grouped into a number of bands, so that it is possible to see how the most important x% of rules perform on training and test data.
the attribute winnowing option attempts to identify unhelpful attributes and exclude them from classifiers.
this application has seven classes (possible types of forest cover), and the cases are described in terms of 12 numeric and two multi-valued discrete attributes.
these versions allow the use of more than 2gb of memory, as required by some extremely large data mining tasks.
release 1.06 - cross-reference facility (see5) - this has been extended to allow classifiers to be related to cases in .test and .cases files in addition to the .data file.
faster classification with rulesets the process for finding all the rules that are satisfied by a case has been enhanced.
the memory required to generate rules from large datasets has been significantly reduced.
for instance, if attribute price has numeric values, the class specifier would indicate four discrete classes: price less than or equal to 100 price greater than 100 but less than or equal to 1000 price greater than 1000 but less than or equal to 5000 price greater than 5000.
release 1.12 - new data values - a new value n/a can be used when the value of an attribute is not applicable to a case.
with fuzzy thresholds, both branches of the tree are explored if the value ofa is close tot; the results are then combined to give a classification that changes more slowly with the value ofa. previous releases of see5/c5.0 had a fuzzy thresholds option, but these soft thresholds were used only in interactive classification.
rules are found more quickly (quite a lot more quickly in some applications).
release 1.12 new data values a new value n/a can be used when the value of an attribute is not applicable to a case.
confidence of ruleset predictions calculation of the confidence of a ruleset prediction has been altered so that it more accurately reflects the ruleset's performance on unseen data.
release 2.03 introduces an optionalcase weight attribute with numeric values; the effect is to bias the development of a classifier to increase accuracy on more important cases.
viewing large results files should now be much faster; some minor format changes will also be apparent.
when (single or boosted) ruleset classifiers are used, a new option shows the rules that are applicable to each case.
a label attribute does not affect classification in any way, but its value is displayed where possible with information about the case such as error messages, cross-referencing results etc.
release 2.03 introduces an optionalcase weight attribute with numeric values; the effect is to bias the development of a classifier to increase accuracy on more important cases.
bug fix: pruning a minor bug that could affect pruning of decision trees has been corrected.
- finally, large rulesets can be interpreted more quickly -- an advantage when using the public c code to deploy applications.
enhanced multi-threading additional sections of see5/c5.0 have been multi-threaded.
- smaller trees for applications with multi-valued discrete attributes - the algorithms for discrete attributes have been further improved.
bug fixes two bugs have been fixed: a value of the "minimum cases" option greater than 25 could occasionally behave as if it was set to 25, and the highest possible threshold of a continuous attribute was sometimes overlooked.
a new winnowing option helps to overcome this problem by investigating the usefulness of all attributes before any classifier is constructed.
release 1.13 new data type times are read and written in the form hh:mm:ss .
many-valued discrete attributes see5/c5.0's handling of discrete attributes with numerous values is faster, and thesubsetting option uses less memory.
can install and use the 64-bit version, even if the server runs 32-bit windows.
- new distribution format for windows - see5 release 2.01 is distributed as a self-contained inno executable.
- improvements to the see5 gui - the output window is now more readable, and can be copied and printed directly (without having to switch to wordpad).
right from the very first release, see5/c5.0 has allowed variable costs to be associated with different types of classification error as describedhere.
- sample locking (see5) - the sampling option introduced in release 1.07 allows random train/test splits of an application's data to be generated automatically.
rulesets are constructed from half the data and tested on the remaining 52,954 cases to estimate their true error rate.
result window (windows version) the system menu for this window has been extended with the option switch to wordpad.
the results summary also incorporates costs directly.
these versions allow the use of more than 2gb of memory, as required by some extremely large data mining tasks.
this is particularly useful when discrete attributes have numerous values and is now much faster than in previous releases.
this enables a fixed percentage of the cases in adata file to be used for training.
to ease the changeover, see5/c5.0 and the new public code will still read model files (.tree and .rules) generated by release 1.11.
enhanced multi-threading release 2.04 will now use up to four processors and so will run faster on the new quad-core cpus and computers with two dual-core processors.
the output window is now more readable, and can be copied and printed directly (without having to switch to wordpad).
in some applications, each case has an identifying code or serial number; this information can be recorded in alabel attribute.
furthermore, the remaining attributes that will be used to build classifiers are now ranked by importance, with an estimate of how predictive accuracy or misclassification cost would increase if individual attributes were removed.
with fuzzy thresholds, both branches of the tree are explored if the value ofa is close tot; the results are then combined to give a classification that changes more slowly with the value ofa. previous releases of see5/c5.0 had a fuzzy thresholds option, but these soft thresholds were used only in interactive classification.
this could (very rarely) lead to different classifiers when the training data were reordered, or when see5/c5.0 was run on computers with multiple cpus.
for example, the time to generate a ruleset for one dataset with 750,000 cases was reduced by more than 10%.
the 32-bit release of see5 will run under either 32-bit or 64-bit windows, so there is no need to change unless your tasks may use more than 2gb of memory.
from this release they have been changed to ascii files, so that models generated on one machine type may be deployed on machines of another type.
- alternative ordering for rulesets - a new option allows rulesets to be ordered by utility, from most important to least important for classification accuracy.
this option invokes the wordpad editor on the output, allowing it to be printed, edited etc.
selecting tests recent releases used a subset of the training data to eliminate some possible tests from consideration.
one noticeable consequence is that decision trees tend to be both smaller and more accurate when there are discrete attributes with many values.
for applications with a .costs file, the unix cross-validation script now reports average misclassification costs.
the results summary also incorporates costs directly.
as before, half of the data -- 290,506 cases -- are used for training and the remainder for testing.
for instance, if attributeprice has numeric values, the class specifier price: 100, 1000, 5000.
this is particularly useful when discrete attributes have numerous values and is now much faster than in previous releases.
every case in this monitoring application is described by 6 numeric-valued attributes and belongs to one of six classes.
bug fix: rulesets and global pruning this bug affected only ruleset classifiers generated with global pruning disabled -- the resulting rulesets might have been over-simplified.
release 2.01 - multi-threading - the core of see5/c5.0 has been rewritten so that it can take advantage of computers with dual processors or intel pcs with hyper-threading technology.
a client pc running 64-bit windows xp/vista/7 can install and use the 64-bit version, even if the server runs 32-bit windows.
- changes to .tree and .rules files - up to release 1.11 decision tree and ruleset models have been stored as binary files.
the use of costs with rulesets or boosted classifiers, however, can sometimes produce quite high error rates.
this can result in speed improvements when the application has many discrete attributes, especially with the discrete value subset option.
a new winnowing option helps to overcome this problem by investigating the usefulness of all attributes before any classifier is constructed.
speed is the most obvious improvement, but the rulesets themselves are sometimes smaller.
the 64-bit version of see5 will run only under 64-bit windows xp, windows vista, or windows 7.
(for the technically minded, the results window no longer uses the rich edit control.)
even without the winnowing option, the classifiers produced by release 1.16 may differ from those generated by earlier releases.
bug fix: case weight attributes and cost files a bug in release 2.04 could cause problems for applications with two classes that used both a case weight attribute and a .costs file.
this can result in speed improvements when the application has many discrete attributes, especially with the discrete value subset option.
most of these changes will be invisible to the user, but some (such as the method for selecting subsets of discrete values) may cause classifiers to differ from those produced by release 1.10.
this invokes a novel method of cross-referencing classifiers and the data from which they were constructed.
for example, the following graphs compare the performance of 1.18 to the previous release (1.17) on three large datasets: sleep stage scoring data (sleep, 105,908 cases, obtained frommlc++).
- bug fix: case weight attributes and cost files - a bug in release 2.04 could cause problems for applications with two classes that used both a case weight attribute and a .costs file.
- bug fix: rulesets and global pruning - this bug affected only ruleset classifiers generated with global pruning disabled -- the resulting rulesets might have been over-simplified.
this option can lead to noticeably better predictive performance and is now recommended for applications with many continuous attributes.
- bug fixes - two bugs have been fixed: - a value of the "minimum cases" option greater than 25 could occasionally behave as if it was set to 25, and - the highest possible threshold of a continuous attribute was sometimes overlooked.
smaller trees for applications with multi-valued discrete attributes the algorithms for discrete attributes have been further improved.
- attribute winnowing - when the number of attributes is large (in the hundreds, say) it becomes harder to distinguish predictive information from chance coincidences.
release 1.20 has been modified so that classifiers of these kinds often have lower error rates without a noticeable increase in misclassification costs.
the client installation program has been modified to set appropriate registry entries on the client, and also leaves a local copy of the help (see5help.chm) in the see5 folder as a workaround in case new windows updates affect htmlhelp.
new class type: thresholded continuous attribute this convenience feature now allows classes to be defined as subranges of a continuous attribute.
- improved scalability - for larger datasets, release 1.17 uses internal sampling to evaluate alternative splitting tests.
the 64-bit version of see5 will run only under 64-bit windows xp, windows vista, or windows 7.
the client installation program has been modified to set appropriate registry entries on the client, and also leaves a local copy of the help (see5help.chm) in the see5 folder as a workaround in case new windows updates affect htmlhelp.
release 1.20 has been modified so that classifiers of these kinds often have lower error rates without a noticeable increase in misclassification costs.
release 1.09 - confidence values for decision trees - when a case is classified by a decision tree, the calculation of the classification confidence has been changed slightly.
other changes and bug fixes there have been minor modifications to the way soft thresholds for decision trees are found.
the releases perform similarly for 25,000 training cases, but 1.17's advantage increases with size -- at around 200,000 cases, 1.17 is almost twice as fast as 1.16.
a timestamp is rounded to the nearest minute and implicitly defined attributes can be used to compute functions of timestamps such as the number of minutes between two of them.
when (single or boosted) ruleset classifiers are used, a new option shows the rules that are applicable to each case.
for example, release 1.19 uses 143mb during processing of theforest application, less than half the 296mb needed by release 1.18.
this shows, for each attribute, the percentage of training cases for which the value of that attribute is both known and also used in classifying the case.
release 1.16 the windows gui has also been improved slightly and the public code has a new option governing output format.
see5/c5.0 now uses a modified test selection strategy when the training data contains thousands of cases or more.
the linux gui calls the native linux c5.0, so there is no performance penalty.
warning: the new code will not work correctly with releases before 1.10, nor will release 1.10 work with the old source code.
for example, a test that separates one class from another in a small subset of the training cases might be genuinely interesting; on the other hand, if hundreds of alternatives have been tried, it is more likely that a similar separation can be found even when none of the tests is helpful for prediction.
(the list of classes is still ok, so you don't have to change existing names files.)
to ease the changeover, see5/c5.0 and the new public code will still read model files (.tree and .rules) generated by release 1.11.
in a customer retention application, for example, the importance of a case describing a customer might depend on the size of the customer's account.
(this bug only trivially affected the output of see5/c5.0 and had no noticeable impact on the trees' predictive accuracy.)
the data are divided into a training set of 99,762 cases and a test set of 99,761.
cross-reference facility (windows version) see5 now has a new button that looks like a large `x'.
as a result, you may observe changes in the rulesets generated from your data.
boosting is now noticeably faster and more resistant to noise in the data.
- for thresholded class attributes (see below), a value of "?" for the continuous target attribute in a .cases file caused problems.
the linux gui calls the native linux c5.0, so there is no performance penalty.
- bug fix (windows only): interactive interpreter - previous releases could sometimes give incorrect results for boosted classifiers.
- forest cover type data (forest, 581,012 cases, also from uci kdd archive).
simplified output for rules when the "rules" option is selected, the output now omits information about decision trees.
these concern the display of implicitly-defined discrete attributes when the value is unknown, and the possible change of classifier settings when the "cross-reference" or "making predictions" windows are invoked immediately after a cross-validation.
the data are divided into a training set of 99,762 cases and a test set of 99,761.
release 2.06 new algorithm for softening thresholds see5/c5.0 decision trees have an option to soften threshold tests for continuous attributes; values near the threshold cause both the low and high branches to be evaluated and combined probabilistically.
release 1.20a fixed three bugs in 1.20: in the see5 gui, the "use classifier" button for interactive interpretation would not work with ruleset classifiers.
this mechanism is intended to make classification models more compact without impairing their predictive accuracy.
the information in the cross-referencing window at any point in time can now be saved as a text file.
batch-mode version of see5 guis are great, but it's sometimes useful to be able to run see5
the 32-bit release of see5 will run under either 32-bit or 64-bit windows, so there is no need to change unless your tasks may use more than 2gb of memory.
finally, large rulesets can be interpreted more quickly -- an advantage when using the public c code to deploy applications.
bug fixes two bugs have been fixed that (sometimes) cause problems with combinations of options: use of the discrete value subsetting option together with the rulesets option for applications with ordered discrete attributes; and use of the attribute winnowing option together with the sampling option and samples greater than 50% (windows only).
as before, half of the data -- 290,506 cases -- are used for training and the remainder for testing.
release 1.05 rulesets the method for generating and selecting rules has been `tweaked' slightly to improve performance on some datasets.
release 1.10 attributes defined by formulas it is sometimes convenient to define the value of an attribute as a function of other attribute values rather than by giving the value explicitly in .data files.
most of these changes will be invisible to the user, but some (such as the method for selecting subsets of discrete values) may cause classifiers to differ from those produced by release 1.10.
- winnowing improved - winnowing (pre-filtering the attributes) is now faster and somewhat more conservative.
right from the very first release, see5/c5.0 has allowed variable costs to be associated with different types of classification error as describedhere.
- batch-mode version of see5 - guis are great, but it's sometimes useful to be able to run see5 non-interactively from a ms-dos command window.

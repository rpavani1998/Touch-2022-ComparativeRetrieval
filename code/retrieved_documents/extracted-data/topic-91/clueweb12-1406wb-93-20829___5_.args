a weak learning algorithm is one that is only somewhat better than random guessing in terms of error rates (i.e., the error rate is just below 50%).the result is then a panel of models used to make a decision on new data by combining the ``expertise'' of each model in such a way that the more accurate experts carry more weight.the key is the use of a weak learning algorithm--essentially any weak learner can be used.as a meta learner boosting employs some other simple learning algorithm to build the models.this model building followed by boosting is repeated until the specific generated model performs no better than random.the algorithm is quite simple, beginning by building an initial model from the training dataset.an example might be decision trees of depth 1 (i.e., decision stumps).the roc curve other examples 10 fold cross validation area under curve calibration curves reporting generating open document format getting started with odfweave openoffice.org macro support generating html generating pdf with latex configuration figure sizes fraud analysis archetype analysis overview boosting builds a collection of models using a ``weak learner'' and thereby reduces misclassification error, bias, and variance (, ,).a new model is then built with these boosted entities, which we might think of as the problematic entities in the training dataset.formatted output automatically generate filenames reading a large file manipulating data manipulating data as sql using sqlite odbc data database connection excel access clipboard data spatial data simple mapthose entites in the training data which the model was unable to capture (i.e., the model mis-classifies those entites) have their weights boosted.clustered box plot further resources map displays further resources preparing data data selection and extraction training and test datasets data cleaning review data selectively changing vector values replace indices by names missing values remove levels from a factor variable manipulations remove columns reorder columns remove non-numeric columns remove variables with no variance cleaning the wine dataset cleaning the cardiac dataset cleaning the survey dataset imputation nearest neighboursreverse a list sorting unique values loading data interactive responses interactive data entry available datasetsthe pdf version is a formatted comprehensive draft book (with over 800 pages).further information summary overview example algorithm resources and further reading bagging support vector machine formalities tutorial example tuning parameters examples resources and further reading overview examples resources and further reading linear regression
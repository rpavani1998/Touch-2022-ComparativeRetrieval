results show that the clustered datasets are on average fifty percent smaller than the original datasets without loss of classification accuracy which is significantly better than random selection.while this has created sophisticated classification algorithms, many do not cope with increasing data set sizes.their ingenuity stems from updating sufficient statistics, only addressing growth when decisions can be made that are guaranteed to be almost identical to those that would be made by conventional batch learning methods.the ensemble method investigation shows that combining trees can be worthwhile, but only when sufficient memory is available, and improvement is less likely than in traditional machine learning.machine learning offers promise of a solution, but the field mainly focusses on achieving high accuracy when data supply is limited.our experiments demonstrate that both methods are faster, find smaller subsets and can even increase the classification accuracy.moreover, instead of assigning semantic roles one at a time, an algorithm is proposed to assign all labels simultaneously; leveraging dependencies between roles and eliminating the problem of duplicate assignment.unfortunately these methods are computationally expensive, quadratic in the number of data points in fact, so cannot be applied directly.when the data set sizes get to a point where they could be considered to represent a continuous supply, or data stream, then incremental classification algorithms are required.semi- supervised learning tries to exploit this abundance of unlabeled training data to improve classification.to measure improvement, a comprehensive framework for evaluating the performance of data stream algorithms is developed.the base classifiers are then learned using features extracted from these randomly transformed versions of the training data, and the result is a highly diverse ensemble of image classifiers.existing systems for semantic role labeling use machine learning methods to assign roles one-at-a-time to candidate arguments.clusters generated without class information usually agree well with the true class labels of cluster members, i.e. class distributions inside clusters generally differ significantly from the global class distributions.to find patterns in these datasets it would be useful to be able to apply modern methods of classification such as support vector machines.
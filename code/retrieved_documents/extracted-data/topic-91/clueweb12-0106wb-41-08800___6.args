this paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in roc space) than varying the loss ratios in ripper or class priors in naive bayes.
hence, bagging with samples the size of the data is impractical.
the use of distributed disjoint partitions in learning is significantly less complex and faster than bagging.
experiments with decision tree and neural network classifiers on various datasets show that, given the same size partitions and bags, disjoint partitions result in performance equivalent to, or better than, bootstrap aggregates (bags).
the random subspaces approach has the added advantage of requiring less careful tweaking.
an empirical evaluation using five real data sets confirms the validity of our approach compared to some other combination of multiple classifiers algorithms.
finally, the methods proposed for thinning again show that ensembles can be made smaller without loss in accuracy.
experimental results are presented using the largest database employed to date in 3d face recognition studies, over 4,000 scans of 449 subjects. ...
this work shows that a random subspaces ensemble can outperform a well-tuned single classifier for a typical 2-d face recognition problem.
rank-one recognition rates of 97.2% and verification rates of 93.2% at 0.1% false accept rate are reported and compared to other methods published on the face recognition grand challenge v2 data set.
experiments on the forest cover data set show that this parallel mixture is more accurate than a single svm, with 90.72% accuracy reported on an independent test set.
in this comment, we show that a simple ensemble of decision trees results in a higher accuracy, 94.75%, and is computationally efficient.
in addition, we also compare the random subspace methodology to an ensemble of subsamples of image data.
we describe an ensemble learning approach that accurately learns from data which has been partitioned according to the arbitrary spatial requirements of a large-scale simulation wherein classifiers may be trained only the data local to a given partition.
as a result, the class statistics can vary from partition to partition; some classes may even be missing from some partitions.
our results indicate that, in such applications, the simple approach of creating a committee of n classifiers from disjoint partitions each of size 1/n (which will be memory resident during learning) in a distributed way results in a classifier which has a bagging-like performance gain.
this paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in roc space) than only under-sampling the majority class.
the main goal of the paper is to see if the random subspace methodology can do as well, if not better, than the single classifier constructed on the tuned face space.
we introduce a new system for 3d face recognition based on the fusion of results from a committee of regions that have been independently matched. ...
bagging forms a committee of classifiers by bootstrap aggregation of training sets from a pool of training data.
boosting lite - handling larger datasets and slower base classifiers, lawrence o. hall, robert e. banfield, kevin w. bowyer and w. philip kegelmeyer.
it has been shown, in different domains, that random subspaces combined with weak classifiers such as decision trees and nearest neighbor classifiers can provide an improvement in accuracy.
we also construct ensembles of classifiers learned from such actively sampled image sets, which further provides improvement in the recognition rates. ...
we evaluate thinning algorithms on ensembles created by several techniques on 22 publicly available datasets.
rank-one recognition rates of 97.2% and verification rates of 93.2% at 0.1% false accept rate are reported and compared to other methods published on the face recognition grand challenge v2 data set.
the random subspaces approach has the added advantage of requiring less careful tweaking.
when compared to other methods, our percentage correct diversity measure algorithm shows a greater correlation between the increase in voted ensemble accuracy and the diversity value. ...
it is based on combining the match scores from matching multiple overlapping regions around the nose.
finally, the methods proposed for thinning again show that ensembles can be made smaller without loss in accuracy.
the main goal of the paper is to see if the random subspace methodology can do as well, if not better, than the single classifier constructed on the tuned face space.
this result is somewhat surprising and illustrates the general value of experimental comparisons using different types of classifiers.
as a result, the class statistics can vary from partition to partition; some classes may even be missing from some partitions.
while this accuracy is impressive, the referenced paper does not consider alternative types of classifiers.
statistical tests were performed on experimental results from 57 publicly available data sets. ...
we evaluate thinning algorithms on ensembles created by several techniques on 22 publicly available datasets.
it is based on combining the match scores from matching multiple overlapping regions around the nose.
experimental results are presented using the largest database employed to date in 3d face recognition studies, over 4,000 scans of 449 subjects. ...
we describe an ensemble learning approach that accurately learns from data which has been partitioned according to the arbitrary spatial requirements of a large-scale simulation wherein classifiers may be trained only the data local to a given partition.
voting many classifiers built on small subsets of data is a promising approach for learning from massive data sets, one that can utilize the power of boosting and bagging.
when compared to other methods, our percentage correct diversity measure algorithm shows a greater correlation between the increase in voted ensemble accuracy and the diversity value. ...
statistical tests were performed on experimental results from 57 publicly available data sets. ...
we also construct ensembles of classifiers learned from such actively sampled image sets, which further provides improvement in the recognition rates. ...
we examine ensemble algorithms (boosting lite and ivoting) that provide accuracy approximating a single classifier, but which require significantly fewer training examples. ...
our method of over-sampling the minority class involves creating synthetic minority class examples.
it has been shown, in different domains, that random subspaces combined with weak classifiers such as decision trees and nearest neighbor classifiers can provide an improvement in accuracy.
we introduce a new system for 3d face recognition based on the fusion of results from a committee of regions that have been independently matched. ...
this work shows that a random subspaces ensemble can outperform a well-tuned single classifier for a typical 2-d face recognition problem.
we examine ensemble algorithms (boosting lite and ivoting) that provide accuracy approximating a single classifier, but which require significantly fewer training examples. ...
in addition, we also compare the random subspace methodology to an ensemble of subsamples of image data.
experiments show this approach is fast, accurate, and scalable.
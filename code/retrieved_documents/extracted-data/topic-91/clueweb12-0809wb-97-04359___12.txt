these model the point distances to cluster centroids in the transformed space.
we also assessed the degree to which real gene expression  data fits the multivariate gaussian mixture assumptions on several common data  transformations.
in fields  such as bioinformatics (e.g. microarray gene expression data, proteomic  sequencing) and text mining (document clustering), clustering high-dimensional  data sets is a common problem.
one main advantage of our approach is the ability to automatically select the number of clusters present in the document collection via bayes factors.
we also assessed the degree to which real gene expression data fits the multivariate gaussian mixture assumptions on several common data transformations.
human brain mapping, 29  (2008) 422-440.
furthermore, these results also suggested that there is a trade-off between weak dependence and expected margins: to compensate for low expected margins, there should be low mutual dependence among the classifiers making up the combination.
in it we benchmarked the performance of model-based clustering on several synthetic and real gene expression data sets for which external validation were available.
murua, a., stanberry, l. and stuetzle, w., (2008)
a key advantage of model-based refractionation over other competing clustering of large data sets approaches is that it does not require the number of clusters be known a priori.
in contrast to previous work, our conditional-potts clustering model introduces a complete principled way to do clustering.
in the absence of such techniques, efficient and reliable clustering methods whose score functions are based on similarity or distance matrices are advantageous.
our penalized adaptive potts model clustering implicitly models the clusters densities via kernel density estimators (a by-product of using a mercer kernel within a potts model), and uses bayesian computational techniques (markov chain monte carlo) to estimate the clusters and their number, and other parameters of the model.
i showed that if both this  dependence is low and the expected margins are large, then decision rules based  on linear combinations of these classifiers can achieve error rates that  decrease exponentially fast.
this work also connects  kernel-based methods to non-parametric kernel density estimation.
murua, a. martin, d. (2005) "s+bayes" library.
the problem is of combinatorial nature.
this is  accomplished through the elucidation of an informative prior for the model  parameters, and also by the development of a wang-landau flat-histogram-like  stochastic algorithm to estimate the posterior of the parameters.
however our acoustic events were defined in terms of labeled graphs and hence  inherited a partial ordering.
amit, y. and murua, a. (2001), "speech recognition using randomized relational decision trees," ieee transactions on speech and audio processing, 9, 333-341.. in this work we recognized speech signals using a large collection of coarse acoustic events which describe temporal relations between a few local cues in the spectrogram.
this latter procedure proved successful even when dealing with di cult situations comprising sets with large numbers of small clusters.
this research is partly based on the belief that the key to the success of  modeling complex data structures lies in the embedding of kernel-based methods  within parametric and bayesian models.
we called this procedure model-based fractionation.
among them are linear, poisson and binomial models.
the main idea is to reduce the  data size by working with meta-data form by clusters of data points.
our work has been presented at several conferences all over  the world.
international journal of statistics and tomography, 8 (2008) 1-25.
however to the best of our knowledge kernels have not been used in  mixture models.
kernels are now everywhere present in statistics as far as a dot product is  at hand.
but our approach has the advantage of suggesting the number of  clusters (through the bayesian information criterion bic) as well as an  appropriate model.
in contrast to previous work, our conditional-potts  clustering model introduces a complete principled way to do clustering.
in particular, the link between the potts and the random cluster model models allows us to elucidate, using random graph theory, a very informative prior for the temperature parameter.
our work has been presented at several conferences all over the world.
each meta-data is formed recursively and carries the information contained in the empirical moments of the associated cluster.
this work was  presented as an invited talk at the world congress of the bernoulli society.
the main idea is to reduce the data size by working with meta-data form by clusters of data points.
it uses these two findings to present a powerful kernel-based clustering methodology based on potts model, kernel density estimation, and consensus clustering.
each  meta-data is formed recursively and carries the information contained in the  empirical moments of the associated cluster.
murua alejandro research the goal of every clustering (or unsupervised classification) method is to  estimate the "true" clustering structure or partition underlying the  feature space.
kernel methods and their extensions such as kernel k-means, multiway normalized cut (mncut), and, recently, adaptive potts model clustering use the similarity matrix information in an effective way, often yielding good results.
we also reviewed fractionation, an idea originally conceived by cutting, karger, pedersen and tukey (1992) for non-parametric hierarchical clustering of large data sets, and described an adaptation of it that extends model-based clustering to large data sets.
some examples of key applications guiding this research are finding groups  of genes associated to certain diseases (e.g. leukemia or lung cancer) in order  to devise effective individualized treatments; discovering the di erent  developmental stages of sensory organs (e.g. vision); finding the role of  proteins through association with proteins whose functions are known; finding  relevant association between drugs and potential adverse effects; and  discovering neuronal firing patterns in the brain using fmri images.
human brain mapping, 29 (2008)
doug martin and i have conceived, developed and implemented several  bayesian hierarchical generalized linear mixed models.
international journal of  statistics and tomography, 8 (2008)
this estimate is usually unveiled by optimizing a score function that gives a figure-of-merit on admissible partitions of the data.
tantrum, j., murua, a. and stuetzle, w. (2004), "hierarchical  model-based clustering of large datasets through fractionation and  refractionation," information systems, 29, 315-326.
nested within our model are the two special cases of a  mixture of exponentials and the kernel k-means method.
this latter  procedure proved successful even when dealing with di cult situations  comprising sets with large numbers of small clusters.
our method is based on  mixtures of gamma distributions.
several experiments showed that the three common methods used to construct multiple classifiers from the same training data -bagging, boosting and randomized trees (random forests)- produce mutually weakly dependent trees.
we showed that suitable chosen transformations can greatly  enhance normality in gene expression data, and that models have varying  performance on data sets that are transformed di erently.
within the same framework it reveals (and re-introduces) the potts model as one of the simplest to conceive kernel-based clustering method.
the goal is to facilitate bayesian inference techniques to a larger audience than just statisticians in academia.
in this work i introduced a useful notion of weak dependence between many  classifiers built using the same training data.
murua alejandro research the goal of every clustering (or unsupervised classification) method is to estimate the "true" clustering structure or partition underlying the feature space.
however our acoustic events were defined in terms of labeled graphs and hence inherited a partial ordering.
we showed that suitable chosen transformations can greatly enhance normality in gene expression data, and that models have varying performance on data sets that are transformed di erently.
selected research contributions .
murua, a., and wicker, n., "kernel-based mixture models for  classification," submitted for publication.
this result has been cited in the engineering and  pattern recognition literature as m "murua's theory.
stanberry, l., murua, a. and cordes, d., "functional connectivity  mapping using the ferromagnetic potts spin model."
the fruit of this work has been compiled in the  so-called s+bayes software library (written in c++ with graphical user  interface in s-plus).
the fruit of this work has been compiled in the so-called s+bayes software library (written in c++ with graphical user interface in s-plus).
in this work i introduced a useful notion of weak dependence between many classifiers built using the same training data.
the model arises as an embedding of the potts density for label membership probabilities into an extended bayesian model for joint data and label membership probabilities.
sound machine learning and statistical methods for variable selection and/or data reduction based on the intrinsic structure of the data are needed.
but our approach has the advantage of suggesting the number of clusters (through the bayesian information criterion bic) as well as an appropriate model.
the goal of my current research is to develop sound machine learning and probabilistic methods to find (1) intrinsic structure (clustering), and (2) patterns (variable selection or functionals that intrinsically describe and interpret the data in few dimensions), in mainly complex and/or high-dimensional data and large datasets.
we showed that on real gene expression data model-based clustering performed as well as leading heuristic clustering methods.
it also performed much better than hmm in our experiments.
note that one of the main di culties with genome  data is the usually small sample size together with the high-dimensionality of  the data.
murua, a., and wicker, n., (2010)
nested within our model are the two special cases of a mixture of exponentials and the kernel k-means method.
we showed that on real gene expression data  model-based clustering performed as well as leading heuristic clustering  methods.
this allowed us to use multiple randomized trees  to access the large pool of acoustic events in a systematic manner.
tantrum, j., murua, a. and stuetzle, w. (2003), "assessment and  pruning of hierarchical model-based clustering," proceedings of the 9th  international conference on knowledge discovery and data mining (kdd03), 197-205 .
murua, a. (2002), "upper bounds for error rates associated to linear combination of classifiers, "ieee transactions on pattern analysis and machine intelligence, 9, 591-602.
the goal is to facilitate bayesian inference techniques  to a larger audience than just statisticians in academia.
in the absence of such techniques, efficient and reliable clustering methods  whose score functions are based on similarity or distance matrices are  advantageous.
murua, a. (2002), "upper bounds for error rates associated to  linear combination of classifiers, "ieee transactions on pattern analysis  and machine intelligence, 9, 591-602.
in the present work we show that they can be useful for classification purposes.
yeung, k.y., fraley, c., murua, a., raftery, a. and ruzzo, l. (2001),  "model-based clustering and data transformations for gene expression  data," bioinformatics, 17, 977-987.
within the  same framework it reveals (and re-introduces) the potts model as one of the  simplest to conceive kernel-based clustering method.
we also reviewed fractionation, an idea  originally conceived by cutting, karger, pedersen and tukey (1992) for  non-parametric hierarchical clustering of large data sets, and described an  adaptation of it that extends model-based clustering to large data sets.
"the conditional-potts clustering model," submitted for publication.
this is a pioneering work on exploratory analysis of gene-expression  microarray data.
sound machine learning and statistical methods for variable selection  and/or data reduction based on the intrinsic structure of the data are needed.
this work also connects kernel-based methods to non-parametric kernel density estimation.
the library is freely available from insightful corporation.
i showed that if both this dependence is low and the expected margins are large, then decision rules based on linear combinations of these classifiers can achieve error rates that decrease exponentially fast.
we also conceived a further extension of fractionation, called model-based refractionation.
we suggest using the  log-likelihood ratio or the bayesian information criterion to select an  appropriate parsimonious model for the data at hand.
a key advantage of  model-based refractionation over other competing clustering of large data sets  approaches is that it does not require the number of clusters be known a  priori.
we suggest using the log-likelihood ratio or the bayesian information criterion to select an appropriate parsimonious model for the data at hand.
in fields such as bioinformatics (e.g. microarray gene expression data, proteomic sequencing) and text mining (document clustering), clustering high-dimensional data sets is a common problem.
the trees  were aggregated to produce a classifier.
among them are linear,  poisson and binomial models.
our penalized adaptive potts model clustering implicitly  models the clusters densities via kernel density estimators (a by-product of  using a mercer kernel within a potts model), and uses bayesian computational  techniques (markov chain monte carlo) to estimate the clusters and their  number, and other parameters of the model.
this explicitly models the data as being drawn from a gaussian mixture.
in the present work we show that they can be useful for  classification purposes.
a comparison with other  popular classification methods such as support vector machines, shows that our  method is very competitive and computationally efficient.
we were later on invited to submit our extended work to the information systems journal.
several experiments showed that the three common  methods used to construct multiple classifiers from the same training data  -bagging, boosting and randomized trees (random forests)- produce mutually  weakly dependent trees.
murua, a., stanberry, l. and stuetzle, w., (2008) "on potts model  clustering, kernel k-means and density estimation," journal of  computational & graphical statistics, 17 (2008), 629-658.
doug martin and i have conceived, developed and implemented several bayesian hierarchical generalized linear mixed models.
they o er a flexibility in the modeling process through the kernel trick which enables capturing interesting features in some cases more easily than just using the standard methods.
for example, the bayesian parametric paradigm gives a framework for introducing structural constraints, whereas the kernel based methods introduces flexibility (e.g. local, spatial and/or temporal adaptation).
these model the point distances to cluster  centroids in the transformed space.
this work was presented as an invited talk at the world congress of the bernoulli society.
this paper has been  identified by thomson-isi as one of the most cited papers in the research area  of gene expression data, and it continues to be cited in most journals that  publish work on bioinformatics.
murua, a., stuetzle, w., tantrum, j. and sieberts, s., "model based document classification and clustering."
this result has been cited in the engineering and pattern recognition literature as m "murua's theory.
the trees were aggregated to produce a classifier.
they o er a flexibility in the modeling process  through the kernel trick which enables capturing interesting features in some  cases more easily than just using the standard methods.
we showed that the learning stage of this procedure is much more efficient than that for hidden markov models (hmm).
we were later on  invited to submit our extended work to the information systems journal.
the goal of my current research is to develop sound machine learning and  probabilistic methods to find (1) intrinsic structure (clustering), and (2)  patterns (variable selection or functionals that intrinsically describe and  interpret the data in few dimensions), in mainly complex and/or  high-dimensional data and large datasets.
part of this work was first presented at the 2002 international  conference on data mining and knowledge discovery (kdd’02) in edmonton,  canada, where it was selected as one of the best papers.
one main advantage of our approach is  the ability to automatically select the number of clusters present in the  document collection via bayes factors.
however to the best of our knowledge kernels have not been used in mixture models.
this explicitly models the data  as being drawn from a gaussian mixture.
part of this work was first presented at the 2002 international conference on data mining and knowledge discovery (kdd’02) in edmonton, canada, where it was selected as one of the best papers.
some examples of key applications guiding this research are finding groups of genes associated to certain diseases (e.g. leukemia or lung cancer) in order to devise effective individualized treatments; discovering the di erent developmental stages of sensory organs (e.g. vision); finding the role of proteins through association with proteins whose functions are known; finding relevant association between drugs and potential adverse effects; and discovering neuronal firing patterns in the brain using fmri images.
amit, y. and murua, a. (2001), "speech recognition using randomized  relational decision trees," ieee transactions on speech and audio  processing, 9, 333-341.. in this work we recognized speech signals using a large collection of  coarse acoustic events which describe temporal relations between a few local  cues in the spectrogram.
the distances are readily computed using  the kernel trick.
"on potts model clustering, kernel k-means and density estimation," journal of computational & graphical statistics, 17 (2008), 629-658. stanberry, l., murua, a. and cordes, d., "functional connectivity mapping using the ferromagnetic potts spin model."
this work uncovers the link between several seemingly unconnected kernel-based methods (e.g. kernel k-means, multiway normalized cut).
this is accomplished through the elucidation of an informative prior for the model parameters, and also by the development of a wang-landau flat-histogram-like stochastic algorithm to estimate the posterior of the parameters.
note that one of the main di culties with genome data is the usually small sample size together with the high-dimensionality of the data.
tantrum, j., murua, a. and stuetzle, w. (2003), "assessment and pruning of hierarchical model-based clustering," proceedings of the 9th international conference on knowledge discovery and data mining (kdd03), 197-205 .
tantrum, j., murua, a. and stuetzle, w. (2004), "hierarchical model-based clustering of large datasets through fractionation and refractionation," information systems, 29, 315-326.
for example, the bayesian parametric  paradigm gives a framework for introducing structural constraints, whereas the  kernel based methods introduces flexibility (e.g. local, spatial and/or  temporal adaptation).
the invariance to changes in duration of speech signal  events was addressed by defining temporal relations in a rather coarse manner,  allowing a large degree of slack.
the distances are readily computed using the kernel trick.
a comparison with other popular classification methods such as support vector machines, shows that our method is very competitive and computationally efficient.
in this work we develop a complete methodology for document classification and clustering based on model-based clustering.
the library is freely  available from insightful corporation.
the second  paper presents an important application to the exploration of brain  connectivity via our adaptive potts model clustering methodology described in  the first paper.
we  called this procedure model-based fractionation.
kernel methods and their extensions such as kernel k-means,  multiway normalized cut (mncut), and, recently, adaptive potts model clustering  use the similarity matrix information in an effective way, often yielding good  results.
yeung, k.y., fraley, c., murua, a., raftery, a. and ruzzo, l. (2001), "model-based clustering and data transformations for gene expression data," bioinformatics, 17, 977-987.
the invariance to changes in duration of speech signal events was addressed by defining temporal relations in a rather coarse manner, allowing a large degree of slack.
it uses these  two findings to present a powerful kernel-based clustering methodology based on  potts model, kernel density estimation, and consensus clustering.
"the conditional-potts clustering  model," submitted for publication.
this research is partly based on the belief that the key to the success of modeling complex data structures lies in the embedding of kernel-based methods within parametric and bayesian models.
this work uncovers the link between several seemingly unconnected  kernel-based methods (e.g. kernel k-means, multiway normalized cut).
in this work we develop a complete methodology for document classification  and clustering based on model-based clustering.
murua, a., stuetzle, w., tantrum, j. and sieberts, s., "model based  document classification and clustering."
this allowed us to use multiple randomized trees to access the large pool of acoustic events in a systematic manner.
this paper has been identified by thomson-isi as one of the most cited papers in the research area of gene expression data, and it continues to be cited in most journals that publish work on bioinformatics.
the second paper presents an important application to the exploration of brain connectivity via our adaptive potts model clustering methodology described in the first paper.
in it we benchmarked the performance of model-based clustering  on several synthetic and real gene expression data sets for which external  validation were available.
kernels are now everywhere present in statistics as far as a dot product is at hand.
we showed that the learning stage of  this procedure is much more efficient than that for hidden markov models (hmm).
furthermore, these results also suggested that there is  a trade-off between weak dependence and expected margins: to compensate for low  expected margins, there should be low mutual dependence among the classifiers  making up the combination.
murua, a., and wicker, n., "kernel-based mixture models for classification," submitted for publication.
we also conceived a further  extension of fractionation, called model-based refractionation.
the model arises as an embedding of the potts density for label membership  probabilities into an extended bayesian model for joint data and label  membership probabilities.
our method is based on mixtures of gamma distributions.
this is a pioneering work on exploratory analysis of gene-expression microarray data.
in  particular, the link between the potts and the random cluster model models  allows us to elucidate, using random graph theory, a very informative prior for  the temperature parameter.
this estimate is usually unveiled by optimizing a score function  that gives a figure-of-merit on admissible partitions of the data.
the third paper presents a bayesian kernel-based clustering method.
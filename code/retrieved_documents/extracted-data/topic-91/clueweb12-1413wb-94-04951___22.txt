in some cases, a weighted sum of hypotheses expands the space of  functions that can be represented.
a second way to force diversity is to provide a different subset of the  input features in each call to the learning algorithm.
dietterich and bakiri (1995) report that this technique  improves the performance of both decision-tree and backpropagation learning  algorithms on a variety of difficult classification problems.
starting at the so-called root (i.e., top) of the tree, we first check whetherx2 =
in statistics, such ensembles are known asgeneralized additive models ( hastie and tibshirani, 1990).
dietterich and bakiri (1995) report that this technique improves the performance of both decision-tree and backpropagation learning algorithms on a variety of difficult classification problems.
in such cases, there may be several different hypotheses that all give  the same accuracy on the training data, and the learning algorithm must choose  one of these to output.
after each of the k classifiers has voted, the class with the highest number of votes is selected as the prediction of the ensemble.
the kth learned classifier attempts to predict bitk of these codewords (a prediction of −1 is treated as a binary value of 0).
consider the words desk, think, and hook, where thek is pronounced, and the words back, quack, and knave, where the k is silent (in back and quack, we will suppose that thec is responsible for the k sound).
a contrasting view of an ensemble is that it is an additive model, that is, it predicts the class of a new data point by taking a weighted sum of a set of component models.
a contrasting view of an ensemble is that it is an additive model,  that is, it predicts the class of a new data point by taking a weighted sum of  a set of component models.
this relabeled data is then given to the learning algorithm, which constructs a classifierhk.
the ensemble’s prediction is the classj whose codewordcj is closest (measured by the number of bits that agree) to the k-bit output string.
so  heuristic methods must be employed.
the resulting weighted error isr = ∑id(xi)yih k(xi), where hk(xi) is the label predicted by hypothesishk.
the best experimental results have been obtained with very large decision trees and neural networks.
the input data  can then be relabeled so that any of the original classes in setak are  given the derived label −1 and the original classes in setbk are  given the derived label +1.
recent experiments suggest that breiman’s combination of bagging and the random subspace method is the method of choice for decision trees: it gives excellent accuracy and works well even when there is substantial noise in the training data.
this work shows that the error of an ensemble on new data points is bounded by the fraction of training data points for which the margin is less than some quantity θ > 0 plus a term that grows as ignoring constant factors and some log terms.
−1 “leaf,” which predicts thatk will be silent.
each new hypothesis is constructed by a learning algorithm that seeks to minimize the classification error on aweighted training data set.
freund, y., and schapire, r. e., 1997, a decision-theoretic generalization of on-line learning and an application to boosting,j. comput.
the input data can then be relabeled so that any of the original classes in setak are given the derived label −1 and the original classes in setbk are given the derived label +1.
adaboost is probably the best method to apply, but favorable results have been  obtained just by training several networks from different random starting  weight values, and bagging is also quite effective.
the majority of research into ensemble methods has focused on constructing  ensembles of decision trees.
freund and schapire (1996) showed improved performance on 22 benchmark problems, equal performance on one problem, and worse performance on four problems.
consider the words desk, think, and hook,  where thek is pronounced, and the words back, quack, and knave, where the k is silent (in back and quack, we  will suppose that thec is responsible for the k sound).
the classification decision of the combined  classifierh is +1 if h(x) ≥ 0 and −1 otherwise.
then each of these words can be represented by the following data points:x 1 x 2 x 3 x 4 y e s _ _
if the base learning algorithm produces less expressive hypotheses than decision trees, then the adaboost method is recommended.
dietterich, t. g., 2000, an experimental comparison of three methods for  constructing ensembles of decision trees: bagging, boosting, and randomization, machine learn., 40:139–158.
one of the most efficient and widely applied learning algorithms searches  the hypothesis space consisting of decision trees.
experimental measurements of bias and variance have confirmed this.
many experiments have employed so-called decision stumps, which are decision trees with only one internal node.
the resulting  weighted error isr = ∑id(xi)yih k(xi), where hk(xi) is the label  predicted by hypothesishk.
as an example, consider the problem of learning to pronounce the letter k in english.
it works  by incrementally adding one hypothesis at a time to an ensemble.
the 32 networks were based on eight different subsets of  the 119 available input features and four different network sizes.
dietterich, t. g., 1997, machine learning research: four current directions, ai magazine, 18:97–136.
the input feature subsets were selected (by hand) to group together features that were basedon different image processing operations (such as principal component analysis and the fast fourier transform).
the primary exception are data sets in which there is a high level of mislabeled training data points.
why ensemble methods work learning algorithms that output only a single hypothesis suffer from three  problems that can be partly overcome by ensemble methods: the statistical  problem, the computational problem, and the representation problem.
a simple vote of all of these equally good classifiers can reduce this risk.
in addition, the base learning algorithm must be sensitive to the encoding of the output values.
ensemble learning algorithms work by running a base learning algorithm multiple times, and forming a vote out of the resulting hypotheses.
in this approach, the choice of one component hypothesis influences the choice of other hypotheses and of the weights assigned to them.
the primary exception  are data sets in which there is a high level of mislabeled training data  points.
one of the most efficient and widely applied learning algorithms searches the hypothesis space consisting of decision trees.
breiman, l., friedman, j. h., olshen, r. a., and stone, c. j., 1984, classification and regression trees, monterey, ca: wadsworth and brooks. ◆ cherkauer, k. j., 1996, human expert-level performance on a scientific image analysis task by a system using combined artificial neural networks, in working notes of the aaai workshop on integrating multiple learned models (p. chan, ed.), menlo park, ca: aaai press, pp.
in such cases, adaboost will put very high weights on the noisy data  points and learn very poor classifiers.
breiman, l., 2001, random forests, machine learn., 45:5–32.
ensemble learning algorithms take a different approach.
in this formula, m is  the number of training data points, andd is a measure of the expressive  power of the hypothesis space from which the individual classifiers are drawn,  known as the vc-dimension.
then each of these words can  be represented by the following data points: x 1 x 2 x 3 x 4 y e  s  _  _
ho (1998) introduced the random subspace method for growing collections of decision trees (“decision forests”).
breiman (2001) combines bagging with the random subspace method to grow random decision forests that give excellent performance.
road map: learning in artificial networks related reading: modular and hierarchical learning systems  ♢ radial basis function networks references bauer, e., and kohavi, r., 1999, an empirical comparison of voting  classification algorithms: bagging, boosting, and variants,machine learn. , 36:105–139.
however, because the output coding can create difficult  two-class learning problems, it is important that the base learner be very  expressive.
statist., 28:337–407.
this view suggests developing algorithms that choose  the component models and the weights so that the weighted sum fits the data  well.
an equivalent way of thinking about this method is that each class j is encoded as ak-bit codeword cj, where bit k is 1 if j ∈ bk and 0 otherwise.
experimentally, adaboost has been shown to be very effective at increasing the margins on the training data points; this result suggests that adaboost will make few errors on new data points.
a decision tree for pronouncing the  letterk.
intuitively, this formula says that if the ensemble learning algorithm can  achieve a large “margin of safety” on each training data point  while using only a weighted sum of simple classifiers,then the resulting voted  classifier is likely to be very accurate.
it works by incrementally adding one hypothesis at a time to an ensemble.
hence, ensemble methods can reduce both the bias and the variance of  learning algorithms.
let dk(xi) be the weight on data pointxi during iteration k of the algorithm.
in most experimental studies (  freund and schapire, 1996 ;  bauer and  kohavi, 1999;  dietterich, 2000 ), adaboost (and algorithms based on it) gives  the best performance on the vast majority of data sets.
the quantity yih(xi) is called the margin , because it is the amount by whichxi is correctly classified.
experimental evidence has shown that ensemble methods are often much more  accurate than any single hypothesis.
performance on 14 out of 33 benchmark tasks (and no change on the remaining 19  tasks).
the second approach to designing ensembles is to construct the hypotheses in a coupled fashion so that the weighted vote of the hypotheses gives a good fit to the data.
statisti., 26:1651–1686. vapnik, v., 1995, the nature of statistical learning theory, new york: springer-verlag.
dietterich and bakiri (1995) describe a technique called error-correcting output coding.
−1  “leaf,” which predicts thatk will be silent.
starting at the so-called root (i.e.,  top) of the tree, we first check whetherx2 =
these and other studies are  summarized in dietterich (1997) .
freund, y., and schapire, r. e., 1997, a decision-theoretic generalization  of on-line learning and an application to boosting,j. comput.
current research is focusing on methods  for extending adaboost to work in high noise settings.
if  not, we follow the right branch to they =
finally, the representational problem arises when the hypothesis space does not contain any hypotheses that are good approximations to the true functionf .
the kth learned classifier  attempts to predict bitk of these codewords (a prediction of −1 is  treated as a binary value of 0).
various heuristics are applied to choose which test to include in each iteration and when to stop growing the tree.
experimental evidence has shown that ensemble methods are often much more accurate than any single hypothesis.
the weight assigned to this hypothesis is  computed by to compute the weights for the next iteration, the weight of training data  pointi is set to where zk is chosen to make dk+1 sum to 1.
the first approach is to construct each hypothesis independently in such a  way that the resulting set of hypotheses is accurate and diverse, that is, each  individual hypothesis has a reasonably low error rate for making new  predictions and yet the hypotheses disagree with each other in many of their  predictions.
another closely related learning algorithm is the hierarchical mixture-of-experts method (seemodular and hierarchical learning systems).
if so, we follow the left branch to another
supervised learning can be applied to many problems,  including handwriting recognition, medical diagnosis, and part-of-speech  tagging in language processing.
many experiments have  employed so-called decision stumps, which are decision trees with only one  internal node.
an algorithm that exhibits the computational problem is sometimes described has havingcomputational variance.
the goal of the  learning algorithm is to find a good approximationh to f that can  be applied to assign labels to newx values.
hence, ensemble methods can reduce both the bias and the variance of learning algorithms.
statisti., 26:1651–1686.
◆ schapire, r. e., freund, y., bartlett, p., and lee, w. s., 1998, boosting  the margin: a new explanation for the effectiveness of voting methods,ann.
this relabeled data is then given to the learning  algorithm, which constructs a classifierhk.
if not, we follow the right (“no”) branch to another test: isx3 = n ?
suppose  we define a vector of features that consists of the two letters prior to thek and the two letters that follow the k.
methods for designing good error-correcting codes can be applied to choose the codewordscj (or, equivalently, subsets ak and bk).
if not, the feature x3 is tested to see if it is the letter n. k is pronounced only if x2 is not c and x3 is notn.
in a hierarchical mixture, individual hypotheses are combined by a gating network that decides, based on the features of the data point, what weights should be employed.
for example, breiman (1996) introduced the bagging ( bootstrap aggregating) method, which works as follows.
and a learning  algorithm that suffers from the representational problem is said to have high bias.
the  answer is to have eachhk classify x.
if the learning algorithm isunstable—that is, if small changes in the training data lead to large changes in the resulting hypothesis— then bagging will produce a diverse ensemble of hypotheses.
in all cases, these algorithms find one best hypothesis h and output it as the “solution” to the learning problem.
this approach directly addresses the representational problem discussed above.
a fourth way of generating accurate and diverse ensembles is to inject  randomness into the learning algorithm.
a decision tree for pronouncing the letterk.
one line of explanation is based on the margin analysis developed by vapnik  (1995) and extended by  schapire et al. (1998) .
the best understood form of statistical learning is known assupervised learning (see learning and statistical inference).
in addition, because the internal nodes of the tree test only a single variable, this creates axis-parallel rectangular decision regions that can have high bias.
finally, the representational problem arises when the hypothesis space does  not contain any hypotheses that are good approximations to the true functionf .
this creates a resampled data set in which some  data points appear multiple times and other data points do not appear at all.
quinlan, j. r., 1993, c4.5: programs for empirical learning, san francisco: morgan kaufmann.
rather than finding one best hypothesis to explain the data, they construct aset of hypotheses (sometimes called acommittee or ensemble) and then have those hypotheses “vote” in some fashion to predict the label of new data points.
in iterationk, the underlying learning algorithm constructs  hypothesishk to minimize the weighted training error.
ho (1998)  introduced the random subspace method for growing collections  of decision trees (“decision forests”).
the classification decision of the combined classifierh is +1 if h(x) ≥ 0 and −1 otherwise.
showed  improved performance on 22 benchmark problems, equal performance on one  problem, and worse performance on four problems.
such ensembles can overcome both the statistical and computational problems discussed above.
current research is exploring ways of  integrating error-correcting output codes directly into the adaboost algorithm.
first, the  bound is not tight, so it may be hiding the real explanation for  adaboost’s success.
suppose that the number of classes,c, is  large.
another closely related learning algorithm is the hierarchical  mixture-of-experts method (seemodular and hierarchical learning systems).
there are  two main approaches to designing ensemble learning algorithms.
third, it is possible to  design algorithms that are more effective than adaboost at increasing the  margin on the training data, but these algorithms exhibit worse performance  than adaboost when applied to classify new data points.
initially, all training data pointsi are given a weight d 1(xi) =
then they consider expanding  the tree by replacing one of the leaves by a test of a second feature (in this  case, the right leaf was replaced with a test ofx3).
◆ cherkauer, k. j., 1996, human expert-level performance on a scientific  image analysis task by a system using combined artificial neural networks, in working notes of the aaai workshop on integrating multiple learned models (p. chan, ed.), menlo park, ca: aaai press, pp.
a fourth way of generating accurate and diverse ensembles is to inject randomness into the learning algorithm.
◆ hastie, t. j., and tibshirani, r. j., 1990, generalized additive models , london: chapman and hall.
then they consider expanding the tree by replacing one of the leaves by a test of a second feature (in this case, the right leaf was replaced with a test ofx3).
adaboost is probably the best method to apply, but favorable results have been obtained just by training several networks from different random starting weight values, and bagging is also quite effective.
more precisely, an ensemble method constructs a set of hypotheses {h1, … , hk}, chooses a set of weights {w 1, … , wk}, and constructs the “voted” classifier h (x) = w1h1(x) + · · · + wkhk(x).
supervised learning can be applied to many problems, including handwriting recognition, medical diagnosis, and part-of-speech tagging in language processing.
dietterich (2000) showed that randomized trees gave significantly improved performance on 14 out of 33 benchmark tasks (and no change on the remaining 19 tasks).
in such cases, adaboost will put very high weights on the noisy data points and learn very poor classifiers.
to determine which hypothesish is best, a learning algorithm can measure how well h matches f on the training data points, and it can also assess how consistenth is with any available prior knowledge about the problem.
breiman (2001)   combines bagging with the random subspace method to grow random decision  forests that give excellent performance.
if not, we follow the right branch to they =
learning describes many different activities, ranging from concept learning (q.v.) to reinforcement learning (q.v.).
the resulting ensemble classifier was significantly more accurate than any of the individual neural networks.
then an unweighted  vote of the hypotheses determines the final classification of a data point.
if the margin is positive, then the sign ofh(xi) agrees with the sign ofyi.
this tree can be used to classify a new data point as follows.
the resulting ensemble classifier was  significantly more accurate than any of the individual neural networks.
a learning algorithm that suffers from the statistical problem is said to have highvariance.
if so, then we  follow the left (“yes”) branch to they =
the goal of the learning algorithm is to find a good approximationh to f that can be applied to assign labels to newx values.
when thel classifiers are applied to classify a new pointx, their predictions are combined into a k -bit binary string.
minimizing j causes the margin to be maximized.
in neural  network and decision tree algorithms, for example, the task of finding the  hypothesis that best fits the training data is computationally intractable,
in statistics,  such ensembles are known asgeneralized additive models (  hastie and  tibshirani, 1990).
+1 leaf, where the tree  indicates thatk should be pronounced.
there are two main approaches to designing ensemble learning algorithms.
the answer is to have eachhk classify x.
∑kwkhk(xi) has the same sign as yi , the correct label ofxi.
then new learning problems can be constructed by randomly partitioning  thec classes into two subsets, ak and bk.
breiman (1997)  showed that this algorithm is a form of gradient  optimization in function space with the goal of minimizing the objective  function
+1, then each class inbk receives a vote.
in addition to the ensemble methods described here, there are other  nonensemble learning algorithms that are similar.
and a learning algorithm that suffers from the representational problem is said to have high bias.
the statistical problem arises when the learning algorithm is searching a space of hypotheses that is too large for the amount of available training data.
breiman, l., 1996, bagging predictors, machine learn., 24:123–140. ◆ breiman, l., 1997, arcing the edge, technical report 486, department of statistics, university of california, berkeley.
experimentally, adaboost has been  shown to be very effective at increasing the margins on the training data  points; this result suggests that adaboost will make few errors on new data  points.
figure 1  shows a decision  tree that explains the data points given above.
the goal is to construct a weighted sum of hypotheses such thath(xi) = ∑kwkhk(xi) has the same sign as yi , the correct label ofxi.
related nonensemble learning methods
the exact reasons for adaboost’s success are not fully understood.
in this setting, each data point consists of a vector of features (denotedx) and a class label y, and it is assumed that there is some underlying functionf such that y = f( x) for each training data point (x, y).
the best understood form of  statistical learning is known assupervised learning (see learning and  statistical inference).
for multiclass problems, the error-correcting output coding algorithm can  produce good ensembles.
such ensembles can overcome both the  statistical and computational problems discussed above.
given a set ofm training data points, bagging chooses in each iteration  a set of data points of sizem by sampling uniformly with replacement  from the original data points.
current research is focusing on methods for extending adaboost to work in high noise settings.
dietterich, t. g., 2000, an experimental comparison of three methods for constructing ensembles of decision trees: bagging, boosting, and randomization, machine learn., 40:139–158.
current research is exploring ways of integrating error-correcting output codes directly into the adaboost algorithm.
◆ dietterich, t. g., and bakiri, g., 1995, solving multiclass learning problems via error-correcting output codes,j. artif.
freund, y., and schapire, r. e., 1996, experiments with a new boosting  algorithm, inprocedings of the 13th international conference on machine  learning, san francisco: morgan kaufmann, pp.
the 32 networks were based on eight different subsets of the 119 available input features and four different network sizes.
suppose we define a vector of features that consists of the two letters prior to thek and the two letters that follow the k.
for example, the backpropagation  algorithm can be run many times, starting each time from a different random  setting of the weights.
an algorithm that exhibits the computational problem  is sometimes described has havingcomputational variance.
in addition to decision trees, there are many other representations for hypotheses that have been studied, includingperceptrons, adalines, and backpropagation (q.v.), radial basis function networks (q.v.), gaussian processes (q.v.), graphical models, helmholtz machines, and support vector machines (q.v.).
+1 leaf, where the tree indicates thatk should be pronounced.
the adaboost algorithm, introduced by  freund and schapire (1996 ,  1997) ,  is an extremely effective method for constructing an additive model.
◆ ho, t. k., 1998, the random subspace method for constructing decision  forests,ieee trans.
let dk(xi) be the  weight on data pointxi during iteration k of the  algorithm.
one way to force a learning algorithm to construct multiple hypotheses is to run the algorithm several times and provide it with somewhat different training data in each run.
experimental measurements of bias and variance have  confirmed this.
because the generalization ability of a single feedforward neural network is usually very good, neural networks benefit less from ensemble methods.
1/m, where m is the number of data  points.
if such an ensemble of hypotheses can be constructed, it is easy to see that it will be more accurate than any of its component classifiers, because the disagreements will cancel out.
in a  hierarchical mixture, individual hypotheses are combined by a gating network  that decides, based on the features of the data point, what weights should be  employed.
the statistical problem arises when the learning algorithm is searching a  space of hypotheses that is too large for the amount of available training  data.
more precisely, an ensemble method constructs a set of  hypotheses {h1, … , hk}, chooses a set of weights {w 1, … , wk}, and constructs the “voted” classifier h (x) = w1h1(x) + · · · + wkhk(x).
then new learning problems can be constructed by randomly partitioning thec classes into two subsets, ak and bk.
there is a risk that the chosen hypothesis will not predict future data points well.
in neural network and decision tree algorithms, for example, the task of finding the hypothesis that best fits the training data is computationally intractable, so heuristic methods must be employed.
in addition, the base learning algorithm  must be sensitive to the encoding of the output values.
methods for designing good error-correcting codes  can be applied to choose the codewordscj (or, equivalently, subsets ak and bk).
as  with the statistical problem, a weighted combination of several different local  minima can reduce the risk of choosing the wrong local minimum to output.
available: http://citeseer.nj.nec.com/breiman97arcing.html .
each new  hypothesis is constructed by a learning algorithm that seeks to minimize the  classification error on aweighted training data set.
vapnik, v., 1995, the nature of statistical learning theory, new  york: springer-verlag.
if the base learning algorithm produces less expressive hypotheses than  decision trees, then the adaboost method is recommended.
available: http://citeseer.nj.nec.com/breiman97arcing.html .
by repeating this process k times (generating different subsets ak and bk), an ensemble  ofk classifiers h1, … , hk is obtained.
one line of explanation is based on the margin analysis developed by vapnik (1995) and extended by schapire et al. (1998) .
for a good discussion of decision trees, see the books by  quinlan (1993) and  breiman et al. (1984) .
if the learning algorithm isunstable—that is, if small changes in  the training data lead to large changes in the resulting hypothesis— then  bagging will produce a diverse ensemble of hypotheses.
there is a risk that the chosen hypothesis will not  predict future data points well.
quinlan, j. r., 1993, c4.5: programs for empirical learning, san  francisco: morgan kaufmann.
for example, breiman (1996)  introduced the bagging ( bootstrap aggregating) method, which works as follows.
similar recommendations apply to ensembles constructed using the naive bayes and fisher’s linear discriminant algorithms.
first, feature x2 is tested to see if it is the letter c.
for multiclass problems, the error-correcting output coding algorithm can produce good ensembles.
in order to learn complex functions with decision stumps, it is important to exploit adaboost’s ability to directly construct an additive model.
decision tree learning algorithms are known to suffer from high variance, because they make a cascade of choices (of which variable and value to test at each internal node in the decision tree) such that one incorrect choice has an impact on all subsequent decisions.
in this setting, each data point consists of a vector  of features (denotedx) and a class label y, and it is assumed  that there is some underlying functionf such that y = f( x) for each training data point (x, y).
res., 2:263–286. freund, y., and schapire, r. e., 1996, experiments with a new boosting algorithm, inprocedings of the 13th international conference on machine learning, san francisco: morgan kaufmann, pp.
by repeating this process k times (generating different subsets ak and bk), an ensemble ofk classifiers h1, … , hk is obtained.
a second way to force diversity is to provide a different subset of the input features in each call to the learning algorithm.
as with the statistical problem, a weighted combination of several different local minima can reduce the risk of choosing the wrong local minimum to output.
the adaboost algorithm, introduced by freund and schapire (1996 , 1997) , is an extremely effective method for constructing an additive model.
review of ensemble algorithms ensemble learning algorithms work by running a base learning algorithm  multiple times, and forming a vote out of the resulting hypotheses.
in addition to decision trees, there are many other representations for  hypotheses that have been studied, includingperceptrons, adalines, and  backpropagation (q.v.), radial basis function networks (q.v.), gaussian  processes (q.v.), graphical models, helmholtz machines, and support vector  machines (q.v.).
in order to learn complex functions with decision stumps, it is  important to exploit adaboost’s ability to directly construct an additive  model.
friedman, hastie, and tibshirani (2000) expand on breiman’s analysis from  a statistical perspective.
the  algorithms are very stable, which means that even substantial (random) changes  to the training data do not cause the learned discrimination rule to change  very much.
for example, any method for  constructing a classifier as a weighted sum of basis functions (see, e.g., radial basis function networks) can be viewed as an additive ensemble where  each individual basis function forms one of the hypotheses.
the input  feature subsets were selected (by hand) to group together features that were  basedon different image processing operations (such as principal component  analysis and the fast fourier transform).
◆ dietterich, t. g., and bakiri, g., 1995, solving multiclass learning  problems via error-correcting output codes,j. artif.
«« previous  next »» terms of use |
this method chooses a random subset of the features at each node of the tree, and constrains the tree-growing algorithm to choose its splitting rule from among this subset.
breiman (1997) showed that this algorithm is a form of gradient optimization in function space with the goal of minimizing the objective function the quantity yih(xi) is called the margin , because it is the amount by whichxi is correctly classified.
the goal is to  construct a weighted sum of hypotheses such thath(xi) =
+1 if k is pronounced and −1 if k is silent, and where “_” denotes positions beyond the ends of the word.
if not, we  follow the right (“no”) branch to another test: isx3 = n ?
road map: learning in artificial networks related reading: modular and hierarchical learning systems ♢ radial basis function networks bauer, e., and kohavi, r., 1999, an empirical comparison of voting classification algorithms: bagging, boosting, and variants,machine learn. ,
a learning algorithm that suffers from the statistical problem is said to  have highvariance.
this approach directly addresses the representational problem  discussed above.
for example, in a  project to identify volcanoes on venus, cherkauer (1996)  trained an ensemble  of 32 neural networks.
the best experimental results have been obtained with very large  decision trees and neural networks.
in some cases, a weighted sum of hypotheses expands the space of functions that can be represented.
first, the bound is not tight, so it may be hiding the real explanation for adaboost’s success.
third, it is possible to design algorithms that are more effective than adaboost at increasing the margin on the training data, but these algorithms exhibit worse performance than adaboost when applied to classify new data points.
to determine  which hypothesish is best, a learning algorithm can measure how well h matches f on the training data points, and it can also assess how  consistenth is with any available prior knowledge about the problem.
the first approach is to construct each hypothesis independently in such a way that the resulting set of hypotheses is accurate and diverse, that is, each individual hypothesis has a reasonably low error rate for making new predictions and yet the hypotheses disagree with each other in many of their predictions.
a decision tree learning algorithm searches the space of such trees by first considering trees that test only one feature (in this casex2 was chosen) and making an immediate classification.
hence, by taking a weighted vote of  hypotheses, the learning algorithm may be able to form a more accurate  approximation tof.
these and other studies are summarized in dietterich (1997) .
this tree can be used to  classify a new data point as follows.
similar recommendations apply to ensembles  constructed using the naive bayes and fisher’s linear discriminant  algorithms.
second, even when adaboost is applied to large decision trees and neural networks, it is observed to work very well even though these representations have high vc-dimension.
then an unweighted vote of the hypotheses determines the final classification of a data point.
decision tree algorithms can be randomized by adding  randomness to the process of choosing which feature and threshold to split on.
now, given a new data point x, how should it be classified?
consequently, ensembles of decision tree classifiers perform much better  than individual decision trees.
we will discuss each of these two approaches in turn.
if hk(x) =  −1, then each class inak receives a vote.
decision tree algorithms can be randomized by adding randomness to the process of choosing which feature and threshold to split on.
breiman, l., 1996, bagging predictors, machine learn.,  24:123–140. ◆ breiman, l., 1997, arcing the edge, technical report 486, department  of statistics, university of california, berkeley.
ordinary machine learning algorithms work by searching through a space of possible functions, calledhypotheses, to find the one function, h , that is the best approximation to the unknown functionf.
however, because the output coding can create difficult two-class learning problems, it is important that the base learner be very expressive.
the algorithms are very stable, which means that even substantial (random) changes to the training data do not cause the learned discrimination rule to change very much.
rather than finding  one best hypothesis to explain the data, they construct aset of  hypotheses (sometimes called acommittee or ensemble) and then  have those hypotheses “vote” in some fashion to predict the label  of new data points.
second, even when adaboost is applied to large  decision trees and neural networks, it is observed to work very well even  though these representations have high vc-dimension.
both of these learn a single linear discrimination rule.
the nearest neighbor  algorithm does not satisfy this constraint, because it merely identifies the  training data pointxi nearest to the new point x and  outputs the corresponding valueyi as the prediction for h(x ), regardless of howyi is encoded.
if not, the feature x3 is tested to see if it is the letter n. k is pronounced only if x2 is not c and x3  is notn.
if such an ensemble of hypotheses can be constructed, it is easy  to see that it will be more accurate than any of its component classifiers,  because the disagreements will cancel out.
decision tree learning algorithms are known to  suffer from high variance, because they make a cascade of choices (of which  variable and value to test at each internal node in the decision tree) such  that one incorrect choice has an impact on all subsequent decisions.
given a set ofm training data points, bagging chooses in each iteration a set of data points of sizem by sampling uniformly with replacement from the original data points.
ordinary machine learning algorithms work by searching through a space of  possible functions, calledhypotheses, to find the one function, h , that is the best approximation to the unknown functionf.
if hk(x) =  +1, then each class inbk receives a vote.
a third way to force diversity is to manipulate the output labels of the training data.
she  reports improved performance on 16 benchmark data sets.
when thel classifiers are applied to  classify a new pointx, their predictions are combined into a k -bit binary string.
intuitively, this formula says that if the ensemble learning algorithm can achieve a large “margin of safety” on each training data point while using only a weighted sum of simple classifiers,then the resulting voted classifier is likely to be very accurate.
this usually gives better results than bagging and other  accuracy/diversity methods.
privacy  policy | contact | faq © 2010
for a good discussion of decision trees, see the books by quinlan (1993) and breiman et al. (1984) .
the function h is called aclassifier, because it assigns class labels y to input data pointsx.
the weight assigned to this hypothesis is computed by to compute the weights for the next iteration, the weight of training data pointi is set to where zk is chosen to make dk+1 sum to 1.
the value of θ can be chosen to minimize the  value of this expression.
after each of the k classifiers has voted, the class with the highest number of votes is selected  as the prediction of the ensemble.
methods for coordinated construction of ensembles in all of the methods described above, each hypothesis hk in the  ensemble is constructed independently of the others by manipulating the inputs,  the outputs, or the features, or by injecting randomness.
consequently, ensembles of decision tree classifiers perform much better than individual decision trees.
hence, methods like bagging that rely on instability do not produce diverse ensembles.
the value of θ can be chosen to minimize the value of this expression.
these heuristics (such as gradient descent) can get stuck in local minima and hence fail to find the best hypothesis.
◆ schapire, r. e., freund, y., bartlett, p., and lee, w. s., 1998, boosting the margin: a new explanation for the effectiveness of voting methods,ann.
the function h is  called aclassifier, because it assigns class labels y to input  data pointsx.
in iterationk, the underlying learning algorithm constructs hypothesishk to minimize the weighted training error.
machine intell., 20:832–844.
dietterich (2000) showed that randomized trees gave significantly improved
in  addition, because the internal nodes of the tree test only a single variable,  this creates axis-parallel rectangular decision regions that can have high  bias.
in addition to the ensemble methods described here, there are other nonensemble learning algorithms that are similar.
the nearest neighbor algorithm does not satisfy this constraint, because it merely identifies the training data pointxi nearest to the new point x and outputs the corresponding valueyi as the prediction for h(x ), regardless of howyi is encoded.
for example, any method for constructing a classifier as a weighted sum of basis functions (see, e.g., radial basis function networks) can be viewed as an additive ensemble where each individual basis function forms one of the hypotheses.
a third way to force diversity is to manipulate the output labels of the  training data.
hence, by taking a weighted vote of hypotheses, the learning algorithm may be able to form a more accurate approximation tof.
for example, in a project to identify volcanoes on venus, cherkauer (1996) trained an ensemble of 32 neural networks.
a decision tree learning algorithm searches the space of such trees by  first considering trees that test only one feature (in this casex2 was  chosen) and making an immediate classification.
methods for independently constructing ensembles one way to force a learning algorithm to construct multiple hypotheses is  to run the algorithm several times and provide it with somewhat different  training data in each run.
in this formula, m is the number of training data points, andd is a measure of the expressive power of the hypothesis space from which the individual classifiers are drawn, known as the vc-dimension.
this method chooses a  random subset of the features at each node of the tree, and constrains the  tree-growing algorithm to choose its splitting rule from among this subset.
suppose that the number of classes,c, is large.
dietterich and bakiri (1995)  describe a technique called  error-correcting output coding.
the second approach to designing ensembles is to construct the hypotheses  in a coupled fashion so that the weighted vote of the hypotheses gives a good  fit to the data.
friedman, hastie, and tibshirani (2000) expand on breiman’s analysis from a statistical perspective.
+1 if k is pronounced and −1 if k is  silent, and where “_” denotes positions beyond the ends of the word.
a simple vote of all of these equally good  classifiers can reduce this risk.
this view suggests developing algorithms that choose the component models and the weights so that the weighted sum fits the data well.
if  the margin is positive, then the sign ofh(xi) agrees with  the sign ofyi.
1/m, where m is the number of data points.
freund and schapire (1996)
if so, then we follow the left (“yes”) branch to they =
for example, the backpropagation algorithm can be run many times, starting each time from a different random setting of the weights.
the algorithm operates as follows.
this usually gives better results than bagging and other accuracy/diversity methods.
friedman, j. h., hastie, t., and tibshirani, r., 2000, additive logistic regression: a statistical view of boosting,ann.
breiman, l., friedman, j. h., olshen, r. a., and stone, c. j., 1984, classification and regression trees, monterey, ca: wadsworth and  brooks.
she reports improved performance on 16 benchmark data sets.
recent experiments suggest that breiman’s  combination of bagging and the random subspace method is the method of choice  for decision trees: it gives excellent accuracy and works well even when there  is substantial noise in the training data.
learning algorithms that output only a single hypothesis suffer from three problems that can be partly overcome by ensemble methods: the statistical problem, the computational problem, and the representation problem.
various heuristics  are applied to choose which test to include in each iteration and when to stop  growing the tree.
this differs from adaboost and other additive ensembles, where the weights are determined once during training and then held constant thereafter.
the computational problem arises when the learning algorithm cannot  guarantee finding the best hypothesis within the hypothesis space.
the majority of research into ensemble methods has focused on constructing ensembles of decision trees.
introduction introduction learning describes many different activities, ranging from concept  learning (q.v.) to reinforcement learning (q.v.).
friedman, j. h., hastie, t., and tibshirani, r., 2000, additive logistic  regression: a statistical view of boosting,ann. statist.,  28:337–407. ◆ hastie, t. j., and tibshirani, r. j., 1990, generalized additive models , london: chapman and hall.
if hk(x) = −1, then each class inak receives a vote.
in such cases, there may be several different hypotheses that all give the same accuracy on the training data, and the learning algorithm must choose one of these to output.
hence, methods like bagging that rely on instability do not produce  diverse ensembles.
because the generalization ability of a single feedforward neural network  is usually very good, neural networks benefit less from ensemble methods.
this differs from adaboost and other additive ensembles, where the  weights are determined once during training and then held constant thereafter.
in all of the methods described above, each hypothesis hk in the ensemble is constructed independently of the others by manipulating the inputs, the outputs, or the features, or by injecting randomness.
◆ ho, t. k., 1998, the random subspace method for constructing decision forests,ieee trans.
the computational problem arises when the learning algorithm cannot guarantee finding the best hypothesis within the hypothesis space.
there are three ways in which this analysis has been criticized.
this work shows that the error  of an ensemble on new data points is bounded by the fraction of training data  points for which the margin is less than some quantity θ > 0 plus a  term that grows as ignoring constant factors and some log terms.
figure 1 shows a decision tree that explains the data points given above.
this creates a resampled data set in which some data points appear multiple times and other data points do not appear at all.
the ensemble’s prediction is the classj whose  codewordcj is closest (measured by the number of bits that agree) to the k-bit output string.
in this approach, the choice of one component hypothesis influences the  choice of other hypotheses and of the weights assigned to them.
in most experimental studies ( freund and schapire, 1996 ; bauer and kohavi, 1999; dietterich, 2000 ), adaboost (and algorithms based on it) gives the best performance on the vast majority of data sets.
these heuristics (such as gradient descent)  can get stuck in local minima and hence fail to find the best hypothesis.
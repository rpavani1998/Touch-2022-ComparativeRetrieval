stochastic discrimination, random forests and boosting tend to form more accurate classifiers than decision trees alone, since they are able to better capture the true structure of the data.decision trees and dependency networks have the greatest range of usability, since trees handle mixed data types, as well as missing and hidden variables.conditional independence allows a probability distribution yielding large memory and computational savings to be factored into the resulting model.bayesian networks use conditional independence to factor a distribution, resulting in large memory and computational savings.neuroscience helped bring about effective non-linear classification, regression and reinforcement learning in the form of neural networks and adaptive filters.this has produced mature tools that will return multiples of the time and money spent learning them: binary decision trees (cart, c4.5) and their statistically boosted variants (adaboost, mart, random forests); discrete graphical models in the form of diagnostic networks and influence diagrams; and support vector machines for prediction and feature selection.one technique, described in the context of random forests but universal to any classifier, determines which features are important for prediction accuracy.often just applying a decision tree and thinking clearly about the results will generate 80% of the savings.with the explosion in internal and external networks, which allow the pooling of data, advantages such as cost savings, sophisticated trouble-shooting and automatic diagnostics are available with even the simplest applications.svms can be powerful prediction and classification machines.the potential applications of bayesian networks in semiconductor manufacturing are vast, but greatest in the introduction of cost-based decision-making influence diagrams.statistical boosting"bayesian networks use conditional independence to factor a distribution, resulting in large memory and computational savings.""bayesian networks use conditional independence to factor a distribution, resulting in large memory and computational savings."using the importance variable method, these classifiers can be used in place of single trees and still find important dependencies in the data.by using a classifier, the label that derives from each feature can be learnt and predicted.
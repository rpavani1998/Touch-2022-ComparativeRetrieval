this paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in roc space) than varying the loss ratios in ripper or class priors in naive bayes.
random subspaces are a popular ensemble construction technique that  improves the accuracy of weak classifiers.
hence, bagging with samples the size of the data is impractical.
the use of distributed disjoint partitions in learning is significantly less complex and faster than bagging.
actively exploring face space(s) for improved face recognition, nitesh v. chawla and kevin w. bowyer, aaai 2007, vancouver, july  2007.
actively exploring face space(s) for improved face recognition, nitesh v. chawla and kevin w. bowyer, aaai 2007, vancouver, july 2007.
experiments with decision tree and neural network classifiers on various datasets show that, given the same size partitions and bags, disjoint partitions result in performance equivalent to, or better than, bootstrap aggregates (bags).
a region ensemble for 3d face recognition, timothy faltemier, kevin  w. bowyer and patrick j. flynn, ieee transactions on information  forensics and security, 3(1):62-73, march 2008.
this paper presents a method for combining classifiers that uses estimates of each individual classifier's local accuracy in small regions of the feature space surrounding an unknown sample.
the random subspaces approach has the added advantage of requiring  less careful tweaking.
synthetic minority over-sampling technique, nitesh chawla, kevin w. bowyer, lawrence o. hall, and w. philip kegelmeyer, journal of artificial intelligence research 16, 2002, 321-357.
is error-based pruning redeemable?, lawrence o. hall, kevin w. bowyer, robert e. banfield and steven eschrich, and richard collins, international journal of artificial intelligence tools, 12 (3), september 2003, 249-264.
an empirical evaluation using five real data sets confirms the validity of our approach compared to some other combination of multiple classifiers algorithms.
ieee transactions on pattern analysis and machine intelligence 29 (1),  173-180, january 2007.
multiple nose region matching for 3d face recognition under varying facial expression, kyong i. chang, kevin w. bowyer, and patrick j. flynn, ieee transactions on pattern analysis and machine intelligence, 28 (10), 1695-1700, october 2006.
we  experimentally evaluate bagging and seven other randomization-based approaches  to creating an ensemble of decision tree classifiers.
finally, the methods proposed for thinning again show that ensembles can be made smaller without loss in accuracy.
ieee transactions on pattern analysis and machine intelligence 29 (1), 173-180, january 2007.
random subspaces and subsampling for 2-d face recognition, nitesh v.  chawla and kevin w. bowyer, computer vision and pattern recognition  (cvpr 2005), san diego, june 2005, ii: 582-589.
experimental results are presented using the largest database employed to date in 3d face recognition studies, over 4,000 scans of 449 subjects. ...
this work shows that a random subspaces ensemble can  outperform a well-tuned single classifier for a typical 2-d face recognition  problem.
kevin w. bowyer - data mining and classifier ensembles detecting and ordering salient regions, larry shoemaker, robert  banfield, lawrence o. hall, kevin w. bowyer and w. philip kegelmeyer, data mining and knowledge discovery 12 (1-2), january 2011, 259-290.
a region ensemble for 3d face recognition, timothy faltemier, kevin w. bowyer and patrick j. flynn, ieee transactions on information forensics and security, 3(1):62-73, march 2008.
rank-one recognition rates of 97.2% and verification rates of 93.2% at 0.1%  false accept rate are reported and compared to other methods published on the  face recognition grand challenge v2 data set.
a comparison of decision tree ensemble creation techniques, robert  e. banfield, lawrence o. hall, kevin w. bowyer, and w. philip kegelmeyer.
experiments on the forest cover data set show that this parallel mixture is more accurate than a single svm, with 90.72% accuracy reported on an independent test set.
multiple nose region matching for 3d face recognition under varying facial  expression, kyong i. chang, kevin w. bowyer, and patrick j. flynn, ieee transactions on pattern analysis and machine intelligence, 28 (10),  1695-1700, october 2006.
learning to predict gender from irises, vince thomas, nitesh v.  chawla, kevin w. bowyer and patrick j. flynn, ieee international  conference on biometrics: theory, applications, and systems (btas 07),  september 2007.
ensemble diversity measures and their application to thinning, robert e. banfield, lawrence o. hall, kevin w. bowyer, and w. philip kegelmeyer, information fusion 6 (1), march 2005, 49-62.
in this comment, we show that a simple ensemble of decision trees results in a higher accuracy, 94.75%, and is computationally efficient.
this paper employs machine  learning techniques to develop models that predict gender based on the iris  texture features. ...
in  addition, we also compare the random subspace methodology to an ensemble of  subsamples of image data.
a comparison of decision tree ensemble creation techniques, robert e. banfield, lawrence o. hall, kevin w. bowyer, and w. philip kegelmeyer.
we describe an ensemble learning approach that accurately  learns from data which has been partitioned according to the arbitrary spatial  requirements of a large-scale simulation wherein classifiers may be trained  only the data local to a given partition.
as a result, the class statistics can vary from partition to partition; some classes may even be missing from some partitions.
our results indicate that, in such applications, the simple approach of creating a committee of n classifiers from disjoint partitions each of size 1/n (which will be memory resident during learning) in a distributed way results in a classifier which has a bagging-like performance gain.
this paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in roc space) than only under-sampling the majority class.
random subspaces and subsampling for 2-d face recognition, nitesh v. chawla and kevin w. bowyer, computer vision and pattern recognition (cvpr 2005), san diego, june 2005, ii: 582-589.
the main goal of the paper is to see if the random subspace  methodology can do as well, if not better, than the single classifier  constructed on the tuned face space.
we experimentally evaluate bagging and seven other randomization-based approaches to creating an ensemble of decision tree classifiers.
in this paper, we apply the random subspace methodology to the 2-d face recognition task.
we also suggest a methodology for determining the best mix of individual classifiers.
http://dx.doi.org/10.1007/s10618-010-0194-6 pdf of this paper.
we introduce a new system for 3d face recognition based on the fusion of results from a committee of regions that have been independently matched. ...
learning to predict gender from irises, vince thomas, nitesh v. chawla, kevin w. bowyer and patrick j. flynn, ieee international conference on biometrics: theory, applications, and systems (btas 07), september 2007.
bagging forms a committee of classifiers by bootstrap aggregation of training sets from a pool of training data.
ensembles of classifiers from spatially disjoint data, robert e. banfield, lawrence o. hall, kevin w. bowyer, and w. philip kegelmeyer, springer-verlag lncs 3541: 6th international workshop on multiple classifier systems (mcs 2005), monterey, ca, june 2005, 196-205.
boosting lite - handling larger datasets and slower base classifiers, lawrence o. hall, robert e. banfield, kevin w. bowyer and w. philip kegelmeyer.
it has been shown, in different domains, that random subspaces combined with weak classifiers such as decision trees and nearest neighbor classifiers can provide an improvement in accuracy.
detecting and ordering salient regions, larry shoemaker, robert banfield, lawrence o. hall, kevin w. bowyer and w. philip kegelmeyer, data mining and knowledge discovery 12 (1-2), january 2011, 259-290.
we also propose the use of a validation set for tuning the face space, to avoid bias in the accuracy estimation.
we also  construct ensembles of classifiers learned from such actively sampled image  sets, which further provides improvement in the recognition rates. ...
we combine a fast ensemble learning algorithm  with probabilistic majority voting in order to learn an accurate classifier  from such data. ...
we evaluate thinning algorithms on ensembles created by several techniques on 22 publicly available datasets.
rank-one recognition rates of 97.2% and verification rates of 93.2% at 0.1% false accept rate are reported and compared to other methods published on the face recognition grand challenge v2 data set.
the random subspaces approach has the added advantage of requiring less careful tweaking.
this paper employs machine learning techniques to develop models that predict gender based on the iris texture features. ...
using classifier ensembles to label spatially disjoint data, larry  shoemaker, robert e. banfield, lawrence o. hall, kevin w. bowyer and w. philip  kegelmeyer, information fusion 9(1), 120-133, january 2008.
combination of multiple classifiers using local accuracy estimates, kevin s. woods, w. philip kegelmeyer and kevin w. bowyer, ieee transactions on pattern analysis and machine intelligence 19 (4), 405-410, april 1997.
we propose a framework for building hundreds or thousands of such classifiers on small subsets of data in a distributed environment.
when compared to other methods, our percentage correct diversity measure algorithm shows a greater correlation between the increase in voted ensemble accuracy and the diversity value. ...
comments on "a parallel mixture of svms for very large scale problems," xiaomei liu, lawrence o. hall, and kevin w. bowyer, neural computation 16 (7), july 2004, 1345-1351.
it is  based on combining the match scores from matching multiple overlapping regions  around the nose.
we propose a learning framework that  actively explores creation of face space(s) by selecting images that are  complementary to the images already represented in the face space.
finally, the methods proposed for thinning again show that ensembles  can be made smaller without loss in accuracy.
distributed learning with bagging-like performance, nitesh chawla, thomas e. moore, lawrence o. hall, kevin w. bowyer, w. philip kegelmeyer, and clayton springer, pattern recognition letters 24 (1-3), 2003, 455-471.
the main goal of the paper is to see if the random subspace methodology can do as well, if not better, than the single classifier constructed on the tuned face space.
we describe an ensemble approach to learning from arbitrarily partitioned data.
random subspaces are a popular ensemble construction technique that improves the accuracy of weak classifiers.
appendix to the paper.
a simple alternative to bagging is to partition the data into disjoint subsets.
multiple classifier systems (mcs) 2007, prague, may 2007.
learning ensembles from bites: a scalable and accurate approach, nitesh chawla, lawrence o. hall, kevin w. bowyer and w. philip kegelmeyer, journal of machine learning research 5, april 2004, 421-451.
this result is somewhat surprising and illustrates the general value of experimental comparisons using different types of classifiers.
as a result, the class statistics can  vary from partition to partition; some classes may even be missing from some  partitions.
using classifier ensembles to label spatially disjoint data, larry shoemaker, robert e. banfield, lawrence o. hall, kevin w. bowyer and w. philip kegelmeyer, information fusion 9(1), 120-133, january 2008.
while this accuracy is impressive, the referenced paper does not consider alternative types of classifiers.
statistical tests were performed on experimental results from 57 publicly available data sets. ...
we evaluate thinning algorithms on ensembles created by  several techniques on 22 publicly available datasets.
we  describe an ensemble approach to learning salient regions from arbitrarily  partitioned data. ...
we propose a learning framework that actively explores creation of face space(s) by selecting images that are complementary to the images already represented in the face space.
it is based on combining the match scores from matching multiple overlapping regions around the nose.
ensembles of classifiers from spatially disjoint data, robert e.  banfield, lawrence o. hall, kevin w. bowyer, and w. philip kegelmeyer, springer-verlag lncs 3541:
an algorithm is proposed for 3d face recognition in the presence of varied facial expressions.
an algorithm is proposed  for 3d face recognition in the presence of varied facial expressions.
experimental results are presented using the largest database  employed to date in 3d face recognition studies, over 4,000 scans of 449  subjects. ...
we combine a fast ensemble learning algorithm with probabilistic majority voting in order to learn an accurate classifier from such data. ...
we describe an ensemble learning approach that accurately learns from data which has been partitioned according to the arbitrary spatial requirements of a large-scale simulation wherein classifiers may be trained only the data local to a given partition.
voting many classifiers built on small subsets of data is a promising approach for learning from massive data sets, one that can utilize the power of boosting and bagging.
many applications (e.g., protein structure prediction) involve use of datasets that are too large to handle in the memory of the typical computer.
6th international workshop on multiple classifier  systems (mcs 2005), monterey, ca, june 2005, 196-205.
when compared to other  methods, our percentage correct diversity measure algorithm shows a greater  correlation between the increase in voted ensemble accuracy and the diversity  value. ...
statistical tests were  performed on experimental results from 57 publicly available data sets. ...
we also construct ensembles of classifiers learned from such actively sampled image sets, which further provides improvement in the recognition rates. ...
in this paper, we apply the random subspace methodology to the 2-d face  recognition task.
we combine a fast ensemble learning algorithm with scaled probabilistic majority voting in order to learn an accurate classifier ...
we describe an ensemble approach to learning from  arbitrarily partitioned data.
we examine ensemble algorithms (boosting lite and  ivoting) that provide accuracy approximating a single classifier, but which  require significantly fewer training examples. ...
we also propose the use of a validation  set for tuning the face space, to avoid bias in the accuracy estimation.
our method of over-sampling the minority class involves creating synthetic minority class examples.
it has been shown, in different  domains, that random subspaces combined with weak classifiers such as decision  trees and nearest neighbor classifiers can provide an improvement in accuracy.
we introduce a new system for 3d face recognition based on the fusion of  results from a committee of regions that have been independently matched. ...
this work shows that a random subspaces ensemble can outperform a well-tuned single classifier for a typical 2-d face recognition problem.
we examine ensemble algorithms (boosting lite and ivoting) that provide accuracy approximating a single classifier, but which require significantly fewer training examples. ...
in addition, we also compare the random subspace methodology to an ensemble of subsamples of image data.
we combine a fast ensemble learning algorithm with scaled  probabilistic majority voting in order to learn an accurate classifier ...
we describe an ensemble approach to learning salient regions from arbitrarily partitioned data. ...
experiments show this approach is fast, accurate, and scalable.
the previous discussion though highlighted the credit-rating agencies failure to assess the risk of consumer asset-backed securities and the basel requirements to stress test portfolios of consumer loans.
for example, there are classification trees in which some of the variables are a 'score' obtained using another method.
in credit cards, this would mean varying the interest rate charged, the credit limit offered, whether an annual fee is charged and whether bonuses such as air miles are given for purchases made with the card.
thus, currently lgd modelling is like estimating a moving target.
therefore, once again the target variable was whether the borrower would default in the next 12 months, but now it was possible to use information on the borrower's recent (usually last 12 months) repayment and purchase performance.
in the competing risk approach, one has several ways in which a loan could finish—default, early repayment, normal repayment—and one can model each of these separately using the fact that as far as a default is concerned, a borrower who pays off early at timet has a history censored at that time.
in the proportional hazard model, the hazard function for default at time periodt into the loan for a borrower with characteristicsx decomposes into the product of the baseline hazard function multiplied by an enhanced risk due to the borrower's characteristics, namely, hence, s(x) can be considered as a risk score in that the higher the score the less likely the borrower is to default.
it considered data on applicants of 2 years ago, and looked at their performance over the subsequent year.
this model can work both as a parametric model in which the baseline hazard function is of a specific family of distributions or semi parametrically using the results ofcox (1972).
after sometime, the process is repeated and a new scorecard is constructed.
these characteristics include socio-economic data such as age and residential status; credit bureau information such as whether the applicant is on the electoral role; and in the case of behavioural scores, performance data such as the number of missed payments in the last 12 months.
the critical issue is how to build a model of the credit risk of portfolios of consumer loans, which includes economic and market conditions and so can then be run under the extreme scenarios suggested by the regulators.
the first term on the rhs of (6) is the 'prior' score—the score of a randomly selected individual from the population; this score is then increased or decreased by the score that is based on the data that are unique to a particular individual.
whereas in many retail environments the horizon may be just until the next purchase or possibly just for a few years, in the consumer finance area, lifetime can really mean lifetime—pension products for example.
it considered data on applicants of 2 years ago, and looked at their performance over the subsequent year.
there are also affordability issues, as the interest rate charged can affect the ability of the borrower to repay, as was seen in the subprime mortgage crisis, in which many of the borrowers only defaulted when the interest rates went from the initial low rates to the higher rates that came in after 2 or 3 years of the loan.
the models forecast how likely the applicant for credit is to be 'bad' and to default on the loan within a given time period.
still, that does not stop people from trying, although it would be more useful if the experiments were carried out on the sizes of samples, 10 000–50 000, usually used in scorecard building, rather than on the small samples of less than 1000, which are easily available in public literature.
until then, despite its importance to the individual consumer, and the fact that it was using an increasing number of those who had trained in operational research (or) and statistics, the modelling underlying it was hardly discussed in any finance course, and the number of research papers in the area was minute compared with that on the corporate credit market or on the pricing of exotic equity-based options.
one advantage of the survival analysis approach is that the competing risk idea means that one can use the same data to estimate several different events.
this is a much faster-moving environment than that for normal loans, as the loans are of such short durations, and their repayment depends on the borrower's ability and desire to pay back the loan that month.
translating from point-in-time default rates to through the cycle default rates highlights the time dependency of a score which we outlined in challenge 2.
before doing so, it is worth recalling what a credit score is and what properties it has.
moreover such loans are increasingly receiving special legislation which requires proof that their risk-assessment systems are robust.
those borrowers who do not default on the loan within the chosen time period are 'good'.
what often happens is that the paper that introduces a new method can show that there is some small improvement by using it rather than using an existing method, but one is always slightly concerned that this may be down to the expertise of the authors in their own method and the fact that they do not take such care with existing methods.
the approach also assumed that the relationship between loan/borrower characteristics and credit worthiness was stable at least over a 4- of 5-year period.
even in the initial decision, lenders now have a number of variants of a loan product they can offer, be it platinum, gold, silver or standard credit cards, or tracker, fixed rate and variable rate mortgages, and within each they can decide what credit limit to offer and what interest rate and fee (the price components) to charge.
log odds scores are produced when one uses logistic regression to determine the classification scorecard but can also be obtained from other approaches by scaling, therefore it is reasonable to assume a scorecard has such a property.
one can do this by sensitivity analysis in which one changes the value of one of the factors that impacts on the model, or by scenario analysis.
this involves estimating how the score s(t,x) changes over time, which brings us back to challenge 2.
this should mean that consumer finance will have a much higher profile in university finance and operational research courses in the future, so that entrants to the finance industry are aware of the need for models and the challenges of building models to solve the problems in this area.
a proper or sufficient score s(x) captures as much information for predicting the probability of a performance outcome, say good/bad, as does the original data vector,x, so that when appropriate we will drop the x dependence of the score and write one form of a score is the log odds score, where hence, a log odds score could have values from minus infinity (when p(g| x)=0) to plus infinity when (p(g|x)=1).
the aim of these actions, however, is to improve the profitability of the customer, but there might be other measures rather than default risk in the next 12 months, which give a better handle on profit.
secondly, the detailed investigations of the subprime mortgage crisis showed that the credit scores changed as the economic conditions worsened ( demyanyk and van hemert, 2008).
it is easy then to see that, given the hazard function, we can calculate the probability of default over any time period because if one uses the proportional hazards or accelerated life models of survival analysis, one is able to obtain a score that describes the 'risk' of a consumer defaulting over any and all time horizons.
university of southampton, to appear inproduction and operation management.
one could argue that the pricing models of challenge 4 are a start but the marketing aspects of the model are not widely used, apart from the work on multiple features in credit cards ( thomaset al, 2006).
its philosophy was pragmatic, in that it only wanted to predict, not explain, and hence used any characteristic that improved the discriminating power of the system.
the accord requires its ratings to have many of the properties of the existing credit-scoring systems.
this suggests that the probability of default does vary, as economic conditions vary, even if the credit worthiness of the borrower is not changing and has required some ingenuity by lenders to translate a credit score, which is clearly a point in time (pit) estimate, into the through the cycle (ttc) estimate.
thus, one needs to build scorecards that can respond very quickly to changes in economic and market behaviour and to immediate changes in the borrower's behaviour, and circumstances.
moreover, laws such as the equal credit opportunity acts in the us have outlawed discrimination in the giving of credit unless there are statistical models that can defend such decisions.
the traditional way of defining a bad was a borrower who became 90 days overdue in the next 12 months.
there is a sumerian clay tablet recording how two farmers borrowed money to purchase grain with the promise of paying back more at harvest time.
however, that does not prevent building credit risk models for portfolios of consumer loans, which have strong parallels with the corporate portfolio models (thomas, 2009b).
in particular, the lack of models for the credit risk of portfolios of consumer loans and not modelling how economic conditions affect credit scores is now recognised as having exacerbated the credit crunch of 2008/2009.
the approach also assumed that the relationship between loan/borrower characteristics and credit worthiness was stable at least over a 4- of 5-year period.
the tremendous increase in computer storage capacity and the requirement of the basel accord that banks have sufficient historical data to validate their credit scoring models have meant that banks are now willing and able to store much more consumer finance data over much longer periods than they used to do.
there are also affordability issues, as the interest rate charged can affect the ability of the borrower to repay, as was seen in the subprime mortgage crisis, in which many of the borrowers only defaulted when the interest rates went from the initial low rates to the higher rates that came in after 2 or 3 years of the loan.
in fact, it is only worthwhile for banks to move to these internal ratings-based systems, if they use them for their consumer lending, as the main saving in capital compared with the alternative externally imposed capital ratios is in consumer lending.
credit scoring models in the credit union environment using neural networks and genetic algorithms.
a survey of credit and behavioural scoring; forecasting financial risk of lending to consumers.
so, not only is it proving difficult to get reasonable estimates of lgd and rr using existing data, but building models to optimise or at least improve the collections process is likely to mean that rrs in the future will be significantly improved on those found in these data.
so, not only is it proving difficult to get reasonable estimates of lgd and rr using existing data, but building models to optimise or at least improve the collections process is likely to mean that rrs in the future will be significantly improved on those found in these data.
however, that does not prevent building credit risk models for portfolios of consumer loans, which have strong parallels with the corporate portfolio models (thomas, 2009b).
although behavioural scoring was an obvious extension of application scoring, it was also an opportunity missed.
clearly, standard risk-assessment systems cannot work for people who have no history of being advanced credit previously and no involvement with a banking system.
it is true that some methods performed slightly better than others—neural nets, support vector machines, logistic regression—but the differences were small and often the hypothesis that two scorecards were equally good at discriminating could not be rejected.
top of page conclusion given the turmoil in the financial markets during 2007 and 2008, which has at last made practitioners and researchers realise how large a proportion of the banking industry is based on consumer lending, there is no question that research in this area will be very active for the foreseeable future.
the initial approaches have looked at linear and logistic regression, non-linear transformation so as to fit beta or log-log distributions, mixture models (especially to identify the 'won't pay' (lgd=1), and even quantile regression ideas (somers and whittaker, 2007).
for example, there are some marketing models that seek to assess the 'emotions' of the customer from their interactions with the company (coussement and van den poel, 2009) but there is no risk-assessment model that includes the customers 'emotions'.
one way that lenders are seeking to increase their profit is by offering generic loan products such as credit cards, but by tailoring the details of the product for each individual.
the models used by marketers to segment customers and to estimate propensity of purchase are very similar to the ones used by the risk teams to determine how many different scorecards to develop and then to estimate the likelihood of default for each customer.
cox showed that one can first calculate the score without making any assumptions about the distribution and then use the kaplan meier approach to estimate the empirical distribution forh0(t) that best fits the data.
figure 1 shows how the total household borrowing in the us overtook that of total business borrowing in the late 1980s, and that by 2004 the total borrowing on mortgages had also exceeded the total business borrowing, although that has drawn level again in 2008.figure 2 similarly shows the growth in consumer borrowing in the uk in the 15 years since 1992.
it also presupposes that the score to probability of default transformation stays as a log odds transformation and ignores what happens when scores are recalibrated during the cycle.
a second challenge is that the accord requires estimates of the long-run average of the 12-month default rate (the ttc default rate) for a segment of borrowers while a credit score estimates the default rate in the next 12 months (the pit estimate).
specifying the score of an event is equivalent to specifying its probability because we can write the probability in terms of the score: one interesting feature of a log odds score is that it separates out completely the information about the population from information about the individual borrower being scored.
instead, what has happened is that lenders score separately a number of the events that affect profitability.
in the proportional hazard model, the hazard function for default at time periodt into the loan for a borrower with characteristicsx decomposes into the product of the baseline hazard function multiplied by an enhanced risk due to the borrower's characteristics, namely, hence, s(x) can be considered as a risk score in that the higher the score the less likely the borrower is to default.
thus, one needs to build lifetime value models that can cope with the changes in economic and market conditions over long time intervals as well as forecasting the changes in the customer's situation and priorities.
one could argue that the pricing models of challenge 4 are a start but the marketing aspects of the model are not widely used, apart from the work on multiple features in credit cards ( thomaset al, 2006).
all these models concentrate on the purchase aspects—time to and value of next purchase and churn—and do not include the default risk elements that can affect profitability in a major way.
secondly, the detailed investigations of the subprime mortgage crisis showed that the credit scores changed as the economic conditions worsened ( demyanyk and van hemert, 2008).
this is not possible nor sensible for portfolios of consumer loans as default there does not depend on the value of assets but on cash flow considerations and personal attitudes to debt.
cox showed that one can first calculate the score without making any assumptions about the distribution and then use the kaplan meier approach to estimate the empirical distribution forh0(t) that best fits the data.
the credit scoring toolkit theory and practice for retail credit risk management and decision automation.
therefore, once again the target variable was whether the borrower would default in the next 12 months, but now it was possible to use information on the borrower's recent (usually last 12 months) repayment and purchase performance.
a third problem is the basel accord's instance on stress testing, which means predicting the future performance of a portfolio of loans under extreme economic conditions.
for a log odds score, equation(4) shows how this probability is related to the credit score of the applicant.
even in the initial decision, lenders now have a number of variants of a loan product they can offer, be it platinum, gold, silver or standard credit cards, or tracker, fixed rate and variable rate mortgages, and within each they can decide what credit limit to offer and what interest rate and fee (the price components) to charge.
building credit scoring models using genetic programming.
buckinx et al (2007) use the transactional information to estimate the customer loyalty to the organisation, whilevan den poel and larivière (2004) model which product features prevent customers churning to another organisation.
thus, bankruptcy scores were developed where a bad was someone who went bankrupt in the next 12 months.
bayesian kernel based classification for financial distress detection.
this performance was used to determine whether the applicant was bad (the specific risk occurred) or good (it did not occur).
construction of a k-nearest neighbour credit scoring system.
moreover, profitability is as much about marketing as about risk assessment, and hence there is a need to combine the work done by financial organisations' marketing and risk assessment or groups.
lookahead scorecards for new fixed term credit products.
the loan is taken out usually at the middle or towards the end of the month and the lender is given a post-dated cheque or a way of accessing the borrower's current account on the day the pay cheque is paid in at the end of the month.
the accord requires its ratings to have many of the properties of the existing credit-scoring systems.
for example, many of the newer methods essentially construct non-linear scorecards with interactions between characteristics, but experts in the linear approaches to credit scorecard building—logistic and linear regression—tend to know from experience about such interactions and allow for them by building separate scorecards for different segments of the population or by introducing interaction variables.
still, that does not stop people from trying, although it would be more useful if the experiments were carried out on the sizes of samples, 10 000–50 000, usually used in scorecard building, rather than on the small samples of less than 1000, which are easily available in public literature.
this is because of adverse selection ( ausubel, 1999; calem et al, 2006) in which more bads apply for consumer credit at higher interest rates than might be expected.
usage scores assess how much a borrower will use the loan product.
one can model purchasing as well as attrition and default events separately and then seek to combine them to get a customer lifetime value approach (challenge 10).
proportional hazards analysis behavioural scores.
in the past few years, credit scoring had been changing, as lenders want credit scoring to support their business objectives of profitability and market share.
consider a very simple example where 1 unit is lent, the cost of capital for the lender isrf (the risk free rate), the loss given default (the fraction of the amount outstanding at default which is finally lost) isld, and the lender will charge an interest rater(p), which is related to the probability p of the applicant being a good.
as mentioned in challenge 2, survival analysis can also be used to introduce economic conditions into scorecards.
such two-stage models could also be used for other secured loans like car finance.
moreover, behavioural scoring only used static characteristics about the customer's past performance and used these to estimate the customer's status at a fixed time in the future.
the models used by marketers to segment customers and to estimate propensity of purchase are very similar to the ones used by the risk teams to determine how many different scorecards to develop and then to estimate the likelihood of default for each customer.
improving customer attrition prediction by integrating emotions from client/company interaction emails and evaluating multiple classifiers.
for example, up to 5 years ago, most banks had hardly any data on the outcome of their collections and recoveries process, but the need to estimate lgd for all consumer loans means that such data are now carefully recorded and analysed.
this suggests that the probability of default does vary, as economic conditions vary, even if the credit worthiness of the borrower is not changing and has required some ingenuity by lenders to translate a credit score, which is clearly a point in time (pit) estimate, into the through the cycle (ttc) estimate.
thus, bankruptcy scores were developed where a bad was someone who went bankrupt in the next 12 months.
moreover, behavioural scoring only used static characteristics about the customer's past performance and used these to estimate the customer's status at a fixed time in the future.
this performance was used to determine whether the applicant was bad (the specific risk occurred) or good (it did not occur).
optimizing the collections process in consumer credit.
thus, there seems to be a great deal more research that is required to develop more appropriate good/bad assessments both in terms of expanding from default to profitability and in removing any pre-defined time horizon on the time over which the customer is assessed.
thus, currently lgd modelling is like estimating a moving target.
the assumption that credit worthiness is time independent over intervals of 3 or 4 years meant that credit scores have been built using the socio-demographic characteristics of the borrower, the credit bureau information about the borrower, details of the loan and even the repayment performance of the borrower on the loan, but not using anything about the current economic and market conditions.
a proper or sufficient score s(x) captures as much information for predicting the probability of a performance outcome, say good/bad, as does the original data vector,x, so that when appropriate we will drop the x dependence of the score and write one form of a score is the log odds score, where hence, a log odds score could have values from minus infinity (when p(g| x)=0) to plus infinity when (p(g|x)=1).
log odds scores are produced when one uses logistic regression to determine the classification scorecard but can also be obtained from other approaches by scaling, therefore it is reasonable to assume a scorecard has such a property.
this sample was then used to build a classification system that best separated the goods from the bads, using the characteristics of the loan and the borrower.
this is a much faster-moving environment than that for normal loans, as the loans are of such short durations, and their repayment depends on the borrower's ability and desire to pay back the loan that month.
in the competing risk approach, one has several ways in which a loan could finish—default, early repayment, normal repayment—and one can model each of these separately using the fact that as far as a default is concerned, a borrower who pays off early at timet has a history censored at that time.
similarly, lenders are more likely to adjust the product or offer alternative or extra products during their relationship with the customer and so are anxious to know what impact such changes will have on the default risk and the profitability of the customer.
whereas in many retail environments the horizon may be just until the next purchase or possibly just for a few years, in the consumer finance area, lifetime can really mean lifetime—pension products for example.
in credit cards, this would mean varying the interest rate charged, the credit limit offered, whether an annual fee is charged and whether bonuses such as air miles are given for purchases made with the card.
one also usually assumes that the score has a monotonic increasing relationship with the probability of being good; hence, if a borrower has a higher score than a second borrower, the first borrower has a higher probability of being good than does the second.
the aim of these actions, however, is to improve the profitability of the customer, but there might be other measures rather than default risk in the next 12 months, which give a better handle on profit.
in particular, the lack of models for the credit risk of portfolios of consumer loans and not modelling how economic conditions affect credit scores is now recognised as having exacerbated the credit crunch of 2008/2009.
the first term on the rhs of (6) is the 'prior' score—the score of a randomly selected individual from the population; this score is then increased or decreased by the score that is based on the data that are unique to a particular individual.
microcredit involves giving very small loans to those in poverty in order to help them develop a business, which will sustain them and their family and so bring them out of poverty.
by applying monte carlo simulation, using different future economic scenarios, one can then use such a model to estimate portfolio-level default rates.
clearly, for lending to consumers, these internal risk-rating systems are application and behavioural scoring systems.
this is because of adverse selection ( ausubel, 1999; calem et al, 2006) in which more bads apply for consumer credit at higher interest rates than might be expected.
the most powerful characteristics are whether the borrowers have recently been in arrears and the current information from the credit bureau on their overall credit performance.
so one needs to be confident in the translation of score to probability of default and to use the standard chi-square and normal distribution-type tests to validate the model by backtesting to compare actual numbers of defaults with predicted ones (bcbs, 2005b).
its philosophy was pragmatic, in that it only wanted to predict, not explain, and hence used any characteristic that improved the discriminating power of the system.
the critical assumption in credit scoring is that the score is all that is required for predicting the probability of the applicant being good.
so a scorecard built on a 2-year-old sample is used to determine which applicants to take for the next few years.
specifying the score of an event is equivalent to specifying its probability because we can write the probability in terms of the score: one interesting feature of a log odds score is that it separates out completely the information about the population from information about the individual borrower being scored.
as mentioned earlier, the introduction of the new banking regulations, the basel ii accord (bcbs, 2005a), concerning the amount of capital that banks need to set aside to cover their risk, has had a major impact on credit scoring.
it also presupposes that the score to probability of default transformation stays as a log odds transformation and ignores what happens when scores are recalibrated during the cycle.
applying this in equation (3) gives thus, a log odds score is the sum of a term depending only on the population odds (spop=lnopop) and a term that depends of the information of the borrowerx.
attrition scores assess whether the borrower will cancel the loan product shortly.
the use of interaction terms and time-dependent coefficients, which proved so successful there, can obviously be taken across to building economy-based credit scorecards.
given the amount of research that has gone into corporate credit risk models, one suspects that there will be considerably more research into these consumer equivalents, given the realisation by bankers now of how much more is being lent to households than to companies.
before doing so, it is worth recalling what a credit score is and what properties it has.
the lender should then accept applicants with high credit scores if and accept applicants with low credit scores if credit scoring began in the 1950s when it was realised that statistical classification methods—the first being discriminant analysis (fisher, 1936 )—could be used to classify loans into goods (non-defaulting) and bads (defaulting), using the characteristics of the loan and the borrowers.
it is true that some methods performed slightly better than others—neural nets, support vector machines, logistic regression—but the differences were small and often the hypothesis that two scorecards were equally good at discriminating could not be rejected.
in the past few years, credit scoring had been changing, as lenders want credit scoring to support their business objectives of profitability and market share.
lenders want to use 'credit scoring' to help make these variable pricing decisions and to determine the long-term profitability of a customer under different lender actions.
one also usually assumes that the score has a monotonic increasing relationship with the probability of being good; hence, if a borrower has a higher score than a second borrower, the first borrower has a higher probability of being good than does the second.
working paper centre for risk research.
consider a very simple example where 1 unit is lent, the cost of capital for the lender isrf (the risk free rate), the loss given default (the fraction of the amount outstanding at default which is finally lost) isld, and the lender will charge an interest rater(p), which is related to the probability p of the applicant being a good.
the overall goal of marketing and credit risk modelling is to improve the profitability of the customer to the financial organisation by improving customer relationship management.
challenge 7: modelling the credit risk of portfolios of consumer loans credit scoring has proved very successful at assessing the relative risk of individual borrowers defaulting.
for a log odds score, equation(4) shows how this probability is related to the credit score of the applicant.
modelling lgd for unsecured personal loans; decision tree approach.
although it should have had some effect on the lending that precipitated the subprime mortgage crisis if it had been in effect then, it would not have dealt with the liquidity risk or the fact that some lenders thought securitisation meant they can absolve themselves of the risks of their lending.
applying this in equation (3) gives thus, a log odds score is the sum of a term depending only on the population odds (spop=lnopop) and a term that depends of the information of the borrowerx.
tang et al (2007) built a survival analysis model that included the interactions between economic and socio-demographic variables to estimate changes in the purchases of pension products.
moreover, it concentrated on a very specific risk—the chance a borrower will become 90 days overdue in their repayments in the next 12 months.
what often happens is that the paper that introduces a new method can show that there is some small improvement by using it rather than using an existing method, but one is always slightly concerned that this may be down to the expertise of the authors in their own method and the fact that they do not take such care with existing methods.
the use of interaction terms and time-dependent coefficients, which proved so successful there, can obviously be taken across to building economy-based credit scorecards.
a comparative study of data mining methods in consumer loans credit scoring management.
however, it also requires much more of credit scoring, with its emphasis on validating the probability of default estimates rather than just ensuring the ranking of borrowers is accurate, which was how credit scoring systems were previously judged.
the methods used are very similar—almost all the methods mentioned in challenge 1 could be applied to build marketing prediction models.
this involves estimating how the score s(t,x) changes over time, which brings us back to challenge 2.
buckinx et al (2007) use the transactional information to estimate the customer loyalty to the organisation, whilevan den poel and larivière (2004) model which product features prevent customers churning to another organisation.
after sometime, the process is repeated and a new scorecard is constructed.
this sample was then used to build a classification system that best separated the goods from the bads, using the characteristics of the loan and the borrower.
microcredit involves giving very small loans to those in poverty in order to help them develop a business, which will sustain them and their family and so bring them out of poverty.
instead, what has happened is that lenders score separately a number of the events that affect profitability.
however, the idea that banks need to build models of the credit risk of their lending and the output of these is used to set their capital requirements—the internal based rating approach—will remain.
tang et al (2007) built a survival analysis model that included the interactions between economic and socio-demographic variables to estimate changes in the purchases of pension products.
full figure and legend (176k) such a growth in consumer lending could not have been possible without an automated approach to assessing the credit risk that the loan to an individual consumer would not be repaid.
a second challenge is that the accord requires estimates of the long-run average of the 12-month default rate (the ttc default rate) for a segment of borrowers while a credit score estimates the default rate in the next 12 months (the pit estimate).
although it should have had some effect on the lending that precipitated the subprime mortgage crisis if it had been in effect then, it would not have dealt with the liquidity risk or the fact that some lenders thought securitisation meant they can absolve themselves of the risks of their lending.
similarly, lenders are more likely to adjust the product or offer alternative or extra products during their relationship with the customer and so are anxious to know what impact such changes will have on the default risk and the profitability of the customer.
credit scoring has proved very successful at assessing the relative risk of individual borrowers defaulting.
a third problem is the basel accord's instance on stress testing, which means predicting the future performance of a portfolio of loans under extreme economic conditions.
this will prove a vital tool in meeting several of the challenges outlined previously.
one advantage of the survival analysis approach is that the competing risk idea means that one can use the same data to estimate several different events.
this will prove a vital tool in meeting several of the challenges outlined previously.
lenders want to use 'credit scoring' to help make these variable pricing decisions and to determine the long-term profitability of a customer under different lender actions.
benchmarking state-of-the-art classification algorithms for credit scoring.
at the other extreme of time scale is payday loans.
this was because the risk models developed in the 1950s and 1960s still seemed to be working well and were surprisingly robust to changes in economic conditions.
for example, there are some marketing models that seek to assess the 'emotions' of the customer from their interactions with the company (coussement and van den poel, 2009) but there is no risk-assessment model that includes the customers 'emotions'.
the traditional way of defining a bad was a borrower who became 90 days overdue in the next 12 months.
the methods used are very similar—almost all the methods mentioned in challenge 1 could be applied to build marketing prediction models.
modelling credit risk of portfolio of consumer loans.
thus, one needs to build scorecards that can respond very quickly to changes in economic and market behaviour and to immediate changes in the borrower's behaviour, and circumstances.
this was because the risk models developed in the 1950s and 1960s still seemed to be working well and were surprisingly robust to changes in economic conditions.
the use of multiple measurements in taxonomic problems.
the review paper by baesens et al (2008) explains how or models and data-mining methods are used for a number of such classification problems, particularly in credit scoring.
the loan is taken out usually at the middle or towards the end of the month and the lender is given a post-dated cheque or a way of accessing the borrower's current account on the day the pay cheque is paid in at the end of the month.
consumer finance, credit scoring, risk-based pricing, classification techniques, customer lifetime valuetop of page consumer finance was the sleeping giant of the modern economy, until it awoke with a vengeance in 2007 and showed what impact problems with the risk assessment of consumer borrowing and the consequent mis-pricing of financial instruments based on this borrowing could have.
moreover, in several countries, one has to be able to explain why one rejects an applicant for credit, and therefore 'black box' methods such as neural nets and support vector machines would not be allowed.
survival analysis methods for personal loan data.
in particular, how does the take probability vary according to the risk score of the applicant and the rate charged by the lender.
this should mean that consumer finance will have a much higher profile in university finance and operational research courses in the future, so that entrants to the finance industry are aware of the need for models and the challenges of building models to solve the problems in this area.
the review paper by baesens et al (2008) explains how or models and data-mining methods are used for a number of such classification problems, particularly in credit scoring.
this is not possible nor sensible for portfolios of consumer loans as default there does not depend on the value of assets but on cash flow considerations and personal attitudes to debt.
it does have the advantage, however, although that it may be possible to use these intermediate components to estimate profitability as well as default risk.
full figure and legend (15 k) total consumer borrowing (calculated by credit action based on bank of england statistics).full figure and legend (176k) such a growth in consumer lending could not have been possible without an automated approach to assessing the credit risk that the loan to an individual consumer would not be repaid.
the growth in the internet and the telephone as ways of undertaking the application process means applications are essentially private and so the product can be 'customised' to depend on the applicants' characteristics, allowing for variable pricing.
moreover such loans are increasingly receiving special legislation which requires proof that their risk-assessment systems are robust.
the tremendous increase in computer storage capacity and the requirement of the basel accord that banks have sufficient historical data to validate their credit scoring models have meant that banks are now willing and able to store much more consumer finance data over much longer periods than they used to do.
total consumer borrowing (calculated by credit action based on bank of england statistics).
moreover, the data that banks are now storing systematically on the outcomes of their collections process are being used to develop models of the sequence and timing of the collections operations so as to optimise the rr (de almeida filho et al, 2008).
under these new regulations, banks are allowed to use the estimates from their own internal risk-rating systems in the formula, which determines the minimum capital they have to set aside to cover the credit risk in their lending.
the initial approaches have looked at linear and logistic regression, non-linear transformation so as to fit beta or log-log distributions, mixture models (especially to identify the 'won't pay' (lgd=1), and even quantile regression ideas (somers and whittaker, 2007).
so a scorecard built on a 2-year-old sample is used to determine which applicants to take for the next few years.
thus, researchers are looking to see whether they can devise classification trees that mimic the performance of the 'black box' and hence give reasons for assuming the applicant is bad and should be rejected.
challenge 6: meeting the regulatory challenge, particularly that in the basel accord as mentioned earlier, the introduction of the new banking regulations, the basel ii accord (bcbs, 2005a), concerning the amount of capital that banks need to set aside to cover their risk, has had a major impact on credit scoring.
those borrowers who do not default on the loan within the chosen time period are 'good'.
lgd is related to rr (ie, the percentage of the debt outstanding which the collections department recovers) by lgd=1-rr.
having decided on what risk is being assessed—say repayments being more than 90 days overdue in the next 12 months—those for which that event occurs are bads and the others are goods.
credit scoring with macroeconomic variables using survival analysis.
propensity scores assess how likely it is that the lender can up sell or cross sell other products to the borrower.
the standard classification methods result in a scorecard and a cut-off so that those with scores above the cut-off are considered good (and would be accepted if they apply) and those below it are classified as bad (and would be rejected if they apply).
until then, despite its importance to the individual consumer, and the fact that it was using an increasing number of those who had trained in operational research (or) and statistics, the modelling underlying it was hardly discussed in any finance course, and the number of research papers in the area was minute compared with that on the corporate credit market or on the pricing of exotic equity-based options.
clearly, for lending to consumers, these internal risk-rating systems are application and behavioural scoring systems.
overtook that of total business borrowing in the late 1980s, and that by 2004 the total borrowing on mortgages had also exceeded the total business borrowing, although that has drawn level again in 2008.figure 2 similarly shows the growth in consumer borrowing in the uk in the 15 years since 1992.
the large samples used to build commercial scorecards, it is likely that most methods will find one of these almost optimal scorecards.
payday loans are small, very short-term loans with extremely high interest rates that are effectively advances on a borrower's next pay packet.
keywords: consumer finance, credit scoring, risk-based pricing, classification techniques, customer lifetime value top of page introduction consumer finance was the sleeping giant of the modern economy, until it awoke with a vengeance in 2007 and showed what impact problems with the risk assessment of consumer borrowing and the consequent mis-pricing of financial instruments based on this borrowing could have.
in fact, it is only worthwhile for banks to move to these internal ratings-based systems, if they use them for their consumer lending, as the main saving in capital compared with the alternative externally imposed capital ratios is in consumer lending.
support vector machines for credit scoring and discovery of significant features.
the critical issue is how to build a model of the credit risk of portfolios of consumer loans, which includes economic and market conditions and so can then be run under the extreme scenarios suggested by the regulators.
by applying monte carlo simulation, using different future economic scenarios, one can then use such a model to estimate portfolio-level default rates.
the first is the need to validate the probability of default predictions that the scorecard makes rather, than the relative ranking of the borrowers, which was what is important in deciding which applicants for credit to accept.
propensity scores assess how likely it is that the lender can up sell or cross sell other products to the borrower.
a two-stage hybrid credit scoring model using artificial neural networks and multivariate adaptive regression splines.
clearly, standard risk-assessment systems cannot work for people who have no history of being advanced credit previously and no involvement with a banking system.
the critical assumption in credit scoring is that the score is all that is required for predicting the probability of the applicant being good.
one can model purchasing as well as attrition and default events separately and then seek to combine them to get a customer lifetime value approach (challenge 10).
this idea of building a large number of models and choosing what the majority predicts could be used with all the classification methodologies, not just classification trees.
for example, there are classification trees in which some of the variables are a 'score' obtained using another method.
under these new regulations, banks are allowed to use the estimates from their own internal risk-rating systems in the formula, which determines the minimum capital they have to set aside to cover the credit risk in their lending.
however, the idea that banks need to build models of the credit risk of their lending and the output of these is used to set their capital requirements—the internal based rating approach—will remain.
so proven ability to handle such short-term loans, and the local economic situation are important features.
it does have the advantage, however, although that it may be possible to use these intermediate components to estimate profitability as well as default risk.
another area in which researchers are seeking to find improved credit scoring methods—that is find the silver bullet which will be 'the' best way of building scorecards—is ensemble methods.
so one needs to be confident in the translation of score to probability of default and to use the standard chi-square and normal distribution-type tests to validate the model by backtesting to compare actual numbers of defaults with predicted ones (bcbs, 2005b).
the models forecast how likely the applicant for credit is to be 'bad' and to default on the loan within a given time period.
attrition scores assess whether the borrower will cancel the loan product shortly.
similarly, the fourth issue that the basel accord has highlighted, the need to model the recovery rate (rr) (or alternatively the loss given default (lgd), where rr=1-lgd) of what percentage of a defaulted loan will subsequently be recovered is also so important that it deserves to be considered as a separate challenge (challenge 8).
so proven ability to handle such short-term loans, and the local economic situation are important features.
another area in which researchers are seeking to find improved credit scoring methods—that is find the silver bullet which will be 'the' best way of building scorecards—is ensemble methods.
credit scoring with a data mining approach based on support vector machines.
this is possible because the use of the internet and the telephone as application channels means the application process is much more private and so varying offers can be made without applicants being aware of what is being offered to others.
firstly, it is not used to support a specific decision but rather it is used by the lender as part of a customer relationship strategy to determine whether to increase credit limits, seek to up sell or cross-sell other products.
translating from point-in-time default rates to through the cycle default rates highlights the time dependency of a score which we outlined in challenge 2.
thus, one needs to build lifetime value models that can cope with the changes in economic and market conditions over long time intervals as well as forecasting the changes in the customer's situation and priorities.
such two-stage models could also be used for other secured loans like car finance.
it is easy then to see that, given the hazard function, we can calculate the probability of default over any time period because if one uses the proportional hazards or accelerated life models of survival analysis, one is able to obtain a score that describes the 'risk' of a consumer defaulting over any and all time horizons.
for example, many of the newer methods essentially construct non-linear scorecards with interactions between characteristics, but experts in the linear approaches to credit scorecard building—logistic and linear regression—tend to know from experience about such interactions and allow for them by building separate scorecards for different segments of the population or by introducing interaction variables.
stress testing retail loan portfolios with dual-time dynamics.
the lender should then accept applicants with high credit scores if and accept applicants with low credit scores if credit scoring began in the 1950s when it was realised that statistical classification methods—the first being discriminant analysis (fisher, 1936 )—could be used to classify loans into goods (non-defaulting) and bads (defaulting), using the characteristics of the loan and the borrowers.
moreover, it concentrated on a very specific risk—the chance a borrower will become 90 days overdue in their repayments in the next 12 months.
all these models concentrate on the purchase aspects—time to and value of next purchase and churn—and do not include the default risk elements that can affect profitability in a major way.
moreover, laws such as the equal credit opportunity acts in the us have outlawed discrimination in the giving of credit unless there are statistical models that can defend such decisions.
similarly, the fourth issue that the basel accord has highlighted, the need to model the recovery rate (rr) (or alternatively the loss given default (lgd), where rr=1-lgd) of what percentage of a defaulted loan will subsequently be recovered is also so important that it deserves to be considered as a separate challenge (challenge 8).
the most powerful characteristics are whether the borrowers have recently been in arrears and the current information from the credit bureau on their overall credit performance.
the growth in the internet and the telephone as ways of undertaking the application process means applications are essentially private and so the product can be 'customised' to depend on the applicants' characteristics, allowing for variable pricing.
firstly, it is not used to support a specific decision but rather it is used by the lender as part of a customer relationship strategy to determine whether to increase credit limits, seek to up sell or cross-sell other products.
decision tree of consumer lending decisions.
the relationship between default and economic cycle for retail portfolios across countries.
these characteristics include socio-economic data such as age and residential status; credit bureau information such as whether the applicant is on the electoral role; and in the case of behavioural scores, performance data such as the number of missed payments in the last 12 months.
moreover, in several countries, one has to be able to explain why one rejects an applicant for credit, and therefore 'black box' methods such as neural nets and support vector machines would not be allowed.
lgd is related to rr (ie, the percentage of the debt outstanding which the collections department recovers) by lgd=1-rr.
for example, up to 5 years ago, most banks had hardly any data on the outcome of their collections and recoveries process, but the need to estimate lgd for all consumer loans means that such data are now carefully recorded and analysed.
there is a sumerian clay tablet recording how two farmers borrowed money to purchase grain with the promise of paying back more at harvest time.
thus, researchers are looking to see whether they can devise classification trees that mimic the performance of the 'black box' and hence give reasons for assuming the applicant is bad and should be rejected.
as mentioned in challenge 2, survival analysis can also be used to introduce economic conditions into scorecards.
the previous discussion though highlighted the credit-rating agencies failure to assess the risk of consumer asset-backed securities and the basel requirements to stress test portfolios of consumer loans.
the standard classification methods result in a scorecard and a cut-off so that those with scores above the cut-off are considered good (and would be accepted if they apply) and those below it are classified as bad (and would be rejected if they apply).
having decided on what risk is being assessed—say repayments being more than 90 days overdue in the next 12 months—those for which that event occurs are bads and the others are goods.
payday loans are small, very short-term loans with extremely high interest rates that are effectively advances on a borrower's next pay packet.
predicting customer loyalty using the internal transactional database.
however, it also requires much more of credit scoring, with its emphasis on validating the probability of default estimates rather than just ensuring the ranking of borrowers is accurate, which was how credit scoring systems were previously judged.
although behavioural scoring was an obvious extension of application scoring, it was also an opportunity missed.
using neural network rule extraction and decision tables for credit-risk evaluation.
this idea of building a large number of models and choosing what the majority predicts could be used with all the classification methodologies, not just classification trees.
the overall goal of marketing and credit risk modelling is to improve the profitability of the customer to the financial organisation by improving customer relationship management.
customer attrition analysis for financial services using proportional hazard models.
moreover, the data that banks are now storing systematically on the outcomes of their collections process are being used to develop models of the sequence and timing of the collections operations so as to optimise the rr (de almeida filho et al, 2008).
given the amount of research that has gone into corporate credit risk models, one suspects that there will be considerably more research into these consumer equivalents, given the realisation by bankers now of how much more is being lent to households than to companies.
thus, there seems to be a great deal more research that is required to develop more appropriate good/bad assessments both in terms of expanding from default to profitability and in removing any pre-defined time horizon on the time over which the customer is assessed.
the assumption that credit worthiness is time independent over intervals of 3 or 4 years meant that credit scores have been built using the socio-demographic characteristics of the borrower, the credit bureau information about the borrower, details of the loan and even the repayment performance of the borrower on the loan, but not using anything about the current economic and market conditions.
moreover, profitability is as much about marketing as about risk assessment, and hence there is a need to combine the work done by financial organisations' marketing and risk assessment or groups.
this model can work both as a parametric model in which the baseline hazard function is of a specific family of distributions or semi parametrically using the results ofcox (1972).
one can do this by sensitivity analysis in which one changes the value of one of the factors that impacts on the model, or by scenario analysis.
this is possible because the use of the internet and the telephone as application channels means the application process is much more private and so varying offers can be made without applicants being aware of what is being offered to others.
in particular, how does the take probability vary according to the risk score of the applicant and the rate charged by the lender.
usage scores assess how much a borrower will use the loan product.
loss given default validation studies on the validation of internal rating systems.
the first is the need to validate the probability of default predictions that the scorecard makes rather, than the relative ranking of the borrowers, which was what is important in deciding which applicants for credit to accept.

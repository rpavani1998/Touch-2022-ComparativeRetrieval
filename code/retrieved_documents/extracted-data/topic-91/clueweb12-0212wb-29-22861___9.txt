while this has created sophisticated classification algorithms, many  do not cope with increasing data set sizes.
unfortunately most of the theoretically well-founded algorithms that have been described in recent years are cubic or worse in the total number of both labeled and unlabeled training examples.
thus we combine probabilistic graphical models and  semi-supervised learning.
the idea is that, for each base image classifier in the ensemble, a random image transformation is generated and applied to all of the images in the labeled training set.
the basic problem is one of identifying who did what to whom for  each predicate in a sentence.
the nearest neighbour problem is of practical significance in a number of fields.
this approach is evaluated  on a benchmark pedestrian detection dataset and shown to be effective.}, pdf = { http://www.cs.waikato.ac.nz/~ml/publications/2007/mayopcm2007.pdf} } @inproceedings{mutter07, author = {s. mutter and b. pfahringer},
the two-tiered algorithm described shows good scalability due to the randomized nature of the first step and the availability of efficient propositional clustering algorithms for the second step.} } @mastersthesis{evans07:_clust_class, author =
{nripendra pradhananga}, title = {effective linear-time feature selection},  school = {
in this setting, the effectiveness of an algorithm cannot simply be assessed by accuracy alone.
in this paper we apply modifications to the standard llgc algorithm to improve efficiency to a point where we can handle datasets with hundreds of thousands of training data.
our experiments demonstrate that both methods are faster, find  smaller subsets and can even increase the classification accuracy.
in this paper  we investigate a simple approach based on randomized propositionalization,  which allows for applying standard clustering algorithms like kmeans to  multi-relational data.
image complexity in this sense refers to the degree to which the target objects are occluded and/or non-dominant (i.e. not in the foreground) in the image, and also the degree to which the images are cluttered with non-target objects.
however, it remains the case that even the most popular of the  techniques proposed for its solution have not been compared against each other.
thus labeling is a two-step process: identify  constituent phrases that are arguments to a predicate, then label those  arguments with appropriate thematic roles.
moreover, instead of assigning semantic roles one at a time, an algorithm is proposed to assign all labels simultaneously; leveraging dependencies between roles and eliminating the problem of duplicate assignment.
abstract = { prediction intervals for class  probabilities are of interest in machine learning because they can quantify the  uncertainty about the class probability estimate for a test instance.
machine learning offers promise of a solution, but the field mainly focusses on achieving high accuracy when data supply is limited.
to find patterns in  these datasets it would be useful to be able to apply modern methods of  classification such as support vector machines.
the frequency and scale of data collection means  that there are now many large datasets being generated.
scaling up  semi-supervised learning: an efficient and effective llgc variant}, booktitle =  {proc 11th pacific-asia conference on knowledge discovery and data mining},  series = {nanjing, china}, year = {2007}, pages = {236-247}, http = { http://dx.doi.org/10.1007/978-3-540-71701-0_25}, publisher = {springer},  abstract = {domains like text classification can easily supply large amounts of  unlabeled data, but labeling itself is expensive.
it observes  that the only valid arguments to a predicate are unembedded constituent phrases  that do not overlap that predicate.
it compares the results of using this framework against using random selection on a large number of classification and regression problems.
machine learning offers promise of a solution, but  the field mainly focusses on achieving high accuracy when data supply is  limited.
so fast, in fact, that the main problem  is making sense of it all.
the hoeffding  tree algorithm is a state-of-the-art method for inducing decision trees from  data streams.
the resulting tree will be the same,  just how it is built is different.
also, many techniques, including the old and popular ones, can be implemented  in a number of ways, and often the different implementations of a technique  have not been thoroughly compared either.
second, the search for each candidate argument is exponential with respect to the number of words in the sentence.
2007.bib @comment{{automatically generated - do not modify!}} @phdthesis{kirkby07:_improv_hoeff_trees, author = {richard kirkby}, title = {
to measure improvement, a comprehensive framework for evaluating the performance of data stream algorithms is developed.
remco r. bouckaert and milan studen{\'y}}, title =
abstract = {abstract.
the results indicate that a voting ensemble of support vector machines, random forests, and boosted decision trees provide the best performance with auc values of up to 0.92 and equal error rate accuracies of up to 85.7\% in stratified 10-fold cross validation experiments on the graz02 complex image dataset.}, pdf = { http://www.cs.waikato.ac.nz/~ml/publications/2007/mayociras2007.pdf} } @inproceedings{mayo07:_random_convol_ensem, author = {michael mayo}, title = {random convolution ensembles}, booktitle = {advances in multimedia information processing - pcm 2007, 8th pacific rim conference on multimedia, lecture notes in computer science 4810}, publisher = {springer}, editor =
finally, we investigate pruning.
{140-151}, year = 2007, series = {warsaw, poland}, publisher = {springer}, pdf  = {http://www.cs.waikato.ac.nz/~ml/publications/2007/kibriyaandfrankpkdd07.pdf },
cover trees are one of the most novel and promising data structures for nearest neighbour search that have been proposed in the literature.} } @mastersthesis{pradhananga07:_effec_linear_time_featur_selec, author =
we have proposed two ensemble creation methods, feature selection ensemble and random feature ensemble.
it involves finding, for a  given point q, called the query, one or more points from a given set of points  that are nearest to the query q.
the modifications are priming of the unlabeled data, and most importantly, sparsification of the similarity matrix.
biological sequence data is, on the one hand, highly structured.
the bayesian model, which is used in conjunction with the ensemble learning algorithm and the standard nearest-neighbour classifier, is evaluated on artificial datasets and modified real datasets.} }
http://www.cs.waikato.ac.nz/~ml/publications/2007/mayopcm2007.pdf} } @inproceedings{mutter07, author = {s. mutter and b. pfahringer}, title =
there are several drawbacks to this general approach.
despite this guarantee, decisions are still subject to limited lookahead and  stability issues.
the study on numeric attributes demonstrates that sacrificing accuracy for space at the local level often results in improved global accuracy.
cover trees are one of the most novel and promising data structures for nearest  neighbour search that have been proposed in the literature.} } @mastersthesis{pradhananga07:_effec_linear_time_featur_selec, author =
thus we combine probabilistic graphical models and semi-supervised learning.
since the initial inception of the problem a  great number of algorithms and techniques have been proposed for its solution.
clustering generally is not  straightforward to evaluate, but preliminary experimental results on a number  of standard ilp datasets show promising results.
often we  are interested in finding an object near to a given query object.
a novel method for creating diverse ensembles of image classifiers is proposed.
the best  implementations of these structures are then compared against each other and  against two other techniques, annulus method and cover trees.
annulus method is  an old technique that was rediscovered during the research for this thesis.
first, a number of methods for handling continuous numeric features are compared.
we apply our models to genotype- phenotype modelling problems.
we present two methods which have the interesting complementary properties that one method performs well to prove that $t$ is implied by $l$, while the other performs well to prove that $t$ is not implied by $l$. however, both methods do not well perform the opposite.
second, tree prediction strategy is investigated to evaluate the utility of various methods.
{bernhard  pfahringer and geoffrey holmes and richard kirkby}, title = {new options for  hoeffding trees}, booktitle =
it compares the results of using this framework against using  random selection on a large number of classification and regression problems.
racing algorithms for conditional independence  inference}, journal = {int. j. approx.
in best-first top-down  induction of decision trees, the best split is added in each step (e.g. the  split that maximally reduces the gini index).
finally, the possibility of improving accuracy using ensemble methods is explored.
{new options for hoeffding trees}, booktitle = {proc 20th australian conference on artificial intelligence}, year = {2007}, pages = {90-99}, http = { http://dx.doi.org/10.1007/978-3-540-76928-6_11}, publisher = {springer},
unfortunately these methods are computationally expensive, quadratic in the number of data points in fact, so cannot be applied directly.
we show that on some datasets a pruned option tree can be smaller and more accurate than a single tree.}, series = {gold coast, australia} } @inproceedings{dblp:conf/ilp/andersonp07, author =
experimental results are provided as evidence  to show that a combination of the proposed argument identification and  multi-argument classification algorithms outperforms all existing systems that  use the same syntactic information.} } @inproceedings{kibriya07:_empir_compar_exact_neares_neigh_algor, author  = {ashraf m. kibriya and eibe frank}, title = {
pre-pruning, post-pruning, cart-pruning can be performed this way to compare.} } @mastersthesis{yu07:_predic_inter_class_probab, author =
department of computer science, university of waikato}, year = 2007,  http
results show that the clustered datasets are on average fifty percent smaller  than the original datasets without loss of classification accuracy which is  significantly better than random selection.
the frequency and scale of data collection means that there are now many large datasets being generated.
exhaustive search is impractical since it searches  every possible combination of features.
the two-tiered algorithm  described shows good scalability due to the randomized nature of the first step  and the availability of efficient propositional clustering algorithms for the  second step.} } @mastersthesis{evans07:_clust_class, author =
in particular, issues are encountered when applying the popular boosting method to streams. } } @article{bouckaert07:_racin, author = {
both methods apply a feature selection algorithm to create individual classifiers of the ensemble.
chi-san althon lin}, title = {syntax-driven argument identification and multi-argument classification for semantic role labeling}, school = {department of computer science, university of waikato}, year = 2007, http = {http://hdl.handle.net/10289/2602},
the resulting tree will be the same, just how it is built is different.
existing systems for semantic role  labeling use machine learning methods to assign roles one-at-a-time to  candidate arguments.
abstract = { modern information technology allows information to be collected at a far greater rate than ever before.
their ingenuity stems from updating sufficient statistics, only  addressing growth when decisions can be made that are guaranteed to be almost  identical to those that would be made by conventional batch learning methods.
we investigate a heuristic, forward, wrapper-based approach, called linear sequential selection, which limits the search space at each iteration of the feature selection process.
it observes that the only valid arguments to a predicate are unembedded constituent phrases that do not overlap that predicate.
improving hoeffding trees}, school = {department of computer science,  university of waikato}, year = 2007, http = {http://hdl.handle.net/10289/2568},
this is in contrast to the  standard depth-first traversal of a tree.
improving hoeffding trees}, school = {department of computer science, university of waikato}, year = 2007, http = {http://hdl.handle.net/10289/2568},
some empirical evidence is provided that suggest this racing algorithms method performs considerably better than an existing method based on so-called skeletal characterization of the respective implication.
thus labeling is a two-step process: identify constituent phrases that are arguments to a predicate, then label those arguments with appropriate thematic roles.
the experimental results  provide meaningful comparisons of accuracy and processing speeds between  different modifications of the hoeffding tree algorithm under various memory  limits.
exhaustive search is impractical since it searches every possible combination of features.
also, many techniques, including the old and popular ones, can be implemented in a number of ways, and often the different implementations of a technique have not been thoroughly compared either.
the algorithm is called randomized linear sequential selection.
the former to handle structured data and the latter  to deal with unlabelled data.
reasoning}, volume = 45, number = 2,  year = 2007, pages = {386-401}, pdf = { http://www.cs.waikato.ac.nz/~ml/publications/2007/racing2.pdf}, abstract = {
some empirical  evidence is provided that suggest this racing algorithms method performs  considerably better than an existing method based on so-called skeletal  characterization of the respective implication.
{a discriminative approach to structured biological data}, booktitle = {proc nzcsrsc'07, the fifth new zealand computer science research student conference}, year = 2007, month = {april}, series = {
first, more than one candidate can be assigned the same role, which is  undesirable.
biological sequence data is, on the one  hand, highly structured.
clustering relational data based on randomized  propositionalization}, booktitle = {proc 17th international conference on  inductive logic programming}, year = {2007}, pages = {39-48}, http = { http://dx.doi.org/10.1007/978-3-540-78469-2_8}, series = {corvallis, or},  publisher = {springer},
given that semantic role labeling occurs  after parsing, this thesis proposes an algorithm that systematically traverses  the parse tree when looking for arguments, thereby eliminating the vast  majority of impossible candidates.
the bayesian model, which is used in conjunction with the ensemble  learning algorithm and the standard nearest-neighbour classifier, is evaluated  on artificial datasets and modified real datasets.} }
semi- supervised learning tries to exploit this abundance of unlabeled training data to improve classification.
{proc 20th australian conference on artificial  intelligence}, year = {2007}, pages = {90-99}, http = { http://dx.doi.org/10.1007/978-3-540-76928-6_11}, publisher = {springer},
{hoeffding trees are state-of-the-art for processing high-speed data streams.
{clustering for classification}, school = {department of computer science, university of waikato}, year = 2007, http = {http://hdl.handle.net/10289/2403}, abstract =
we have proposed two ensemble creation  methods, feature selection ensemble and random feature ensemble.
they also show that there is no  free lunch, for each dataset it is important to choose a clustering method  carefully.} } @mastersthesis{kibriya07:_fast_algor_neares_neigh_searc, author =
the base classifiers are then learned using features extracted from these randomly transformed versions of the training data, and the result is a highly diverse ensemble of image classifiers.
the classification  learning task requires selection of a subset of features to represent patterns  to be classified.
in this paper we investigate a simple approach based on randomized propositionalization, which allows for applying standard clustering algorithms like kmeans to multi-relational data.
in this paper we explore hoeffding option trees, a regular hoeffding tree containing additional option nodes that allow several tests to be applied, leading to multiple hoeffding trees as separate paths.
{this paper introduces the first author's phd pro ject which has  just got out of its initial stage.
we introduce randomization in the search space.
this gives rise to a parallel algorithm in which both methods race against each other in order to determine effectively whether $t$ is or is not implied.
{clustering for classification}, school = {department of computer science,  university of waikato}, year = 2007, http = {http://hdl.handle.net/10289/2403},  abstract =
http://hdl.handle.net/10289/2315}, abstract = {the classification learning task requires selection of a subset of features to represent patterns to be classified.
improvements to many aspects of the hoeffding tree  algorithm are demonstrated.
the best implementations of these structures are then compared against each other and against two other techniques, annulus method and cover trees.
we show how to control tree growth in order to generate a mixture of paths, and empirically determine a reasonable number of paths.
since the initial inception of the problem a great number of algorithms and techniques have been proposed for its solution.
we describe how random rules are generated and then turned into boolean-valued features.
a random decision tree ensemble  learning algorithm is also proposed, whose prediction output constitutes the  neighbourhood that is used by the bayesian model to produce a pi for the test  instance.
the results indicate  that a voting ensemble of support vector machines, random forests, and boosted  decision trees provide the best performance with auc values of up to 0.92 and  equal error rate accuracies of up to 85.7\% in stratified 10-fold cross  validation experiments on the graz02 complex image dataset.}, pdf = { http://www.cs.waikato.ac.nz/~ml/publications/2007/mayociras2007.pdf} } @inproceedings{mayo07:_random_convol_ensem, author = {michael mayo},  title = {random convolution ensembles}, booktitle = {advances in multimedia  information processing - pcm 2007, 8th pacific rim conference on multimedia,  lecture notes in computer science 4810}, publisher = {springer}, editor =
2007.bib @comment{{automatically generated - do not modify!}} @phdthesis{kirkby07:_improv_hoeff_trees, author = {richard kirkby},  title = {
bernhard pfahringer and geoffrey holmes and richard kirkby}, title =
this thesis seeks to obviate these problems by approaching  semantic role labeling as a multi-argument classification process.
this is in contrast to the standard depth-first traversal of a tree.
remco r. bouckaert and milan  studen{\'y}}, title
the algorithm is called randomized linear sequential  selection.
hamilton, new zealand}, pdf = { http://www.cs.waikato.ac.nz/~ml/publications/2007/mutternzcsrsc07_short.pdf}, abstract =
{reuben evans}, title =
our experiments have shown that both methods work well with  high-dimensional data.} } @mastersthesis{shi07:_best_decis_tree_learn, author =
this thesis seeks to obviate these problems by approaching semantic role labeling as a multi-argument classification process.
existing systems for semantic role labeling use machine learning methods to assign roles one-at-a-time to candidate arguments.
al}, year = 2007, pages = {216-225}, abstract = {
in best-first top-down induction of decision trees, the best split is added in each step (e.g. the split that maximally reduces the gini index).
however, it remains the case that many of the proposed algorithms have not been  compared against each other on a wide variety of datasets.
our results suggest that there is generally little gain  in using metric trees or cover trees instead of kd-trees for the standard nns  problem.} } @inproceedings{mayo07:_effec_class_detec_objec, author = {michael mayo},  title = {
nearest neighbour search (nns) is an old problem that is of practical importance in a number of fields.
given that semantic role labeling occurs after parsing, this thesis proposes an algorithm that systematically traverses the parse tree when looking for arguments, thereby eliminating the vast majority of impossible candidates.
the runtime of heuristic and random searches are better but the problem still persists when dealing with high-dimensional datasets.
{xiaofeng yu}, title = {prediction intervals for class probabilities}, school = {
we also explore the idea of ensemble learning.
in particular, issues are encountered when  applying the popular boosting method to streams. } } @article{bouckaert07:_racin, author = {
in this article, we consider the computational aspects of deciding whether a conditional independence statement $t$ is implied by a list of conditional independence statements $l$ using the implication related to the method of structural imsets.
moreover, instead of assigning semantic  roles one at a time, an algorithm is proposed to assign all labels  simultaneously; leveraging dependencies between roles and eliminating the  problem of duplicate assignment.
the idea is  that, for each base image classifier in the ensemble, a random image  transformation is generated and applied to all of the images in the labeled  training set.
semi- supervised learning  tries to exploit this abundance of unlabeled training data to improve  classification.
this thesis proposes a framework whereby a variety  of clustering methods can be used to summarise datasets, that is, reduce them  to a smaller but still representative dataset so that these advanced methods  can be applied.
given the unobservability of class probabilities, a bayesian  approach is employed to derive a complete distribution of the class probability  of a test instance based on a set of class observations of training instances  in the neighbourhood of the test instance.
this thesis proposes a probabilistic model for calculating such prediction intervals.
{hoeffding trees are state-of-the-art for processing high-speed data  streams.
our experiments have shown that both methods work well with high-dimensional data.} } @mastersthesis{shi07:_best_decis_tree_learn, author =
clusters generated without  class information usually agree well with the true class labels of cluster  members, i.e. class distributions inside clusters generally differ  significantly from the global class distributions.
this research  attempts to fill this gap to some extent by presenting a detailed empirical  comparison of three prominent data structures for exact nns: kd-trees, metric  trees, and cover trees.
clustering of relational data has so far  received a lot less attention than classification of such data.
the objective of this project is to  investigate whether it is possible to determine an appropriate tree size on  practical datasets by combining best-first decision tree growth with  cross-validation-based selection of the number of expansions that are  performed.
the problem  is old, and a large number of solutions have been proposed for it in the  literature.
annulus method is an old technique that was rediscovered during the research for this thesis.
in this paper we apply  modifications to the standard llgc algorithm to improve efficiency to a point  where we can handle datasets with hundreds of thousands of training data.
{a discriminative approach to structured biological data}, booktitle = {proc  nzcsrsc'07, the fifth new zealand computer science research student  conference}, year = 2007, month = {april}, series = {
both methods  apply a feature selection algorithm to create individual classifiers of the  ensemble.
pre-pruning, post-pruning, cart-pruning can be performed this way to  compare.} } @mastersthesis{yu07:_predic_inter_class_probab, author =
the idea  is that all likely class probability values of the test instance are included,  with a pre-specified confidence level, in the calculated prediction interval.
third, single-role assignment  cannot take advantage of dependencies known to exist between semantic roles of  predicate arguments, such as their relative juxtaposition.
in order to simulate continuous operation, classes of synthetic data are generated providing an evaluation on a large scale.
title = {best-first decision tree learning}, school = {department of computer science, university of waikato}, year = 2007, http = { http://hdl.handle.net/10289/2317},
the prediction strategy shown to perform best adaptively chooses between standard majority class and naive bayes prediction in the leaves.
abstract = { prediction intervals for class probabilities are of interest in machine learning because they can quantify the uncertainty about the class probability estimate for a test instance.
on the other hand there are large amounts of  unlabelled data.
we then empirically evaluate a spectrum of hoeffding tree variations: single trees, option trees and bagged trees.
the objective of this project is to investigate whether it is possible to determine an appropriate tree size on practical datasets by combining best-first decision tree growth with cross-validation-based selection of the number of expansions that are performed.
nearest neighbour search (nns) is an old problem that  is of practical importance in a number of fields.
{ashraf masood kibriya}, title = {fast algorithms for nearest neighbour  search}, school = {department of computer science, university of waikato}, year  = 2007, http = {http://hdl.handle.net/10289/2463}, abstract = {
our experiments demonstrate that both methods are faster, find smaller subsets and can even increase the classification accuracy.
department of computer science, university of waikato}, year = 2007, http
the nearest  neighbour problem is of practical significance in a number of fields.
the base classifiers are then learned using features extracted  from these randomly transformed versions of the training data, and the result  is a highly diverse ensemble of image classifiers.
this is because the performance of the classifier and the  cost of classification are sensitive to the choice of the features used to  construct the classifier.
this thesis proposes a probabilistic model for calculating such prediction  intervals.
the hoeffding tree algorithm is a state-of-the-art method for inducing decision trees from data streams.
second, tree prediction strategy is investigated  to evaluate the utility of various methods.
we report promising results on large  text classification problems.} } @inproceedings{dblp:conf/ausai/pfahringerhk07, author =
we apply our models to genotype- phenotype  modelling problems.
effective classifiers for detecting objects}, booktitle = {proc.
http://hdl.handle.net/10289/2315}, abstract = {
we also  explore the idea of ensemble learning.
furthermore, unlike previous  methods, the method is able to handle more than five variables.} } @phdthesis{lin07:_syntax, author =
first, a number of methods for handling continuous  numeric features are compared.
we describe how random rules are generated and then  turned into boolean-valued features.
abstract =  {semantic role labeling is an important stage in systems for natural language  understanding.
title = {best-first decision tree learning}, school = {department of computer  science, university of waikato}, year = 2007, http = { http://hdl.handle.net/10289/2317}, abstract = {
title = {prediction intervals for class probabilities}, school = {department of  computer science, university of waikato}, year = 2007, http = { http://hdl.handle.net/10289/2436},
improvements to many aspects of the hoeffding tree algorithm are demonstrated.
so fast, in fact, that the main problem is making sense of it all.
furthermore, unlike previous methods, the method is able to handle more than five variables.} } @phdthesis{lin07:_syntax, author =
experimental results are provided as evidence to show that a combination of the proposed argument identification and multi-argument classification algorithms outperforms all existing systems that use the same syntactic information.} } @inproceedings{kibriya07:_empir_compar_exact_neares_neigh_algor, author = {ashraf m. kibriya and eibe frank}, title = {
clustering generally is not straightforward to evaluate, but preliminary experimental results on a number of standard ilp datasets show promising results.
the idea is that all likely class probability values of the test instance are included, with a pre-specified confidence level, in the calculated prediction interval.
clusters generated without class information usually agree well with the true class labels of cluster members, i.e. class distributions inside clusters generally differ significantly from the global class distributions.
the study on numeric attributes demonstrates that sacrificing accuracy  for space at the local level often results in improved global accuracy.
while this has created sophisticated classification algorithms, many do not cope with increasing data set sizes.
finally, the possibility of  improving accuracy using ensemble methods is explored.
in particular we predict the set of single nucleotide polymorphisms which underlie a specific phenotypical trait.} } @inproceedings{dblp:conf/pakdd/pfahringerlr07, author = {bernhard pfahringer and claire leschi and peter reutemann}, title = {
abstract = { modern information technology allows information to be collected  at a far greater rate than ever before.
however, it remains the case that many of the proposed algorithms have not been compared against each other on a wide variety of datasets.
third, single-role assignment cannot take advantage of dependencies known to exist between semantic roles of predicate arguments, such as their relative juxtaposition.
this approach is evaluated on a benchmark pedestrian detection dataset and shown to be effective.}, pdf = {
given the unobservability of class probabilities, a bayesian approach is employed to derive a complete distribution of the class probability of a test instance based on a set of class observations of training instances in the neighbourhood of the test instance.
consideration needs to be given to the memory available to the algorithm and the speed at which data is processed in terms of both the time taken to predict the class of a new data sample and the time taken to include this sample in an incrementally updated classification model.
often we are interested in finding an object near to a given query object.
the experimental results provide meaningful comparisons of accuracy and processing speeds between different modifications of the hoeffding tree algorithm under various memory limits.
unfortunately most of the theoretically well-founded algorithms  that have been described in recent years are cubic or worse in the total number  of both labeled and unlabeled training examples.
they also show that there is no free lunch, for each dataset it is important to choose a clustering method carefully.} } @mastersthesis{kibriya07:_fast_algor_neares_neigh_searc, author = {ashraf masood kibriya}, title = {fast algorithms for nearest neighbour search}, school = {department of computer science, university of waikato}, year = 2007, http = {http://hdl.handle.net/10289/2463}, abstract = {
a novel method  for creating diverse ensembles of image classifiers is proposed.
clustering of relational data has so far received a lot less attention than classification of such data.
this thesis proposes a framework whereby a variety of clustering methods can be used to summarise datasets, that is, reduce them to a smaller but still representative dataset so that these advanced methods can be applied.
{clustering relational data based on randomized propositionalization}, booktitle = {proc 17th international conference on inductive logic programming}, year = {2007}, pages = {39-48}, http = { http://dx.doi.org/10.1007/978-3-540-78469-2_8}, series = {corvallis, or}, publisher = {springer}, abstract = {
hamilton, new zealand},  pdf = { http://www.cs.waikato.ac.nz/~ml/publications/2007/mutternzcsrsc07_short.pdf},  abstract =
in  this article, we consider the computational aspects of deciding whether a  conditional independence statement $t$ is implied by a list of conditional  independence statements $l$ using the implication related to the method of  structural imsets.
abstract = {semantic role labeling is an important stage in systems for natural language understanding.
the  modifications are priming of the unlabeled data, and most importantly,  sparsification of the similarity matrix.
the former to handle structured data and the latter to deal with unlabelled data.
{grant anderson and  bernhard pfahringer}, title
the problem is old, and a large number of solutions have been proposed for it in the literature.
the basic problem is one of identifying who did what to whom for each predicate in a sentence.
in particular we predict the set of single nucleotide  polymorphisms which underlie a specific phenotypical trait.} } @inproceedings{dblp:conf/pakdd/pfahringerlr07, author = {bernhard  pfahringer and claire leschi and peter reutemann}, title = {
this research presents a detailed  investigation of different implementations of two popular nearest neighbour  search data structures, kdtrees and metric trees, and compares the different  implementations of each of the two structures against each other.
we report promising results on large text classification problems.} } @inproceedings{dblp:conf/ausai/pfahringerhk07, author =
second, the search for each candidate argument is exponential with  respect to the number of words in the sentence.
when the data set sizes get to a point where they could be considered to represent a continuous supply, or data stream, then incremental classification algorithms are required.
{syntax-driven argument identification and multi-argument classification for  semantic role labeling}, school = {department of computer science, university  of waikato}, year = 2007, http = {http://hdl.handle.net/10289/2602},
this is because the performance of the classifier and the cost of classification are sensitive to the choice of the features used to construct the classifier.
to measure  improvement, a comprehensive framework for evaluating the performance of data  stream algorithms is developed.
the aim of this thesis is to improve this algorithm.
{advances in technology have provided industry with an array of devices for collecting data.
effective classifiers for detecting objects}, booktitle = {proc. of  the fourth international conference on computational intelligence, robotics,  and autonomous systems (ciras '07)}, year = 2007, abstract = {several  state-of-the-art machine learning classifiers are compared for the purposes of  object detection in complex images, using global image features derived from  the ohta color space and local binary patterns.
however, it remains the case that even the most popular of the techniques proposed for its solution have not been compared against each other.
{this paper introduces the first author's phd pro ject which has just got out of its initial stage.
the  prediction strategy shown to perform best adaptively chooses between standard  majority class and naive bayes prediction in the leaves.
despite this guarantee, decisions are still subject to limited lookahead and stability issues.
in order to simulate  continuous operation, classes of synthetic data are generated providing an  evaluation on a large scale.
to find patterns in these datasets it would be useful to be able to apply modern methods of classification such as support vector machines.
the ensemble method investigation shows that combining trees can be worthwhile, but only when sufficient memory is available, and improvement is less likely than in traditional machine learning.
this research presents a detailed investigation of different implementations of two popular nearest neighbour search data structures, kdtrees and metric trees, and compares the different implementations of each of the two structures against each other.
it involves finding, for a given point q, called the query, one or more points from a given set of points that are nearest to the query q.
results show that the clustered datasets are on average fifty percent smaller than the original datasets without loss of classification accuracy which is significantly better than random selection.
we show how  to control tree growth in order to generate a mixture of paths, and empirically  determine a reasonable number of paths.
our results suggest that there is generally little gain in using metric trees or cover trees instead of kd-trees for the standard nns problem.} } @inproceedings{mayo07:_effec_class_detec_objec, author = {michael mayo}, title = {
chi-san althon lin}, title =
when the data set sizes get to a  point where they could be considered to represent a continuous supply, or data  stream, then incremental classification algorithms are required.
{grant anderson and bernhard pfahringer}, title =
{racing algorithms for conditional independence inference}, journal = {int. j. approx.
and fourth, execution times for existing algorithm are excessive, making them unsuitable for real-time use.
{advances in technology have provided industry with an array of  devices for collecting data.
image complexity in this sense  refers to the degree to which the target objects are occluded and/or  non-dominant (i.e. not in the foreground) in the image, and also the degree to  which the images are cluttered with non-target objects.
we investigate a heuristic, forward, wrapper-based  approach, called linear sequential selection, which limits the search space at  each iteration of the feature selection process.
we present two methods which have the interesting  complementary properties that one method performs well to prove that $t$ is  implied by $l$, while the other performs well to prove that $t$ is not implied  by $l$. however, both methods do not well perform the opposite.
in this  setting, the effectiveness of an algorithm cannot simply be assessed by  accuracy alone.
{nripendra pradhananga}, title = {effective linear-time feature selection}, school = {
the runtime of heuristic and random  searches are better but the problem still persists when dealing with  high-dimensional datasets.
department of computer science, university of waikato}, year = 2007, http = { http://hdl.handle.net/10289/2436},
an empirical comparison of exact  nearest neighbour algorithms}, booktitle = {proc 11th european conference on  principles and practice of knowledge discovery in databases}, pages =
an empirical comparison of exact nearest neighbour algorithms}, booktitle = {proc 11th european conference on principles and practice of knowledge discovery in databases}, pages = {140-151}, year = 2007, series = {warsaw, poland}, publisher = {springer}, pdf = {http://www.cs.waikato.ac.nz/~ml/publications/2007/kibriyaandfrankpkdd07.pdf },
first, more than one candidate can be assigned the same role, which is undesirable.
we then empirically evaluate a spectrum  of hoeffding tree variations: single trees, option trees and bagged trees.
in this paper we explore hoeffding option trees, a regular  hoeffding tree containing additional option nodes that allow several tests to  be applied, leading to multiple hoeffding trees as separate paths.
of the fourth international conference on computational intelligence, robotics, and autonomous systems (ciras '07)}, year = 2007, abstract = {several state-of-the-art machine learning classifiers are compared for the purposes of object detection in complex images, using global image features derived from the ohta color space and local binary patterns.
reasoning}, volume = 45, number = 2, year = 2007, pages = {386-401}, pdf = { http://www.cs.waikato.ac.nz/~ml/publications/2007/racing2.pdf}, abstract = {
within the framework memory size is fixed in order to simulate realistic application scenarios.
scaling up semi-supervised learning: an efficient and effective llgc variant}, booktitle = {proc 11th pacific-asia conference on knowledge discovery and data mining}, series = {nanjing, china}, year = {2007}, pages = {236-247}, http = { http://dx.doi.org/10.1007/978-3-540-71701-0_25}, publisher = {springer}, abstract = {domains like text classification can easily supply large amounts of unlabeled data, but labeling itself is expensive.
within the framework memory size is fixed in  order to simulate realistic application scenarios.
their ingenuity stems from updating sufficient statistics, only addressing growth when decisions can be made that are guaranteed to be almost identical to those that would be made by conventional batch learning methods.
consideration needs to be given to the memory available to the  algorithm and the speed at which data is processed in terms of both the time  taken to predict the class of a new data sample and the time taken to include  this sample in an incrementally updated classification model.
we show that on some datasets a pruned option  tree can be smaller and more accurate than a single tree.}, series = {gold  coast, australia} } @inproceedings{dblp:conf/ilp/andersonp07, author =
this research attempts to fill this gap to some extent by presenting a detailed empirical comparison of three prominent data structures for exact nns: kd-trees, metric trees, and cover trees.
unfortunately these methods are  computationally expensive, quadratic in the number of data points in fact, so  cannot be applied directly.
a random decision tree ensemble learning algorithm is also proposed, whose prediction output constitutes the neighbourhood that is used by the bayesian model to produce a pi for the test instance.
we introduce randomization in  the search space.
the ensemble method  investigation shows that combining trees can be worthwhile, but only when  sufficient memory is available, and improvement is less likely than in  traditional machine learning.
this gives rise  to a parallel algorithm in which both methods race against each other in order  to determine effectively whether $t$ is or is not implied.
and fourth,  execution times for existing algorithm are excessive, making them unsuitable  for real-time use.
on the other hand there are large amounts of unlabelled data.
it is widely used in  academia for teaching and research.
these blocks represent the levels of the factor associated with the layer.
many sensors  are now multispectral or even hyperspectral, increasing the size of the data  set by up to $10^2$.
statistical and  machine learning methods can provide an efficient means of estimating the  bioreponses of these compounds in order to expedite drug design.
a usual approach  is the multi-level random-effects models.
in the coming decade the census bureau is planning  a program which integrates three components: collecting long form data in a  national survey, the american community survey (acs); enhancing the master  address file and tiger, a geographic database to bring into compliance with gps  coordinates; and a re-engineered 2010 short form census.
one evaluation process generates different trees by weighted random selection of prioritized variables at each partitioning step.
a gui  was constructed to support the numerical and graphical input/output structure.
however, this testing method is used based on intuition  rather than probability theory.
computationally intensive techniques for a fully bayesian, decision  theoretic approach to financial forecasting and portfolio selection author: andrew simpson (university of newcastle) darren j.  wilkinson (university of newcastle)
as a result we have to  account for rank deficiency of the system matrix due to spatial or temporal  correlation of some variables.
we examine a class of algorithms for sampling  relational data, analyze the characteristics of the samples they produce, and  show that all have important pitfalls for unwary researchers.
subsequently, it has been extensively  investigated in literatures.
the method is useful for number of other estimation problems.
using both the standard bootstrap and the  moving block bootstrap, these new bootstrap control charts are valid for  monitoring independent data as well as dependent data.
we tested our method  for different cost models and application domains from the uci ml repository  and, for the majority of the tasks, its probability estimates improve over the  probability estimates of both decision trees and bagged probability estimation  trees (unpruned, uncollapsed, and smoothed decision trees; b-pets) - one of the  best existing class probability estimators.
abstract: we establish the relationship between the support vector machine and the  bayes rule.
natascha  vukasinovic (utah state university)
data were also obtained from the u.s. bureau of the  census' 1995-1998 current population surveys and adjusted total population data  files.
we consider the asymptotic  expansion of $r=\prod_{i=1}^p x_i^{a_i}$ where $x_i$ are positive random  variables, which are independent but not identically distributed.
in this presentation, we describe how we have created an environment for interactive statistical documents.
our algorithm builds on the basic ideas of the lazy decision tree  classification algorithm introduced by friedman et al. (1996).
the situation of interest here is the  case when the number of clusters is small but the number of hierarchical levels  in each cluster is large.
particular interests focus on the heterogeneous and  unbalanced clustering.
web-based reporting in industry,  ``live'' documents in research, and interactive worksheets in education  material are in many ways ideal uses of the web.
the effect due to multiple stripping of  the membrane was also inspected.
the accuracy of the above approximations are illustrate in several  examples and the results are compared to the exact, when available, or monte  carlo results.
compression and analysis of very large imagery data sets using spatial  statistics
this approach can be easily extended to unbalanced hierarchical structures and heterogeneous clusters.
the u.s. census bureau's maf/tiger system, internal and external  interfaces author: robert marx (u.s. census bureau)
accomplishing the needed improvements will increase the effectiveness of the  sponsoring and participating organizations that depend on the census bureau's  statistical data and its geographic infrastructure.
conditional on the fitted common shape model, it is possible to fit and  test nonlinear mixed effects using standard methods.
when  we apply rough sets to pidd using rosetta software, the predictive accuracy is  82.6\%, which is better than other data mining methods that we are aware of.
statistical software and web-based applications (contributed session ) session time: saturday, june 16th, 9:00am - 10:45am room
the centers, $c$, are determined by qr  factorization with column pivoting of right singular vectors of $g$. it is  shown that the selected $c$'s reflect the best compromise between structural  stability and residual minimization.
``squashing flat files flatter''
decision-support and forecasting (contributed session)
the  generalized central limit theorem is used to provide an approximation for the  distribution of r and study conditions under which this approximation is valid.
bootqc is a microsoft excel add-in file written in visual basic for  applications (vba), and its graphical user interfaces can provide an easy  access to online data analysis by applying bootstrap methods to generating  statistical quality control charts and threshold charts.
since the concerned data is characterized by large numbers of descriptors and  very few data points, we adapt svmr model selection and bagging strategies in  order to avoid overfitting.
the  basic study is a comparison of results run in both assays.
many times several pages of numbers must be used to describe the data, which  makes finding the numerical output that is of interest tedious or difficult.
our  empirical results show that ensembles generated using this approach yield  results that are competitive in accuracy and superior in computational cost.
author: james a. shine (george mason university and us army topgraphic  engineering center)
to cope with such situations we introduce the idea of relaxed  iterative projection generalized additive models (rip-gam).
the new method improved the prediction  accuracy of a single decision tree algorithm.
the  conclusion of the data analysis and its biological interpretation will be  reported.
lazy class probability estimators dragos d. margineantu (oregon state university) thomas g.  dietterich (oregon state university)
using pseudo-predictors to improve the performance of a classification  rule author: majid mojirsheibani (carleton university)
processing images for classification or mapping purposes  thus poses an increasing computational challenge.
location: viejo room an environment for creating interactive statistical documents author: samuel e. buttrey (naval postgraduate school) deborah nolan (university of california, berkeley)
it supports exact and approximate inference, parameter and  structure learning, and static and temporal models.
abstract: statistical analyses of large-scale data can often be hard to interpret.
pls achieves dimension  reduction by constructing components to maximize the covariance between the  response (survival times) and the linear combination of the covariates (gene  expressions) sequentially.
this method uses two continued  fractions for computation, one for the incomplete gamma function and the other  for the complement of the incomplete gamma function.
several satellites currently  offer resolution of 1 meter per pixel or better.
perfect random tree classifiers adele cutler (utah state university) guohua zhao (at\&t) abstract: ensemble classifiers are some of the most accurate general-purpose  classifiers available.
abstract: in this paper we propose an interactive hierarchical visualization system  that, at each level of the hierarchy, provides the user not only with the data  projections, but also with the corresponding magnification factor and  directional curvature plots.
what backfitting  gam and rip-gam have in common is the use of the same s-plus functions provided  for generalized additive modelling such as s(), the spline smoother, and other  features.
visual representations such as maps, graphs, and charts  can aid in this process.
since 1988, many dozens of  publications using various algorithms have resulted in accuracy rates of 66\%  to 81\%.
this latter approach was named data squashing by dumouchel, volinsky, johnson,  cortes and pregibon (1999)
the number of  basis functions ($m$), their widths ($s$) centers ($c$) and weights ($w$).
(the chinese university of hong kong) abstract: we present a temporal radial basis function (rbf) network for recursive  function estimation.
interval computation of gamma probabilities and their inverses author: trong wu (southern illinois university edwardsville) abstract: a new method for computing the gamma cumulative distribution functions  and their inverses is presented in this paper.
skewing blocks with off-target factors is explored.
in this presentation, we describe how we have created an environment for  interactive statistical documents.
these  fixed/mixed non-parametric/semi-parametric models are widely used in practice  to analyze data arise in many areas of investigation such as medicine,  epidemiology, pharmacokinetics and social science.
there are two  basic approaches to this problem: either switch to a different  hardware/software/analysis strategy or else substitute a smaller dataset for  the large one.
furthermore, by  extrapolating the estimates of $\lambda(t)$ into the future, the volterra  integral equation can be used to project the future course of the $\mu(t)$  function.
unfortunately, much of the effort in creating these types of documents  has focussed on re-inventing existing statistical software, and often with  inferior results.
as an application, we show how maxent in graphical models can provide a practical tool for the assessment of model parameters in graphical models that are build in collaboration with domain experts.
uniqueness of the vandermonde matrix with the perfect condition  number is characterized for the numerical behavior of the cf.
multi-layer structured correlation designs for heterogeneous and  unbalanced clustered data author: edward c. chao (insightful corporation) abstract: data with high dimensional hierarchical structures often occur in  longitudinal studies, geographical studies or family studies.
session  time:saturday, june 16th, 9:00am - 10:45am room location:  capistrano room cost growth models for nasa's programs author: tze-san lee (western illinois university) l. dale thomas (national aeronautics and space administration)
abstract: the bayes net toolbox (bnt) for matlab is a software package for directed  graphical models.
an adequate statistic for the exponentially distributed censoring data author: p. s. nair (creighton university) s-c cheng (creighton  university) abstract: in the problem of estimating the parameter of the underlying probability  distribution, a sufficient statistic should be one that summarizes and exhausts  in itself all the relevant information on the parameter that is contained in  the sample.
poisson process with $k(t,u)$  as the probability distribution of the translation from the linear poisson at u  to the planar poisson at t. we then construct the likelihood function for  $\lambda(t)$ in terms of the planar poisson point process at t and the  probability distribution $k(t,u)$.
in some cases  it is possible to compress data several orders of magnitude without  substantially degrading results of subsequent analysis.
a simple data transformation “tamed” the data to come reasonably  close to satisfying those assumptions by several ad hoc criteria.
the bayes net toolbox for matlab author: kevin murphy (uc berkeley)
while the method can be used for many different types of  databases, the proposed method will be described in terms of `occupation' and  `mortality'.
but it is not possible for such a study to describe the bias or  variability of either assay.
the performance of our  proposed network is also demonstrated with a comparison to a classic one.
we have also written some new programs  to fill in the gaps.
in a well-known, publicly  available leukemia patients data set published by gollub, et al., means and  standard deviations varied over several orders of magnitude and the  cost-constrained, modest sample size precludes using asymptotic approximations.
yielding more focused results for the transformed data than for the raw data.
criteria are response dispersion, efficiency, and column  rank.
the result of this research  shows that the program's initial cost estimate is the only significant  predictor for the program's annual absolute cost growth, while the weighted  average of technology readiness level from the program's components is the only  significant predictor for the program's annual relative cost growth.
since macromolecules such as proteins have a very large number of  interdependent torsional angles and the distributions of many of them could be  multimodal and even skewed, we discuss theoretically and computationally  challenging problems that arise in the simultaneous modeling of these angles  based on data from molecular dynamics simulations.
for example, ecotrin is  a brand of aspirin, which belongs to the class of nonsteoridal  anti-inflammatory drug, which in turn belongs to the larger class of analgesic  drugs.
functionality to geographic information systems by means of various coupling  mechanisms between established statistical software packages and a gis.
support vector machine regression in chemometrics author: kristin p. bennett (rensselaer polytechnic institute)
in this  paper, we outline an alternative approach where the functionality is built from  scratch, using a combination of small libraries of dedicated functions, rather  than relying on the full scope of existing software suites.
taming genetic microarray data: a paradigm using a well-known case study author: howard t thaler (memorial sloan-kettering cancer center)
this is  a self-validated computation.
in some cases it is possible to compress data several orders of magnitude without substantially degrading results of subsequent analysis.
abstract: this paper considers the problem of bayesian modelling and forecasting  for multivariate financial time series.
both bootstrap control charts and threshold systems are demonstrated in  an analysis of aviation surveillance data collected by the faa from several air  carriers.
it is  assumed that $m$ is small enough so that $x$ can be processed by the desired  hardware/software, and that the software can make appropriate use of the weight  variable.
formally, data squashing is a form of lossy compression that  attempts to preserve statistical information.
the extra column in $x$ is a column of weights,  $w_i$, $i = 1, \ldots, m$, where $w_i > 0$ and $\sum_i w_i = n$.
abstract: tree ensemble or voting methods using re-sampling technique have been  highlighted recently in statistical classification and data mining.
the paper provides some perspective by raising a few general issues.
these methods are  briefly reviewed in the context of comparing two in-vitro assays for measuring  clotting times where a new method is intended to replace an old method.
for example, prices of related stocks  exhibit dependencies between series, as well as the usual dependencies over  time.
in this paper we propose a tree-based scan statistic for database  surveillance use, to be used when the independent variable can be defined in  the form of a hierarchical tree.
abstract: physicians typically compare a laboratory result for an individual  patient with previous values and population-based reference ranges to determine  the significance of any change.
based on these results, we  developed a probability framework for ic testing method and applied it to  determine the sample size requirement.
by the next census in  2010 advances in technology will provide opportunities for further successes.
analysts that would look at the data on much smaller problems inevitably end up looking at caricatures of the data.
specifically, we use singular value decomposition to study the  complexity of the interpolation ($g$) and design ($s$) matrices which form the  foundation of the sg algorithm proposed here.
for example one paper indicates using a recursive partitioning algorithm on a problem with over two million variables.
likewise, we are looking  for a statistic as one that is exhaustive of all the relevant information on  the future random variable that is available in the current observable random  variables.
parameters of the models are used to compress the original image.
national science foundation overview session time: friday, june 15th, 4:15pm - 6:00pm room location: laguna room session chair: james rosenberger, penn  state unraveling and defining biocomplexity william k michener (university of new mexico) james l rosenberger (penn state university)
the biocomplexity and the biocomplexity in the  environment funding programs will be described and opportunities for  statisticians, mathematicians and computer scientists discussed.
abstract: index of coincidence (ic) testing method has been used in cryptoanalysis  of vigenere cipher.
each record  contains additional information such as store department, price, etc. together  with identifying information such as the particular checkout scanner and, for  some transactions, customer identification.
for evaluating the quality of the predictions in a cost-sensitive context, we employed the paired bdeltacost procedure introduced by margineantu \& dietterich (2000).
algorithms were enhanced with  analytical versions of matrices and were implemented using the java programming  language.
one goal of this software  development is to collect existing programs and make them user friendly so that  more researchers can use them with ease.
methods for  data squashing will be presented and compared.
the proposed procedure involves two  steps: (i) an iterative method for generating a sequence of classifiers from  the initial one, and (ii) a combining procedure that ``pools together'' the  sequence of constructed classifiers in order to produce a new classifier which  is far more effective (in an asymptotic sense) than the initial one.
the proposed method is illustrated by looking at whether death from silicosis is particularly common among specific occupations as classified by the census bureau, without preconceived idea of what specific occupation or group of occupations may be related to increased risk, if any at all.
as another example, in the occupational classification system of the  census, `statisticians' are a subset of `mathematical and computer scientists',  which are a subset `professional specialty occupations', which in turn are a  subset of `managerial and professional specialty occupations'.
by incorporating that information it is then  straightforward to obtain point estimates of the bias and variability of the  new assay.
positive weight factors  used in model evaluation were read from a stored table.
we finish our  talk with recommendations for future similar courses.
more generally the views can be smoothed regression surfaces and various approaches can be used to graphically define multivariate partitions.
it is also expected that this  method performs reasonably well with fewer number of re-samples compared to the  popular bagging or boosting methods.
the “optimal” model was chosen as the one with the  smallest, statistically significant, weighted estimated risk as compared to the  null (no-change) model.
lin (1989) to determine if they are ``in  agreement''.
in order for some of  these algorithms to work efficiently, a variety of kalman filtering, smoothing  and simulation-smoothing techniques are required, as na{\"\i}ve  implementations suffer from problems associated with slow mixing and  convergence.
the entropy evaluation  requires probabilistic modeling of conformations in the internal coordinates.
however, statistical properties  of the data produced present challenges in the interpretation of statistical  analyses and identification of true outliers associated with causal genes.
theoretical and computational challenges in entropy evaluation of  macromolecules author: harshinder singh (west virginia university) e. james harner (west virginia university) eugene demchuk (niosh/held)
the performance of various  bootstrap and exact-distribution estimates is compared, first in the context of  ``demonstrating agreement'', and in the context examining if the new assay has  actually improved performance over the old assay.
the importance of the mathematical and statistical  sciences can be seen for integrating the components of reductionist research  into a quantitative model which provides a predictive outcome with appropriate  measures of uncertainly.
the  procedure has been tested on real mr data, with volume estimates within 6\% of  those derived from doctors' hand segmentations.
in these cases, a layer of correlation corresponds to modeling  the correlation structure within a factor in the hierarchical structures.
thus projection pursuit or related algorithms can help the analysts to select views.
the view used for partitioning may not be well chosen.
a case study from prostate cancer and simulation studies show this approach is more efficient than the existing gee methods and multivariate methods.
the demonstration uses the software bootqc (liu and teng (2000)).
the rules in these stochastic parameterized  grammars (spg’s) each have the power of a boltzmann probability  distribution, suggesting the use of mean field theory and other methods for  model inversion.
input to the computer program included a vector of sequential  readings and desired statistical significance level.
designing experiments for causal networks author: william d heavlin (advanced micro devices)
most of the existing methods for constructing  control charts are parametric in nature, and their applicability is much  restricted by the requirement of predetermined models such as normal  distributions.
room location: laguna room a statistical view of the support vector machine author: yi lin (university of wisconsin, madison)
adel k. el-naggar (university of texas m.d. anderson cancer center)
a  layer may consist of multiple blocks, and blocks of the same layer have the  same parameterization.
a nonparametric alternative based on bootstrap methods was  proposed in liu and tang (1996).
in this paper we introduce the use the  proportional hazard (ph) regression (cox 1972) in conjunction with dimension  reduction by partial least squares (pls), since the number of covariates $p$  exceeds the number of samples $n$. this setting is typical of gene expression  data from dna microarrays.
the multivariate dynamic linear state space models of west and harrison  (1997) are often appropriate for explaining log-price behaviour.
the initial phase of cnmap will provide reports at the state level for a  number of nutritional indicators such as: (1) the percentage of individuals  meeting recommended daily allowances of a select group of nutrients; (2) the  percentage of individuals meeting minimum requirements for pyramid servings  food groups; (3) the percentage of american households receiving food stamps;  (4) the percentage of individuals using supplements.
it is a hierarchical, regularized method based on classical  statistics that produces a rigorous confidence interval for lesion volume.
this paper describes the use of spatial statistics to compress the size of  large 1-meter imagery data sets.
and finally she performs statistical computations and renders visual displays using the statistical software that is embedded within the reader's browser.
the author focuses on the presentation and display of these components,  including the usual multi-media elements such as text, images and sounds.
furthermore, by using cauchy inequality and one-sample u-statistic  theory, we proved that ic is gaussian distributed.
these blocks represent the levels of the factor  associated with the layer.
data-driven and optimal denoising of a signal and recovery of its  derivative using multiwavelets author: nathaniel tymes, jr (university of new mexico) sam  efromovich (university of new mexico)
model complexity based design of radial basis function networks with  data mining applications author: miyoung shin (syracuse university and etri(korea)) amrit  l. goel (syracuse university ) abstract: radial basis function (rbf) models, a particular class of neural  networks, have recently become popular for pattern recognition tasks because of  their fast learning capability and good mathematical properties (best and  universal approximation).
in order to  compute good probability estimates, multiple tests are performed in each node  on the query instance.
abstract: one of the chief obstacles to effective data mining is the clumsiness of  managing and analyzing data in very large files.
a similar basic problem of statistics is that of predicting a  future (that is, not yet observed) random variable on the basis of some  existing observable random variables when the parameter of the underlying  probability distribution does not concern us directly.
among these  databases, some have variables that naturally form a hierarchical tree  structure, such as pharmaceutical drugs or occupations.
this network is a dynamic hybrid system which consists of  two sub-rbf networks.
the images were taken over locations in the  united states using a camis (computerized airborne multispectral imaging  system) instrument flown in an airplane and registered by trained image  analysts.
abstract: several recent efforts have focused on adding exploratory data analysis
a  recent improvement in visual statistics has been the use of the internet.
algorithms based on the approach are embedded in gee methods.
unfortunately, most of the existing classification  algorithms give poor class probability estimates because they were specifically  designed to maximize the classification accuracy and, as a result, the learned  models will output probability values that are more extreme (i.e., close to 0  and 1).
author: carol m. van horn (u.s. census bureau)
) abstract: we present a suite of user friendly s-plus functions for fitting, among  others, (a) smoothing spline models for independent and correlated gaussian  data, and for independent binomial, poisson and gamma data; (b) semi-parametric  regression models; (c) non-parametric mixed effects models; and (d)  semi-parametric nonlinear mixed effects models.
the array data are used to characterize genetic differences  between individuals or between types of tissue.
in fact, the inverses  can be considered random gamma variates if a uniform random number generator is  used to generate the probabilities over the interval
this criterion  maximizes dispersion among multiple responses, using a distance-in-space  coefficients (disco) model.
abstract: for many practical applications one would rather have learning algorithms  compute accurate values of probabilities for each possible class, instead of a  single class label.
here we define ``large dataset''  as one that cannot be analyzed using some particular desired combination of  hardware and software because of computer memory constraints.
in  particular, this method is useful for selecting the shrinkage factor for the  two stage testimator in view of increasing the efficiency of such testimator.
the objective of this paper  is to describe our first experiences with a novel randomized tree induction  method that uses a subset of samples at a node to determine the split.
however, some of these  statistical tools when used in conjunction with an appropriate dimension  reduction method can be effective.
in  the presentation, we will share many lessons we have learned in acquiring,  displaying, and analyzing the microarray data.
this paper introduces a new lazy learning algorithm --- the lazy option trees --- based on which we derive a method for computing good class probability estimates.
this paper describes the use of spatial statistics to compress the size of large 1-meter imagery data sets.
abstract: the publicly available pima indian diabetic database (pidd) at the  uc-irvine machine learning lab has become a standard for testing data mining  algorithms to see their accuracy in predicting diabetic status from the 8  variables given.
a better approach is to allow the author to create a document using the  common authoring tools (e.g. {\latex}, ms word or htm editors) and to  conveniently insert dynamic and interactive components from other languages.
abstract: recent work in classification indicates that significant improvements in  accuracy can be obtained by growing an ensemble of classifiers and having them  vote for the most popular class.
bootqc: bootstrap for statistical quality control and applications to  aviation safety analysis author: regina y. liu (rutgers university) hueychung teng (rutgers university) abstract: control charts are widely used as effective online monitoring tools in  statistical quality control.
this paper focuses on ensembles of decision  trees that are created with a randomized procedure based on sampling.
suppose that the original or  ``mother'' dataset is a matrix $y$ having $n$ rows or entities and $n$ columns  or variables.
the sample maximum of the data is  of interest in settings such as insurance and finance; we produce a  normalization of this statistic, which, in conjunction with subsampling  methods, will allow for asymptotically correct estimation of its cumulative  distribution function.
data mining diabetic databases: are rough sets a useful addition?
abstract: under two cost growth indices, annual absolute and relative cost growth,  probability-based models were constructed for basis functions of nasa's  technology readiness levels through the use of johnson's four parameter system  of bounded, unbounded, or lognormal distributions.
sampling and resampling methods (contributed session) session  time:saturday, june 16th, 11:05am - 12:50pm room location: laguna  room session chair: ori rosen, university of pittsburgh resampling time series with seasonal components author: dimitris politis (university of california, san diego)
a better approach is to allow the author to create a document using the common authoring tools (e.g. {\latex}, ms word or htm editors) and to conveniently insert dynamic and interactive components from other languages.
dimitris politis (ucsd)
simulations were used to assess the speed and portability of the java  implementation, termed change detector.
the different languages are all reasonably standard tools and each is used for the purposes for which it was designed.
here it is proposed that many  useful generative probabilistic models can appropriately be expressed in the  form of stochastic grammars, which recursively generate sets of words with  numerical parameters attached.
comparing two measurement devices: review and extensions to estimate  new device variability author: brian j eastwood (eli lilly and company) abstract: there is much literature available on methods for comparing two  measurement systems that are supposed to be equivalent.
the maf/tiger system is an aging national resource.
census 2000: lessons for census 2010 session organizer: nancy gordon session time: friday, june 15th, 4:15pm - 6:00pm room  location: viejo room session chair: nancy gordon, u. s. census bureau technology and the 2010 census
efficient techniques  were developed for computation of the gasser, sroka, and jenner-steinmetz  (gsjs) variance estimate and associated unbiased risk (i.e. expected loss  function), subset regression of indicator variables on the input vector,  regression residuals, and estimated auto-correlation of model residuals.
abstract: microarray technology, such as the gene-chip expression analysis probe  array (affymetrix), generates expression levels for thousands of genes from a  single specimen.
we propose a modification  of the block bootstrap that successfully addresses the issue of seasonalities,  and show some of its properties.
we try to argue that basing the analysis on a  statistical model can be far more rewarding than using ad hoc methods and cut  off criteria.
abstract: evaluation of entropy is important in biological processes in order to  predict the stability of a molecular conformation.
this makes it a reasonably straightforward environment in which to quickly and simply create interfaces for various different applications and audiences.
multicategory support vector machines yoonkyung lee (university of wisconsin-madison) yi lin (university of wisconsin-madison) grace wahba (university of  wisconsin-madison) abstract: support vector machine (svm) has shown great performance in practice as a  classification methodology recently.
the process of model search  and model fitting often require many passes over a large dataset, or random  access to the elements of a large dataset.
when the data view allows partitioning on more than one predictor variable the approach includes a type of look ahead compared to a one variable at a time algorithm.
converting these numbers into information that is understandable and useful to  someone without an extensive statistical background is also a task that is not  easily accomplished.
more generally graphics can be also be used in the evaluation process.
main results from our example: while rip does not seem to run into  numerical troubles, backfitting has slow or no convergence in some instances.
duncan temple lang (bell  laboratories)
abstract: in this paper, a new methodology based on the likelihood of bootstrap  samples is introduced for improving the efficiency of the two stage shrinkage  testimator of waikar {\it et al\/} (2001) for estimation of a normal mean.
in nested designs, a factor is nested in another and each factor has multiple levels.
the short form data collection holds promise for expanding the  internet and other electronic reporting options as modes for data collection.
abstract: we propose a 3d method to segment magnetic resonance imagery (mri) of  ischemic stroke patients into lesion and background, and hence to estimate  lesion volumes.
while the sieve parametric  form of the model suggests that a conditional likelihood ratio test should be  available for testing whether the shape varies with a time invariant covariate,  the null distribution of the likelihood ratio test may not be chi-squared.
we discuss references (papers and urls) useful for such a  class and summarize student surveys conducted during the course.
conclusion: the proposed paradigm for analyzing microarray gene expression data  yielded more precise, concise and reliable results.
in this  paper, we propose a new ensemble method in decision trees that utilizes  systematic patterns of classifications.
in this talk, we provide an overview of our web-based statistics  course, including detailed discussions of lecture topics, homework assignments,  and student projects.
the  data deviate “wildly” from classical statistical assumptions:  normality, homoscedasticity, and additivity.
compared to the original s-plus implementation,  we found the new java program to be as accurate, easier to use, and faster.
we  conclude that change detector provides an improved statistical program for  automated review of laboratory data in the clinical setting.
the limits are cause for humility in the face of overwhelming quantities of data.
parameters of the models are used to  compress the original image.
vladimir hnizdo (niosh/held)
in this article, we make a comprehensive study of  finding an adequate statistic for the total time on test when the data is  assumed to be exponentially distributed and censored after the r-th failure.
the general form of smoothing  splines based on reproducing kernel hilbert space is used to model  non-parametric functions.
here we take interest in gam fitting of  rather complicated data showing patterns of correlation.
this  approach requires a limited amount of user interaction to initialize.
the edgeworth expansion of the distribution of r is discussed for the  independent and not identically distributed case.
the entire  computation only involves two simple algebraic functions.
our method has produced results that fit the observed aids incidence  better than those produced by other existing methods based on data from u.s.,  canada and australia.
randomization can be introduced by using random samples of the training data  (as in bagging or arcing) and running a conventional tree-building algorithm,  or by randomizing the induction algorithm itself.
thus it  has not been a surprise that the used methods of denoising are modified  universal thresholding procedures developed for uniwavelets.
we demonstrate the use of the methodology to a diffuse large b-cell  lymphoma (dlbcl) textit{complementary} dna (cdna) data set.
consequently, this  threshold system can help achieve more effective regulation of air traffic and  safety.
[kdd'99  proceedings].
we performed duplicate experiments and, in some cases, aligned the  images multiple times to estimate the alignment variability, experiment  variability, and patient variability.
for routine use in clinical settings, we now describe  the improved design and implementation of these numerical algorithms for  sequential change detection of the mean.
rip-gams with an application in human brain research author: michael g. schimek (karl-franzens-university of graz) abstract: backfitting is still the most popular numerical technique for generalized  additive models (gam).
(title and abstract unavailable) latanya sweeney (carnegie mellon university)
while the  number of genes measured exceeded the number of sample specimens 100-fold, a  simple dimensionality-reduction strategy ameliorated the multiplicity problem  and facilitated evaluation of group differences and covariate effects --
however, to our best knowledge, there has been  no course before where statistical issues and the web have been systematically  discussed.
each record in the  data set represents an individual item processed by an individual laser scanner  at a particular store at a particular time on a particular day.
abstract: the success of the 2000 census was due, in part, to the application of  new technologies such as image capture, internet and laptop computers for some  data collection activities.
exploratory graphical methods using the hexbin plot and brushing methods were  applied.
an improved interval  method for computation and implemented as c++ language classes is used.
we developed programming techniques to speed up  the increment in the iterative loops for finding the inverse of the gamma  cumulative distribution function for a given probability.
regression and function approximation (contributed session) session time:saturday, june 16th, 11:05am - 12:50pm room location:  santa ana room session chair: gareth james, usc self-modeling regression with random effects author: naomi s. altman (cornell university) julio villarreal (edvision corporation)
however, in this  presentation only the estimation of the normal mean is considered for related  simulation studies and the discussion of results.
this can be done for traditional as well as graphically defined partitions.
considering the random nature of the trees, pert is  surprisingly accurate.
zixing fang (university of connecticut school of medicine) stephen walsh (university of connecticut school of medicine)
the algorithm also allows tests on continuous attributes, and performs local smoothing in the leaf nodes.
yanqui ren (university of  illinois, urbana-champaign)
in order to compute good probability estimates, multiple tests are performed in each node on the query instance.
with this study it  is possible to determine the relative performance of both assays with respect  to bias and variability (standard deviation) using techniques described in  bland and altman (1981) and
one issue concerns the limits of human sensory input and the resulting conscious awareness.
the ability exists to  generate vast amounts of potential pharmecutical compounds.
subgraph sampling for relational data author: david jensen (university of massachusetts amherst) jennifer neville (university of massachusetts amherst) abstract: sampling is central to evaluating inductive learning algorithms.
the use of penalized regression splines allows for a  generalization in the modeling, estimation, and testing of parameters and is  easily implemented.
she uses html form elements and java components to provide interactive controls with which the reader can manipulate the contents of the document.
abstract: in many longitudinal studies, the response can be modeled as a  (discretely sampled) curve over time for each subject.
linda m. franz (u.s.  census bureau)
for example,  information on patient characteristics, tissue extraction method, choice of  primer, lot number and strip number of the membrane, hybridization procedure,  exposure time, and other parameters of image acquisition all needs to be  recorded.
we tested our method for different cost models and application domains from the uci ml repository and, for the majority of the tasks, its probability estimates improve over the probability estimates of both decision trees and bagged probability estimation trees (unpruned, uncollapsed, and smoothed decision trees; b-pets) - one of the best existing class probability estimators.
some of these have simply focused on  ascertaining the ``quality'' of the data while others have been more narrowly  focussed on simple questions like ``which pairs of items are most frequently  purchased together'' or ``what is the relationship between basket size and  number of baskets.''
wolfgang fink (jpl) abstract: bayesian analysis systems typically have some input language for  describing probabilistic models upon which exact or approximate inference is to  be performed by one or more algorithmic engines.
abstract: we consider an iterative procedure to improve the misclassification error  rate of an initial classification rule.
sampling  in relational data is far more challenging and error-prone than sampling in  non-relational contexts.
it  is shown that $m$ can be obtained as the effective rank of $g$. for this  purpose a new model complexity measure ($d$) is introduced and its relationship  to singular values is derived.
it allows the author to use html, javascript  and r to create the content and the interactivity.
after making the assumption that it is still beneficial to have analysts involved in the analysis process, it seems that thought and computational power could be devoted to producing and prioritizing caricatures that exploit analysts' visual processing strengths.
this is analogous to principal components analysis
trees displays can use graphical representations to show the partition boundaries.
conditional on the fitted common shape model, it is possible to fit and test nonlinear mixed effects using standard methods.
accuracy of the  estimates of $\lambda(t)$ can then be checked by comparing the estimates of  $\mu(t)$, calculated from the volterra equation using the estimated solution of  $\lambda(t)$ and the known distribution $k(t,u)$, with the observed planar  poisson process via statistical goodness of fit tests.
in the development of  cnmap, several statistical and data processing issues were addressed.
the statistical methods  include hierarchical multiple regression modeling with a weighted minimum risk  criteria for model selection to choose models indicating changes in mean values  over time.
this spg viewpoint raises new possibilities  for interacting with subject domain experts to create statistical models and  data analysis algorithms, but raises new challenges for the language or system  implementor in the areas of mathematical notation, algorithm composition (e.g.  using clocked objective functions), and software synthesis.
at this resolution, even a  small geographic area leads to a very large data set; 1 square mile, for  example, is represented by approximately $2.6 \times 10^6$ pixels.
we extend  lindstrom's work by representing the common shape by a penalized regression  spline, and use a parametric random effects model to represent the differences  between curves.
rough sets as a data mining predictive tool has been used in medical  areas since the late 1980s, but not applied to the pidd to our knowledge.
here we assume that the former strategy is unavailable or  undesirable and consider ways of constructing a smaller substitute dataset.
the sheer size of the data set has forced us to go beyond  simple ``data mining'' methods and become involved in ``meta-mining:'' the post  processing of the results of basic analyses.
calculations suggest that one reason why pert performs  so well is that although the trees are extremely weak, they are also almost  uncorrelated.
chunlei ke (st. jude  medical
abstract: in this presentation we discuss biocomplexity, a word which describes a  new research focus which evolved during the past three years at the nsf and  which fosters interdisciplinary research to understand and model the complex  interrelationships underlying biological systems.
statistical modelling of micro array data author: ziad taib (biostatistics, astrazeneca r\&d m{"o}lndal) abstract: we summarize some statistical issues encountered when attempting to  analyse gene expression data.
models of spatial variation are first computed on an entire image,  then on subsampled sets of the image.
session  time:saturday, june 16th, 9:00am - 10:45am room location: santa ana  room session chair: william christenen, smu ciphertext size requirement of ciphertext-only attack on vigenere cipher author: song guo (college of  computer science, northeastern university)
the proposed method is illustrated by looking  at whether death from silicosis is particularly common among specific  occupations as classified by the census bureau, without preconceived idea of  what specific occupation or group of occupations may be related to increased  risk, if any at all.
the  sequence of classifiers in step (i) are generated based on repeated  augmentation of the feature vector with some carefully constructed  pseudo-predictors.
therefore the bayes rule is not directly  available in practice, but can be used as an ideal benchmark of any  classification procedure.
these  include maintaining confidentiality when reporting survey data geographically,
session time: saturday, june 16th, 9:00am - 10:45am
the suggested  approach is modular and completely freestanding, allowing the use of data  formats from different vendors.
ibnu syabri (university of illinois, urbana-champaign)
a course on web-based statistics author: juergen symanzik (utah state university)
creating ensembles of decision trees through sampling author: chandrika kamath (lawrence livermore national laboratory) erick cantu-paz (lawrence livermore national laboratory)
emphasizing  tolerance design applications, this work presents an optimal design algorithm  when the variables are organized as a causal network.
however, current algorithms for learning the model  parameters tend to produce inconsistent designs due to their {\it ad-hoc\/} and  trial-and-error nature.
usually there is much historical information  available for the ``old'' method.
image analysis operations are then performed on  the original and compressed images and performance is compared.
abstract: the spectacular growth and acceptance of the web has made it a very  attractive medium for interactive documents.
the design approach is  algorithmic, optimizing wynn’s entropy criterion.
abstract: causal networks are generalizations of ishikawa diagrams.
and finally  she performs statistical computations and renders visual displays using the  statistical software that is embedded within the reader's browser.
one of the most important uses of accurate class probability estimates in machine learning and data mining is prediction in the presence of arbitrarily large costs associated with the different kinds of errors.
abstract: gene expression (ii) (contributed session) session time: friday, june 15th, 4:15pm - 6:00pm room location: capistrano room session chair:maryann hill, uc irvine assessing patient survival using microarray gene expression data
abstract: community nutrition map (cnmap) is a web application to display  nutritional and demographic information for geographic areas within the united  states using a compilation of data from a variety of sources.
to accomplish this mission, the census bureau  has been using the master address file and the topologically integrated  geographic encoding and referencing (maf/tiger) system for more than fifteen  years to support its various census and sample survey activities.
m. cristina pereyra (university of new mexico) joseph d. lakey (new mexico state  university)
via  partial least squares proportional hazard regression author: danh v. nguyen (uc davis) david m. rocke (uc  davis)
a layer may consist of multiple blocks, and blocks of the same layer have the same parameterization.
one sub-network models the relationship between the  current network output and the past ones, and the other sub-network describes  the relationship between the current network output and the inputs.
according to the physicians  with whom we are working, these results are clinically useful to evaluate  stroke therapies.
these types of documents  frequently display dynamic, statistical output both in the form of text and  plots.
the major objective is to  identify differentially expressed genes between normal and tumor tissues.
both the mechanics and the asymptotic validity of the  proposed procedure are discussed.
we propose designing correlation with multi-layer  structures, in which, each layer represents a unique parameterization for  correlation with a type chosen from generic structures such as ar,  exchangeable, stationary etc.
(pca) but the optimization criterion in pca is variance rather than covariance  in pls.
the author focuses on the presentation and display of these components, including the usual multi-media elements such as text, images and sounds.
while the method can be used for many different types of databases, the proposed method will be described in terms of `occupation' and `mortality'.
this talk will describe a number  of analyses we have undertaken.
mining large datasets johannes gehrke (cornell university) abstract: this talk has two parts.
it is shown that there  cannot exist computationally better tool than cf in terms of numerical  stability.
multi-level models might have difficulty in fitting data  with unbalanced clusters.
in this paper we develop a new mathematical framework  for rbf design.
compared to other ensemble methods,  pert is very fast to fit.
assist: a package for spline smoothing in s-plus template author: yuedong wang (univ of california)
combining decision trees using systematic patterns author: hyunjoong kim (worcester polytechinc institute)
the  new illumitek software tool, nvizn, the follow up to the graphics production  library, is an interactive tool that allows a user to expand or narrow the  focus of a visual representation and to order, overlay, and rearrange the  format of the data, as fast as the user’s connection can process without  needing a statistical computing background.
i will first survey recent work in scalable  decision tree construction over massive training databases.
selection of shrinkage factor for the two stage testimator of the  normal mean using bootstrap likelihood author: makarand v. ratnaparkhi (wright state university, dayton)
results of data analysis using various tools were compared.
the  differential expressed genes were identified by computing the raw folds of  change, the t-statistics, and change with respect to the interquartile range.
adjusting the significance tests of human defined partitions for multiple comparison is an open research question with some algorithm emulation possibilities.
thus these s-plus functions deal with many different  situations in a unified fashion.
the algorithm also allows tests on continuous  attributes, and performs local smoothing in the leaf nodes.
we show that the support vector machine approaches  the bayes rule in an asymptotic sense.
the approach taken to cope with the high  dimensionality is to reduce the dimension via some dimension reduction  (component extraction) method in the first stage and then estimate the survival  distribution using a ph regression model in the second stage.
series approximations in risk analysis author: reza modarres (the george washington university) costas  christophi (the george washington university)
the primary  method of component extraction considered is pls.
hierarchical visualization of environmental data on the web using nvizn author: lacey jones (utah state university)
there is no use of  transcendental functions, auxiliary functions, power series, or newton's method  in the computation.
abstract: in the case of time series with a seasonal component, the well-known  block bootstrap procedure is not directly applicable.
assigning proper false  alarm rates to these bootstrap control charts, this paper develops a meaningful  threshold system for regulating and monitoring aviation safety data.
thus we suggest an  alternative denoising procedure based on the efromovich-pinsker algorithm.
to provide a statistical basis for this  process, we previously developed an approach to sequentially analyze laboratory  test results and identify departures from past values.
we have proposed  multicategory svms, which extend binary svm to multicategory case, and  encompass binary svm as a special case.
even though svm implements the optimal  classification rule asymptotically in binary case, one-versus-rest approach to  solve multicategory case using svm is not optimal.
the bayes rule is the optimal classification rule if the underlying  distribution of the data is known.
a hierarchical interactive visualization system author: peter tino (aston university, uk) ian nabney (aston  university, uk) yi sun (aston university, uk)
multicategory svm implements the  optimal classification rule as sample size gets large, overcoming the  suboptimality of conventional one-versus-rest approaches.
the total data set contains  billions of items which can be aggregated into hundreds of millions of  transactions for millions of repeat customers.
the $n$-dimensional distribution of the rows of $x$ weighted by the  $w_i$ is intended to approximate the distribution of the rows of $y$ well  enough that statistical analysis of $x$ is an acceptable substitute for the  desired analysis of $y$. a squashing procedure is evaluated by how much more  closely modeling of the squashed pseudo-data approximates results from the full  data than do the results from a random sample of the same size.
abstract: several asymptotic approximation methods for computing the distribution  of a multiplicative risk model are discussed.
data sources  include the united states department of agriculture's 1994-96 and 1998  continuing surveys of food intakes by individuals and the department's food and  nutrition service web site.
vasant b. waikar (maimi university, oxford, ohio) frederick j.  schuurmann (miami university, oxford,ohio)
in standard situations, however, both procedures produce the same estimation  results).
abstract: the u.s. census bureau's overall mission is to be the preeminent  collector and provider of timely, relevant, and quality data about the people  and economy of the united states.
we will also discuss methods for selecting  the number of iterations.
our algorithm builds on the basic ideas of the lazy decision tree classification algorithm introduced by friedman et al. (1996).
abstract: predicting the biological activity of a compound from its chemical  structure is a fundamental problem in drug design.
data mining (contributed session) session time: saturday,  june 16th, 11:05am - 12:50pm room location: capistrano room a tree-based scan statistic for database disease surveillance author: martin kulldorff (university of connecticut school of medicine)
in this paper, we studied the statistical  properties of the ic test and proved that ic is an unbiased estimator of $sum  p_i^2$.
on perfect stability in characteristic function author: jinhyo kim (cheju national university, south korea) bongsu  ko (cheju national university, south korea) abstract: algorithm stability is used to support the notion that the cf is superior  to the mgf in terms of numerically stable behavior.
we illustrate this on an example from human  brain research.
directional curvatures  capture the local folding patterns in the projection manifold.
in these cases, a layer of correlation corresponds to modeling the correlation structure within a factor in the hierarchical structures.
in this paper we propose a tree-based scan statistic for database surveillance use, to be used when the independent variable can be defined in the form of a hierarchical tree.
this paper describes the opportunities, the benefits, and the challenges the  census bureau faces using technology in the 2010 census.
obtaining proper estimates when combining different sources of information,  proper use of sampling weights, and using static or dynamic web page design.
the reader accesses the interactive and dynamic functionality of the document via a plug-in for netscape that embeds r within it.
abstract: gene expression in head and neck cancer patients was assessed by using  the research genetics cdna membranes gf200 and gf211.
the paper closes by describing an approach to laying out trees based on their similarities.
she  uses html form elements and java components to provide interactive controls  with which the reader can manipulate the contents of the document.
while the sieve parametric form of the model suggests that a conditional likelihood ratio test should be available for testing whether the shape varies with a time invariant covariate, the null distribution of the likelihood ratio test may not be chi-squared.
image analysis operations are then performed on the original and compressed images and performance is compared.
the different languages are all reasonably  standard tools and each is used for the purposes for which it was designed.
looking at the 392 complete cases, guessing all are  non-diabetic gives an accuracy of 65.1\%.
in terms for constructing trees, a dynamic example shows the approach of using grand tour, brushing, alphablending and graphical partitioning to build trees.
smooth quadratures of volterra integral equations with applications to  estimation of hiv infection rates and projection of aids incidence
an iterative two-step algorithm is proposed for fitting the  model.
the development of community nutrition map (cnmap) author: alvin b. nowverl (usda-ars-bhnrc-cnrg)
the parametric distributions (such as weibull and  gamma distributions and several new functions) and non-parametric distributions  (such as linear and cubic spline functions) for $k(t,u)$ are so chosen as to  take into account the fact that the hiv screening test was available only since  1985 and the treatment made available to the hiv positive patients only after  1987.
magnification factors quantify the extend to which  the areas are magnified on projection to the data space.
the class  probability estimates are improved using breiman's bagging.
this algorithm provides a  consistent approach for determining the rbf parameters, viz.
in this paper  we develop a support vector machine regression (svmr) methodology for  estimating the bioresponse of molecules based on the large sets of descriptors.
examples of biocomplexity  research are given, research paradigms are described, and essential components  for success are presented.
lessons learned from analyzing the differential gene expression data  between normal and tumor tissues in head and neck cancer patients author: j. jack lee (university of texas m.d. anderson cancer center) hyung woo kim (university of texas m.d. anderson cancer center) feng zhan (university of texas m.d. anderson cancer center)
classification methods (contributed session)
author: joseph l. breault (tulane university; alton ochsner medical  foundation)
the expanded use of  technology in 2010 will greatly reduce the census bureau's reliance on paper  questionnaires.
we analyzed data from patients treated  with cisplatin-based chemotherapy regimens ($n=60$) to determine significant  changes in hematocrit values.
algorithms based on the  approach are embedded in gee methods.
in this paper we have employed various parametric and nonparametric  distributions for $k(t,u)$ and applied the above method to estimate the hiv  infection rates and to make short term projections of aids incidence using  hiv/aids diagnosis data.
therefore, one can expect it is easy to implement.
in this talk, i will describe some of the features that  distinguish it from other bayes net software packages, some of the advantages  and disadvantages of using matlab, plus plans for future work.
we will also discuss a  saddlepoint approximation to the distribution and quantile functions of the  model.
the  threshold system can serve as a set of standards for evaluating the performance  of aviation entities, and provide guidelines for identifying unexpected  performances and assigning appropriate corrective measures.
in nested designs, a factor is nested in another and each factor has  multiple levels.
a case study from prostate cancer and  simulation studies show this approach is more efficient than the existing gee  methods and multivariate methods.
computational methods and tools (contributed session)
the census  bureau needs to prepare for significantly more automation in the 21st century,  including: improvement to street and other map feature locations; addition of  accurate housing unit locations, and enhanced feature change detection  methodology to provide more timely updates; and modernizing the processing  environment from ``home grown'' systems to one based on cots and gis software.
the software also utilizes the  internet to give opportunities for easy access to anyone with an internet  connection and a graphical user interface without having to purchase a personal  statistical computing package at an impractical price.
its implementation in s-plus is stable and sufficient  for a large number of fitting problems.
the results are established under very  mild condition, allowing arbitrary number of discontinuities in the underlying  conditional probability function.
for evaluating the quality of the  predictions in a cost-sensitive context, we employed the paired bdeltacost  procedure introduced by margineantu \& dietterich (2000).
a key constraint is block self-containment--the  blocks are analyzable without reference to one another--complemented by  all-block analyses.
this is in contrast with most other  asymptotic results in the statistical literature, where the underlying  conditional probability functions are assumed to be smooth to a given order.
model composition arises at the level of multiple rules in a  grammar, and also at the level of entire grammars called as subroutines to  implement a rule in other grammars.
for more  details, see http://http.cs.berkeley.edu/~murphyk/bayes/bnt.htm massive data sets session  time:friday, june 15th, 4:15pm - 6:00pm room location: costa mesa  room session chair: matthew schonlau, rand data squashing: constructing summary data sets author: william dumouchel (at\&t shannon labs)
research and testing will involve handheld computers equipped with gps for the  creation of an initial address list and for use in nonresponse and other field  followup activities, enabling field workers to enter responses directly into a  computer file.
they help create a better understanding through visual  representation of the information and processes the data is explaining.
in addition,  the maf/tiger database has been used as the foundation of the burgeoning  geographic information system (gis) industry in the united states to support  the analytical programs and gis activities managed by other federal agencies,  numerous state, local, and tribal governments, the private sector, and academic  organizations.
java implementation of multiple linear regression models for  patient-specific longitudinal data to monitor chemotherapy-induced anemia author: christine e. mclaren (university of california, irvine) wagner truppel (university of california, irvine) randall f.  holcombe (chao family comprehensive cancer center)
in the second part,  i will address algorithms for mining high-speed data streams.
finally, the weights are computed by the  usual pseudo inverse method.
abstract: we consider data from an infinite order moving average time series model  with inputs in a stable domain of attraction.
this makes it a reasonably straightforward environment in which to quickly and  simply create interfaces for various different applications and audiences.
standardization by background correction and  by the nonparametric regression based loess method was examined.
this approach can be easily extended to unbalanced  hierarchical structures and heterogeneous clusters.
abstract: multiwavelets are relative newcomers into the world of wavelets.
inference for sample maxima in the presence of serrial correlation and  heavy-tailed distributions author: tucker mcelroy (university of california, san diego)
the visual approach uses linear combinations of predictor variables.
often these curves have  a common shape function and individual subjects differ from the common shape by  a transformation of the time and response scales.
oleg smirnov (university of texas at dallas)
the  visualization system is constructed in a statistically principled framework.
the problem of  bayesian inference for the underlying states and covariance matrices has been  examined by a variety of algorithms in the literature.
we introduce a new ensemble classifier, pert, which is  an ensemble of perfectly-fit random trees.
$p$ far exceed the number of samples $n$ render most  traditional statistical tools of little direct use.
the reason is that systems such as s and sas cannot be  integrated into the reader's browser.
the paper emphasizes graphical possibilities and does not evaluate the quality of analyst defined trees.
models of spatial variation are first computed on an entire image, then on subsampled sets of the image.
on the other hand,  the specific of a multiwavelet discrete transform is that typical errors are  not identically distributed and independent normal errors.
curt m. breneman (rensselaer  polytechnic institute) mark j. embrechts (rensselaer polytechnic  institute )
abstract: many scientific research problems arise as solutions to volterra integral  equations of the first kind in which the given function $\mu(t)$ is expressed  as the integral over $(0,t]$ of the product of the known kernel $k(t,u)$ and  the unknown function $\lambda(u)$ with respect to $u$, where $\mu(t)$ and  $\lambda(t)$ are positive for $0 \leq t \leq \tau$, for some positive $\tau$,  $k(t,u)$ is positive for $t \geq u$ and is 0 for $t<u$ and is square  integrable on ${(t,u)| 0 \leq t \leq \tau,0 \leq u \leq \tau}$. in biomedical  research problems, data for $\mu(t)$ and $k(t,u)$ often come as step functions  of time and so the method of quadratures may be employed to solve the equation  to estimate $\lambda(t)$ as a step function of time.
i will explain how generative and non-generative models differ and why the difference is important.
the reader accesses the  interactive and dynamic functionality of the document via a plug-in for  netscape that embeds r within it.
many statistical fitting algorithms  assume that the entire dataset being analyzed fits into computer memory,  restricting the number of feasible analyses.
the squashed dataset is a matrix $x$ having $m$ rows and $n+1$  columns, where $m \ll n$.
before the data analysis,  experimental condition need to be carefully documented.
in each  sub-network, the kernel parameters of the hidden layer and those in the output  layer are all adaptively determined globally by using an  expectation-maximization (em) algorithm (xu 1998).
in addition, it includes several devices to visualize spatial  autocorrelation in lattice (or regional) data, such as the moran scatterplot  and lisa maps.
it combines within an overall framework of  fully dynamically linked windows a cartographic representation of data on a map  with traditional statistical graphics, such as histograms, box plots, and  scatterplots.
visualization and image data (contributed session) session time: saturday, june 16th, 11:05am - 12:50pm room location: viejo room session chair: michael minnotte, utah state a statistical approach to the segmentation of mr imagery and volume  estimation of stroke lesions author: benjamin stein (university of massachusetts) joseph  horowitz (university of massachusetts)
abstract: many statistics courses have been taught that make use of web-based  statistical tools such as teachware tools, electronic textbooks, and  statistical software on the web.
however, this method tends  to yield negative and/or erratic values as solutions of $\lambda(t)$. to obtain  non-negative and smooth estimates of $\lambda(t)$, we shall treat $\lambda(t)$  as the mean of a linear (non-homogeneous) poisson process and $\mu(t)$ as the  mean of the translated planar (non-homogeneous)
exploratory analysis of retail sales of billions of items author: william f. eddy (carnegie mellon university) dunja mladenic (j.  stefan institute, slovenia and carnegie mellon univ., usa) scott ziolko (carnegie mellon university) abstract: we report some preliminary analyses of a data set collected over the past  year from a grocery chain containing hundreds of stores.
in addition, statistical  prediction models were built for the programs.
visualizing spatial autocorrelation with dynamically linked windows author: luc anselin (university of illinois, urbana-champaign)
using distributional assumptions or bootstrap estimates one can then  obtain confidence interval estimates and conduct hypothesis tests about the  absolute and relative performance of the two assays.
rough sets are a useful addition to the analysis of diabetic databases.
the proposed approach compares very favorably with  partial least squares (pls), a well-known and commonly used method in  chemometrics, on the performance of quantitative structure-activity  relationships (qsar) analysis based on real chemistry data.
the class probability estimates are improved using breiman's bagging.
lindstrom (1995) represented  the common shape by a free-knot regression spline, and used a parametric random  effects model to represent the differences between curves.
an em (expectation-maximization) formula is  derived for iterative estimation of $\lambda(t)$. smooth nonparametric maximum  likelihood estimates of $\lambda(t)$ are then obtained by applying the em  algorithm coupled with a smoothing step at each iteration.
abstract: many databases exist by which it is possible to study the relationship  between health events and various potential risk factors.
the causal network is  transformed into a causal map, which represents all factors and responses as  points in a common d-dimensional metric space.
specifically, for a given vector of response values  which are times to event (death or censored times) and $p$ gene expressions  (covariates) we address the issue of how to assess (estimate) the survival  experience (curve) when $n \ll p$.
i will be using this new  software by creating hierarchical visual images, such as maps and charts, of  data modeling concentration estimates of 148 hazardous air pollutants for the  60,803 census tracts in the continental united states obtained through the  epa’s cumulative exposure project.
this paper introduces a new lazy learning algorithm --- the lazy option  trees --- based on which we derive a method for computing good class  probability estimates.
abstract: high dimensional data sets from microarray experiments where the number  of variables (genes)
edward l. kambour (prostrategic solutions)
one of the most important uses of accurate class probability estimates in  machine learning and data mining is prediction in the presence of arbitrarily  large costs associated with the different kinds of errors.
the images were taken over locations in the united states using a camis (computerized airborne multispectral imaging system) instrument flown in an airplane and registered by trained image analysts.
since fluctuations in the rotational angle (torsional) coordinates make a  pivotal contribution to the overall configurational entropy of the molecule, we  review circular probability modeling approaches for modeling the torsional  angles.
an adaptive-learned temporal radial basis function network for  recursive function estimation author: yiu ming cheung (the chinese university of hong kong) lei  xu
the bnt web site receives an average of 300  hits per week.
these explanations differ from common statistical analysis in that they are both based on ``non-generative'' models of the world.
the notion of adequate statistics initiated by fisher and skibinsky  would deal with this concern.
the results clarify the mechanism beneath the support vector machine, and  highlight the advantage and limitation of the support vector machine  methodology.
an experiment with real dataset is carried  out to see the performance of the new method.
apart from being freestanding, this new program (dynesda2)  implements a number of other advances, such as the capability to brush polygon  coverages, simultaneous linking of multiple maps with multiple statistical  graphics, and interactive lisa maps.
abstract: as remote sensing instruments evolve, the size of imagery data sets  derived from remote sensing continues to increase.
some well known special cases are polynomial  splines including the popular cubic splines, periodic splines, spherical  splines, thin-plate splines, l-splines, generalized additive models, smoothing  spline anova models, and self-modeling nonlinear regression models.
the proposed method  deals with equal misclassification cost and unequal cost case in unified way.
author: john j. hsieh (university of toronto)
it allows the author to use html, javascript and r to create the content and the interactivity.
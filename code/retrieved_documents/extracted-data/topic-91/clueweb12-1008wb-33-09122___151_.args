these quantities are important when you compare models with a different number of parameters: the log-likelihood will always increase if you add more variables, falling in the "overfit" trap.important variables may be missing, that can change the results of the regression and their interpretation.often, you can solve the problem by transforming the variables (so that the outliers and influential observations disappear, so that the residuals look normal, so that the residuals have the same variance -- quite often, you can do all this at the same time), by altering the model (for a simpler or more complex one) or by using another regression (gls to account for heteroskedasticity and correlated residuals, robust regression to account for remaining influencial observations).one idea, to increase the quality on an estimator (a non-linear and unstable one, e.g., a regression tree) is simply to compute its "mean" on several bootstrap samples.those values tells us how an error on a predictive variable prapagates to the predictions.the leverage effect can yield incorrect results.sometimes, they come from mistakes (they should be identified and corrected), sometimes, they are perfectly normal but extreme.in some cases, you can even get contradictory results: depending on the order of the predictive variables, you can find that z sometimes depends on x, sometimes not.bagging (bootstrap aggregation) one idea, to increase the quality on an estimator (a non-linear and unstable one, e.g., a regression tree) is simply to compute its "mean" on several bootstrap samples.if the sample is too small, you will not be able to estimate much, in such situationm you have to restrict yourself to simple (simplistic) models, such as linear models, becaus the overfitting risk is too high.we get 0.9546, i.e., we are in the prediction interval in (more than) 5% of the cases -- but the prediction interval is huge: it tells us that we cannot predict much.some points might bear an abnormally high influence on the regression results.# we select a "good" regression model, using the bic as a # criterion, by starting from a model and adding or # removing variables at random, if this improves the bic.we also get the confusion matrix, that gives the "distances" between the classes: we can use it to plot the classes in the plane, with a distance analysis algorithm (mds (multidimensional scaling), etc. -- we have already mentionned this).if you know where the change occurs, you just split your sample into several chuks and perform a regression on each (to make sure that a change occured, you can test the equality of the coefficients in the chunks).
it has been shown, in different domains, that random subspaces combined with weak classifiers such as decision trees and nearest neighbor classifiers can provide an improvement in accuracy.experiments with decision tree and neural network classifiers on various datasets show that, given the same size partitions and bags, disjoint partitions result in performance equivalent to, or better than, bootstrap aggregates (bags).our results indicate that, in such applications, the simple approach of creating a committee of n classifiers from disjoint partitions each of size 1/n (which will be memory resident during learning) in a distributed way results in a classifier which has a bagging-like performance gain.we describe an ensemble learning approach that accurately learns from data which has been partitioned according to the arbitrary spatial requirements of a large-scale simulation wherein classifiers may be trained only the data local to a given partition.voting many classifiers built on small subsets of data is a promising approach for learning from massive data sets, one that can utilize the power of boosting and bagging.in this comment, we show that a simple ensemble of decision trees results in a higher accuracy, 94.75%, and is computationally efficient.we also construct ensembles of classifiers learned from such actively sampled image sets, which further provides improvement in the recognition rates. ...experiments show this approach is fast, accurate, and scalable.this result is somewhat surprising and illustrates the general value of experimental comparisons using different types of classifiers.when compared to other methods, our percentage correct diversity measure algorithm shows a greater correlation between the increase in voted ensemble accuracy and the diversity value. ...the random subspaces approach has the added advantage of requiring less careful tweaking.this paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in roc space) than only under-sampling the majority class.this work shows that a random subspaces ensemble can outperform a well-tuned single classifier for a typical 2-d face recognition problem.experiments on the forest cover data set show that this parallel mixture is more accurate than a single svm, with 90.72% accuracy reported on an independent test set.an empirical evaluation using five real data sets confirms the validity of our approach compared to some other combination of multiple classifiers algorithms.
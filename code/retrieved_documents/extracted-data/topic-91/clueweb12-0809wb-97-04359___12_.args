sound machine learning and statistical methods for variable selection and/or data reduction based on the intrinsic structure of the data are needed.in the absence of such techniques, efficient and reliable clustering methods whose score functions are based on similarity or distance matrices are advantageous.several experiments showed that the three common methods used to construct multiple classifiers from the same training data -bagging, boosting and randomized trees (random forests)- produce mutually weakly dependent trees.this research is partly based on the belief that the key to the success of modeling complex data structures lies in the embedding of kernel-based methods within parametric and bayesian models.for example, the bayesian parametric paradigm gives a framework for introducing structural constraints, whereas the kernel based methods introduces flexibility (e.g. local, spatial and/or temporal adaptation).the goal is to facilitate bayesian inference techniques to a larger audience than just statisticians in academia.furthermore, these results also suggested that there is a trade-off between weak dependence and expected margins: to compensate for low expected margins, there should be low mutual dependence among the classifiers making up the combination.we showed that suitable chosen transformations can greatly enhance normality in gene expression data, and that models have varying performance on data sets that are transformed di erently.i showed that if both this dependence is low and the expected margins are large, then decision rules based on linear combinations of these classifiers can achieve error rates that decrease exponentially fast.in it we benchmarked the performance of model-based clustering on several synthetic and real gene expression data sets for which external validation were available.in this work we develop a complete methodology for document classification and clustering based on model-based clustering.a comparison with other popular classification methods such as support vector machines, shows that our method is very competitive and computationally efficient.it uses these two findings to present a powerful kernel-based clustering methodology based on potts model, kernel density estimation, and consensus clustering.this allowed us to use multiple randomized trees to access the large pool of acoustic events in a systematic manner.a key advantage of model-based refractionation over other competing clustering of large data sets approaches is that it does not require the number of clusters be known a priori.
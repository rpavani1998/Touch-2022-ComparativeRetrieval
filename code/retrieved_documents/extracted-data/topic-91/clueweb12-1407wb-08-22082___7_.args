the randomness introduced by the random forest model builder in the dataset selection and in the variable selection delivers considerable robustness to noise, outliers, and over-fitting, when compared to a single tree classifier.the generalisation error rate from random forests tends to compare favourably to boosting approaches, yet the approach tends to be more robust to noise in the training dataset, and so tends to be a very stable model builder, not suffering the sensitivity to noise in a dataset that single decision tree induction does.the randomness also delivers substantial computational efficiencies.random forests are often used when we have very large training datasets and a very large number of input variables (hundreds or even thousands of input variables).in building each decision tree model based on a different random subset of the training dataset a random subset of the available variables is used to choose how best to partition the dataset at each node.in summary, a random forest model is a good choice for model building for a number of reasons.the random forest model builder can also report on the input variables that are actually most important in determining the values of the output variable.also, at each node in the process of building the decision tree, only a small fraction of all of the available variables are considered when determining how to best partition the dataset.together, the resulting decision tree models of the forest represent the final ensemble model where each decision tree votes for the result, and the majority wins.the general observation is that the random forest model builder is very competitive with nonlinear classifiers such as artificial neural nets and support vector machines.thirdly, because many trees are built and there are two levels of randomness and each tree is effectively an independent model, the model builder tends not to overfit to the training dataset.in building a single decision tree the model builder may select a random subset of the training dataset.the data does not need to be normalised and the approach is resiliant to outliers.each decision tree is built from a random subset of the training dataset, using what is called replacement (thus it is doing what is known as bagging), in performing this sampling.second, if we have many input variables, we generally do not need to do any variable selection before we begin model building.
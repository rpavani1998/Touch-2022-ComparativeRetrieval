decision tree learning algorithms are known to suffer from high variance, because they make a cascade of choices (of which variable and value to test at each internal node in the decision tree) such that one incorrect choice has an impact on all subsequent decisions.recent experiments suggest that breimanâ€™s combination of bagging and the random subspace method is the method of choice for decision trees: it gives excellent accuracy and works well even when there is substantial noise in the training data.experimental evidence has shown that ensemble methods are often much more accurate than any single hypothesis.decision tree algorithms can be randomized by adding randomness to the process of choosing which feature and threshold to split on.in neural network and decision tree algorithms, for example, the task of finding the hypothesis that best fits the training data is computationally intractable, so heuristic methods must be employed.the first approach is to construct each hypothesis independently in such a way that the resulting set of hypotheses is accurate and diverse, that is, each individual hypothesis has a reasonably low error rate for making new predictions and yet the hypotheses disagree with each other in many of their predictions.in neural network and decision tree algorithms, for example, the task of finding the hypothesis that best fits the training data is computationally intractable,dietterich and bakiri (1995) report that this technique improves the performance of both decision-tree and backpropagation learning algorithms on a variety of difficult classification problems.experimentally, adaboost has been shown to be very effective at increasing the margins on the training data points; this result suggests that adaboost will make few errors on new data points.hence, ensemble methods can reduce both the bias and the variance of learning algorithms.there is a risk that the chosen hypothesis will not predict future data points well.these heuristics (such as gradient descent) can get stuck in local minima and hence fail to find the best hypothesis.dietterich (2000) showed that randomized trees gave significantly improved performance on 14 out of 33 benchmark tasks (and no change on the remaining 19 tasks).third, it is possible to design algorithms that are more effective than adaboost at increasing the margin on the training data, but these algorithms exhibit worse performance than adaboost when applied to classify new data points.as with the statistical problem, a weighted combination of several different local minima can reduce the risk of choosing the wrong local minimum to output.
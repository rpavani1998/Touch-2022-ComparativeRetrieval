question: is there an ordering of the vertices of a weighted graph such  that ?
scheduling, code optimization, permanent evaluation, quadratic  programming, etc. open: graph isomorphism, composite number, minimum length triangulation.
do there exist languages which can be verified in polynomial time and still take exponential time on deterministic machines?
one way is by , where m is best a large prime not too close to , which would just mask off the high bits.
the amount of space we need to store a song can be measured in either the words or characters needed to memorize it.
most likely you would set up 13 piles and put all cards with the same number in one pile.
if and only if the expression is satisfiable.
the following algorithm implements it: for k=1 to n for i=1 to n for j=1 to n this obviously runs in time, which asymptotically is no better than a calls to dijkstra's algorithm.
this is really the exact same problem as the previous integer  programming problem, slightly concealed by: the linear algebra notation - each row is one constraint.
hence, any problem which solves this problem is equivalent to a machine  which recognizes whether an instance is in the language!
listening to part 22-10 p versus np the precise distinction between whether a problem is in p or np is somewhat technical, requiring formal language theory and turing  machines to state correctly.
that means each integer is mapped to a point on the  parabola .
listening to part 22-5 further, many strange and impossible-to-believe things have been shown to  be true if someone in fact did find a fast satisfiability algorithm.
17 12 6 19 23 8 5 10 - before 6 8 5 10 23 19 12 17 - after partitioning places all the elements less than the pivot in the left part of the array, and all elements greater than the pivot in the right part of the array.
build an example first: (5, 2, 8, 7, 1, 6, 4) ask yourself what would you like to know about the first n-1 elements to tell you the answer for the entire sequence?
if it is one, includes the cooresponding edge/number to give two per column.
any problem innp has a non-deterministic tm program which solves it in polynomial time, specificallyp(n).
listening to part 21-8 polynomial vs. exponential time n f(n) = n f(n) =
example: the traveling salesman problem: given a weighted graph g, what tour minimizes .
this new graph has a traveling salesman tour of costn iff the graph is hamiltonian.
another example is showing isomorphism is no easier  for bipartite graphs: for any graph, replacing an edge with makes it bipartite.
to use the notation of the general backtrack algorithm, , and v is a solution whenever .
k do x = next[x] return x note: the while loop might require two lines in some programming languages.
listening to part 21-11
listening to part 22-5 further, many strange and impossible-to-believe things have been shown to be true if someone in fact did find a fast satisfiability algorithm.
because we replaced a chain of edges by the edge, the triangle inequality ensures the tour only gets shorter.
what happens if i scan from left to right?
the problem instances are encoded as strings and strings in the language if and only if the answer to the decision problem is yes!
an instance is a problem with the input parameters specified.
using heaps, both of these operations can be done within time, balancing the work and achieving a better tradeoff.
thus by definition a is a heap!
this can be done in o(n m) time, by doing a dfs or bfs to loop through all edges, with a constant time test per edge, and a total ofn iterations.
we had to show that all problems innp could be  reduced to sat to make sure we didn't miss a hard one.
using binary search and the decision version of the problem we can find  the optimal tsp solution.
dynamic programming (the viterbi algorithm) can be used on the sentences to obtain the same results, by finding the shortest paths in the underlying dag.
for convenience, from now on we will talk only about decision problems.
any ip solution gives a sat solution.
let a = 1, b= 3/2 and f(n)
proof: vc in in np - guess a subset of vertices, count  them, and show that each edge is covered.
our simple dynamic programming algorithm improved to capacity of pdf-417 by an average of !
in an unweighted graph, the cost of a path is just the number of edges on the shortest path, which can be found ino(n+m) time via breadth-first search.
lecture sound../sounds/lec7-8a.au array based sets unsorted arrays listening to part 7-10 sorted arrays what are the costs for a heap?
hamiltonian path - for any graph problems whose hardness depends upon ordering.
listening to part 1-7 lecture schedule subject topics readingsubject topics reading what is an algorithm?
note the need for absolute values.
previous: lecture 19 - satisfiability lecture 21 - vertex cover listening to part 24-7 36.5-2 - given an integer  matrix a, and in integer m -vectorb, the 0-1 integer programming problem asks whether there is an  integern-vector x with elements in the set (0,1) such that  .
therefore, any cover of just these  edges must include one vertex per edge, or half the greedy cover!
saying is equivalent to saying that .
the class of languages which we can recognize in time  polynomial in the size of the string or a deterministic turing machine (without  guessing module) is calledp. the class of languages we can recognize in time polynomial in the length  of the string or a non-deterministic turing machine is callednp.
there is no going back once we reject someone.
to show that longest path or hamiltonian path is np-complete, add start and stop vertices and distinguish the first and last selector vertices.
reading from the equation, a=4, b=2, and .
but what is the fraction?
for a given alphabet of symbols 0, 1, &, we can form  an infinite set ofstrings or words by arranging them in any order:  `&10', `111111',`&&&', and `&'.
are we doing string matching in the text or substring matching?
note that the theory of np-completeness does not stipulate that  it is hard to get close to the answer, only that it is hard to get the optimal  answer.
difficult analysis shows that it takes time, where is the inverse ackerman function and number of atoms in the universe)=5.
in the simplest implementation, we can simply mark each vertex as tree and non-tree and search always from scratch: select an arbitrary vertex to start.
i, since we cannot excuse deleting the firsti characters of the pattern without cost.
lines 4-6 identify x as the non-nil decendant, if any.
the proof of cook's theorem, while quite clever, was certainly difficult  and complicated.
we must transform every instance of a known np-complete problem  to an instance of the problem we are interested in.
a vertex cover instance  consists of a graph and a constantk, the minimum size of an acceptable  cover.
we do not care about small values ofn.
while the graph has edges pick an arbitrary edge v, u add both u and v to the cover delete all edges incident on either u and v if the graph is represented by an adjacency list this can be implemented in o(m+n) time.
for a cover to have n+2c vertices, all the cross edges must be  incident on a selected vertex.
tsp remains hard even when the distances are euclidean distances in the plane.
collision resolution by chaining the easiest approach is to let each element in the hash table be a pointer to a list of keys.
in a weighted graph, the weight of a path between two vertices is the sum of the weights of the edges on a path.
to construct all subsets, set up an array/vector of n cells, where the value of is either true or false, signifying whether the ith item is or is not in the subset.
since any sat solution will also satisfy the 3-sat instance and  any3-sat solution sets variables giving a sat solution - the problems  are equivallent.
non-deterministic turing machines suppose we buy a guessing module peripherial for our turing  machine, which looks at a turing machine program and problem instance and in  polynomial time writes something it says is an answer.
how can we find the last bit of the missing integer?
assign each edge in g weight 1, any edge not ing weight 2.
if we take the derivative of both sides, ... multiplying both sides of the equation by x gives the identity we need: substituting x = 1/2 gives a sum of 2, so build-heap uses at most 2n comparisons and thus linear time.
listening to part 2-12 the complexity of songs suppose we want to sing a song which lasts for n units of time.
either make sure you understand your data, or use a good worst-case or randomized algorithm!
listening to part 7-6 stack implementation
bfs will not work on weighted graphs because sometimes visiting more edges can lead to shorter distance, ie.
when m = 366, this probability sinks below 1/2 when n = 23 and to almost 0 when .
whenever your objects are ordered in a left-to-right way, you should smell dynamic programming!
while theoretically any np-complete problem will do, choosing the correct one can make it much easier.
for i = 1 to n/2 do find(1)
listening to part 19-6 union-find programs our analysis that kruskal's mst algorithm is requires a fast way to test whether an edge links two vertices in the same connected component.
in a clique, there are always between two vertices.
selection sort scans throught the entire array, repeatedly finding the smallest remaining element.
however, intuitively a problem is in p, (ie. polynomial) if it  can be solved in time polynomial in the size of the input.
a finite set of tape symbols a finite set of states  for the machine, including the start state  and  final states a transition function, which takes the current machine state, and current  tape symbol and returns the new state, symbol, and head position.
list-insert(l, x) next[x] = head[l] if head[l] <> nil then prev[head[l]] =
the key problem is establishing a correspondence between features in the two images.
at each entry point to a gadget, check if the other vertex is in the cover and traverse the gadget accordingly.
proceed: systems like mathematica and maple have packages for doing this.
try backsubstituting: the term should now be obvious.
listening to part 26-2 other np-complete problems partition - can you partition n integers into two subsets so  that the sums of the subset are equal?
for various reasons, it is known that satisfiability is a hard problem.
for a convex polygon, or part thereof: evaluation proceeds as in the matrix multiplication example - values of t, each of which takes o(j-i) time if we evaluate the sections in order of increasing size.
we claim this graph has a hc iff g has a vc of  sizek.
given a weighted graph g and integer k, does there exist a traveling salesman tour with cost k?
guess a subset of the input number and simply add them up.
in the eight queens, we prune whenever one queen threatens another.
once you get the hang of it, it is surprisingly straightforward and pleasurable to do.
the stack and queue orders are just special cases of orderings.
if the root of a red-black tree is black can we just color it red?
listening to part 11-8 constructing all subsets how many subsets are there of an n-element set?
clearly, , since for any dtm program we can run it on a non-deterministic machine, ignore what the guessing module is doing, and it will just as fast.
we can always add more vertices to the cover to bring it up to size k .
convert g to an instance of the bo-billy problem y. call the subroutine bo-billy on y to solve this instance.
the theory of np-compleness, developed by stephen cook and richard karp,  provided the tools to show that all of these problems were really the same  problem.
all of these problems are equally hard, and the more you can restrict the problem you are reducing, the less work your reduction has to do.
what is the best way to encode text for this design?
our integer  programming problem will have twice as many variables as the sat instance, one  for each variable and its compliment, as well as the following inequalities: listening to part 24-4 for each variable  in the set problem, we will add the following  constraints: and both ip variables are restricted to values of 0 or 1, which makes them  equivalent to boolean variables restricted to true/false.
since memory on any computer is limited,o(nm) space is more of a bottleneck than o (nm) time.
assign each edge in g weight 1, any edge not  ing weight 2.
the principle behind dijkstra's algorithm is that if is the shortest path froms to t, then had better be the shortest path from s to x.
after each comparison, we can throw away half the possible number of keys.
what does a red-black tree with two real nodes look like?what does a red-black tree with two real nodes look like?
thus we can sort the elements to the left of the pivot and the right of the pivot independently!
example 1:  and a clause is satisfied when at least one literal in it is true.
just changing a problem a little can make the difference between it being  inp or np-complete: p np-complete eulerian circuit  hamiltonian circuit edge cover  vertex cover
we can set up n buckets, each responsible for an interval of m /n numbers from 1 to m
most interesting optimization problems can  be phrased as decision problems which capture the essence of the computation.
listening to part 22-8
the tricky part is to cover with as small a set as possible.
example: sequential strips in triangulations.
thus it is usually cleaner and easier to talk about upper and lower bounds of the function.
to reduce s(n), we must structure the song differently.
since running our algorithm on a machine which is twice as fast will effect the running times by a multiplicative constant of 2 - we are going to have to ignore constant factors anyway.
thus sorting each bucket takeso(1) time!
proof: as before, suppose e is not in t. adding e to t makes a cycle.
this gives us a recursive sorting algorithm, since we can use the partitioning approach to sort each subproblem.
with precomputing the list of possible moves, this program could search 1,000 positions per second.
we want the limit the height of our trees which are effected by unions .
my  favorites are: 3-sat - that old reliable...
for example, the problem of finding a  path of lengthk is really hamiltonian path.
all reasonable encodings will be within polynomial size of each other.
combinatorial problems may have this property but may use too much memory/time to be efficient.
if we were allowed to visit cities more than once, doing a depth-first  traversal of a mst, and then walking out the tour specified is at most twice  the cost of mst.
after all, 1-sat is trivial!
listening to part 26-6 select the right source problem for the right reason.
the command line arguments were: latex2html all.tex.
singly linked to doubly-linked list is as a conga line is to a can-can line.
we know nothing about the lower bound of this!
listening to part 5-4 since the partitioning step consists of at most n swaps, takes time linear in the number of keys.
but what does it buy us?
put the edges in a heap count=0 while (count < n-1) do get next edge (v,w) if (component (v) component(w))
theorem: clique is np-complete proof: if you take a graph and find its vertex cover, the remaining vertices form an independent set, meaning there are no edges between any two vertices in the independent set, for if there were such an edge the rest of the vertices could not be a vertex cover.
we could modify our algorithm to first test whether the input is the special instance we know how to solve, and then output the canned answer.
listening to part 2-8 properties of logarithms recall the definition, .
most interesting optimization problems can be phrased as decision problems which capture the essence of the computation.
in the traditional version of tsp - a salesman wants to plan a drive to  visit all his customers exactly once and get back home.
listening to part 8-15 red-black tree definition red-black trees have the following properties: listening to part 8-16
make your target problem as hard as possible.
listening to part 21-10 convex hull and sorting a nice example of a reduction goes from sorting numbers to the convex hull problem: listening to part 21-11 since this parabola is convex, every point is on the convex hull.
listening to part 26-3 polynomial or exponential?
each edge in the initial graph will be represented by the following component: we claim this graph has a hc iff g has a vc of sizek.
listening to part 22-7 instance: a collection of clause c where each clause contains  exactly3 literals, boolean variable v. question: is there a truth assignment to v so that each clause is  satisfied?
listening to part 7-18
for any other problem, we can prove itnp-hard by polynomially transforming sat to it!
soon as we establish the shortest path from s to a new node x , we go through each of its incident edges to see if there is a better way froms to other nodes thru x. listening to part 20-5 for i=1 to n, for each edge (s,v), dist[v]=d(s, v) last=s while ( ) select v such
every tree has at least two leaves, meaning that there is always an edge which is adjacent to a leaf.
example not np - how many tsp tours are there in g of  length less thank.
does the first idea which comes into my head work?
am i using all information that i am given about the problem?
kruskal's algorithm builds up connected components.
they depend on the particular input.
the  original numbers.
as implemented by a graduate student project, this backtrack search eliminates of the search space, when the pieces are ordered by decreasing mobility.
every vertex cover gives a satisfying truth assignment.
right if then swap(a[i],a[max]) heapify(a,max)
question: does the graph contain a clique of j vertices, ie. is  there a subset ofv of size j such that every pair of vertices  in the subset defines an edge ofg?
all are o(1) time operations.
example: the traveling salesman problem: given a weighted graph g, what tour  minimizes  .
go back to the definition!to show , we must show o and .
when talking about graph problems, it is most natural to work from a graph problem - the onlynp-complete one we have is vertex cover!
listening to part 8-18 red-black tree height lemma: a red-black tree with n internal nodes has height at most .
other algorithms deal correctly with negative cost edges.
listening to part 5-5 quicksort animations listening to part 5-6 pseudocode sort(a) quicksort(a,1,n) quicksort(a, low, high)
listening to part 25-4 36.5-1 prove that subgraph isomorphism is np-complete.
listening to part 25-3 in an independent set, there are no edges between two vertices.
the goal of all this is going to be a formal way to talk about the set of  problems which can be solved in polynomial time, and the set that cannot be.
better, use hamiltonian cycle, i.e. where all the weights are 1 or .
the diagonal moves up to the right as the computation progresses.
in the algorithms we have studied so far, correctness tended to be easier than efficiency.
but if any literal is true, we have n-3 free variables and n-3 remaining 3-clauses, so we can satisfy each of them.
however, a very simple strategy (heuristic) can get us a cover at most twice that of the optimal.
each candidate is ranked from 1 to n, and all permutations are  equally likely.
listening to part 12-10 what about dynamic programming?
while  theoretically any np-complete problem will do, choosing the correct one can  make it much easier.
polynomial time reductions a decision problem is np-hard if the time complexity on a  deterministic machine is within a polynomial factor of the complexity of any  problem innp.
suppose you have a robot arm equipped with a tool, say a soldering iron.
example: this graph contains a clique of size 5.
improving the complexity is an open question but there is a super-slick dynamic programming algorithm which also runs in  .
example:  and t=3754 answer: 1+16+64+256+1040+1093+1284 =
on each element of thekth diagonal |j-i| =
as we have seen, finding the minimum vertex cover is np-complete.
listening to part 22-2 what is a problem?
dynamic programming is a technique for efficiently computing recurrences by storing partial results.
for the previous example: listening to part 13-3 procedure matrixorder for i=1 to n do m[i, j]=0 for diagonal=1 to n-1 for i=1 to n-diagonal do j=i+diagonal faster(i,j)=k return [m(1, n)] procedure showorder(i, j) if (i=j) write ( ) else k=factor(i, j) write ``('' showorder(i, k) write ``*'' showorder (k+1, j) write ``)''
then t(n) can be bounded asymptotically as follows: listening to part 3-10 examples of the master theorem which case of the master theorem applies?
iff(n) dominates g(n), f is much larger (ie. slower) thang.
let n be the number of points in the set
listening to part 6-8 real world distributions consider the distribution of names in a telephone book.
once you get the hang of it,  it is surprisingly straightforward and pleasurable to do.
we partitioned the full audio track into sound clips, each corresponding to one page of lecture notes, and linked them to the associated text and images.
an  implementation of dijkstra's algorithm would be faster for sparse  graphs, and comes from using a heap of the vertices (ordered by distance), and  updating the distance to each vertex (if necessary) in time for each edge out  from freshly known vertices.
within the for statement, "key:=a[j]" is executed n-1 times.
which vertex can we never go wrong picking?
example 2: , although you try, and you try, and you try and you try, you can get no satisfaction.
given a solution to this ip instance, all variables will be 0 or 1.
our transformation will use boolean variables to maintain the state of the tm: variable range intended meaningvariable range intended meaning note that there are literals, a polynomial number if p(n) is polynomial.
let the n selected vertices from the first stage coorespond to  true literals.
c, , and are all constants independent of n. all of these definitions imply a constant beyond which they are satisfied.
integer partition - the one and only choice for problems whose hardness  seems to require using large numbers.
when we union, we can make the tree with fewer nodes the child.
a logarithm is simply an inverse exponential function.
since the extra big-oh costs of doubly-linkly lists is zero, we will usually assume they are, although it might not be necessary.
danny heep next: lecture 5 - quicksort up: table of contents
the pivot fits in the slot between them.
listening to part 22-8 if k=2, meaning  , create one new variable  and two new clauses:  , if k=3, meaning  , copy into the 3-sat instance as it is.
exact analysis of insertion sort count the number of times each line of pseudocode will be executed.
bin packing - how many bins of a given size do you need to hold n items of variable size?
satisfying one clause in dfs satisfied the whole formula.
that integer partition is a number problem, as opposed to the graph and logic problems we have seen to date.
n =o(n), by the master theorem.
we are concerned with the difference between algorithms which are polynomial and exponential in the size of the input.
listening to part 24-6 things to notice the reduction preserved the structure of the problem.
often, we can prove performance bounds on heuristics, that the resulting answer is withinc times that of the optimal one.
connect by an edge connect the two endpoints by an edge.
a good hash function: the first step is usually to map the key to a big integer, for example
listening to part 26-2 other np-complete problems open: graph isomorphism, composite number, minimum length triangulation.
picking the first or last candidate gives us a probability of 1/n of getting the best.
the total time is o(mn), but can we do better?
an algorithmic problem is specified by describing the set of instances it must work on and what desired properties the output must have.
maximum cliqueinstance: a graph g=( v, e) and integer .
what kind of machine is necessary to recognize this language?
listening to part 27-7 finding the optimal spouse
in fact, heapify performs better than , because most of the heaps we merge are extremely small.
dynamic programming computes recurrences efficiently by storing partial results.
satisfying one clause in dfs  satisfied the whole formula.
no boolean variable and its complement will both be true,  so it is a legal assignment with also must satisfy the clauses.
vertex cover - for any graph problems whose hardness depends upon selection.
by reduction from satisfiability any set instance has boolean variables and clauses.
why are recurrences good things?
example: although this tree has height , the total sum at each level decreases geometrically, so: the recursion tree framework made this much easier to see than with algebraic backsubstitution.
assume it starts at one of the k selector vertices.
possible permutations and computes the length of the longest edge for each gives an easy algorithm.
the moral: ``if you are gonna do the crime, make it worth the time!''
the original numbers.
the optimal strategy is clearly to sample some fraction of the candidates,  then pick the first one who is better than the best we have seen.
do we need to keep all o(mn) cells, since if we evaluate the recurrence filling in the columns of the matrix from left to right, we will never need more than two columns of cells to do what we need.
show that for large enough c and n. assume that it is true forn/2, then starting with basis cases t(2)=4, t(3)=5, lets us complete the proof for .
saving space in dynamic programming is very important.
next: lecture 20 - integer up: table of contents previous: lecture 18 - shortest lecture 20 - integer programming listening to part 22-6 36.4-5 give a polynomial-time algorithm to satisfy boolean formulas  in disjunctive normal form.
if we could find the correct pile for each card in constant time, and each pile getso(1) cards, this algorithm takes o(n) time.
now suppose my reduction translates g to y in o( p(n)): if my bo-billy subroutine ran in o(p'(n))
a problem in np for which a polynomial time algorithm would imply all languages innp are in p is called np -complete.
previous: lecture 17 - minimum lecture 19 - satisfiability listening to part 21-7 the theory of np-completeness several times this semester we have encountered problems for which we  couldn't find efficient algorithms, such as the traveling salesman problem.
the problem is np-complete, meaning that it is exceedingly unlikely that you will be able to find an algorithm with polynomial worst-case running time.
often, we can prove performance bounds on heuristics, that the resulting  answer is withinc times that of the optimal one.
solve the recurrence using the master theorem.
indeed, the dirty little secret of np-completeness proofs is that they are usually easier to recreate than explain, in the same way that it is usually easier to rewrite old code than the try to understand it.
the numbers from the edges will be  .
* if y has other fields, copy them, too.
set the literals correspondly to 1 variable true and the 0 to false.
proving a lower bound on the optimal solution is the key to getting an  approximation result.
all you must remember in this song is this template of size , and the current value ofn.
this model is useful and accurate in the same sense as the flat-earth model (whichis useful)!
listening to part 7-9 pointer based implementation we can maintain a dictionary in either a singly or doubly linked list.
our transformation will use boolean variables to maintain the state of the  tm: variable  range  intended meaning q[i, j]   at timei, m is in state h[i,j]  at time i, the read-write head is scanning tape square j s[i,j,k]  at time i, the contents of tape squarej is symbol note that there are  literals, a polynomial number if p(n)  is polynomial.
however, notice that the increase in level as a function of the amount of money you steal growslogarithmically in the amount of money stolen.
although using median-of-three turns the sorted permutation into the best case, we lose if insertion sort is better on the given data.
heapsort(a) build-heap(a)
certainly for any constant c we can find an n such that this is not true.
turing machines and cook's theorem cook's theorem proves that satisfiability is np-complete by  reducing all non-deterministic turing machines tosat.
i can  solve the bandersnatch problem ino(p(n)+p'(n ))
the key observation about this gadget is that there are only three  ways to traverse all the vertices: note that in each case, we exit out the same side we entered.
we realized that for any prefix, you want an optimal encoding which might leave you in every possible mode.
we know a problem is in np if we have a ndtm program to solve it  in worst-case timep[n], where p is a polynomial and n is the size of the input.
listening to part 20-12 the floyd-warshall algorithm an alternate recurrence yields a more efficient dynamic programming formulation.
it cannot be more because the target in that digit isk and it cannot be less because, with at most3 1's per edge/digit-column, no sum of these can carry over into the next column.
example: since the maximum value of f(v) given the constraints is , there is no solution.
the best worst-case bounds come from balanced binary trees, such as red-black trees.
we will now have to add clauses to ensure that these variables takes or the values as in the tm computation.
as an experiment in using the internet for distance learning, we have digitized the complete audio of all 23 lectures, and have made this available on the www.
listening to part 4-21 discrete event simulationsin simulations of airports, parking lots, and jai-alai - priority queues can be used to maintain who goes next.
listening to part 4-19 priority queues a priority queue is a data structure on sets of keys supporting the following operations: these operations can be easily supported using a heap.
if c is a clique in g, v-c is a vertex cover ing'.
number the vertices from1 to n. let be the shortest path from i to j using only vertices from 1, 2,...,k as possible intermediate vertices.
listening to part 4-20 applications of priority queues heaps as stacks or queues both stacks and queues can be simulated by using a heap, when we add a new time field to each item and order the heap according it this time field.
so a logarithmic number of multiplications  suffice for exponentiation.
in fact, the optimal is obtained by sampling the first n/e candidates.
listening to part 21-8 n f(n) = n f(n) =
n bottles of beer on the wall, n bottles of beer.
in designing algorithms for optimization problem - we must prove that the algorithm in fact gives the best possible solution.
steps 5, 6, 7 are harder to count.
the  non-leaf, since it is the only one which can also cover other edges!
a stack supports last-in, first-out operations: push and pop.
listening to part 24-9 for each variable, we create two vertices connected by an edge: at least two vertices per triangle must be in the cover to take care of edges in the triangle, for a total of at least2c vertices.
even  better, use hamiltonian path instead of cycle.
let be the height of the tree
the goal is to find a permutation of the vertices on the line which minimizes the maximum length of any edge.
further, any cover uses at least half as many vertices as the greedy cover.
although it works correctly on the previous example, other data causes trouble: listening to part 1-12 a correct algorithm
what is the simplest implementation?
we can calculate in linear time by storing small values: for i=1 to n moral: we traded space for time.
since robots are expensive, we need to find the order which minimizes the time (ie. travel distance) it takes to assemble the circuit board.
the group 6 clauses enforce the transition function of the machine.
the $10,000 question is whether a polynomial time simulation exists, or in  other words whetherp=np?.
if you are trying to route or schedule something, this is likely  your lever.
ex: the traveling salesman problem the guessing module can easily write a permutation of the vertices in polynomial time.
each instance of an optimization or decision problem can be encoded as  string on some alphabet.
further since neighboring points on the convex hull have neighboringx values, the convex hull returns the points sorted byx-coordinate, ie.
backtracking ensures correctness by enumerating all possibilities.
can we do better with sparse graphs?
proof: we must show that any problem in np is at least as hard as sat.
theorem: hamiltonian circuit is np-complete proof: clearly hc is in np-guess a permutation and check it out.
although this implementation uses an array, a linked list would eliminate the need to declare the array size in advance.
we seek to maximize our probability of getting the single best possible  spouse.
since it is a satisfying truth assignment, at least one of the three cross edges associated with each clause must already be covered - pick the other two vertices to complete the cover.
the fact that we can ignore minor differences in encoding is important.
(even for a fixed size problem) how does insertion sort do on sorted permutations?
this is essentially the same as prim's algorithm.
each edge in the initial graph will be  represented by the following component: all further connections to this gadget will be through vertices   , ,  and  .
we use a data structure called an incidence matrix to represent the graphg .
but it should be clear that the series converges.
if an arbitrary node is red can we color it black?if
every top-notch algorithm expert in the world (and countless other, lesser  lights) have tried to come up with a fast algorithm to test whether a given set  of clauses is satisfiable, but all have failed.
permutations of an n-element set - we cannot use dynamic programming to store the best solution for each subpermutation.
a simple  finite machine can check if the last symbol is zero: no memory is required, except for the current state.
the following problem arises often in manufacturing and transportation testing applications.
listening to part 14-10 dynamic programming and morphing morphing is the problem of creating a smooth series of intermediate images given a starting and ending image.
next: lecture 22 - techniques up: table of contents
lemma: if union(t,v) attaches the root of v as a subtree oft iff the number of nodes in t is greater than or equal to the number inv, after any sequence of unions, any tree with h/4 nodes has height at most .
listening to part 11-9 constructing all permutations how many permutations are there of an n-element set?
unfortunately, ease of expression moves in the reverse order.
to be interesting, an algorithm has to solve a general, specified problem.
to demonstrate how one goes about proving a problem hard, i accept the  challenge of showing how a proof can be built on the fly.
in a vertex cover we  need to have at least one vertex for each edge.
ask yourself questions.
a turing machine has a finite-state-control (its program), a two way  infinite tape (its memory) and a read-write head (its program counter)
thus t could not have been the mst.
the k chains correspond to the vertices in the cover.
limiting them to four means that i know a lot about these problems - which variants of these problems are hard and which are soft.
initialize each to to start.
note that the pivot element ends up in the correct place in the total order!
listening to part 20-9 dynamic programming and shortest paths the four-step approach to dynamic programming is: from the adjacency matrix, we can construct the following matrix: , if and is not in e d[i,j] =
using binary search and the decision version of the problem we can find the optimal tsp solution.
this is just like computing .
to prove it is complete, a reduction from must be provided.
total size of new graph: ge+k vertices and 12e+2kn+2e edges  construction is polynomial in size and time.
previous: lecture 6 - linearnext: lecture 8 - binary up: table of contents previous: lecture 6 - linear listening to part 8-1 9.1-3 show that there is no sorting algorithm which sorts at least instances ino(n) time.
the numbers from the edges will be .
this new graph has a traveling salesman tour of  costn iff the graph is hamiltonian.
a problem is np-complete if it is np-hard and in np .
listening to part 20-11 since the shortest path between any two nodes must use at most n edges (unless we have negative cost cycles), we must repeat that proceduren times (m=1 to n) for an algorithm.
we win whenever the best candidate occurs before any number from2 to i in the last n (1- 1/f) / f candidates.
let m(i,j) be the minimum number of multiplications necessary to compute .
this means the cost equals .
asymptotically, the base of the log does not matter: thus, , and note that is just a constant.
the supercomputer people pull this trick on the linpack benchmarks!
thus twenty comparisons suffice to find any name in the million-name manhattan phone book!
since we seek maximize our chances of getting the best, it never pays to pick someone who is not the best we have seen.
but if we get unlucky with our order of insertion or deletion, we could get linear height!
the total time iso(mn).
however, we can approximate the optimal euclidean tsp tour using minimum  spanning trees.
theorem: clique is np-complete proof: if you take a graph and find its vertex cover, the remaining  vertices form an independent set, meaning there are no edges between any two  vertices in the independent set, for if there were such an edge the rest of the  vertices could not be a vertex cover.
(c) if there are positive constants and c such that to the right of , the value of f(n) always lies on or above .
vertex: listening to part 18-8 finding the minimum weight fringe-edge takes o(n) time - just bump through fringe list.
how many 1's in a particular row?
a queue supports first-in, first-out operations: enqueue and dequeue.
an element in if ( ) is solution, print!
listening to part 3-7 try backsubstituting until you know what is going on also known as the iteration method.
and no is called adecision problem.
consider , , it has history = 2, degree = 1, and coefficients of 2 and 1.
each edge ing is incident on one or two cover vertices.
listening to part 3-3 recurrence relations many algorithms, particularly divide and conquer algorithms, have time complexities which are naturally modeled by recurrence relations.
there are several ways to characterize the shortest path between two nodes  in a graph.
let be the length of the shortest path from i to j using at mostm edges.
since there can be an exponential number of them, we cannot count them all in polynomial time.
guessing a subgraph of g and proving it is isomorphism to h takes  time, so it is in np.
heuristics heuristics are rules of thumb; fast methods to  find a solution with no requirement that it be the best one.
this is why we will seek to avoid exponential time algorithms.
non-deterministic turing machines suppose we buy a guessing module peripherial for our turing machine, which looks at a turing machine program and problem instance and in polynomial time writes something it says is an answer.
since this parabola is convex, every point is on the convex hull.
note that the  reduction did notsolve the problem - it just put it in a different  format.
in a vertex cover we need to have at least one vertex for each edge.
multiplying an matrix by a matrix (using the common algorithm) takes multiplications.
how about unsorted permutations?
yes, if for all n (b) is is ?
by the early 1970s, literally hundreds of problems were stuck in this limbo.
we can  repeat the process until the tree as 0 or 1 edges.
proof, by induction: now assume it is true for all tree with black height  0
listening to part 13-6 a 3-approximate match a match with one of each of three edit operations is:
we had to show that all problems innp could be reduced to sat to make sure we didn't miss a hard one.
while theoretically anynp-complete problem can be reduced to any other one, choosing the correct one makes finding a reduction much easier.
dynamic programmingdynamic programming is a technique for computing recurrence relations efficiently by sorting partial results.
i offer the following advice to those needing to prove the hardness of a  given problem: make your source problem as simple (i.e. restricted) as possible.
listening to part 14-2 16-1:
ms listening to part 21-9 suppose i gave you the following algorithm to solve the bandersnatch problem: bandersnatch(g)
for i = 1 to n/2 do union(i,i+1)
listening to part 7-5 stacks and queues sometimes, the order in which we retrieve data is independent of its content, being only a function of when it arrived.
lines 7-8 give x a new parent.
listening to part 24-5 we must show: in any sat solution, a true literal corresponds to a 1 in the ip, since if the expression is satisfied, at least one literal per clause in true, so the sum in the inequality is 1.
this has a hamiltonian path from start to stop iff the original  graph has a vertex cover of sizek.
let the first stage vertices define the truth  assignment.
it  cannot be more because the target in that digit isk and it cannot be  less because, with at most3 1's per edge/digit-column, no sum of these  can carry over into the next column.
to prove completeness, we show that vertex cover integer partition.
question: does the graph contain a clique of j vertices, ie. is there a subset ofv of size j such that every pair of vertices in the subset defines an edge ofg?
improving the complexity is an open question but there is a super-slick dynamic programming algorithm which also runs in .
the time-to-serve is a roughly linear function of the totallevel.
listening to part 8-4 binary search trees a binary search tree labels each node in a binary tree with a single key such that for any nodex, and nodes in the left subtree of x have keys and all nodes in the right subtree of x have key's .
selecting the right source problem makes a big difference is how difficult  it is to prove a problem hard.
theorem: 3-sat is np-complete proof: 3-sat is np - given an assignment, just check that each clause is covered.
however, the loops are so tight and it is so short and simple that it runs better in practice by a constant factor.
it repeatedly adds the smallest edge to the spanning tree that does not create a cycle.
since the number of nodes is related to the height, the height of the final tree will increase only if both subtrees are of equal height!
to prove completeness, we show 3-sat and vc.
to prove completeness, we show that vertex cover  integer partition.
if we scan from left to right, we get an open tour which uses all points to the left of our scan line.
a binary tree is a rooted tree where each node contains at most two children.
local replacement - make local changes to the structure.
go back to the definition!
we keep calculating the same value over and over!
to show it is complete, we use vertex cover.
example: pivot about 10.
not according to the big oh.
insert(a) insert(b) insert(c) insert(d) we can't easily use randomization - why?
such distribution techniques can be used on strings instead of just numbers.
however, the loops are so tight and it is so  short and simple that it runs better in practice by a constant factor.
we will take this program and create from it an instance of satisfiability  such that it is satisfiable if and only if the input string was in the language.
lines 14-16 if the removed node is deleted, copy.
a simple finite machine can check if the last symbol is zero: observe that solving decision problems can be thought of as formal language recognition.
thus tsp is np-complete  if we can show hc isnp-complete.
each turing machine has access to a two-way infinite tape (read/write) and  a finite state control, which serves as the program.
the master theorem: let and b>1 be constants, let f(n) be a function, and let t(n) be defined on the nonnegative integers by the recurrencethe master theorem: let and b>1 be constants, let f(n) be a function, and let t(n) be defined on the nonnegative integers by the recurrence where we interpret n/b to mean either or .
we can run dijkstra's algorithm n times (once from each possible start vertex) to solve all-pairs shortest path problem in .
let  be the length of the shortest path from i to j using at mostm edges.
we set up k additional vertices and connect each  of these to then start points and n end points of each chain.
chromatic number - how many colors do you need to color a graph?
if all edge weights are positive, thesmallest edge incident to s, say (s,x), defines d(s,x).
question: is better than ?
listening to part 12-9 computing fibonacci numbers implementing it as a recursive procedure is easy but slow!
listening to part 22-4 consider the following logic problem: instance: a set v of variables and a set of clauses c overv. question: does there exist a satisfying truth assignment for c?
example: pivot about 10 | 17 12 6 19 23 8 5 | 10 | 5 12 6 19 23 8 | 17 5 | 12 6 19 23 8 | 17 5 | 8 6 19 23 | 12 17 5 8 | 6 19 23 | 12 17 5 8 6 | 19 23 | 12 17 5 8 6 | 23 | 19 12 17 5 8 6 ||23 19 12 17 5 8 6 10 19 12 17 23 as we scan from left to right, we move the left bound to the right when the element is less than the pivot, otherwise we swap it with therightmost unexplored element and move the right bound one step closer to the left.
all leaves can be identified and trimmed in o(n) time during  a dfs.
cook's theorem proved satisfiability was np-hard by using a polynomial time reduction translating each problem innp into an instance of sat: the proof of cook's theorem, while quite clever, was certainly difficult and complicated.
thus  this is still within twice optimal!
instance:  ,  ,  ,  ,  , solution:  cost= 27
listening to part 7-3 8.3-2 how many calls are made to random in randomized quicksort in the best and worst cases?each call to random occurs once in each call to partition.
running the same program twice on the same permutation causes it to do exactly the same thing, but running it on different permutations of the same data causes a different sequence of comparisons to be made on each.
we claim that any subtree rooted at x has at least - 1 internal nodes, where bh(x) is the black height of node x .
what if we know  for all i,j? since w[k, k]=0 this gives us a recurrence, which we can evaluate in a bottom up fashion: for i=1 to n for j=1 to n for k=1 to n =min(  ,  ) this is an  algorithm just like matrix multiplication, but it only goes  fromm to m+1 edges.
yes, so case 1 applies and .
vc in g integer partition in sgiven k vertices covering g, pick the k cooresponding vertex/numbers.
by the early 1970s, literally hundreds of problems were stuck in this  limbo.
binary search is an example of an algorithm.
thus dynamic programming can only be efficient when there are not too many partial results to compute!
listening to part 6-6 bucketsort suppose we are sorting n numbers from 1 to m, where we know the numbers are approximately uniformly distributed.
p = unescessaraly t =
listening to part 3-6 guess a solution and prove by induction to guess the solution, play around with small values for insight.
note that the shortest path fromi to j, , using at mostm edges consists of the shortest path from i to k using at most m-1 edges + w(k, j) for some k. listening to part 20-10 this suggests that we can compute all-pair shortest path with an induction based on the number of edges in the optimal path.
we can also store the value ofk in another triangle matrix to reconstruct to order of the optimal parenthesisation.
listening to part 18-9 kruskal's algorithm since an easy lower bound argument shows that every edge must be looked at to find the minimum spanning tree, and the number of edges , prim's algorithm is optimal in the worst case.
while there are still unvisited points i =
listening to part 3-9
consider ``the k days of christmas''.
the reduction from vertex cover will create n+m numbers from g .
previous: lecture 21 - vertex lecture 23 - approximation algorithms and cook's theorem listening to part 26-1 36.5-5 prove that hamiltonian path is np-complete.
for sorting, we can check if the values are already ordered, and if so output them.
moral: there cannot be too many good cases for any sorting algorithm!
asymptotically, any polynomial function of n does not matter:note that since , and .
listening to part 27-4 things to notice
the translation was initiated by algorithms on mon jun 2 09:21:39 edt 1997
observe that any  convex hull algorithm also gives us a complicated but  correct sorting algorithm as well.
listening to part 18-10 why is kruskal's algorithm correct?
listening to part 21-10 a nice example of a reduction goes from sorting numbers to the convex hull  problem: we must translate each number to a point.
it must then go through one of the chains of gadgets until it reaches a different selector vertex.
unnecessarily finding such a matching seems like a hard problem because we must figure out where you addblanks, but we can solve it with dynamic programming.
we must transform every instance of a known np-complete problem to an instance of the problem we are interested in.
it is  trivial to finda vertex cover of a graph - just take all the vertices.
if k>3, meaning  , create n-3 new variables and n-2 new clauses in a chain:  , ... if none of the original variables in a clause are true, there is no way to  satisfy all of them using the additional variable:
in our work on reconstructing text typed on an (overloaded) telephone keypad, we had to select which of many possible interpretations was most likely.
example 1: and a clause is satisfied when at least one literal in it is true.
listening to part 20-9 dynamic programming and shortest paths the four-step approach to dynamic programming is: characterize the structure of an optimal solution.
if we could do convex hull in better than , we could sort faster than - which violates our lower bound.
so a logarithmic number of multiplications suffice for exponentiation.
previous: lecture 16 - applicationsnext: lecture 18 - shortest up: table of contents previous: lecture 16 - applications listening to part 20-7 25.1-1 give two more shortest path trees for the following graph: there are two choices for how to get to the third vertex x, both of which cost 5.
indeed, the dirty  little secret of np-completeness proofs is that they are usually easier to  recreate than explain, in the same way that it is usually easier to rewrite old  code than the try to understand it.
in both cases, d[i,0] =
deleting any edge from a tsp tour leaves a path, which is a tree of  weight at least that of the mst!
deleting another edge from this cycle leaves a connected graph, and if it is one frome-e' the cost of this tree goes down.
listening to part 11-7 recursive backtracking recursion can be used for elegant and easy implementation of backtracking.
i+1 let be the closest unvisited point to visit return to from this algorithm is simple to understand and implement and very efficient.
this large number must be reduced to an integer whose size is between 1 and the size of our hash table.
i found it an enjoyable experience.
listening to part 11-11 covering the chess board
by the earlier analysis, any cover must have at least n+2c vertices.
each turing machine has access to a two-way infinite tape (read/write) and a finite state control, which serves as the program.
we must keep track of two things - (1) the size of the remaining argument to the recurrence, and (2) the additive stuff to be accumulated during this call.
we can represent all the possibilities in a triangle matrix.
worst caseif the input is sorted in descending order, we will have to slideall of the already-sorted elements, so , and step 5 is executed next: lecture 2 - asymptotic notation up: table of contents
question: is there an ordering of the vertices of a weighted graph such that ?
proof: we must show that any problem in np is at least as  hard as sat.
listening to part 7-15 good and bad hash functions the first three digits of the social security number the birthday paradox no matter how good our hash function is, we had better be prepared for collisions, because of the birthday paradox.
a problem is a general question, with parameters for the input and conditions on what is a satisfactory answer or solution.
why not use this reduction to give a polynomial-time algorithm for 3-sat?
what do we know about g'(n)
maximum clique instance: a graph g=( v, e) and  integer .
by recurring on the remaining candidate numbers, we get the answer in t (n) =
recall the sorting lower bound of  .
the euclidean traveling-salesman problem is the problem of determining the shortest closed tour that connects a given set ofn points in the plane.
however, it breaks down when we try to use it for2-sat, since there is no way to stuff anything into the chain of clauses.
exponential functions, like the amount owed on a n year mortgage at an interest rate of per year, are functions which grow distressingly fast, as anyone who has tried to pay off a mortgage knows.
all one must memorize is: on the kth day of christmas, my true love gave to me, on the first day of christmas, my true love gave to me, a partridge in a pear tree
you take one down and pass it around n-1 bottles of beer on the ball.
listening to part 26-4 techniques for proving np-completeness restriction - show that a special case of the problem you are  interested in isnp-complete.
in spring 1996, i taught my analysis of algorithms course via enginet, the suny stony brook distance learning program.
this simulation is not as efficient as a normal stack/queue implementation, but it is a cute demonstration of the flexibility of a priority queue.
listening to part 27-6
the sharper the consequences for doing what is undesired, the easier it is to prove if and only if.
listening to part 25-7 we might get only one instance of each edge in a cover - but we are free  to take extra edge/numbers to grab an extra1 per column.
listening to part 6-5 non-comparison-based sorting all the sorting algorithms we have seen assume binary comparisons as the basic primative, questions of the form ``isx before y?''.
we will transform each clause independantly based on itslength .
listening to part 26-7 amplify the penalties for making the undesired transition.
formal language theory concerns the study of how powerful a machine you  need to recognize whether a string is from a particular language.
an algorithm is the thing which stays the same whether the program is in pascal running on a cray in new york or is in basic running on a macintosh in kathmandu!
listening to part 7-19 performance on set operations with either chaining or open addressing: pragmatically, a hash table is often the best data structure to maintain a dictionary.
the easiest way to do this is to be bold with your penalties, to punish anyone trying to deviate from your proposed solution.
but this is too slow!
theorem: integer partition is np-complete.
the shifflett's of charlottesville
every tree has at least two leaves, meaning that there is always an edge  which is adjacent to a leaf.
many other seemingly  smarter ones can give a far worse performance in the worst case.
the easiest argument says that g contains a hp but no hc iff (x ,y) in g such that adding edge (x, y) to g causes to have a hc, so calls to a hc function solves hp.
finally, we will connect each literal in the flat structure to the  corresponding vertices in the triangles which share the same literal.
rough analysis of heapify heapify on a subtree containing n nodes takes
remember the direction to reduction!
correctnessfor any algorithm, we must prove that it always returns the desired output for all legal instances of the problem.
if d[i,j] = 0, if i=j this tells us the shortest path going through no intermediate nodes.
if there weren clauses and m total literals  in the sat instance, this transform takeso(m) time, so sat and 3-sat.
each of these complexities defines a numerical function - time vs. size!
rules for algorithm design the secret to successful algorithm design, and problem solving in general, is to make sure you ask the right questions.
adding the bounding constants shows .
next: none up: table of contents previous: lecture 22 - techniques about this document ...
think strategically at a high level, then build gadgets to enforce  tactics.
listening to part 22-9 having at least 3-literals per clause is what makes the problem  difficult.
listening to part 18-6
the average-case complexity of the algorithm is the function defined by an average number of steps taken on any instance of sizen.
the sharper the consequences for doing what is undesired,  the easier it is to prove if and only if.
listening to part 12-6 optimization problems
delete all edges from the graph except the edges we  selected.
such a translation from instances of one type of problem to instances of  another type such that answers are preserved is called areduction.
however, it isnot correct!
this problem is intimately relates to the traveling salesman.
do there exist languages which can be  verified in polynomial time and still take exponential time on deterministic  machines?
even better, follows from using fibonacci heaps, since they permit one to do a decrease-key operation ino(1) amortized time.
let be the length of the longest sequence ending with the ith character: sequence 5 2 8 7 3 1 6 4sequence 5 2 8 7 3 1 6 4 to find the longest sequence - we know it ends somewhere, so length = listening to part 14-5 the principle of optimality to use dynamic programming, the problem must observe the principle of optimality, that whatever the initial state is, remaining decisions must be optimal with regard the state following from the first decision.
what do we know about g''(n) =
if so, we need to be able to identify fringe vertices and the minimum cost edge associated with it, fast.
fraud and deceit; forgery; offenses involving altered or counterfeit instruments other than counterfeit bearer obligations of the united states.
note that negative cost cycles render the problem of finding the shortest path meaningless, since you can always loop around the negative cost cycle more to reduce the cost of the path.
in a  clique, there are always between two vertices.
note the little-oh - it means ``grows strictly slower than''.
the non-leaf, since it is the only one which can also cover other edges!
listening to part 2-3 names of bounding functions now that we have clearly defined the complexity functions we are talking about, we can talk about upper and lower bounds on it: got it?
tree-search(x, k) if (x = nil) and (
and no is  called adecision problem.
listening to part 22-10 p versus np the precise distinction between whether a problem is in p or np is somewhat technical, requiring formal language theory and turing machines to state correctly.
if the read-write head is not on tape squarej at time i , it doesn't change ....
the storage size for n depends on its value, but bits suffice.
next: lecture 23 - approximation up: table of contents
tree-insert(t,z) y = nil x = root[t] while do y = x if key[z]  z) then /
i usually consider four and only four problems as candidates for my hard  source problem.
listening to part 14-8 dynamic programming and high density bar codes symbol technology has developed a new design for bar codes, pdf-417 that has a capacity of several hundred bytes.
claim: this graph will have a vertex cover of size n+2c if and only if the expression is satisfiable.
both can be used to traverse a tree, but the order is completely different.
in bucket sort, our hash function mapped the key to a bucket based on the first letters of the key.
differences may be: approximate matching is important in genetics as well as spell checking.
there are  literals and  clauses in all, so the transformation is done in  polynomial time!
the buckets will correspond to letter ranges instead of just number ranges.
i prefer to describe the ideas of an algorithm in english, moving to pseudocode to clarify sufficiently tricky details of the algorithm.
listening to part 27-4 things to notice example: pick one of the two vertices instead of both (after all, the middle edge is already covered)
assume true to k-1 nodes.
recursively define the value of an optimal solution.
we can do unions and finds in , good enough for kruskal's algorithm.
thus we want to know what kind of luck to expect.
since we seek maximize our chances of getting the best, it never pays to  pick someone who is not the best we have seen.
next: lecture 19 - satisfiability up: table of contents
we could try all possible orderings of the points, then select the ordering which minimizes the total length: for each of the n!
with uniformly distributed keys, the expected number of items per bucket is 1.
if i know that  is a lower-bound to compute bandersnatch, then  must be a  lower-bound to compute bo-billy.
don't be afraid to add extra constraints or freedoms in order to make your problem more general (at least temporarily).
example not np - how many tsp tours are there in g of length less thank.
``if you pick this, then you have to pick up this huge set which  dooms you to lose.''
it turns out that resolution gives a polynomial time algorithm for2-sat.
you want to morph an eye to an eye, not an ear to an ear.
we can check if it is correct by summing up the weights of the  special edges in the permutation and see that it is less thank.
we want a subset of vertices  which covers each edge.
is that all she wrote?
but now that we have a known np-complete problem in sat.
listening to part 25-5 integer partition (subset sum) instance: a set of integers s and a target integer t.
formal language theory concerns the study of how powerful a machine you need to recognize whether a string is from a particular language.
exactly one of the ip variables associated with a given sat variable is 1.
depends on the vertex degree.
clearly,  , since for any dtm program we can run it on a non-deterministic  machine, ignore what the guessing module is doing, and it will just as fast.
matrix multiplication is not communitive, so we cannot permute the order of the matrices without changing the result.
the group 6 clauses enforce the transition function of the  machine.
because we replaced a chain of edges  by the edge, the triangle inequality ensures the tour only gets shorter.
listening to part 12-12 example consider , where a is , b is , c is , and d is .
our integer programming problem will have twice as many variables as the sat instance, one for each variable and its compliment, as well as the following inequalities: listening to part 24-4 for each variable in the set problem, we will add the following constraints: both ip variables are restricted to values of 0 or 1, which makes them equivalent to boolean variables restricted to true/false.
listening to part 2-4 o, , and (a) if there exist positive constants , , and such that to the right of , the value off(n) always lies between and inclusive.
although the heuristic is simple, it is not stupid.
theorem: integer programming is np-hard proof:
note that the theory of np-completeness does not stipulate that it is hard to get close to the answer, only that it is hard to get the optimal answer.
component design - these are the ugly, elaborate constructions listening to part 26-5 the art of proving hardness proving that problems are hard is an skill.
a post-processing clean-up step (delete any unecessessary vertex) can  only improve things in practice, but might not help the bound.
listening to part 6-7 we can use bucketsort effectively whenever we understand the distribution of the data.
``how can i force that either a or b but not both are chosen?''
a problem is in np if, given the answer, it is possible to verify  that the answer is correct within time polynomial in the size of the input.
listening to part 13-11 how much space do we need?
previous: lecture 3 - recurrence listening to part 5-1 4-2 find the missing integer from 0 to n using o(n )
this means that exactly one of and are true!
polynomial time reductions a decision problem is np-hard if the time complexity on a deterministic machine is within a polynomial factor of the complexity of any problem innp.
instead, use tsp on instances restricted to the triangle inequality.
while (there are non-tree vertices) select minimum weight edge between tree and fringe add the selected edge and vertex to the tree
in real life, certain people cut in line.
because now all nodes may not have the same black height.
the problem instances are encoded as strings and strings  in the language if and only if the answer to the decision problem is yes!
to construct all n! permutations, set up an array/vector of n cells, where the value of is an integer from 1 to n which has not appeared thus far in the vector, corresponding to theith element of the permutation.
there are only substrings between 1 and n.
thus we have encoded the information about the initial graph.
to avoid visiting a vertex more than once, each chain is associated with a  selector vertex.
turing machines and cook's theorem cook's theorem proves that satisfiability is np-complete by reducing all non-deterministic turing machines tosat.
the identify for the sum of a geometric series is
what happened to our lower bound!
however, we can approximate the optimal euclidean tsp tour using minimum spanning trees.
listening to part 1-11 closest pair tour always walking to the closest point is too restrictive, since that point might trap us into making moves we don't want.
the possible ip instances which result are a small subset of the possible  ip instances, but since some of them are hard, the problem in general must be  hard.
the reason - any binary tree with n leaves has n-1 internal nodes, each of which corresponds to a call to partition in the quicksort recursion tree.
(this is why base-4 number were chosen).
since the tour is a hc, all gadgets are traversed.
neat, sweet, and np-complete!
why isn't it trivial?
here, four of the eight vertices are enough to cover.
listening to part 24-10 claim: this graph will have a vertex cover of size n+2c
but do they reduce the space complexity?
it is not obvious that ip  np, since the numbers assigned to the  variables may be too large to write in polynomial time - don't be too hasty!
the  following algorithm implements it: for k=1 to n for i=1 to n for j=1 to n this obviously runs in  time, which asymptotically is no better than a  calls to dijkstra's algorithm.
listening to part 7-11 unsorted list implementation list-search(l, k) x = head[l] while x <> nil and key[x] <>
thus tsp is np-complete if we can show hc isnp-complete.
either what you are looking for will be there or you might find a closely  related problem to use in a reduction.
sincen can be large, we want to memorize songs which require only a small amount of brain space, i.e. memory.
if we want the cost of comparing all of the pattern against all of the text, such as comparing the spelling of two words, all we are interested in isd[n,m].
that integer partition is a number problem, as opposed to the  graph and logic problems we have seen to date.
to give the cover, at least one cross-edge must be covered, so the truth  assignment satisfies.
since proving an exponential time lower bound for a problem innp would make us famous, we assume that we cannot do it.
this is not a special case of hamiltonian cycle!
list-search'(l, k) x = next[nil[l]] while x <> nil[l] and key[x] <>k do x = next[x] return x list-insert'(l, x) next[x] = next[nil[l]] prev[next[nil[l]]] = x next[nil[l]] = x prev[x] = nil[l] list-delete'(l, x) next[prev[x]] <> next[x] next[prev[x]] = prev[x] listening to part 7-13 hash tables hash tables are a very practical way to maintain a dictionary.
n to 1 do swap(a[1],a[i]) n = n - 1 heapify(a,1)
the ideal union-find tree has depth 1: o(n)?
big numbers is what makes integer partition hard!
the cleanest proof modifies the vc and hc reduction from the book: listening to part 27-3 approximating vertex cover
in general, there are at most nodes of height h, so the cost of building a heap is: since this sum is not quite a geometric series, we can't apply the usual identity to get the sum.
any comparison-based sorting program can be thought of as defining a decision tree of possible executions.
note that there are many possible ways to encode the input graph:  adjacency matrices, edge lists, etc.
we will augment an adjacency list with fields maintaining fringe information.
thuso(m ) space is sufficient to evaluate the recurrence without changing the time complexity at all.
the optimal cover is one vertex, the greedy heuristic is two vertices, while the new/bad heuristic can be as bad asn-1 .
if there is a satisfying truth assignment, that means at least one of the three cross edges from each triangle is incident on a true vertex.
in the traditional version of tsp - a salesman wants to plan a drive to visit all his customers exactly once and get back home.
listening to part 1-14 the ram model algorithms are the only important, durable, and original part of computer sciencebecause they can be studied in a machine and language independent way.
backtrack(a, k) if a is a solution, print(a) else { k =
see if you can use the master theorem to provide an instant asymptotic solution the master theorem: let and b>1 be constants, letf(n) be a function, and let t(n) be defined on the nonnegative integers by the recurrence where we interpret n/b as or .
elementary data structures such as stacks, queues, lists, and heaps will be the ``of-the-shelf'' components we build our algorithm from.
each child can be identified as either a left or right child.
many applications, such as finding the center or diameter of a graph,  require finding the shortest path between all pairs of vertices.
suppose the clause contains k literals.
we know a problem is in np if we have a ndtm program to solve it in worst-case timep[n], where p is a polynomial and n is the size of the input.
we can spend as much time as we want doing other things provided we don't look at extra bits.
listening to part 2-2 exact analysis is hard!
usually, problems don't have to get that large before the faster algorithm wins.
listening to part 20-4 dijkstra's algorithm we can use dijkstra's algorithm to find the shortest path between any two verticess and t in g.
listening to part 1-10 nearest neighbor tour a very popular solution starts at some point and then walks to its nearest neighbor first, then repeats from , etc. until done.
instance: , , , , , a problem with answers restricted to yes
next: lecture 8 - binary up: table of contents
big numbers is what makes integer  partition hard!
permutations of the n points
example 2:  , although you try, and you try, and you try and you try, you can get no  satisfaction.
example: and t=3754 answer: 1+16+64+256+1040+1093+1284 =
you should be asking these kinds of questions.
as we will see, induction provides a useful tool to solve recurrences - guess a solution and prove it by induction.
it contains a list of several hundred problems known to benp-complete.
because it trys all n! permutations, it is extremely slow, much too slow to use when there are more than 10-20 points.
also note, the problem is asking us to minimize the number of bits we read.
every top-notch algorithm expert in the world (and countless other, lesser lights) have tried to come up with a fast algorithm to test whether a given set of clauses is satisfiable, but all have failed.
insertion sort one way to sort an array of n elements is to start with empty list, then successively insert new elements in the proper position: at each stage, the inserted element leaves a sorted list, and after n insertions contains exactly the right elements.
this is the most important question in computer science.
don't let this issue confuse you - the important idea here is of  reductions as a way of proving hardness.
pick and visit an initial point i = 0
integer partition in s vc in g any solution  tos must contain exactly k vertex/numbers.
if we do the reduction the  other way, all we get is a slow way to solvex, by using a subroutine  which probably will take exponential time.
since there can be an exponential number of them,  we cannot count them all in polynomial time.
chaining is easy, but devotes a considerable amount of memory to pointers, which could be used to make the table larger.
might our algorithm be faster than it seems?" typically in our analysis, we will say that since we are doing at most x operations of at most y time each, the total time is o( x y).
the actual alignment - what got matched, substituted, and deleted - can be reconstructed from the pattern/text and table without an auxiliary storage, once we have identified the cell with the lowest cost.
prove that 0-1 integer programming is np-hard (hint: reduce from 3-sat).
listening to part 19-1 lessons from the backtracking contest listening to part 19-3 winning optimizations listening to part 19-10 shortest paths finding the shortest path between two nodes in a graph arises in many different applications: listening to part 20-1 shortest paths and sentence disambiguation
``how can i clean up the things i did not select?''
listening to part 27-9 for a given fraction 1/f, what is the probability of finding the best?
open addressing we can dispense with all these pointers by using an implicit reference derived from a simple function: the reason for using a more complicated science is to avoid long runs from similarly hashed keys.
listening to part 1-9 correctness is not obvious!
but can we do better?
in the check example, the expected number of moves per items is small, sayc.
a 2 3 4 5 6 7 8 9 10 j q k a 2 3 4 5 6 7 8 9 10 j q k a 2 3 4 5 6 7 8 9 10 j q k a 2 3 4 5 6 7 8 9 10 j q k with only a constant number of cards left in each pile, you can use insertion sort to order by suite and concatenate everything together.
tsp remains hard even when the distances are euclidean distances in the  plane.
never use the general traveling salesman problem (tsp) as a target  problem.
theorem: vertex cover is np-complete.
you are trying to translate one problem into another, while making them  stay the same as much as possible.
kruskal's algorithm is also greedy.
listening to part 2-13 the refrain most popular songs have a refrain, which is a block of text which gets repeated after each stanza in the song: bye, bye miss american pie drove my chevy to the levy but the levy was dry them good old boys were drinking whiskey and rye singing this will be the day that i die.
for one of its children might be red.
copyright © 1993, 1994, 1995, 1996, nikos drakos, computer based learning unit, university of leeds.
since the set of3-sat instances is smaller and  more regular than thesat instances, it will be easier to use 3 -sat for future reductions.
however, if we overestimate too much, our bound may not be as tight as it should be!
for i=1 to n-1 do for each pair of endpoints (x,y) of partial paths
= o(g(n)) if there are positive constants and c such that to the right of , the value of f (n) always lies on or below .
what is the shortest tree with leaves?
listening to part 11-12 the backtracking contest: bandwidth the bandwidth problem takes as input a graph g, with n vertices and m edges (ie.
select the n vertices cooresponding to the true literals to be in  the cover.
prove that 0-1 integer programming is np-hard (hint: reduce from 3-sat).this is really the exact same problem as the previous integer programming problem, slightly concealed by: listening to part 24-8 vertex cover instance: a graph g=(v, e), and integer question: is there a subset of at most k vertices such that every has at least one vertex in the subset?
if none of the original variables in a clause are true, there is no way to satisfy all of them using the additional variable:
let p be a pattern string and t a text string over the same alphabet.
by using a more clever algorithm, we eventually were able to prove no solution existed, in less than one day's worth of computing.
although this is slick, observe that even  is slower than running  dijkstra's algorithm starting from each vertex!
listening to part 25-2 when talking about graph problems, it is most natural to work from a graph  problem - the onlynp-complete one we have is vertex cover!
every vertex cover must contain n first stage vertices and 2c second stage vertices.
listening to part 8-5 searching in a binary tree dictionary search operations are easy in binary trees ...
previous: lecture 20 - integernext: lecture 22 - techniques up: table of contents previous: lecture 20 - integer hamiltonian cycle instance: a graph g question: does the graph contains a hc, i.e. an ordered of the vertices ?
listening to part 25-6 how many 1's are there in each column?
listening to part 27-9 for a given fraction 1/f, what is the probability of finding the  best?
different paths from the root to leaves in the decision tree, ie.
so it goes for any constant base.
example p - is there a path from s to t in g of length less than k. example np - is there a tsp tour in g of length less than k. given the tour, it is easy to add up the costs and convince me it  is correct.
previous: lecture 3 - recurrencenext: lecture 5 - quicksort up: table of contents
a triangulation of a polygon is a set of non-intersecting diagonals which partitions the polygon into diagonals.
suppose the clause  contains k literals.
formal languages and the theory of np-completeness the theory of np-completeness is based on formal languages and turing machines, and so we will must work on a more abstract level than usual.
listening to part 6-3 can we sort in better than ?
algorithms are the ideas behind computer programs.
first of all, play with the problem.
using arrays or unsorted linked lists as the data structure, operation a takes o(n) time and operation b takes o(1).
now suppose my reduction translates g to y in o( p(n)): the second argument is the idea we use to prove problems hard!
let the n selected vertices from the first stage coorespond to true literals.
note that if both vertices associated with an edge are in the cover, the  gadget will be traversal in two pieces - otherwise one chain suffices.
in general, adding a new vertex k+1 helps iff a path goes through  it, so
thus satisfying  the constraint is equivalent to satisfying the clause!
if it is one, includes the cooresponding edge/number to give two per  column.
add to t component (v)=component(w) if we can test components in , we can find the mst in !
next: none up: table of contents previous: lecture 22 - techniquesnext: none up: table of contents previous: lecture 22 - techniques this document was generated using the latex2html translator version 96.1 (feb 5, 1996)
thus the following reduction suffices.
to avoid visiting a vertex more than once, each chain is associated with a selector vertex.
divide-and-conquer is seems efficient because there is no overlap, but ...
any problem innp has a non-deterministic tm program which  solves it in polynomial time, specificallyp(n).
think of the decision tree which can do this.
let the first stage vertices define the truth assignment.
return the answer of bo-billy(y) as the answer to g.
the set of all instances which return true for some problem define a language.
queue implementation a circular queue implementation requires pointers to the head and tail elements, and wraps around to reuse array elements.
we seek algorithms which are correct and efficient.
listening to part 4-17 heapsort heapify can be used to construct a heap, using the observation that an isolated element forms a heap of size 1.
how do we find the best order?
whenever the number of unthreated squares exceeds the sum of the maximum number of coverage remaining in unplaced squares, we canprune.
although there are only terms before we get to t(1), it doesn't hurt to sum them all since this is a fast growing geometric series: listening to part 3-8 recursion trees drawing a picture of the backsubstitution process gives you a idea of what is going on.
but how efficient is it?
formal languages and the theory of np-completeness the theory of np-completeness is based on formal languages and turing  machines, and so we will must work on a more abstract level than usual.
+ 1 compute , the candidate kth elements given v. k =
listening to part 27-8 for example, if the input permutation is we see (3,1,2) after three candidates.
line insertionsort(a) #inst.
what if we know for all i,j? since w[k, k]=0 this gives us a recurrence, which we can evaluate in a bottom up fashion: for i=1 to n for j=1 to n for k=1 to n =min( , ) this is an algorithm just like matrix multiplication, but it only goes fromm to m+1 edges.
clique and subgraph isomorphism.
it's already sorted, all 's are 1.
by placing all the things in a priority queue and pulling them off in order, we can improve performance over linear search or sorting, particularly if the weights change.
listening to part 20-11 since the shortest path between any two nodes must use at most n edges (unless we have negative cost cycles), we must repeat that proceduren times (m=1 to n) for an  algorithm.
vc in g integer partition in s given k vertices covering g, pick the k cooresponding  vertex/numbers.
hash functions it is the job of the hash function to map keys to integers.
listening to part 25-7 we might get only one instance of each edge in a cover - but we are free to take extra edge/numbers to grab an extra1 per column.
for any  other problem, we can prove itnp-hard by polynomially transforming sat  to it!
-- 5 in insertion sort, the cost of each insertion is the number of items which we have to jump over.
listening to part 2-11 working with the asymptotic notation suppose
if the keys were uniformly distributed, then each bucket contains very few keys!
any exponential dominates every polynomial.
the improved  tour is1-2-3-5-8-9-6-4-7-10-11-1.
``collisions'' were the set of keys mapped to the same bucket.
thus inverse exponential functions, ie. logarithms, grow refreshingly slowly.
we can repeat the process until the tree as 0 or 1 edges.
one clause can always be satisfied iff it does not contain both a variable and its complement.
in order of increasing precision, we have english, pseudocode, and real programming languages.
step 5 is executed times.
this always is confusing at first - it seems bass-ackwards.
the transformation captures the essence of why ip is hard - it has  nothing to do with big coefficients or big ranges on variables; for restricting  to 0/1 is enough.
now that we have shown3-sat is np-complete, we may use it for further reductions.
while theoretically anynp-complete problem can be reduced to any other  one, choosing the correct one makes finding a reduction much easier.
listening to part 25-1 starting from the right problem as you can see, the reductions can be very clever and very complicated.
a hash function is a mathematical function which maps keys to integers.
if a polynomial time transform exists, then sat must be np -complete, since a polynomial solution to sat gives a polynomial time  algorithm to anything innp.
we can only usek x vertex/numbers, because of  the high order digit of the target.
obviously, this gives a spanning tree, but is it minimal?
once you understand dynamic programming, it is usually easier to reinvent certain algorithms than try to look them up!
in covering the chess board, we prune whenever we find there is a square which wecannot cover given the initial configuration!
listening to part 22-3 example: the traveling salesman decision problem.
listening to part 27-5 the euclidean traveling salesman
thus it can be solved mechanically!
just changing a problem a little can make the difference between it being inp or np-complete: p np-completep np-complete listening to part 26-4 techniques for proving np-completeness listening to part 26-5 the art of proving hardness proving that problems are hard is an skill.
different leaves in the tree.
still, it is my preferred method.
select the selector edges to complete the circuit.
the total effort of bucketing, sorting buckets, and concatenating the sorted buckets together iso(n).
the n-queens problem
10  0.01  s  0.1  s  1  s  3.63 ms 20  0.02  s  0.4  s  1 ms  77.1  years 30  0.03  s  0.9  s  1 sec  years 40  0.04  s  1.6  s  18.3  min 50  0.05  s  2.5  s  13 days 100  0.1  s  10  s  years 1,000  1.00  s  1
for backtracking to be efficient, we must prune the search space.
there are subsets of an n-element set - we cannot use dynamic programming to store the best solution for each.
connect these in a triangle.
even better,  follows from using fibonacci heaps, since they permit one to  do a decrease-key operation ino(1) amortized time.
each edge ing is incident on one or two cover  vertices.
problem: is there a subset of s which adds up exactly to t ?
creating and reading off the points takes o(n) time.
adding the lower bounding constants shows .
(g may have a hp but not cycle)
listening to part 20-3 finding shortest paths
there is a 1/i probability of that, so,
as you can see, the reductions can be very clever and complicated.
when in doubt, keep it simple, stupid (kiss).
listening to part 13-10 boundary conditions for string matching what should the value of d[0,i] be, corresponding to the cost of matching the firsti characters of the text with none of the pattern?
there is no satisfying assigment since must be false (third clause), so must be false (second clause), but then the first clause is unsatisfiable!
after trimming off the covered edges, we have a smaller tree.
listening to part 6-4 once you believe this, a lower bound on the time complexity of sorting follows easily.
to give the cover, at least one cross-edge must be covered, so the truth assignment satisfies.
listening to part 14-7 minimum length triangulation
the idea is simply that looking an item up in an array is once you have its index.
sort(s) call subroutine convex-hull on this point set.
we must construct another graph.
although mergesort is , it is quite inconvenient for implementation with arrays, since we need space to merge.
dynamic programming is best understood by looking at a bunch of different examples.
that is the last bit of the missing integer!
the first use of pruning to deal with the combinatorial explosion was by the king who rewarded the fellow who discovered chess!
the first thing you should do when you suspect a problem might be  np-complete is look in garey and johnson,computers and intractability.
although this looks similar to the previous recurrence, it isn't.
when the tree consists only  of an isolated edge, pick either vertex.
reading from the equation, a=4, b=2, and f(n) =
the easiest argument says that g contains a hp but no hc iff (x ,y) in g such that adding edge (x, y) to g causes to have a hc, so  calls to a hc function solves hp.
how can we determine the second-to-last bit?
at least two vertices per triangle must be in the cover to take care of  edges in the triangle, for a total of at least2c vertices.
this works on the same principle as a roulette wheel!
a recurrence relation is an equation which is defined in terms of itself.
don't be afraid to add extra constraints or freedoms in order to make your  problem more general (at least temporarily).
listening to part 20-12 the floyd-warshall algorithm an alternate recurrence yields a more efficient dynamic programming  formulation.
listening to part 14-3 why will they let us assume that no two x-coordinates are the same?
from the adjacency matrix, we can construct the following matrix: , if  and  is not in e d[i,j] =
if (low  i and 2i+1 >
this very slow growth means it pays to commit one crime stealing a lot of money, rather than many small crimes adding up to the same amount of money, because the time to serve if you get caught is much less.
thus , and since our recursion tree has 0 and 1 as leaves, means we have calls!
note - there are a total of bits, so we are not allowed toread the entire input!
listening to part 27-2 option 1: algorithm fast in the average case examples are  branch-and-bound for the traveling salesman problem, backtracking algorithms,  etc.
below, i give a possible series of questions for you to ask yourself as you try to solve difficult algorithm design problems: listening to part 6-11 listening to part 6-12 listening to part 7-1 8.2-3 argue that insertion sort is better than quicksort for sorting checksin the best case, quicksort takes .
that's the way, uh-huh, uh-huh
the resulting short lists were easily sorted, and could just as easily be searched!
this means that exactly one of and  are true!
deleting any edge from a tsp tour leaves a path, which is a tree of weight at least that of the mst!
as with bucket sort, it assumes we know that the distribution of keys is fairly well-behaved.
there are two aspects to any data structure: the fact that we can describe the behavior of our data structures in terms of abstract operations explains why we can use them without thinking, while the fact that we have different implementation of the same abstract operations enables us to optimize performance.
nil list-delete(l, x) if prev[x] <> nil then next[prev[x]]
listening to part 26-6 selecting the right source problem makes a big difference is how difficult it is to prove a problem hard.
listening to part 14-4
listening to part 7-8 dynamic set operations perhaps the most important class of data structures maintain a set of items, indexed by keys.
there are only n(n-1)/2 possible subtrees of a binary search tree, each described by a maximum and minimum key, so we can use it for optimizing binary search trees.
make sure you understand the direction of reduction now - and think back to this when you get confused.
these can be implemented byunion and find operations: is t= find u= find return (is t=u?)
then t( n) can be bounded asymptotically as follows: exact analysis of heapify
proof: first, we note that integer partition is in np.
even better, use hamiltonian path instead of cycle.
a careful study of what properties we do need for our  reduction tells us a lot about the problem.
next: lecture 21 - vertex up: table of contents
think of the equality as meaning in the set of functions.
because it is so easy to cheat with the best case running time, we usually don't rely too much about it.
limiting them to four means that i know a lot about these  problems - which variants of these problems are hard and which are soft.
although we might further speed the program by an order of magnitude, we need to prune more nodes!
listening to part 4-16 the lessons of heapsort, i "are we doing a careful analysis?
proof: vc in in np - guess a subset of vertices, count them, and show that each edge is covered.
that depends on what data structures are used.
observe that solving decision problems can be thought of as formal  language recognition.
we  use a data structure called an incidence matrix to represent the graphg .
if then the space complexity is still o(n) since it is only halved (if the verse-size = refrain-size): listening to part 2-14 the k days of christmas
to prove it is complete, a reduction from must be  provided.
euclidean geometry satisfies the triangle inequality,  .
to demonstrate how one goes about proving a problem hard, i accept the challenge of showing how a proof can be built on the fly.
listening to part 19-8 proof: by induction on the number of nodes k, k=1 has height0.
the class of languages we can recognize in time polynomial in the length of the string or a non-deterministic turing machine is callednp.
listening to part 20-8 all-pairs shortest path notice that finding the shortest path between a pair of vertices (s, t) in worst case requires first finding the shortest path from s to all other vertices in the graph.
although this is slick, observe that even is slower than running dijkstra's algorithm starting from each vertex!
what bounds can we establish on s(n)?
it must then  go through one of the chains of gadgets until it reaches a different selector  vertex.
listening to part 8-13 balanced search trees all six of our dictionary operations, when implemented with binary search trees, takeo(h), where h is the height of the tree.
if e' is contained in a mstt and e is the smallest edge in e- e' which does not create a cycle, .
hey, i have got a recurrence!
theorem: hamiltonian circuit is np-complete proof: clearly hc is in np-guess a permutation and check  it out.
a problem is a general question, with parameters for the input  and conditions on what is a satisfactory answer or solution.
this is where the dreaded big o notation comes in!
start at the root - if that is not the one we want, search either left or right depending upon whether what we want is or then the root.
example: the traveling salesman problemlet be the cost of the optimal tour fori to 1 that goes thru each of the other cities once here there can be any subset of instead of any subinterval - hence exponential.
example: is the string a binary representation of a even number?
a problem with answers restricted to yes
example p - is there a path from s to t in g of length less than k. example np - is there a tsp tour in g of length less than k. given the tour, it is easy to add up the costs and convince me it is correct.
if c is a clique in g, v-c is a  vertex cover ing'.
best of all, use hamiltonian  path on directed, planar graphs where each vertex has total degree 3.
this document was generated using the latex2html  translator version 96.1 (feb 5, 1996)
we can improve this to  with the observation that any path using at most 2m edges is the function of paths using at most m edges each.
our assessment of each candidate is relative to what we have seen before.
stack-empty(s) if top[s] = 0 then return true else return false push(s, x) pop(s) if stack-empty(s) then error ``underflow'' else return s[top[s] + 1] listening to part 7-7
listening to part 7-4 elementary data structures ``mankind's progress is measured by the number of things we can do without thinking.''
next: lecture 18 - shortest up: table of contents
``how can i force that a is taken  before b?''
listening to part 2-15 100 bottles of beer
theentire semesters lectures, over thirty hours of audio files, fit comfortably onthe algorithm design manual cd-rom, which also includes a hypertext version of the book and a substantial amount of software.
make sure you  understand the direction of reduction now - and think back to this when you get  confused.
prove by induction: height6pt width4pt listening to part 3-5 solving recurrences no general procedure for solving recurrence relations is known, which is why it is an art.
listening to part 2-6 testing dominance f(n) dominates g(n) if , which is the same as sayingg(n)=o(f(n)).
since it is a satisfying truth assignment, at least one of the three  cross edges associated with each clause must already be covered - pick the  other two vertices to complete the cover.
best of all, use hamiltonian path on directed, planar graphs where each vertex has total degree 3.
if 3-sat is np-complete, it implies sat is np-complete but not visa-versa, perhaps long clauses are what makes sat difficult?!
listening to part 25-5 instance: a set of integers s and a target integer t. problem: is there a subset of s which adds up exactly to t ?
note that the number of characters is since every word in a song is at most 34 letters long - supercalifragilisticexpialidocious!
there are two choices for how to get to vertex v, both of which cost 9.
note that a slight modification to this construction would prove 4 -sat, or5-sat,... also np-complete.
alternate between looking for an algorithm or a reduction if you get  stuck.
lines in banks are based on queues, while food in my refrigerator is treated as a stack.
a turing machine has a finite-state-control (its program), a two way infinite tape (its memory) and a read-write head (its program counter)
this heuristic must always produce cover, since an edge is only deleted when it is adjacent to a cover vertex.
listening to part 14-6 when can you use dynamic programming?
complexity  if we use adjacency lists and a boolean array to mark what is  known.
``is bit[j] in a[i]'' queries.
thus finding the largest clique is np-complete: if vc is a vertex cover in g, then v-vc is a  clique ing'.
for convenience, from now on we will talk only about decision  problems.
therefore, any cover of just these edges must include one vertex per edge, or half the greedy cover!
i need a volunteer to pick a random problem from the 400+ hard problems in  the back of garey and johnson.
unfortunately, many words are mispelled.
suppose in the previous example all the keys happened to be 1.
a deque is a double ended queue and supports all four operations: push, pop, enqueue, dequeue.
each of my lectures that semester was videotaped, and the tapes made available to off-site students.
when you can't prove hardness, it likely pays to change your thinking at least for a little while to keep you honest.
expressing algorithmswe need some way to express the sequence of steps comprising an algorithm.
to show that longest path or hamiltonian path is np-complete, add  start and stop vertices and distinguish the first and last selector vertices.
a k-approximate match between p and t is a substring oft with at most k differences.
clearly, satisfiability is in np, since we can guess an assignment of true, false to the literals and check it in polynomial time.
to fill each cell, we need only consider three other cells, not o( n) as in other examples.
this subset of k vertex/numbers must contain at least one edge-list per column, since if not there is no way to account for the two in each column of the target integer, given that we can pick up at most one edge-list using the edge number.
cook's theorem - satisfiability is np-complete!
previous: nonenext: lecture 2 - asymptotic notation up: table of contents
the set of all instances which return true for some  problem define a language.
clearly this reduction can be done in polynomial time.
asymptotic notation are as well as we can practically deal with complexity functions.
for a cover to have n+2c vertices, all the cross edges must be incident on a selected vertex.
exchanging the maximum element with the last element and calling heapify repeatedly gives an sorting algorithm, named heapsort.
the numbers from the vertices will be a base-4 realization of rows from  the incidence matrix, plus a high order digit: ie.
ask all the n integers what their last bit is and see whether 0 or 1 is the bit which occurs less often than it is supposed to.
we can take a shortest path to the next unvisited vertex.
let the number of elements that have to be slide right to insert the j th item.
previous: lecture 20 - integer lecture 22 - techniques for proving hardness hamiltonian cycle question: does the graph contains a hc, i.e. an ordered of the vertices  ?
we  also couldn't prove an exponential time lower bound for the problem.
example: sortinginput: a sequence of n numbers output: the permutation (reordering) of the input sequence such as .
since the goal of the problem is to find a permutation, a backtracking program which iterates through all then!
when you can't prove hardness, it  likely pays to change your thinking at least for a little while to keep you  honest.
for sorting, this means even if (1) the input is already sorted, or (2) it contains repeated elements.
listening to part 24-9 for each variable, we create two vertices connected by an edge: to cover each of these edges, at least n vertices must  be in the cover, one for each pair.
in general, adding a new vertex k+1 helps iff a path goes through it, so
c is satisfied when true.
yes, for , so there exists a c  a[max]) then max =
the shortest path from s to s, d(s,s )=0.
this for this song, .
there are three possible parenthesizations: the order makes a big difference in real computation.
previous: lecture 1 - analyzingnext: lecture 3 - recurrence up: table of contents
with no intermediate vertices, any path consists of at most one edge, so .
when the tree consists only of an isolated edge, pick either vertex.
if there weren clauses and m total literals in the sat instance, this transform takeso(m) time, so sat and 3-sat.
thus if we complement a graph  (have an edge iff there was no edge in the original graph), a clique becomes an  independent set and an independent set becomes a clique!
a problem is in np if, given the answer, it is possible to verify that the answer is correct within time polynomial in the size of the input.
now suppose we have a vertex cover of size  .
refrains made a song easier to remember, since you memorize it once yet sing ito(n) times.
checkers - does black have a forced win from a given position?
greedy algorithms, which makes the best local decision at each step, occasionally produce a global optimum - but you need a proof!
listening to part 20-2 the final system worked extremely well - identifying over 99% of characters correctly based on grammatical and statistical constraints.
the reason is that we will do all our design and analysis for the ram model of computation: we measure the run time of an algorithm by counting the number of steps.
a simple implementation is to represent each set as a tree, with pointers from a node to its parent.
example: a solution to this is , .
listening to part 5-3 partitioning the elements
the search tree labeling enables us to find where any key is.
if , but it is true if , so case 2 applies and .
by adding the other two vertices to the cover, we cover all edges associated with the clause.
next: lecture 20 - integer up: table of contents previous: lecture 18 - shortestnext: lecture 20 - integer up: table of contents previous: lecture 18 - shortest listening to part 22-6 36.4-5 give a polynomial-time algorithm to satisfy boolean formulas in disjunctive normal form.
theorem: 3-sat is np-complete proof: 3-sat is np - given an assignment, just check that  each clause is covered.
the worst case is usually fairly easy to analyze and often close to the average or real running time.
and look, the two parameters which describe my optimal tour are the two endpoints.
we can use an array to store the length of the shortest path to each node.
from the leftmost point in the hull, read off the points from left to right.
if we use an array of buckets, each item gets mapped to the right bucket in o(1) time.
if we do the reduction the other way, all we get is a slow way to solvex, by using a subroutine which probably will take exponential time.
the optimal cover is one vertex, the greedy  heuristic is two vertices, while the new/bad heuristic can be as bad asn-1 .
make union(t, u) find returns the name of the set and union sets the members oft to have the same name as u.
how fast is kruskal's algorithm?
each column (digit) represents an edge.
thus if we complement a graph (have an edge iff there was no edge in the original graph), a clique becomes an independent set and an independent set becomes a clique!
at each entry  point to a gadget, check if the other vertex is in the cover and traverse the  gadget accordingly.
next: lecture 13 - dynamic up: table of contents
but what if we want the cheapest match between the pattern anywhere in the text?
this subset of k vertex/numbers must contain at least one  edge-list per column, since if not there is no way to account for the two in  each column of the target integer, given that we can pick up at most one  edge-list using the edge number.
do while do (*advance*)
since such an edge exists,t could not be a mst.
the initial or boundary condition terminate the recursion.
i usually consider four and only four problems as candidates for my hard source problem.
thus in our discussions, we will assume that all edge weights are positive.
hence, any problem which solves this problem is equivalent to a machine which recognizes whether an instance is in the language!
we have agreed that the best, worst, and average case complexity of an algorithm is a numerical function of the size of the instances.
by sterling's approximation, a better bound is where e=2.718.
in any sat solution, a true literal corresponds to a 1 in the ip,  since if the expression is satisfied, at least one literal per clause in true,  so the sum in the inequality is 1.
each instance of an optimization or decision problem can be encoded as string on some alphabet.
if we construct our heap from bottom to top using heapify, we do not have to do anything with the lastn/2 elements.
play around with the problem by constructing examples to get insight into it.
note that the only way all four of these can be satisfied is if z is true.
k +1 compute while do = an element in = backtrack(a, k) } backtracking can easily be used to iterate through all subsets or permutations of a set.
all leaves can be identified and trimmed in o(n) time during a dfs.
a subset of the set of strings over some alphabet is a formal language .
but how fast is prim's?
one clause can always be satisfied iff it does not  contain both a variable and its complement.
this means we need only store two rows of the table.
bentley suggested simplifying the problem by restricting attention to bitonic tours, that is tours which start at the leftmost point, go strictly left to right to the rightmost point, and then go strictly right back to the starting point.
let g=g' and  , the  complete subgraph onk nodes.
the complexity of prim's algorithm is independent of the number of edges.
level: 6 (b) specific offense characteristics (1) if the loss exceeded $2,000, increase the offense level as follows: loss(apply the greatest) increase in levelloss(apply the greatest) increase in level the federal sentencing guidelines are designed to help judges be consistent in assigning punishment.
previous: lecture 1 - analyzing listening to part 3-1 problem 2.1-2: show that for any real constants a and b , b > 0, to show , we must show o and .
in a real sense, listening to all the audio is analogous to sitting through a one-semester college course on algorithms!
finally, we will connect each literal in the flat structure to the corresponding vertices in the triangles which share the same literal.
a program for a non-deterministic tm is: space on the tape for guessing a solution and certificate to permit  verification.
the best height we could hope to get is , if the tree was perfectly balanced, since
once we have selected a pivot element, we can partition the array in one linear scan, by maintaining three sections of the array:  pivot, and unexplored.
euclidean geometry satisfies the triangle inequality, .
listening to part 14-9 originally, symbol used a greedy algorithm to encode a string, making local decisions only.
listening to part 2-9 federal sentencing guidelines 2f1.1.
since a binary tree of height h has at most leaves, we know , or .
in an order statistic tree, each node x is labeled with the number of nodes contained in the subtree rooted inx.
theorem: let g be a weighted graph and let .
to convince ourselves it really is an answer, we can run another program to check it.
listening to part 8-14 perfectly balanced trees perfectly balanced trees require a lot of work to maintain: therefore, when we talk about "balanced" trees, we mean trees whose height is , so all dictionary operations (insert, delete, search, min/max, successor/predecessor) take time.
to show that our reduction is correct, we must show that: select the n vertices cooresponding to the true literals to be in the cover.
for comparison, note that there are seven shifflett's (of various spellings) in the 1000 page manhattan telephone directory.
let g=g' and , the complete subgraph onk nodes.
thus convex hull must take as well!!!
np observe that any ndtm program which takes time p(n) can  simulated in time on a deterministic machine, by running the checking program   times, once on each possible guessed string.
starting from the right problem
listening to part 19-9 can we do better?
assuming some  arbitrary order to the edges incident on a particular vertex, we can link  successive gadgets by edges forming a chain of gadgets.
if vc is a vertex cover in g, then v-vc is a clique ing'.
with no intermediate vertices, any path consists of at most one  edge, so .
yes, for , so case 3 might apply.
this only gives the cost of the optimal matching.
listening to part 13-4 a dynamic programming solution has three components: listening to part 13-5 approximate string matching a common task in text editing is string matching - finding all occurrences of a word in a text.
note that if both vertices associated with an edge are in the cover, the gadget will be traversal in two pieces - otherwise one chain suffices.
after the bucketing phase, we have: problems like this are why we worry about the worst-case performance of algorithms!
still, with other ideas (some type of pruning or best-first search) it can be effective for combinatorial search.
the numbers from the vertices will be a base-4 realization of rows from the incidence matrix, plus a high order digit: ie. becomes .
the last row could be exactly half filled.
enqueue(q, x) q[tail[q]] x if tail[q] = length[q] then tail[q] 1 else tail[q] tail[q] + 1 dequeue(q) x = q[head[q]] if head[q] = length[q] then head[q] = 1 else head[q] = head[q] + 1 return x a list-based implementation would eliminate the possibility of overflow.
the target integer will be why?
a vertex cover instance consists of a graph and a constantk, the minimum size of an acceptable cover.
d[i, j] = the minimum number of differences between and the segment oft ending at j. d[i, j] is the minimum of the three possible ways to extend smaller strings: once you accept the recurrence it is easy.
i need a volunteer to pick a random problem from the 400+ hard problems in the back of garey and johnson.
this heuristic must always produce cover, since an edge is only deleted  when it is adjacent to a cover vertex.
listening to part 27-2 dealing with np-complete problems option 1: algorithm fast in the average caseexamples are branch-and-bound for the traveling salesman problem, backtracking algorithms, etc. option 2: heuristicsheuristics are rules of thumb; fast methods to find a solution with no requirement that it be the best one.
what we can do is prove that it is at least as hard as any problem in np.
listening to part 24-2 a perpetual point of confusion note carefully the direction of the reduction.
however, how can we avoid revisiting cities?
number the vertices from1 to n. let  be the shortest path from i to j using only  vertices from 1, 2,...,k as possible intermediate vertices.
lecture sound../sounds/lec4-17a.au heapsort animations listening to part 4-18 the lessons of heapsort, ii always ask yourself, ``can we use a different data structure?''
for a given alphabet of symbols 0, 1, &, we can form an infinite set ofstrings or words by arranging them in any order: `&10', `111111',`&&&', and `&'.
since the bounding constants don't necessary cancel, we know nothing about the lower bounds on g'+g'' because we know nothing about lower bounds onf, g. suppose and .suppose
it just makes it more difficult to analyze.
listening to part 8-2 9.1-4 show that the lower bound for sorting still holds with ternary comparisons.
ask the numbers which ended with the correct last bit!
besides, the asymptotic answer won't change so long the fraction is less than one.
previous: lecture 17 - minimumnext: lecture 19 - satisfiability up: table of contents previous: lecture 17 - minimum listening to part 21-7 the theory of np-completeness several times this semester we have encountered problems for which we couldn't find efficient algorithms, such as the traveling salesman problem.
however, a very simple strategy (heuristic) can get us a cover at most twice  that of the optimal.
in optimization problems, we are interested in finding a thing which maximizes or minimizes some function.
we must transform all instances of  clique into some instances of subgraph isomorphism.
making a heuristic more complicated does not necessarily make it better.
clique is a special case of  subgraph isomorphism!
example: pick one of the two vertices instead of both (after all, the  middle edge is already covered)
listening to part 24-3 integer programming instance: a set v of integer variables, a set of inequalities  over these variables, a functionf(v) to maximize, and integer b. question: does there exist an assignment of integers to v such  that all inequalities are true and ?
properly compressed, the full semester's audio requires less than 300 megabytes of storage, which is much less than i would have imagined.
if there is a satisfying truth assignment, that means at least  one of the three cross edges from each triangle is incident on a true vertex.
listening to part 24-8 vertex cover instance: a graph g=(v, e), and integer question: is there a subset of at most k vertices such that every   has at least one vertex in the subset?
given a weighted graph g and integer k, does there exist  a traveling salesman tour with cost k?
all inequalities are  - multiply both sides by -1 to reverse the  constraint from to  if necessary.
thus the algorithm must be correct.
if we were allowed to visit cities more than once, doing a depth-first traversal of a mst, and then walking out the tour specified is at most twice the cost of mst.
thus convex hull must take  as well!!!
thus satisfying the constraint is equivalent to satisfying the clause!
the second argument is the idea we use to prove problems hard!
the 2/3 comes from merging heaps whose levels differ by one.
if you have an algorithm which runs in time, take it, because this is blindingly fast even on very large instances.
this suggests a dynamic programming-like strategy, where we store the distance froms to all nearby nodes, and use them to find the shortest path to more distant nodes.
listening to part 22-7 3-satisfiability instance: a collection of clause c where each clause contains exactly3 literals, boolean variable v. question: is there a truth assignment to v so that each clause is satisfied?
sometimes the reason you cannot prove hardness is that there is an  efficient algorithm to solve your problem!
conclusion: deletion takes time proportional to the height of the tree.
listening to part 13-9 what do we return?
call subroutine convex-hull on this point set.
don't let this issue confuse you - the important idea here is of reductions as a way of proving hardness.
let s(n) be the space complexity of a song which lasts forn units of time.
listening to part 8-3 binary search trees ``i think that i shall never see a poem as lovely as a tree poem's are wrote by fools like me but only g-d can make a tree `` - joyce kilmer binary search trees provide a data structure which efficiently supports all six dictionary operations.
but the goal of this assignment is to find as practically good an algorithm as possible.
however, intuitively a problem is in p, (ie. polynomial) if it can be solved in time polynomial in the size of the input.
no efficient, correct algorithm exists for the traveling salesman problem, as we will see later.
i like it, uh-huh, huh reference: d. knuth, `the complexity of songs', comm. acm, april 1984, pp.18-24 next: lecture 3 - recurrence up: table of contents
proof: our strategy; first we bound the number of nodes in any subtree, then we bound the height of any subtree.
by inspection , since the last n/2 terms of the product are each greater thann/2.
what does the hint mean?
``if you pick this, then you have to pick up this huge set which dooms you to lose.''
we will be using each edge exactly twice.
then inorder-tree-walk(left[x]) print key[x] inorder-tree-walk(right[x]) a-b-c-d-e-f-g-h listening to part 8-10 tree insertion do a binary search to find where it should be, then replace the termination nil pointer with the new item.
listening to part 27-7 finding the optimal spouse listening to part 27-8 for example, if the input permutation is we see (3,1,2) after three candidates.
a direct recursive implementation of this will be exponential, since there is a lot of duplicated work as in the fibonacci recurrence.
it turns out that resolution gives a polynomial time  algorithm for2-sat.
there is no satisfying assigment since  must be false (third clause), so   must be false (second clause), but then the first clause is unsatisfiable!
example: since the maximum value of f(v) given the constraints is  ,  there is no solution.
listening to part 27-1 37.1-3 give an efficient greedy algorithm that finds an optimal  vertex cover of a tree in linear time.
no two of these edges share a vertex.
assuming the initialization for substring matching, we seek the cheapest matching of the full pattern ending anywhere in the text.
np observe that any ndtm program which takes time p(n) can simulated in time on a deterministic machine, by running the checking program times, once on each possible guessed string.
previous: lecture 11 - backtrackingnext: lecture 13 - dynamic up: table of contents previous: lecture 11 - backtracking listening to part 13-1 16.3-5 give an algorithm to find the longest montonically increasing sequence in a sequence ofn numbers.
when none of the three problems below seem  appropriate, i go back to the source.
each element is contained in a node, and thename of the set is the key at the root: listening to part 19-7
never be afraid to ask for another explanation of a problem until it is clear.
since any sat solution will also satisfy the 3-sat instance and any3-sat solution sets variables giving a sat solution - the problems are equivallent.
note that the cost of airfares is an example of a distance  function which violates the triangle inequality.
since only binary comparisons (less than or greater than) are used, the decision tree is a binary tree.
such a translation from instances of one type of problem to instances of another type such that answers are preserved is called areduction.
but the time it takes to sing it is
from a 3 -sat instance withn variables and c clauses, we construct a graph with2n+3c vertices.
neat, sweet, and np-complete.
we will take this program and create from it an instance of satisfiability such that it is satisfiable if and only if the input string was in the language.
thus it requires only space to store the optimal cost for each of them.
read polya's book how to solve it.
recall the sorting lower bound of .
note that the run time changes with the permutation instance!
clearly, satisfiability is in np, since we can guess an  assignment of true, false to the literals and check it in polynomial time.
greedy algorithmsin greedy algorithms, we always pick the next thing which locally maximizes our score.
there are up to n possible candidates we will see over our  lifetime, one at a time.
listening to part 24-3 integer programming instance: a set v of integer variables, a set of inequalities over these variables, a functionf(v) to maximize, and integer b. question: does there exist an assignment of integers to v such that all inequalities are true and ?
with the implicit tree defined by array positions, (i.e. the ith position is the parent of the2ith and (2i+1)st positions) the leaves start out as heaps.
it remains np-complete even for restricted classes of trees.
the cleanest proof modifies the vc and hc reduction from  the book: listening to part 27-3 approximating vertex cover
for each clause, we create three new  vertices, one for each literal in each clause.
what do kids sing on really long car trips?
i have found dynamic programming to be one of the most useful algorithmic techniques in practice: listening to part 12-11 multiplying a sequence of matrices suppose we want to multiply a long sequence of matrices .
no boolean variable and its complement will both be true, so it is a legal assignment with also must satisfy the clauses.
the improved tour is1-2-3-5-8-9-6-4-7-10-11-1.
the dnf formula can become exponentially large and hence the reduction cannot  be done in polynomial time.
by adding the other two vertices to the cover, we cover all edges  associated with the clause.
k = key[x]) then return x if (k  nil)
we  are concerned with the difference between algorithms which are polynomial and  exponential in the size of the input.
plug the recurrence back into itself until you see a pattern.
we can do this matching on a line-by-line basis: listening to part 14-11 this algorithm was incorported into a morphing system, with the following results: listening to part 14-1 problem solving techniques most important: make sure you understand exactly what the question is asking - if not, you have no hope of answer it!!
listening to part 26-8 now watch me try it!
if then and return since all possible orderings are considered, we are guaranteed to end up with the shortest possible tour.
listening to part 12-1 15.1-5 given an element x in an n-node order-statistic binary tree and a natural numberi, how can the i th successor of x be determined in time.
``how can i force that  either a or b but not both are chosen?''
we seek to find the minimum length triangulation.
many applications, such as finding the center or diameter of a graph, require finding the shortest path between all pairs of vertices.
that for each (v,x), last=v complexity if we use adjacency lists and a boolean array to mark what is known.
an implementation of dijkstra's algorithm would be faster for sparse graphs, and comes from using a heap of the vertices (ordered by distance), and updating the distance to each vertex (if necessary) in time for each edge out from freshly known vertices.
sweepline algorithms in computational geometry listening to part 4-22
i offer the following advice to those needing to prove the hardness of a given problem: never use the general traveling salesman problem (tsp) as a target problem.
this problem can be solved if our data structure supports two operations: what we are interested in is get(rank(x)+i).
any edge where both vertices are in the same connected component create a cycle.
if k=1, meaning  , create two new variables  and four new 3 -literal clauses: ,  ,  ,  .
compute this recurrence in a bottom-up fashion.
to enable the robot arm to do a soldering job, we must construct an ordering of the contact points, so the robot visits (and solders) the first contact point, then visits the second point, third, and so forth until the job is done.
knowing the dominance relation between common functions is important because we want algorithms whose time complexity is as low as possible in the hierarchy.
dominates if a > b since complexity 10 20 30 40 50 60complexity 10 20 30 40 50 60 listening to part 2-7 logarithms it is important to understand deep in your bones what logarithms are and where they come from.
(this is why base-4 number were  chosen).
however, we will not use it much in proving the efficiency of our algorithms, since the worst-case time is unpredictable.
there are literals and clauses in all, so the transformation is done in polynomial time!
any instance of sat  can be translated to an instance ofx in polynomial time.
the worst case ``shouldn't'' happen if we understand the distribution of our data.
after adding a vertex to the tree, running through its adjacency list to update the cost of adding fringe vertices (there may be a cheaper way through the new vertex) can be done ino(n) time.
for the traveling salesman, we can check if the points lie on a line, and if so output the points in that order.
listening to part 4-15 proof of convergence series convergence is the ``free lunch'' of algorithm analysis.
let c[k,n] be the optimal cost partial tour where the two endpoints arekw(x,y).
we can check if it is correct by summing up the weights of the special edges in the permutation and see that it is less thank.
prevents any other possibilites).
what order will this generate the subsets of ?
since the composition of two polynomial time reductions can be  done in polynomial time, all we need show is that sat, ie.
every input we see is drawn from the uniform distribution.
there are several ways to characterize the shortest path between two nodes in a graph.
if we could do  convex hull in better than , we could sort faster than  - which violates our  lower bound.
``how can i force that a is taken before b?''
observe that any convex hull algorithm also gives us a complicated but correct sorting algorithm as well.
for each clause  in the sat instance, construct a constraint: thus at least one ip variable must be one in each clause!
thus we cannot go wrong with the greedy strategy the way we could with the traveling salesman problem.
listening to part 3-4 recursion is mathematical induction!
listening to part 7-2 8.3-1 why do we analyze the average-case performance of a randomized algorithm, instead of the worst-case?in a randomized algorithm, the worst case is not a matter of the input but only of luck.
what does this mean?
integer partition in s vc in gany solution tos must contain exactly k vertex/numbers.
further  since neighboring points on the convex hull have neighboringx values,  the convex hull returns the points sorted byx-coordinate, ie.
since any two different permutations of n elements requires a different sequence of steps to sort, there must be at leastn!
*/ return y lines 1-3 determine which node y is physically removed.
(again, the prevention of carrys across digits prevents any other possibilites).
our maximization function and bound are relatively unimportant: b=0.
listening to part 5-2 quicksort
the key observations are listening to part 12-13 a recurrence for this is: if there are n matrices, there are n+1 dimensions.
thus this is still within twice optimal!
is there a song which eliminates even the need to count?is there a song which eliminates even the need to count?
clearly the smallest vertex cover gives the biggest independent  set, and so the problems are equivallent - delete the subset of vertices in one  from the total set of vertices to get the order!
specifically, each piece can threaten a certain maximum number of squares (queen 27, king 8, rook 14, etc.)
ex: the traveling salesman problem the guessing module can easily write a permutation of the vertices in  polynomial time.
listening to part 22-4 satisfiability consider the following logic problem: instance: a set v of variables and a set of clauses c overv. question: does there exist a satisfying truth assignment for c?
= next[x] else head[l] = next[x]
we can run dijkstra's algorithm n times (once from each possible  start vertex) to solve all-pairs shortest path problem in .
thus at least one ip variable must be one in each clause!
my favorites are: listening to part 26-7 you are trying to translate one problem into another, while making them stay the same as much as possible.
each side of each edge gadget is associated with a vertex.
thus finding the maximum independent set must be np-complete!
sometimes the reason you cannot prove hardness is that there is an efficient algorithm to solve your problem!
we can only usek x vertex/numbers, because of the high order digit of the target.
all of  these problems are equally hard, and the more you can restrict the problem you  are reducing, the less work your reduction has to do.
example: a solution to this is  ,  .
note that time complexity is every bit as well defined a function as or you bank account as a function of time.
cook's theorem proved satisfiability was np-hard by using a  polynomial time reduction translating each problem innp into an  instance of sat: since a polynomial time algorithm for sat would imply a  polynomial time algorithm for everything innp, sat is np -hard.
however, bad things happen when we assume the wrong distribution.
an arbitrary node is red can we color it black?
extract the optimal solution from computed information.
we are interested in minimizing the time it takes to execute any sequence of unions and finds.
hence, the best case time is where c and d are constants.
listening to part 1-15 best, worst, and average-case the worst case complexity of the algorithm is the function defined by the maximum number of steps taken on any instance of sizen.
if next[x] <> nil then prev[next[x]]
there are a variety of implementations of these dictionary operations, each of which yield different time bounds for various operations.
lines 9-10 modify the root node, if necessary lines 11-13 reattach the subtree, if necessary.
we can improve this to with the observation that any path using at most 2m edges is the function of paths using at most m edges each.
another idea would be to repeatedly connect the closest pair of points whose connection will not cause a cycle or a three-way branch to be formed, until we have a single chain with all the points in it.
listening to part 12-4 rules of the game listening to part 12-5 producing efficient programs
to convince ourselves it  really is an answer, we can run another program to check it.
deletion in an open addressing scheme is ugly, since removing one element can break a chain of insertions, making some elements inaccessible.
(g may have a hp but  not cycle)
notice that this reduction could not be performed in polynomial time if the number were written in unary 5=11111.
since we can guess a solution to sat, it is innp and thus np -complete.
as you can see, the reductions can be very clever and very complicated.
dynamic programming works best on objects which are linearly ordered and cannot be rearranged - characters in a string, matrices in a chain, points around the boundary of a polygon, the left-to-right order of leaves in a search tree.
red-black trees are binary search trees where each node is assigned a color, where the coloring scheme helps us maintain the height as .
since proving an  exponential time lower bound for a problem innp would make us famous,  we assume that we cannot do it.
if 3-sat is  np-complete, it implies sat is np-complete but not visa-versa, perhaps long  clauses are what makes sat difficult?!
listening to part 3-2 problem 2.1-4: (a) is ?
listening to part 21-9 the main idea suppose i gave you the following algorithm to solve the bandersnatch problem: bandersnatch(g)
a faster algorithm running on a slower computer will always win for sufficiently large instances, as we shall see.
unfortunately, because we won't have the full matrix we cannot reconstruct the alignment, as above.
note that this is a more restricted problem than sat.
if then , , d = dist(x,y)
n 0 1 2 3 4 5 6 7n 0 1 2 3 4 5 6 7
we win whenever the best candidate occurs before any number  from2 to i in the last n (1- 1/f) / f candidates.
suppose you were given a deck of playing cards to sort.
sort(s) for each , create point .
since the set of3-sat instances is smaller and more regular than thesat instances, it will be easier to use 3 -sat for future reductions.
we also couldn't prove an exponential time lower bound for the problem.
thus we need a data structure for maintaining sets which can test if two elements are in the same and merge two sets together.
add up the executed instructions for all pseudocode lines to get the run-time of the algorithm: what are the ?
this is the first and easiest place to go wrong.
we want a subset of vertices which covers each edge.
the translation was initiated by algorithms on mon jun 2 09:21:39 edt 1997 algorithms
by analyzing the bit patterns of the numbers from0 to n which end with this bit.
note that the shortest path fromi to j,  , using  at mostm edges consists of the shortest path from i to k using at most m-1 edges + w(k, j) for some k. listening to part 20-10 this suggests that we can compute all-pair shortest path with an induction  based on the number of edges in the optimal path.
the dnf formula can become exponentially large and hence the reduction cannot be done in polynomial time.
doing this for all  vertices in the original graph createsn intertwined chains with n entry points and n exits.
from a 3 -sat instance withn variables and c clauses, we construct a  graph with2n+3c vertices.
note that there can be an exponential number of shortest paths between two nodes - so we cannot report all shortest paths efficiently.
previous: none listening to part 2-1 problem 1.2-6: how can we modify almost any algorithm to have a good best-case running time?to improve the best case, all we have to do it to be able to solve one instance of each size efficiently.
the theory of np-compleness, developed by stephen cook and richard karp, provided the tools to show that all of these problems were really the same problem.
the goal of all this is going to be a formal way to talk about the set of problems which can be solved in polynomial time, and the set that cannot be.
in both, we have general and boundary conditions, with the general condition breaking the problem into smaller and smaller pieces.
note that you can do inductive proofs with the big-o's notations - just be sure you use it right.
because it is usually very hard to compute the average running time, since we must somehow average over all the instances, we usually strive to analyze the worst case running time.
the optimal strategy is clearly to sample some fraction of the candidates, then pick the first one who is better than the best we have seen.
for each vertex in the cover, start traversing the chain.
suppose i+1 is the highest ranked person in the first n/ f candidates.
however, there are only n(n-1)/2 continguous substrings of a string, each described by a starting and ending point, so we can use it for string problems.
previous: lecture 21 - vertexnext: lecture 23 - approximation up: table of contents previous: lecture 21 - vertex listening to part 26-1 36.5-5 prove that hamiltonian path is np-complete.
in practice, the fastest sorting algorithm is quicksort, which uses partitioning as its main idea.
my approach is: realize that linear, finite history, constant coefficient recurrences always can be solvedcheck out any combinatorics or differential equations book for a procedure.
an  example is the reduction .
claim: the cost of a mst is a lower bound on the cost of a tsp tour.
the $10,000 question is whether a polynomial time simulation exists, or in other words whetherp=np?.
to show that our reduction is correct, we must show that: every satisfying truth assignment gives a cover.
listening to part 24-5 we must show: any sat solution gives a solution to the ip problem.
now that we have shown3-sat is np-complete, we may use it  for further reductions.
thus case 2 of the master theorem applies.
thus if we can maintain which vertices are in which component fast, we do not have test for cycles!
how many 1's are there in each column?
how can we search for the string closest to the pattern?
the easiest way to do this is to be bold  with your penalties, to punish anyone trying to deviate from your proposed  solution.
every sat defines a cover and every cover truth values for the sat!
note that there are many possible ways to encode the input graph: adjacency matrices, edge lists, etc.
(again, the prevention of carrys across digits
you too can fight the combinatorial explosion!
= prev[x] sentinels boundary conditions can be eliminated using a sentinel element which doesn't go away.
b: pull it out of the array and put it first.
every edge is used exactly twice in the dfs tour: 1.
all reasonable encodings will be within  polynomial size of each other.
we will now have to add clauses to ensure that these variables takes or  the values as in the tm computation.
chromatic number, clique, and independent set all involve  trying to select the correct subset of vertices or edges.
however, it breaks down when we  try to use it for2-sat, since there is no way to stuff anything into  the chain of clauses.
the number of partitions is in any run of quicksort!!
for i = 1 to n a: find the smallest of the first n-i+1 items.
listening to part 1-13 efficiency "why not just use a supercomputer?" supercomputers are for people too rich and too stupid to design efficient algorithms!
listening to part 27-1 37.1-3 give an efficient greedy algorithm that finds an optimal vertex cover of a tree in linear time.
listening to part 2-5 what does all this mean?
listening to part 22-9 having at least 3-literals per clause is what makes the problem difficult.
minimum spanning trees are uneffected by negative cost edges.
listening to part 24-6 things to notice listening to part 24-7 36.5-2 - given an integer matrix a, and in integer m -vectorb, the 0-1 integer programming problem asks whether there is an integern-vector x with elements in the set (0,1) such that .
we must decided either to marry or reject each candidate as we see them.
notice that this reduction could not be performed in polynomial time if  the number were written in unary 5=11111.
fortunately, there is a clever divide-and-conquer algorithm which computes the actual alignment ino(nm) time and o(m) space.
a problem in np for which a polynomial time algorithm would  imply all languages innp are in p is called np -complete.
this option can lead to noticeably better predictive performance and is now recommended for applications with many continuous attributes.this mechanism is intended to make classification models more compact without impairing their predictive accuracy.more efficient memory use for high-dimensional applications memory allocation has been improved for applications with thousands of attributes.this facility can help to identify problems in the training data, and can be very useful for understanding complex classifiers (such as boosted trees or rulesets).release 1.20 has been modified so that classifiers of these kinds often have lower error rates without a noticeable increase in misclassification costs.attributes found to be irrelevant or harmful to predictive performance are disregarded ("winnowed") and only the remaining attributes are used to construct decision trees or rulesets.faster boosting boosting is speedier for large datasets, especially where the number of boosting trials exceeds 20 or so.one noticeable consequence is that decision trees tend to be both smaller and more accurate when there are discrete attributes with many values.furthermore, the remaining attributes that will be used to build classifiers are now ranked by importance, with an estimate of how predictive accuracy or misclassification cost would increase if individual attributes were removed.the use of costs with rulesets or boosted classifiers, however, can sometimes produce quite high error rates.as an added convenience, classifiers constructed using the sampling option are now automatically evaluated on a disjoint set of test cases.for example, a test that separates one class from another in a small subset of the training cases might be genuinely interesting; on the other hand, if hundreds of alternatives have been tried, it is more likely that a similar separation can be found even when none of the tests is helpful for prediction.this invokes a novel method of cross-referencing classifiers and the data from which they were constructed.a label attribute does not affect classification in any way, but its value is displayed where possible with information about the case such as error messages, cross-referencing results etc.this can result in speed improvements when the application has many discrete attributes, especially with the discrete value subset option.
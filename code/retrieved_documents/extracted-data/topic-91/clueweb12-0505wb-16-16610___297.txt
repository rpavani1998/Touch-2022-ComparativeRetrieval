this problem is pervasive in web marketplaces and portals.
we describe the results of extensive experiments using optimized rule-based induction methods on large document collections.
text  categorization: an experiment using phrases}, booktitle =
international  journal of human computer interaction}, volume = {8}, number = {3}, pages =
text categorization is the process of assigning documents to a set of previously fixed categories.
text classification is becoming more important with the proliferation of the internet and the huge amount of data it transfers.
experiments on data sets with different properties (reuters-21578, patent abstracts and patent applications) and with two different algorithms (winnow and rocchio) show that uc-based term selection is not the most aggressive term selection criterium, but that its effect is quite stable across data sets and algorithms.
empirical  comparison with several other existing feature selection methods shows that the  backward elimination variant of csa leads to the most accurate classification  results on an array of datasets."
our results on  two datasets of scanned journals from the making of america collection confirm  the importance of using whole page sequences.
text classification and segmentation using minimum cross-entropy}, booktitle = {proceeding of riao-00, 6th international conference ``recherche d'information assistee par ordinateur''}, editor = {}, address = {paris, fr}, year = {2000}, pages = {}, url = {}, abstract = {several methods for classifying and segmenting text are described.
{yiming yang}, title = {expert network: effective and efficient learning from  human decisions in text categorisation and retrieval}, booktitle = {proceedings  of sigir-94, 17th acm international conference on research and development in  information retrieval}, editor = {w. bruce croft and van rijsbergen, cornelis  j.}, publisher = {springer verlag, heidelberg, de}, address = {dublin, ie},  pages = {13--22}, year = {1994}, url =  {http://www.acm.org/pubs/articles/proceedings/ir/188490/p13-yang/p13-yang.pdf},  abstract = {expert network (expnet) is our approach to automatic categorization  and retrieval of natural language texts.
we verify experimentally that  the integration of wordnet helps ssahc improve its performance, effectively  addresses the classification of documents into categories with few training  documents.
as  output, the system evaluates a set of weighted hypotheses about the type of the  actual letter.
we provide experimental results demonstrating  that the approach can significantly improve performance, and that it does not  impair it.}, } @inproceedings{dagan96, author = {dagan, ido and feldman, ronen  and hirsh, haym}, title = {keyword-based browsing and analysis of large  document sets}, booktitle =
building hierarchical classifiers using class  proximity}, booktitle = {proceedings of vldb-99, 25th international conference  on very large data bases}, publisher =
the utilized concepts are automatically extracted from documents via probabilistic latent semantic analysis.
however, many systems set a  relatively high threshold to reduce retrieval of non-relevant documents, which  results in the ignorance of many relevant documents.
booktitle = {proceedings of cikm-00, 9th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, address = {mclean, us}, editor = {arvin agah and jamie callan and elke rundensteiner}, year = {2000}, pages = {12--19}, url = {ftp://ftp.cs.umn.edu/dept/users/kumar/cikm-ci.ps}, abstract = {retrieval techniques based on dimensionality reduction, such as latent semantic indexing (lsi), have been shown to improve the quality of the information being retrieved by capturing the latent meaning of the words present in the documents.
in the  past few years, researchers investigated various forms of semi-supervised  learning to reduce the burden of manual labeling.
a hierarchical model for clustering and categorising documents}, booktitle =
recent studies have estimated the size of this ''hidden web''  to be 500 billion pages, while the size of the ``crawlable'' web is only an  es-timated two billion pages.
in this work, we investigate the usefulness of explicit control of that  combination within a proposed feature selection framework.
southampton, uk}, url = {http://www.math.unipd.it/~fabseb60//publications/tm05.pdf}, abstract = {
we provide background, present procedures for building  metaclassifiers that take into consideration both reliability indicators and  classifier outputs, and review a set of comparative studies undertaken to  evaluate the methodology.}, } @inproceedings{bennett03, author = {paul n.  bennett}, title = {using asymmetric distributions to improve text classifier  probability estimates}, booktitle = {proceedings of sigir-03, 26th acm  international conference on research and development in information retrieval},  editor = {jamie callan and gordon cormack and charles clarke and david hawking  and alan smeaton}, publisher = {acm press, new york, us}, address = {toronto,  ca}, year = {2003}, pages = {111--118}, url =  {http://doi.acm.org/10.1145/860435.860457},
the best performance was achieved by the feature selection methods based on the feature scoring measure called odds ratio that is known from information retrieval.}, } @phdthesis{mladenic98c, author = {
however, the question remains on whether the same techniques that  are used on the web can be applied to collections of documents containing  citations between scientific papers.
{spie {}-{} the international society for optical engineering},  editor = {
with consideration of these  characteristics, we propose a probabilistic model to incorporate both content  and time information in a unified framework.
by doing so, model probability and classification accuracy come into correspondence, allowing unlabeled data to improve classification performance.
luc de raedt and peter a. flach}, year = {2001}, url = {http://link.springer.de/link/service/series/0558/papers/2167/21670454.pdf}, abstract = {
this paper  addresses the problem with respect to a set of popular algorithms in text  categorization, including support vector machines, k-nearest neighbor, ridge  regression, linear least square fit and logistic regression.
many corpora, such as internet directories, digital libraries, and patent databases are manually organized into topic hierarchies, also called taxonomies.
in this paper we propose a new information-theoretic divisive algorithm for word clustering applied to text classification.
text classification algorithms were used to  automatically classify arbitrary search results into an existing category  structure on-the-fly.
the discrimination between informative keywords  and functional keywords is not crisp.
in this paper, we systematically compare  combination strategies in the context of document filtering, using queries from  the tipster reference corpus.
in addition to  motivating the task and describing the practical details of participating in  the track, this document includes a detailed graphical presentation of the  experimental results and provides a brief overall analysis of the performance  data.}, } @inproceedings{ipeirotis01, author = {panagiotis g. ipeirotis and  luis gravano and mehran sahami}, title = {
our results show that ridge regression seems to be the most promising candidate for rare class problems.}, } @inproceedings{zhdanova02, author = {anna v. zhdanova and denis v. shishkin}, title = {classification of email queries by topic: approach based on hierarchically structured subject domain}, booktitle = {proceedings of ideal-02, 3rd international conference on intelligent data engineering and automated learning}, editor = {hujun yin and nigel allinson and richard freeman and john keane and simon hubbard}, publisher = {springer verlag, heidelberg, de}, address = {manchester, uk}, year = {2002}, pages = {99--104}, note = {published in the ``lecture notes in computer science'' series, number 2412}, url = {http://link.springer.de/link/service/series/0558/papers/2412/24120099.pdf}, abstract =
also the role of linguistic preprocessing seems to provide  positive effects on the performance.
in this paper, we study the  properties of phrasal and clustered indexing languages on a text categorization  task, enabling us to study their properties in isolation from query  interpretation issues.
generalization is an important ability specific to inductive learning that will predict unseen data with high accuracy based on learned concepts from training examples.
booktitle = {proceedings of sigir-02, 25th acm international conference on research and development in information retrieval}, editor = {micheline beaulieu and ricardo baeza-yates and sung hyon myaeng and kalervo j{\"{a}}rvelin}, publisher =
in addition to being highly accurate, this method utilizes the hamming distance from ecoc to provide high-precision results.
our experiments with the acm computing  classification scheme, using documents from the acm digital library, indicate  that gp can discover similarity functions superior to those based solely on a  single type of evidence.
we conduct an empirical study on several document  classification tasks which confirms the value of our methods in large scale  semi-supervised settings."
these text messages are scanned and then  distributed to one of several expert agents according to a certain task  criterium.
our experiments use reuters 21578 database and consist of binary classifications for categories selected from the 89 topics classes of the reuters collection.
so far, documents have been classified according to their contents manually.
categorizing documents with the aid of knowledge-based features leverages information that cannot be deduced from the documents alone.
this has  important implications for document classification when a hierarchic ordering  of topics exists.
{2827--2830}, url =  {http://www-a2k.is.tokushima-u.ac.jp/member/sasaki/frame_home/papers/smc.ps},  abstract = {document categorization, which is defined as the classification of  text documents into one of several fixed classes or categories, has become  important with the explosive growth of the world wide web.
"672--681", } @inproceedings{esuli:2006:dts, author =
two different classification schedules are compared along with two methods of automatically classifying documents into categories.
our experiments demonstrate that this new  approach is able to learn more accurate classifiers than either of its  constituent methods alone.}, } @inproceedings{slonim01, author = {noam slonim  and naftali tishby}, title = {
in this paper we propose a new information-theoretic divisive algorithm for feature/word clustering and apply it to text classification.
our experiments show that this method significantly outperforms the combination of single label approach."
classifying news stories using memory-based reasoning}, booktitle = {proceedings of sigir-92, 15th acm international conference on research and development in information retrieval}, editor =
{dmitry v. khmelev and william j. teahan}, title = {a repetition based measure  for verification of text collections and for text categorization}, booktitle =  {proceedings of sigir-03, 26th acm international conference on research and  development in information retrieval}, editor = {jamie callan and gordon  cormack and charles clarke and david hawking and alan smeaton}, publisher =
a comparative study of classification-based  personal e-mail filtering}, booktitle = {proceedings of pakdd-00, 4th  pacific-asia conference on knowledge discovery and data mining}, editor =  {takao terano and huan liu and arbee l.p. chen}, pages = {408--419}, year =
{2001}, pages = {1286--1291}, url = {}, abstract = {although several attempts  have been made to introduce natural language processing (nlp) techniques in  information retrieval, most ones failed to prove their effectiveness in  increasing performances.
"fumiyo fukumoto and yoshimi  suzuki", title =
we propose a ``hyperbolic som'' (hsom) based on a regular tesselation of the hyperbolic plane, which is a non-euclidean space characterized by constant negative gaussian curvature.
yan liu and yiming yang and jaime carbonell},  title =
"485--492", abstract =
this article uses the structure that is present in the semantic space of topics in order to improve performance in text categorization: according to their meaning, topics can be grouped together into ``meta-topics'', e.g., gold, silver, and copper are all metals.
{journal of the association for computing machinery}, year = {1964}, volume = {11}, number = {2}, pages = {138--151}, url = {http://www.acm.org/pubs/articles/journals/jacm/1964-11-2/p138-borko/p138-borko.pdf}, abstract = {
using an already coded training database of about  50,000 stories from the dow jones press release news wire, and seeker
{learning to classify english text with ilp methods}, booktitle = {advances in inductive logic programming}, editor = {de raedt, luc}, publisher =
booktitle = {proceedings of hicss-01, 34th annual hawaii  international conference on system sciences}, publisher = {ieee computer  society press, los alamitos, us}, editor = {ralph h. sprague}, year = {2001},  address = {maui, us}, pages = {}, url =  {http://dlib.computer.org/conferen/hicss/0981/pdf/09817061.pdf}, abstract =  {with rapid expansion of the numbers and sizes of text repositories and  improvements in global connectivity, the quantity of information available  online as free-format text is growing exponentially.
"seattle, washington", abstract =
we then present a fast, divisive algorithm that monotonically decreases this objective function value.
links clearly contain high-quality semantic clues that are lost upon a purely  term-based classifier, but exploiting link information is non-trivial because  it is noisy.
"proceedings of the twenty-second international conference on  machine learning", year =
ji he and ah-hwee tan and chew-lim tan}, title = {
this paper proposes a  semi-automatic process (interleaved with human suggestions) whose aim is to  minimize (simplify) the work required to the administrators when creating,  modifying, and maintaining directories.
taipei, tw}, url =  {http://acl.ldc.upenn.edu/coling2002/proceedings/data/area-28/co-269.pdf},  abstract = {automatic text categorization is a problem of automatically  assigning text documents to predefined categories.
{miguel e. ruiz and padmini srinivasan}, title = {hierarchical neural networks for text categorization}, booktitle = {proceedings of sigir-99, 22nd acm international conference on research and development in information retrieval}, editor = {
the document  collection on which this comparison takes place is a subset of the annotated  brown corpus semantic concordance.
"vancouver, british  columbia, canada" } @inproceedings{betts:2007:uie, author =
naive use of terms in the link neighborhood of a document can even degrade accuracy.
the system is also applicable to text classification problems other  than telex classification.}, } @inproceedings{li97, author = {
a  framework for filtering news and managing distributed data}, journal = {journal  of universal computer science}, year = {1997}, number = {8}, volume = {3},  pages = {1007--1021}, url =
such categories offer a standardized and universal way for  referring to or describing the nature of real world objects, activities,  documents and so on, and may be used (we suggest) to semantically characterize  the content of documents.
published in the  ``lecture notes in computer science'' series, number 2168}, url =  {http://link.springer.de/link/service/series/0558/papers/2168/21680266.pdf},  abstract =
the algorithm is automatic and unsupervised in both training and application: senses are induced from a corpus without labeled training instances or other external knowledge sources.
}  @inproceedings{liu:2004:tcl, author =
the classifiers provide heuristics to the crawler thus biasing it towards certain portions of the web graph.
it is used in our research to guide and improve the ga-based evolution of the feature subsets.
we report an experiment to study the impact of term redundancy on the performance of text classifier.
recurrent plausibility networks with local memory are developed and examined for learning robust text routing.
"borovets, bulgaria" } @inproceedings{guo:2004:kmb, author =
{micheline beaulieu and ricardo baeza-yates and sung hyon myaeng and kalervo  j{\"{a}}rvelin}, publisher = {acm press, new york, us}, address =
all the presented  experiments are based on unrestricted text downloaded from the world wide web  without any manual text preprocessing or text sampling.
the key idea is to automatically adjust the window  size so that the estimated generalization error is minimized.
wsd is the  identification of the sense of words in context.
booktitle = {proceedings of icml-00, 17th international  conference on machine learning}, editor = {pat langley}, year = {2000}, address  = {
there are two aspects to the key difficulties of this  problem: one is that the meaning of the queries and the semantics of the  predefined categories are hard to determine.
we describe a procedure for generating a hierarchy of classifiers that model the hierarchy structure.
the method maintains a window on the training data.
international congress on phonetic sciences}, address = {barcelona, es}, editor  = {}, publisher = {}, year = {2003}, pages = {}, url = {}, abstract = {}, }  @inproceedings{alkofahi01, author =
= {springer verlag, heidelberg, de}, address = {seoul, ko}, note = {
from a prescribed vocabulary.
"categorizing web pages as a preprocessing step for information extraction", booktitle = "lrec'04", year = "2004" } @inproceedings{hahn:2004:pdt, author =
this paper  focuses on a comparative evaluation of a wide-range of text categorization  methods, including previously published results on the reuters corpus and new  results of additional experiments.
{hyperlink ensembles: a case study in hypertext classification}, journal =
our results show that an index  can be constructed on a desktop workstation with little effect on  categorisation accu-racy compared to a memory-based approach.
evaluation against the reuters-21578 collection shows both techniques have levels of performance that approach benchmark methods, and the ability of one of the classifiers to produce realistic measures of confidence in its decisions is shown to be useful for prioritizing relevant documents.}, } @inproceedings{lee02c, author = {kang hyuk lee and judy kay and byeong ho kang and uwe rosebrock}, title = {a comparative study on statistical machine learning algorithms and thresholding strategies for automatic text categorization}, booktitle = {proceedings of pricai-02, 7th pacific rim international conference on artificial intelligence}, editor = {mitsuru ishizuka and abdul sattar}, publisher = {springer verlag, heidelberg, de}, address = {
keywords: text categorization,  machine learning, digital libraries."
profile allows the user to update on-line the profile and to check the discrepancy between the assessment and the prediction of relevance of the system.
{information retrieval}, number = {1}, volume = {4}, pages = {5--31}, year =  {2001}, url = {http://www.wkap.nl/article.pdf?335913}, abstract = {
cheng and weiguo fan and wei-ying ma", title =
arvin agah and jamie callan and elke rundensteiner}, year = {2000}, pages = {373--374}, url = {http://www.cs.ust.hk/~meretaks/papers/mfll-cikm2000.pdf}, abstract = {naive bayes (nb) classifier has long been considered a core methodology in text classification mainly due to its simplicity and computational efficiency.
however, the intrinsic geometric structure of text data has been ignored by standard kernels commonly used in svms.
the final classification of test documents is determined by a  majority voting from the individual classifications of each feature.
{heterogeneous uncertainty sampling for supervised learning}, booktitle =
one is synonym-based and the other is statistics based.
the described experimental study  shows that the idf transform considerably effects the distribution of  classification performance over feature selection reduction rates, and offers  an evaluation method which permits the discovery of relationships between  different document representations and feature selection methods which is  independent of absolute differences in classification performance.}
in this paper, we present pva, an adaptive personal view information agent system to track, learn and manage, user's interests in internet documents.
} @inproceedings{yin:2006:anb, author =
informative keywords are the ones which  reflect the contents of a document.
term selection according to this criterium is performed by the elimination of noisy terms on a class-by-class basis, rather than by selecting the most significant ones.
once the system has learned this information, a gaussian  function is shaped for each term of a category, in order to assign the term a  weight that estimates the level of its importance for that particular category.
"li, ying and zheng, zijian and dai, honghua (kathy)", title  =
we show that there are in fact differences between  citations and links in this context.
it enables drawing on results from statistics and machine learning in explaining the effectiveness of alternate representations of text, and specifies desirable characteristics of text representations.
when choosing optimal pairs of metrics for each of the  four performance goals, bns is consistently a member of the pair---e.g., for  greatest recall, the pair bns + f1-measure yielded the best performance on the  greatest number of tasks by a considerable margin.}, } @inproceedings{forman04,  author = {
creating segmented databases from free text for  text retrieval}, booktitle = {proceedings of sigir-91, 14th acm international  conference on research and development in information retrieval}, editor =
feature selection techniques are then needed to  identify these words.
extensive experimental evidence has been derived  on real test data and also from well-established academic test sets.
many of its applications have great commercial value.
{improving text classification by shrinkage in a hierarchy of classes},  booktitle = {proceedings of icml-98, 15th international conference on machine  learning}, editor = {jude w. shavlik}, year = {1998}, address = {
given a visual content, the occurrences of visual keywords are  detected, summarized spatially, and coded via singular value decomposition to  arrive at a concise coded description.
we focus on three of them: what representation is used for documents, how is the high number of features dealt with and which learning algorithm is used.
this paper introduces a class of predictive self-organizing neural networks known as adaptive resonance associative map (aram) for classification of free-text documents.
the  application of a logic-based approach to text document classification is  critical when one wishes to be able to justify why a particular document has  been assigned to one class versus the other class.
{proceedings of icml-03, 20th international conference on machine learning},  editor = {}, year = {2003}, address = {washington, dc}, pages = {}, publisher =
the r-measure can be  effectively computed using the suffix array data structure.
support vector machines (svm), by an average of 2\% to 7\%.
much of the diversity resides in its technical terminology, which has also become voluminous.
we propose a ``hyperbolic som''  (hsom) based on a regular tesselation of the hyperbolic plane, which is a  non-euclidean space characterized by constant negative gaussian curvature.
collaboration with friends of the earth allows us  to test our ideas in a non-academic context involving high volumes of  documents.
"193--200", }  @inproceedings{sandler:2005:oul, author =
the chi(2) test, which is sometimes used for selecting terms that are  highly related to a text class, is applied in a novel way when constructing a  category weight vector.
"computational linguistics and intelligent text processing", year = "2005" } @inproceedings{chowdhury:2005:utc, author =
text classification tasks, like text categorization, help the users to access to the great amount of text they find in the internet and their organizations.
we present the results of a number of experiments designed to  evaluate the effectiveness and behavior of different compression-based text  classification methods on english text.
"wang, muyuan and li, zhiwei and lu, lie and ma, wei-ying and zhang, naiyao", title = "web object indexing using domain knowledge", booktitle =
we present a set of small-scale but reasonable experiments in text genre detection, author identification, and author verification tasks and show that the proposed method performs better than the most popular distributional lexical measures, i.e., functions of vocabulary richness and frequencies of occurrence of the most frequent words.
support vector machines (svms) have been recognized as one of the most successful classification methods for many applications including text classification.
"omar zaidan and jason eisner and christine piatko", title =
we present experimental results obtained on the standard \textsf{reuters-21578} benchmark with one classifier learning method (support vector machines), three term selection functions (information gain, chi-square, and gain ratio), and both local and global term selection and weighting.}, } @inproceedings{debole04, author = {franca debole and fabrizio sebastiani}, title = {an analysis of the relative difficulty of reuters-21578 subsets}, year = {2004}, booktitle = {proceedings of lrec-04, 4th international conference on language resources and evaluation}, address = {lisbon, pt}, pages = {}, url = {http://www.math.unipd.it/~fabseb60/publications/lrec04.pdf}, abstract = {
"filtering obfuscated email spam by means of phonetic string  matching", booktitle =
a number of feature selection metrics have been explored in text categorization, among which information gain (ig), chi-square (chi), correlation coefficient (cc) and odds ratios (or) are considered most effective.
our new approach is based on a bayesian network induction which  does not rely on some major assumptions found in a previous method using the  bayesian independence classifier approach.
this article describes an approach to tc based on the  integration of a training collection (reuters-21578) and a lexical database  (wordnet 1.6) as knowledge sources.
= {madrid, es}, pages = {32--38}, url = {ftp://parcftp.xerox.com/pub/qca/genre/paper.acl97.ps.z}, abstract = {as the text databases available to users become larger and more heterogeneous, genre becomes increasingly important for computational linguistics as a complement to topical and structural principles of classification.
text categorization is the task of classifying text into one of several predefined categories.
in this paper, an optimized k-nearest neighbors (knn) classifier that uses intervalization and the p-tree1 technology to achieve a high degree of accuracy, space utilization and time efficiency is proposed: as new samples arrive, the classifier finds the k nearest neighbors to the new sample from the training space without a single database scan.}, } @inproceedings{raskutti01, author = {bhavani raskutti and herman ferr{\'{a}} and adam kowalczyk}, title = {
this paper presents a hierarchical mixture model which extends the standard naive bayes classifier and previous hierarchical approaches.
this is still true even for the maximum entropy (me) method, whose flexible modeling capability has alleviated data sparseness more successfully than the other probabilistic models in many nlp tasks.
more empirical evidence is required to  determine the suitable linguistic levels for modeling each ir subtask (e.g.  information zoning, parsing, feature selection for indexing,...) and the  corresponding use of this information.
the system is also applicable to text classification problems other than telex classification.}, } @inproceedings{li97, author = {
an extended version appears as~\cite{sable00}}, year = {1999},  address = {
this task has several applications, including automated  indexing of scientific articles according to predefined thesauri of technical  terms, filing patents into patent directories, selective dissemination of  information to information consumers, automated population of hierarchical  catalogues of web resources, spam filtering, identification of document genre,  authorship attribution, survey coding, and even automated essay grading.
our algorithm has the following features: it does not need any natural language processing technique and it is robust for noisy data.
however, the overall conclusion of our paper is that support vector machines are still the method of choice if the aim is to maximize accuracy.} } @inproceedings{makrehchi:2005:tcu, author = {masoud makrehchi and mohamed s. kamel}, title = {
the originality of stretch lies principally in  the possibility for unskilled users to define the indexes relevant to the  document domains of their interest by simply presenting visual examples and  applying reliable automatic information extraction methods (document  classification, flexible reading strategies) to index the documents  automatically, thus creating archives as desired.
on the two other sets (reuters-21578 and webkb) the  word-based representation slightly outperforms the word-cluster representation.
however, in many domains labels are highly interdependent.
experimental results show that our neuro-genetic algorithm is able to perform as well as, if not better than, the best results of neural networks to date, while using fewer input features.}, } @inproceedings{zaiane02, author = {osmar r. za{\"{\i}}ane and maria-luiza antonie}, title = {
using web structure for classifying and describing web pages}, booktitle = {
though it is  not a heavy requirement to rely on some existing pn dictionary (often these  resources are available on the web), its coverage of a domain corpus may be  rather low, in absence of manual updating.
this approach has the advantage of being able to recommend previously unrated  items to users with unique interests and to provide explanations for its  recommendations.
these structures can then be applied to individual documents to derive a posterior probability that the document is about a particular target concept.}, } @inproceedings{toutanova01, author =
the method has  been implemented in the context of a client-server application, named  webclassii.
the changes may occur both on the transmission side (the nature of the streams can change), and on the reception side (the interest of a user can change).
classification is traditionally  performed by extracting information for indexing a document from the document  itself.
we present very preliminary results of the application of this model to a standard test collection, evaluating it in supervised mode in order to facilitate comparison with other methods, and showing initial results of its use in unsupervised mode.}, } @proceedings{sahami98a, editor = {mehran sahami}, title = {proceedings of the 1998 workshop on learning for text categorization}, institution = {americal association for artificial intelligence}, note = {available as technical report ws-98-05}, address = {
however, as the number of positive training data  decreases, the boundary of svmc starts overfitting at some point and end up  generating very poor results.
"shenghuo zhu and xiang ji and wei  xu and yihong gong ", title =
} @inproceedings{pekar:2004:cwp, author =
{acm press, new york, us}, pages = {793--797}, url =  {http://www.math.unipd.it/~fabseb60/publications/sac03c.pdf}, abstract = {
in this case, the label of one entity (e.g., the topic of the  paper) is often correlated with the labels of related entities.
adelaide, au}, year = {2002}, pages = {309--320}, note = {
the algorithm is based on the multiperturbation shapley  analysis, a framework which relies on game theory to estimate usefulness.
performance of rankboost is somewhat inferior to that of a state-of-the-art routing algorithm which is, however, more complex and less theoretically justified than rankboost.
{leah s.  larkey}, title = {
we will discuss the statistical and numerical properties of  these algorithms, with a focus on text categorization.
using corpus statistics to remove redundant words in text categorization}, journal = {journal of the american society for information science}, year = {1996}, volume = {47}, number = {5}, pages = {357--369}, url = {http://www3.interscience.wiley.com/cgi-bin/fulltext?id=57757&placebo=ie.pdf}, abstract =
automatic text classification is necessary to  store documents like that.
the best variant, which we call lazyboosting, is tested on the largest sense-tagged corpus available containing 192,800 examples of the 191 most frequent and ambiguous
other extensions to design discriminative multiple-category mfom classifiers for application scenarios with new performance metrics could be envisioned too.}, } @inproceedings{gaussier02, author = {{\'e}ric gaussier and cyril goutte and kris popat and francine chen}, title = {
integrating {wordnet} knowledge to supplement  training data in semi-supervised agglomerative hierarchical clustering for text  categorization}, journal = {international journal of intelligent systems},  pages = {929--947}, year = {2001}, volume = {16}, number = {8}, url =  {http://www3.interscience.wiley.com/cgi-bin/fulltext?id=84503376&placebo=ie.pdf},  abstract = {
this characterization hence must consider the data at hand, both the kernels and also the task, that is the information given by the labels.
with this extended model, we also have improved the well-known  probabilistic classification method based on the bernoulli document generation  model.
we have developed a text classifier that misclassifies only 13\% of  the documents in the reuters benchmark; this is comparable to the best results  ever obtained.
we present an extensive experimental evaluation of both algorithms on three english collections and one chinese corpus.
in this  paper, we measure the importance of sentences using text summarization  techniques.
finally, the future of  automatic classification and some of the practical problems to be faced are  outlined.}, } @article{drucker99, author = {harris drucker and vladimir vapnik  and dongui wu}, title = {support vector machines for spam categorization},  journal = {ieee transactions on neural networks}, year = {1999}, number = {5},  volume = {10}, pages = {1048--1054}, url =  {http://www.monmouth.edu/~drucker/svm_spam_article_compete.pdf}, abstract = {
an unoptimized version of  the algorithm was used to process the trec database in a very short time.
the proposed model  represents an effective feature selection methodology.
amherst, us}, year = {1994}, url = {http://www.cs.utah.edu/~riloff/psfiles/single-thesis.ps}, abstract = {knowledge-based natural language processing systems have achieved good success with many tasks, but they often require many person-months of effort to build an appropriate knowledge base.
organizing search results allows users to focus on items in categories of interest rather than having to browse through all the results sequentially.}, } @inproceedings{chen00a, author = {hao chen and tin kam ho}, title = {evaluation of decision forests on text categorization}, booktitle = {proceedings of the 7th spie conference on document recognition and retrieval}, publisher =
in web classification, most researchers assume that the objects to classify are individual web pages from one or more web sites.
experiments  with the purely theoretical approach and with several heuristic variations show  that heuristic assumptions may yield significant improvements.}, }  @article{fuhr94, author = {norbert fuhr and ulrich pfeifer}, title =
we present an application of hidden markov models to supervised document classification and ranking.
they include two families of  text categorization techniques, namely the k-nearest neighbor (k-nn) algorithm  and linear classifiers.
we discovered that misclassifications by the citation-link based classifiers  are in fact difficult cases, hard to classify even for humans."
we  consider three classification techniques which have decision rules that are  derived via explicit error minimization: linear discriminant analysis, logistic  regression, and neural networks.
given a set of categories, with or without a preexisting hierarchy among them, we consider the problem of assigning documents to one or more of these categories from the point of view of a hierarchy with more or less depth.
using various data sets, their performances are investigated  and compared to a standard centroid-based classifier (tdidf) and a  centroid-based classifier modified with information gain.
this means that two (or several)  different techniques are used to optimize the performances even if a single  algorithm may have more chances to operate the right choices.
automated text categorization is generally cast as a multi-class classification problem.
the results show that the method outperforms svm at multi-class  categorization, and interestingly, that results correlate strongly with  compression-based methods.}, } @inproceedings{kim00, author = {yu-hwan kim and  shang-yoon hahn and byoung-tak zhang}, title = {text filtering by boosting  naive bayes classifiers}, booktitle = {proceedings of sigir-00, 23rd acm  international conference on research and development in information retrieval},  editor = {nicholas j. belkin and peter ingwersen and mun-kew leong}, publisher  = {acm press, new york, us}, address = {athens, gr}, year = {2000}, pages =
} @article{montanesdrcf05, title =
siam international conference on data mining}, publisher = {}, editor = {}, year = {2001}, address = {
the  current implementation of cats may be considered a baseline, on top of which  many enhancements are possible.
in this paper, we present a method for automatic genre classification  that is based on statistically selected features obtained from both  subject-classified and genre classified training data.
jochen d{\"o}rre and peter gerstl and roland seiffert}, title = {
experiments on a large-scale chinese document collection with 71,674 texts show that the f1 metric of categorization performance of bwm-nbs gets to 94.9\% in the best case, which is 26.4\% higher than that of tf*idf, 19.1\% higher than that of tf*idf*ig, and 5.8\% higher than that of bwm under the same condition.
experimental results show that the methods are a  significant improvement over previously used methods in a number of areas.
luc de raedt and peter a. flach},  publisher = {springer verlag, heidelberg, de}, address = {freiburg, de}, year =
the technical  details of the solutions from the three award winning teams are available in  their papers separately in this issue of sigkdd explorations.
moreover, we also compare them to two well-known methods: k-nn and naive bayes.
"2006" } @inproceedings{li:2006:csq, author =
simulation results on both toy-data settings and an actual application on internet chat line discussion analysis is presented by way of demonstration.}, } @inproceedings{kao03, author = {anne kao and lesley quach and steve poteet and steve woods}, title = {
for n training  instances held in memory, the best-known svm implementations take time  proportional to n a , where a is typically between 1.8 and 2.1.
as the result of collaborative  research with friends of the earth, an environmental issues campaigning  organisation, we have developed a general purpose information classification  agent architecture and have applied it to the problem of document  classification and routing.
some attempts have been made at automating this task, most of them based on detecting the similarity between the answer and textual descriptions of the meanings of the candidate codes.
finally, the relative performance of the classifiers being tested provided insights into their strengths and limitations for solving classification problems involving diverse and often noisy web pages.}, } @inproceedings{yang03, author =
we also present preliminary results showing how this model could classify documents with dtds not represented in the training set.}, } @inproceedings{denoyer03a, author = {ludovic denoyer and jean-no{\"{e}}l vittaut and patrick gallinari and sylvie brunessaux and stephan brunessaux}, title = {structured multimedia document classification}, booktitle = {proceedings of doceng-03, acm symposium on document engineering}, publisher = {acm press, new york, us}, editor = {}, year = {2003}, address = {grenoble, fr}, pages = {153--160}, url = {http://doi.acm.org/10.1145/958220.958249}, abstract = {
classification problems are first formulated as optimization via discriminant analysis.
"mexico city, mexico" } @inproceedings{karakos:2007:tjr, author =
published in the ``lecture notes in computer science'' series, number 2291}, pages = {229--247}, url = {http://link.springer.de/link/service/series/0558/papers/2291/22910229.pdf}, abstract =
this paper presents an empirical comparison of twelve feature  selection methods (e.g.\ information gain) evaluated on a benchmark of 229 text  classification problem instances that were gathered from reuters, trec,  ohsumed, etc.
the rapid growth of data in large databases, such as text databases and scientific databases, requires efficient computer methods for automating analyses of the data with the goal of acquiring knowledges or making discoveries.
choose your words carefully: an empirical study of feature selection metrics for text classification}, booktitle = {proceedings of pkdd-02, 6th european conference on principles of data mining and knowledge discovery}, editor =
second, undirected models are well  suited for discriminative training, where we optimize the conditional  likelihood of the labels given the features, which generally improves  classification accuracy.
published in  the ``lecture notes in computer science'' series, number 1642}, editor = {
"joachims, thorsten", title =
both the interest of the user and  the document content change over time.
it is therefore worthwhile to understand whether such good performance is  unique to the svm design, or if it can also be achieved by other linear  classification methods.
published in the ``lecture notes in computer  science'' series, number 2431}, url =  {http://springerlink.metapress.com/openurl.asp?genre=article&issn=0302-9743&volume=2431&spage=150},  abstract = {good feature selection is essential for text classification to make  it tractable for machine learning, and to improve classification performance.
a ph.d. thesis % required:  author, title, school, year % optional: type, address, month, note % %  proceedings % =
the analysis gives theoretical insight into the heuristics used in the rocchio algorithm, particularly the word weighting scheme and the similarity metric.
furthermore, by combining these methods, we significantly reduced the variance in performance of our event tracking system over different data collections, suggesting a robust solution for parameter optimization.}, } @inproceedings{yang00a, author =
our method is unique in that decision lists are automatically  constructed on the basis of the principle of minimizing extended stochastic  complexity (esc), and with it we are able to construct decision lists that have  fewer errors in classification.
it is used here for classifying  xml documents.
based on the hierarchical structure the problem is  divided into subproblems, each representing one on the categories included in  the yahoo hierarchy.
keywords: text categorization, machine learning, digital libraries."
we propose instead combining domain knowledge with training examples in a bayesian framework.
it is concluded that, while there is no significant difference in the predictive efficiency between the bayesian and the factor score methods, automatic document classification is enhanced by the use of a factor-analytically-derived classification schedule.
in order to reduce human efforts, there has been increasing interest in applying active learning for training text classifiers.
{317--324}, url = {http://doi.acm.org/10.1145/956863.956924}, abstract = {
however, these methods of indexing have two major drawbacks: first, they must be laboriously assigned by human indexers.
booktitle = {proceedings of aaai-99, 16th conference of the american association for artificial intelligence}, publisher =
our results show that overall, svms and k-nn lsa perform better than the other methods, in a statistically significant way.}, } @incollection{caropreso01, author = {maria fernanda caropreso and stan matwin and fabrizio sebastiani}, title = {a learner-independent evaluation of the usefulness of statistical phrases for automated text categorization}, year = {2001}, booktitle = {text databases and document management: theory and practice}, editor = {amita g. chin}, publisher = {idea group publishing}, address =
experiments show that the algorithm can feasibly optimize training  sets of thousands of examples and classification hierarchies consisting of  hundreds of nodes.
in this paper we  present a method for detecting the text genre quickly and easily following an  approach originally proposed in authorship attribution studies which uses as  style markers the frequencies of occurrence of the most frequent words in a  training corpus (burrows, 1992).
to cope with this problem, we propose a hybrid technique using latent semantic indexing (lsi) and rough sets theory (rs) to alleviate this situation.
the results show that filtering  significantly improves the recall of the method, and that also has the effect  of significantly improving the overall performance.} } @article{combarromdrm05,  title =
we describe here an n-gram-based approach to  text categorization that is tolerant of textual errors.
"we present  and study the contribution-selection algorithm (csa), a novel algorithm for  feature selection.
feature selection using support vector machines}, booktitle = {proceedings of the 3rd international conference on data mining methods and databases for engineering, finance, and other fields}, year = {2002}, pages = {}, address = {bologna, it}, url = {http://www.brank.org/msr/fsnormal/bologna/bologna-paper-4.pdf}, abstract =
we  find that adding the words in the linked neighborhood to the page having those  links (both inlinks and outlinks) were helpful for all our classifiers on one  data set, but more harmful than helpful for two out of the three classifiers on  the remaining datasets.
"744--751", booktitle = "proceedings of the 14th {acm} international conference on information and knowledge management", year =
papers from the 1996 aaai spring symposium}, institution =
in spite of these differences, both ripper and  sleeping-experts perform extremely well across a wide variety of categorization  problems, generally outperforming previously applied learning methods.
the biological literature presents a difficult challenge to information processing in its complexity, diversity, and in its sheer volume.
using a massively parallel supercomputer, we leverage the  information already contained in the thousands of coded stories and are able to  code a story in about 2 seconds.
classifying text documents by associating terms with text  categories}, booktitle = {proceedings of the 13th australasian conference on  database technologies}, publisher = {acm press, new york, us}, year = {2002},  pages = {215--222}, address = {melbourne, au}, volume = {5}, url = {}, note =
this work also proposes an algorithm for training tsvms efficiently, handling  10,000 examples and more.}, } @article{juan02, author = {juan, alfons and  vidal, enrique}, title = {
exploiting hierarchy in text  categorization}, journal = {information retrieval}, number = {3}, volume = {1},  pages =
four well known learning algorithms: rocchio's algorithm (w.w. cohen and y. singer, 1995), the simple bayesian classifier (sec) (r.o. duda and p.e. hart, 1973), the sleeping experts (se) and winnow (i. dagan et al., 1997) were implemented.
morgan kaufmann publishers, san francisco, us}, url = {http://www.ai.mit.edu/~jrennie/papers/icml03-nb.pdf}, abstract = {naive bayes is often used as a baseline in text classification because it is fast and easy to implement.
in this work, we developed an approach to automatically generate  category themes and reveal the hierarchical structure among them.
{soumen chakrabarti and byron e. dom and rakesh agrawal and prabhakar  raghavan}, title = {scalable feature selection, classification and signature  generation for organizing large text databases into hierarchical topic  taxonomies}, journal = {journal of very large data bases}, year = {1998},  number = {3}, volume = {7}, pages = {163--178}, url =  {http://www.cs.berkeley.edu/~soumen/vldb54_3.pdf}, abstract = {we explore how  to organize large text databases hierarchically by topic to aid better  searching, browsing and filtering.
"koppel moshe and schler, jonathan and argamon, shlomo and messeri, eran", title = "authorship attribution with thousands of candidate authors", booktitle =
the system is based on calculating and comparing profiles of n-gram  frequencies.
abis compares documents with the past situations and finds the similarity scores on the basis of a memory-based reasoning approach.}, } @article{amati99, author = {
text categorization with support vector machines: learning with many relevant features}, booktitle = {proceedings of ecml-98, 10th european conference on machine learning}, publisher = {springer verlag, heidelberg, de}, note = {published in the ``lecture notes in computer science'' series, number 1398}, editor = {
filters of this type have so far been based mostly on manually constructed keyword patterns.
we introduce a probabilistic method for combining  classifiers that considers the context-sensitive reliabilities of contributing  classifiers.
cats is a meta-search engine that utilizes text classification  techniques to improve the presentation of search results.
published in the ``lecture notes in computer science'' series, number  2291}, pages = {213--228}, url =  {http://link.springer.de/link/service/series/0558/papers/2291/22910213.pdf},  abstract = {typical text classifiers learn from example and training documents  that have been manually categorized.
at the end, we also share the results of a survey conducted with this years cup participants.
the second concerns the infoagent, a system for  supporting users in retrieving data in distributed and heterogeneous archives  and repositories.
we take advantage of  this fact by using a hierarchically organized neural network, built up from a  number of independent self-organizing maps in order to enable the true  establishment of a document taxonomy.
text-classification methods have thus far not easily incorporated numerical features.
even though our experimental results do not outperform earlier approaches, they give rise to promising perspectives.}, } @inproceedings{moulinier96a, author = {isabelle moulinier and jean-gabriel ganascia}, title = {
this problem can be tackled since a couple of recent learners (ripper and scar) do not require a preprocessing step.
we also report results of making  actual use of the selected $n$-grams in the context of a linear classifier  induced by means of the rocchio method.}, } @inproceedings{carreras01, author =
we do so by  applying feature selection to the pool of all $k$-grams ($k\leq n$), and  checking how many $n$-grams score high enough to be selected in the top  $\sigma$ $k$-grams.
"new york, ny", publisher =
in many cases, users would like to search for information of a certain 'object', rather than a web page containing the query terms.
{taipei, tw}, url = {http://acl.ldc.upenn.edu/coling2002/proceedings/data/area-28/co-201.pdf}, abstract = {
experiments on 20 newsgroups (20ng), reuters corpus volume 1  (rcv1) and open directory project (odp) data show that ocfs is consistently  better than ig and chi with smaller computation time especially when the  reduced dimension is extremely small."
the model is evaluated on the reuters test  collection and compared to the multinomial naive bayes model.
while previous  work in hierarchical classification focused on virtual category trees where  documents are assigned only to the leaf categories, we propose a top-down  level-based classification method that can classify documents to both leaf and  internal categories.
we conclude by comparing our  approach of representing texts and rules as logic programs to others.}, }  @inproceedings{junker01, author = {markus junker and andreas dengel}, title =  {preventing overfitting in learning text patterns for document categorization},  booktitle = {proceedings of icapr-01, 2nd international conference on advances  in pattern recognition}, publisher = {springer verlag, heidelberg, de}, note =
we  consider the problem of classifying documents not by topic, but by overall  sentiment, e.g., determining whether a review is positive or negative.
feature  engineering for text classification}, booktitle = {proceedings of icml-99, 16th  international conference on machine learning}, editor
} @inproceedings{angelova:2006:gbt, author = "angelova, ralitsa andweikum, gerhard", title = "graph-based text classification: learn from your neighbors", booktitle = "proceedings of the 29th annual international acm sigir conference on research and development in information retrieval", year = "2006", pages =
these metrics are shown to be good predictors of categorization accuracy that can be achieved on a dataset, and serve as efficient heuristics for generating datasets subject to user's requirements.
such a framework provides for a better assessment of the expected performance of a categorizer if the compression rate of the summarizer is known.}, } @inproceedings{koller97, author = {daphne koller and mehran sahami}, title = {
this situation occurs, for  instance, in declassifying documents that have been previously considered  important to national security and thus are currently being kept as secret.
springer verlag, heidelberg, de},  editor = {alexander f. gelbukh}, note = {
in this paper we present empirical  results on the performance of a bayesian classifier and a decision tree  learning algorithm on two text categorization data sets.
{morgan  kaufmann publishers, san francisco, us}, url =  {http://doi.acm.org/10.1145/1015330.1015356}, abstract = {
in the first stage, the queries are enriched such that for each query,  its related web pages together with their category information are collected  through the use of search engines.
this paper investigates three algorithms that potentially could automate this categorization process: 1) a nearest neighbor-like algorithm, 2) c4.5rules, a machine learning decision tree algorithm; and 3)
by  asymmetric misclassification costs we mean that one of the class values is the  target class value for which we want to get predictions and we prefer false  positive over false negative.
the world wide web is a vast source of information accessible to  computers, but understandable only to humans.
while the experimental literature on text categorization emphasizes  effectiveness comparisons, we list a variety of other characteristics of  learning approaches that are equally important to consider.
this paper studies the  effects of boosting in the context of different classification methods for text  categorization, including decision trees, naive bayes, support vector machines  (svms) and a rocchio-style classifier.
we have experimented with five machine learning algorithms: nearest neighbors (nn), naive bayes (nb), decision tree (dt), sparse network of winnows (snow), and support vector machines (svm) using two kinds of features: bag-of-words and bag-ofngrams.
w. bruce croft and van rijsbergen, cornelis j.}, publisher
because the sense distinctions made are coarse, the disambiguation can be accomplished without the expense of knowledge bases or inference mechanisms.
candidate feature subsets are evaluated by using three-layer  feedforward neural networks.
we propose a general class of models for classification and clustering in relational domains that capture probabilistic dependencies between related instances.
experimental results show that this  variant provides an order of magnitude further improvement in training  efficiency.
editor = {carlo zaniolo and peter c. lockemann and marc h. scholl and torsten grust}, year = {2000}, address = {konstanz, de}, publisher = {springer verlag, heidelberg, de}, note = {
our experiments demonstrate that the part-of-speech approach is better than traditional bag of words techniques, particularly in the domain transfer conditions.}, } @inproceedings{fisher03, author = {michelle fisher and richard everson}, title = {when are links useful?
an extended version appears as~\cite{apte94}}, url = {http://www.acm.org/pubs/articles/proceedings/ir/188490/p23-apte/p23-apte.pdf}, abstract = {
furthermore, they are fully automatic, eliminating  the need for manual parameter tuning.}, } @inproceedings{joachims99, author =
the first set of results applies  rankboost to a text representation produced using modern term weighting  methods.
this paper describes our general approach, several machine learning  algorithms for this task, and promising initial results with a prototype  system.}, } @article{creecy92, author = {robert m. creecy and brij m. masand  and stephen j. smith and david l. waltz}, title = {trading mips and memory for  knowledge engineering: classifying census returns on the connection machine},  journal =
rather than  performing lsi's singular value decomposition (svd) process solely on the  training data, we instead use an expanded term-by-document matrix that includes  both the labeled data as well as any available and relevant background text.
we present the utilization of wsd as an aid for tc.
we describe a new  family of topic-ranking algorithms for multi-labeled documents.
the linear combination approach makes use of limited knowledge in the training document set.
we report the results of a study on topic spotting in  conversational speech.
} @inproceedings{koppel:2006:aat, author =
with the recent dramatic increase in electronic access to documents, text categorization-the task of assigning topics to a given document-has moved to the center of the information sciences and knowledge management.
it is easier to find similar documents which use different nomenclature.
the agreement  between the automated grader and the final manual grade was as good as the  agreement between human graders.}, } @inproceedings{larkey99, author =
our contributions in this paper are as follows: (a) we provide an implementation of transductive svm (tsvm) that is significantly more efficient and scalable than currently used dual techniques, for linear classification problems involving large, sparse datasets.
empirical  results support the theoretical findings.
using a massively parallel supercomputer, we leverage the information already contained in the thousands of coded stories and are able to code a story in about 2 seconds.
this can be considered as the effective combination of  documents with no topic or class labels (unlabeled data), labeled documents,  and prior domain knowledge (in the form of the known hierarchic structure), in  providing enhanced document classification performance.}, }  @inproceedings{vinot03, author = {
our algorithm automatically induces a very natural behavior,  where our knowledge about one instance helps us classify related ones, which in  turn help us classify others.
{taipei, tw}, url =  {http://acl.ldc.upenn.edu/coling2002/proceedings/data/area-28/co-201.pdf},  abstract = {this paper proposes a new approach for text categorization, based  on a feature projection technique.
a large collection of automatically  generated datasets are made available for other researchers to use.}, }  @inproceedings{debole03, author = {franca debole and fabrizio sebastiani},  title = {supervised term weighting for automated text categorization}, year =
a combination of all the above results in a multi-stage ned system that  performs much better than baseline single-stage ned systems.}, }  @inproceedings{kwok98, author = {james t. kwok}, title = {automated text  categorization using support vector machine}, booktitle = {proceedings of  iconip'98, 5th international conference on neural information processing},
in particular, in which situations can a combination of kernel be expected to perform better than its components considered separately?
{524--527}, url = {http://doi.acm.org/10.1145/956863.956964}, abstract = {while  there are many aspects to managing corporate knowledge, one key issue is how to  organize corporate documents into categories of interest.
"international series in intelligent  technologies", number = 20, isbn =
our experiments on human subjects indicate that human feedback on feature relevance can identify a sufficient proportion of the most relevant features (over 50% in our experiments).
"1301--1306" }  @inproceedings{lan:2006:pnt, author =
we explore the subsequent risk that k tuned on  partitions will be suboptimal after aggregation and re-training.
modeled as semi-structured documents, e-mail  messages consist of a set of fields with predefined semantics and a number of  variable length free-text fields.
despite a limited number of training examples,  combining an effective feature selection with the chi(2) learning algorithm for  training the text classifier results in an adequate categorization of new  magazine articles.}, } @inproceedings{mooney00, author = {raymond j. mooney and  loriene roy}, title = {content-based book recommending using learning for text  categorization}, booktitle = {proceedings of dl-00, 5th acm conference on  digital libraries}, editor = {}, publisher = {acm press, new york, us}, year =
although we usually estimate the model so that it completely  satisfies the equality constraints on feature expectations with the me method,  complete satisfaction leads to undesirable overfitting, especially for sparse  features, since the constraints derived from a limited amount of training data  are always uncertain.
this model generalizes the multinomial naive bayes and it is derived from a more general model for different access tasks.
the  results showed that our method performs well for reuters text collection when  enough training documents are given and the new measures have indeed considered  the contributions of misclassified documents.}, } @article{sun03, author =  {aixin sun and ee-peng lim and wee-keong ng}, title = {performance measurement  framework for hierarchical text classification}, journal = {journal of the  american society for information science and technology}, year = {2003}, volume  = {54}, number = {11}, pages = {1014--1028}, url =  {http://www.cais.ntu.edu.sg/~sunaixin/paper/sun_jasist03.pdf}, abstract =  {hierarchical text classification or simply hierarchical classification refers  to assigning a document to one or more suitable categories from a hierarchical  category space.
first, a chain augmented naive bayes model relaxes some of the independence assumptions of naive bayes--allowing a local markov chain dependence in the observed variables--while still permitting efficient inference and learning.
the simplicity of the model, the high recall precision rates, and the efficient computation together make expnet preferable as a practical solution for real world applications.}, } @inproceedings{yang95, author = {
published in the ``lecture notes in computer science''  series, number 1040}, url = {http://www-poleia.lip6.fr/~moulinie/wijcai.ps.gz},
the experiments compare the performance of a counterpropagation network against a backpropagation neural network.
{2004}, pages = {342--349}, url = {}, abstract = {}, } @inproceedings{wang99,  author = {hui wang and nguyen h. son}, title = {text classification using  lattice machine},
the results reveal that a new feature  selection metric we call 'bi-normal separation' (bns), outperformed the others  by a substantial margin in most situations.
{text categorization with support vector machines: how to represent  texts in input space?}, journal = {machine learning}, year = {2002}, volume =
"it is  often useful to classify email according to the intent of the sender (e.g.,  'propose a meeting', 'deliver information').
on the other hand, other techniques naturally  extensible to handle multi-class classification are generally not as accurate  as svm.
janez brank and marko grobelnik and natasa mili{\'{c}}-frayling and dunja mladeni{\'{c}}}, title = {
{washington, us}, editor = {david a. evans and luis gravano and otthein herzog and chengxiang zhai and marc ronthaler}, year = {2004}, pages =
} } @inproceedings{kibriya:2004:mnb, author =
second, we present the word-augmented relevancy signatures algorithm that uses lexical items to represent domain-specific role relationships instead of semantic features.
in this paper, we summarize the competition task,  the evaluation method, and the results of the competition.
however, published work on combining text  categorizers suggests that, for this particular application, improvements in  performance are hard to attain.
text categorization can be viewed as a process of category search,  in which one or more categories for a test document are searched for by using  given training documents with known categories.
{edward a. fox and peter  ingwersen and raya fidel}, publisher = {acm press, new york, us}, year =
{information extraction as a basis for high-precision text classification},  journal = {acm transactions on information systems}, year = {1994}, number =  {3}, volume = {12}, pages = {296--333}, url =  {http://www.cs.utah.edu/~riloff/psfiles/single-acm.ps}, abstract = {
we present detailed experimental results using naive bayes and  support vector machines on the 20 newsgroups data set and a 3-level hierarchy  of html documents collected from dmoz open directory.}, } @article{dhillon03,  author = {inderjit dhillon and subramanyam mallela and rahul kumar}, title = {
however, it does show that tfidf  conversion and document length normalization are important.
{parameterized generation of labeled datasets for text categorization based on  a hierarchical directory}, booktitle = {proceedings of sigir-04, 27th acm  international conference on research and development in information retrieval},  editor = {
the \textsf{reuters-21578} test collection, together with its  earlier variants, has been such a standard benchmark for the text  categorization (tc) task throughout the last ten years.
lisa f. rau and paul s. jacobs}, title = {
our results confirm, quantify, and extend previous research using web structure in these areas, introducing new methods for classification and description of pages.}, } @inproceedings{godbole04, author = {shantanu godbole and abhay harpale and sunita sarawagi and soumen chakrabarti}, title = {document classification through interactive supervision of document and term labels}, booktitle = {proceedings of pkdd-04, 8th european conference on principles of data mining and knowledge discovery}, editor = {jean-fran{\c{c}}ois boulicaut and floriana esposito and fosca giannotti and dino pedreschi}, address = {
nlp found helpful (at least for one text categorization task)},
a hyperlink ensemble is formed by obtaining one prediction for each hyperlink that points to a page.
the accuracy of classification achieved with  our method appears better than or comparable to those of existing rule-based  methods.}, } @inproceedings{liao02, author = {yihua liao and v. rao vemuri},  title = {
we are  facing a new challenge due to the fact that web documents have a rich structure  and are highly heterogeneous.
thus, there is the problem of determining what information is relevant to the  user and how this decision can be taken by a supporting system.
henri prade}, year  = {1998}, pages = {473--474}, address = {brighton, uk}, url =  {http://www-ai.ijs.si/dunjamladenic/papers/pww/pwwecai98yr.ps.gz}, abstract =  {the paper describes an approach to automatic web-page classification based on  the yahoo hierarchy.
"learning hierarchical multi-category text  classification models", booktitle = "proceedings of the twenty-second  international conference on machine learning", year =
in the second phase, to clarify the impact of this performance on filtering, different types of user profiles were created by grouping subsets of classes based on their individual classification accuracy rates.
in these experiments we show that, after only one epoch of training, our algorithm performs much better than perceptron-based hierarchical classifiers, and reasonably close to a hierarchical support vector machine.} } @article{diaz04, author = {irene d{\'{\i}}az and jos{\'{e}} ranilla and elena monta{\~{n}}es and javier fern{\'{a}}ndez and el{\'{\i}}as f. combarro}, title = {
text categorization is performed using the kullback-leibler distance between the probability distribution of the document to classify and the probability distribution of each category.
we propose a new hierarchical generative model for textual data,  where words may be generated by topic specific distributions at any level in  the hierarchy.
{1990}, address = {}, pages = {25--37}, url = {}, abstract = {}, }  @article{gray71, author = {w. a. gray and a. j. harley}, title =  {computer-assisted indexing}, journal = {information storage and retrieval},  year = {1971}, volume = {7}, number = {4}, pages = {167--174}, url = {},  abstract = {}, } @inproceedings{guo04, author =
the final classification of test documents is determined by a majority voting from the individual classifications of each feature.
reprinted in karen sparck jones and peter willett (eds.), ``readings in information retrieval'', morgan kaufmann, san francisco, us, 1997, pp.\ 518--526.}, abstract = {
{integrating linguistic resources in an uniform way for text classification  tasks}, booktitle = {proceedings of lrec-98, 1st international conference on  language resources and evaluation}, publisher = {}, editor = {antonio rubio and  natividad gallardo and rosa castro and antonio tejada}, address = {grenada,  es}, pages = {1197--1204}, year = {1998}, url =  {http://www.esi.uem.es/laboratorios/sinai/postscripts/lrec98.ps}, abstract =  {applications based on automatic text classification tasks, like text  categorization (tc), word sense disambiguation (wsd), text filtering or  routing, monolingual or multilingual information retrieval, and text  summarization could obtain serious improvements by integrating linguistic  resources in the current methods.
{montreal, ca}, year = {1997}, pages = {513--530}, note = {an extended version appears as~\cite{amati99}}, url = {http://www.cs.strath.ac.uk/~fabioc/papers/97-riao.pdf}, abstract = {
the data-driven nature of tcs allows it is to satisfy fully the requirements of ease of application development, portability to other applications and maintainability.}, } @article{he03, author = {
{morgan kaufmann publishers, san francisco, us}, url = {http://www-ai.cs.uni-dortmund.de/dokumente/klinkenberg_joachims_2000a.pdf.gz},
this paper presents a new method for graph-based classification, with particular emphasis on hyperlinked text documents but broader applicability.
our experiments on hierarchical classification methods based on svm classifiers and binary naive bayes classifiers showed that svm classifiers perform better than naive bayes classifiers on reuters-21578 collection according to the extended measures.
the method we suggest  is based on combining the lexical knowledge extracted from the ldb interpreted  as a classifier with a learning-based classifier, through sg.
the goal of  these experiments was to automatically discover classification patterns that  can be used for assignment of topics to the individual newswires.
{proceedings of the 5th international conference on soft computing and  information}, publisher =
}  @inproceedings{alm:2005:eft, author =
the described experimental study shows that the idf transform considerably effects the distribution of classification performance over feature selection reduction rates, and offers an evaluation method which permits the discovery of relationships between different document representations and feature selection methods which is independent of absolute differences in classification performance.}
"generalized {lars} as an  effective feature selection tool for text classification with {svm}s",  booktitle =
it connects the statistical properties of  text-classification tasks with the generalization performance of a svm in a  quantitative way.
{micheline beaulieu and ricardo baeza-yates and sung hyon myaeng and kalervo j{\"{a}}rvelin}, publisher = {acm press, new york, us},
we present experimental results on the \textsf{reuters-21578} text categorization collection, showing that for both algorithms the version with discretized continuous attributes outperforms the version with traditional binary representations.}, } @inproceedings{ng97, author =
in this study, we adapt a simple text  classifier (rocchio), using weakly supervised clustering techniques.
}  @inproceedings{zhang:2005:tck, author =
"bonn, germany", url =  "http://www.machinelearning.org/proceedings/icml2005/papers/094_hierarchical_rousuetal.pdf",  abstract =
= {morgan kaufmann publishers, san francisco, us}, year = {2003}, address = {acapulco, mx}, pages = {581--586}, url = {}, abstract = {a new approach to the text categorization problem is here presented.
text mining concerns of discovering unknown patterns or knowledge from a large text repository.
we implemented our classification scheme using decision tree classifiers and self-organizing maps.}, } @inproceedings{siersdorfer04, author = {stefan siersdorfer and sergej sizov and gerhard weikum}, title = {goal-oriented methods and meta methods for document classification and their parameter tuning}, booktitle = {proceedings of cikm-04, 13th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, address = {washington, us}, editor = {david a. evans and luis gravano and otthein herzog and chengxiang zhai and marc ronthaler}, year = {2004}, pages = {59--68}, url = {}, abstract = {}, } @inproceedings{siersdorfer05, author = {stefan siersdorfer and gerhard weikum}, title = {using restrictive classification and meta classification for junk elimination}, booktitle = {proceedings of ecir-05, 27th european conference on information retrieval}, publisher = {springer verlag}, editor = {david e. losada and juan m. fern{'{a}}ndez-luna}, address = {santiago de compostela, es}, year = {2005}, pages = {287--299}, url = {}, abstract = {}, } @inproceedings{siolas00, author = {siolas, georges and d'alche-buc, florence}, title = {support vector machines based on a semantic kernel for text categorization}, booktitle = {proceedings of ijcnn-00, 11th international joint conference on neural networks}, publisher =
in the traditional setting, text categorization is formulated as a concept learning problem where each instance is a single isolated document.
"andrea esuli and fabrizio sebastiani", title =
"zhiwei li and bin wang and mingjing  li and wei-ying ma", title =
these  questions are addressed in an overview of the existing approaches to text  classification.
furthermore we demonstrate that the hsom is able to map large text collections in a semantically meaningful way and therefore allows a ``semantic browsing'' of text databases.}, } @article{paijmans98, author = {paijmans, hans}, title = {
applying an  existing machine learning algorithm to text categorization}, booktitle =
we conclude that, in general, more accurate hierarchical categorisation is possible by using our simple feature selection technique.}, } @inproceedings{wiener95, author = {erik d. wiener and jan o. pedersen and andreas s. weigend}, title = {
our  experiments, make use of the reuters 21578 database of documents and consist of  a binary classification for each of the ten most populous categories of the  reuters database.
in this paper, we propose an approach to speedup the process of text classification based on pruning the training corpus.
acm international conference on research and development in information  retrieval}, editor =
{ashraf m. kibriya and eibe frank and bernhard pfahringer and geoffrey holmes},  title = {multinomial naive bayes for text categorization revisited}, booktitle  = {proceedings of ai-04, 17th australian joint conference on artificial  intelligence}, year = {2004}, pages = {488--499}, address = {cairns,  australia}, series = {lecture notes in artificial intelligence}, volume =
= {morgan kaufmann publishers, san francisco, us}, year = {2003}, address = {acapulco, mx}, } @article{zhang01, author = {tong zhang and frank j. oles}, title = {
we propose a theory of  genres as bundles of facets, which correlate with various surface cues, and  argue that genre detection based on surface cues is as successful as detection  based on deeper structural properties.}, } @inproceedings{khmelev03, author =
visual keywords can be constructed  automatically from samples of visual data through supervised/unsupervised  learning.
the results show that our new approach outperforms  the latest lnn approach and linear classifiers in all experiments.}, }  @inproceedings{lam99, author = {savio l. lam and dik l. lee}, title = {feature  reduction for neural network based text categorization}, booktitle =  {proceedings of dasfaa-99, 6th ieee international conference on database  advanced systems for advanced application}, editor = {
{1992}, url = {http://www.research.att.com/~lewis/papers/lewis92b.ps}, abstract  = {syntactic phrase indexing and term clustering have been widely explored as  text representation techniques for text retrieval.
this approach allows us to tune the combination system on  available but less-representative validation data and obtain smaller  performance degradation of this system on the evaluation data than using a  single-method classifier alone.
we discuss two learning algorithms for text filtering: modified rocchio and a boosting algorithm called adaboost.
w. bruce croft and van rijsbergen, cornelis j.}, publisher = {springer verlag, heidelberg, de}, year = {1994}, address
the feature sets are based  on the ``latent semantics'' of a reference library - a collection of documents  adequately representing the desired concepts.
nevertheless, as we show, it searches a space  that is as rich as the space of all linear separators.
as we show, each of these smaller problems can be solved accurately by  focusing only on a very small set of features, those relevant to the task at  hand.
the baldwin effect concerns the tradeoffs between  learning and evolution.
we show how  to learn such models efficiently from data.
the user interface helps users search in fields  without requiring the knowledge of inquery query operators.
in preparation for the use of text categorization to study text representation, a more effective and theoretically well-founded probablistic text categorization algorithm was developed, building on work by maron, fuhr, and others.
{journal of documentation}, volume = {31}, number = {4}, pages = {246--265},  url = {}, abstract = {}, } @inproceedings{finn02, author = {aidan finn and  nicholas kushmerick and barry smyth}, title = {genre classification and domain  transfer for information filtering}, booktitle = {proceedings of ecir-02, 24th  european colloquium on information retrieval research}, editor = {
in this paper we investigate seven text representations based on  n-grams and single words.
experimental results show that the choice of thresholding strategy can significantly influence the performance of knn, and that the "optimal" strategy may vary by application.
documents are assigned to categories based on these rankings.
new filtering and  disambiguation methods are used as pre-processing to solve the problems caused  by the use of the thesaurus.
proceedings of iaai-90, 2nd conference on innovative applications of artificial intelligence}, publisher = {aaai press, menlo park, us}, editor = {alain rappaport and reid smith}, year = {1990}, pages = {49--66}, url = {}, abstract = {}, address = {boston, us}, } @inproceedings{hayes90a, author = {philip j. hayes and peggy m. andersen and irene b. nirenburg and linda m. schmandt}, title = {{\sc tcs}: a shell for content-based text categorization}, booktitle = {proceedings of caia-90, 6th ieee conference on artificial intelligence applications}, publisher =
text categorization (tc) and information extraction (ie) are two important goals of natural language processing.
"shen, dou and sun, jian-tao and yang, qiang and chen, zheng", title =
empirical evaluation results indicate that the proposed technique, mice, was more effective than the category discovery approach and was insensitive to the quality of original categories.}, } @article{weigend99, author = {andreas s. weigend and erik d. wiener and jan o. pedersen}, title = {
as a classifier, we adopted a variant of k-nearest neighbor (knn) with  supervised term weighting schemes to improve the performance, making our method  among the top-performing systems in the trec official evaluation.
then the final output of the pca is combined with the feature  vectors from the class-profile which contains the most regular words in each  class.
"d. tikk and {gy.} biro", title =  "experiment with a hierarchical text categorization method on the {wipo}  patent collection", booktitle =
applied to trec-7 and trec-8 filtering track documents, the proposed method obtained a significant improvement in lf1, lf2, fl and f3 measures compared to the best results submitted by other trec entries.}, } @inproceedings{kim04, author =
in practice, the assumption is too restrictive since a web page itself may not always correspond to a concept instance of some semantic concept (or category) given to the classification task.
"rome, italy" } @inproceedings{davy:2007:alh, author =
our results show that naive bayes is a weak choice for guiding a topical  crawler when compared with support vector machine or neural network.
these algorithms make extremely fast decisions, because they need to examine only a small number of words in each text document.
booktitle = {proceedings of cikm-99, 8th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, editor = {}, year = {1999}, address = {kansas city, us}, pages = {180--187}, url = {http://www.acm.org/pubs/articles/proceedings/cikm/319950/p180-labrou/p180-labrou.pdf}, abstract = {
once again, lsi slightly improves  performance.
this paper  presents our research work on automatic question classification through machine  learning approaches.
we present experimental results obtained on the standard \textsf{reuters-21578}  benchmark with one classifier learning method (support vector machines), three  term selection functions (information gain, chi-square, and gain ratio), and  both local and global term selection and weighting.}, }  @inproceedings{debole04, author = {franca debole and fabrizio sebastiani},  title = {an analysis of the relative difficulty of reuters-21578 subsets}, year  = {2004}, booktitle = {proceedings of lrec-04, 4th international conference on  language resources and evaluation}, address = {lisbon, pt}, pages = {}, url =  {http://www.math.unipd.it/~fabseb60/publications/lrec04.pdf}, abstract = {
the system has been tested in two applications in particular, one concerning passive invoices and the other bank documents.
then it combines these predictions using a majority  voting.
"june", address = "prague, czech republic" } @inproceedings{kim:2007:cap, author = "soo-min kim and eduard hovy", title =
"on the use of linear programming for unsupervised text  classification", booktitle =
a probabilistic classification procedure computes indexing weights for each relevance description.
we have performed  experiments which results show that the ideas we describe are promising and  deserve further investigation.}, } @inproceedings{gomez02a, author =
"rochester, ny" }  @article{zelikovitz:2007:ewb, author = {
we call $n$-gram a set $g_k$ of $n$ word stems, and we say that $g_k$ occurs in a document $d_j$ when a sequence of words appears in $d_j$ that, after stop word removal and stemming, consists exactly of the $n$ stems in $g_k$, in some order.
this paper presents an empirical comparison of twelve feature selection methods (e.g.\ information gain) evaluated on a benchmark of 229 text classification problem instances that were gathered from reuters, trec, ohsumed, etc.
athena satisfies these requirements through  linear-time classification and clustering engines which are applied  interactively to speed the development of accurate models.
"672--681", }  @inproceedings{esuli:2006:dts, author =
mh$^r$} algorithm.}, } @article{sebastiani02, author  = {fabrizio sebastiani}, title = {machine learning in automated text  categorization}, journal = {acm computing surveys}, volume = {34}, number =
to classify a database, our algorithm does not retrieve or in-spect any documents or pages from the database, but rather just exploits the number of matches that each query probe generates at the database in question.
this paper applies two statistical learning algorithms, the linear least squares fit (llsf) mapping and a nearest neighbor classifier named expnet, to a large collection of medline documents.
it is easier to find similar documents which use  different nomenclature.
in comparison to the previously proposed agglomerative strategies our divisive algorithm is much faster and achieves comparable or higher classification accuracies.
"schiffman, barry and mckeown,  kathleen r.", title =
first, a chain augmented naive bayes model relaxes some of  the independence assumptions of naive bayes--allowing a local markov chain  dependence in the observed variables--while still permitting efficient  inference and learning.
we devise an  algorithm that interleaves labeling features and documents which significantly  accelerates standard active learning in our simulation experiments.
in addition, this paper investigates how these term distributions contribute to weight each term in documents, e.g., a high term distribution of a word promotes or demotes importance or classification power of that word.
an application of the  network model is described, followed by an indexing example and some  experimental results about the indexing performance of the network model.}, }  @article{uren02, author = {victoria s. uren and thomas r. addis}, title = {how  weak categorizers based upon different principles strengthen performance},  journal = {the computer journal}, year = {2002}, volume = {45}, number = {5},  pages = {511--524}, url =  {http://www3.oup.co.uk/computer_journal/hdb/volume_45/issue_05/pdf/450511.pdf},  abstract =
this advantage is achieved by executing morphological and semantic analyses of an incoming text.
we report experiments that compare its performance with that of a  well-known probabilistic classifier.
booktitle = {proceedings of thai-99, 1st european symposium on telematics, hypermedia and artificial intelligence}, editor =
text categorization using weight-adjusted $k$-nearest neighbor classification}, booktitle = {proceedings of pakdd-01, 5th pacific-asia conferenece on knowledge discovery and data mining}, editor = {david cheung and qing li and graham williams}, year = {2001}, publisher = {springer verlag, heidelberg, de}, address = {hong kong, cn}, note = {
we propose a new method of text classification using stochastic  decision lists.
"many classification problems require classifiers to assign each single document into more than one category, which is called multi-labelled classification.
the personal view constructor mines user interests and maps them to a class hierarchy (i.e., personal view).
{william b. cavnar and john m. trenkle}, title = {n-gram-based text categorization}, booktitle = {proceedings of sdair-94, 3rd annual symposium on document analysis and information retrieval}, publisher
using restrictive classification and meta classification for junk  elimination}, booktitle = {proceedings of ecir-05, 27th european conference on  information retrieval}, publisher = {springer verlag}, editor = {david e.  losada and juan m. fern{'{a}}ndez-luna}, address = {santiago de compostela,  es}, year = {2005}, pages = {287--299}, url = {}, abstract = {}, }  @inproceedings{siolas00, author = {siolas, georges and d'alche-buc, florence},  title = {support vector machines based on a semantic kernel for text  categorization}, booktitle = {proceedings of ijcnn-00, 11th international joint  conference on neural networks}, publisher = {ieee computer society press, los  alamitos, us}, editor = {amari, shun-ichi and giles, c. lee and gori, marco and  piuri, vincenzo}, year = {2000}, address = {como, it}, volume = {5}, pages =
{acm press, new york, us}, editor = {}, year = {1999}, address = {san diego,  us}, pages = {398--401}, url =  {http://www.acm.org/pubs/articles/proceedings/ai/312129/p398-dorre/p398-dorre.pdf},  abstract =
we also investigate the usability of our automated learning approach by  actually developing a system that categorizes texts into a tree of categories.
this paper reports a system that hierarchically classifies chinese  web documents without dictionary support and segmentation procedure.
as the standard performance measures assume independence  between categories, they have not considered the documents incorrectly  classified into categories that are similar or not far from the correct ones in  the category tree.
we identify the inductive biases of each classifier and explore how boosting, as an error-driven resampling mechanism, reacts to those biases.
published in the ``lecture notes in computer science'' series, number 2734}, abstract = {
the technique can effectively rank the words in the unlabeled set according to their importance.
reprinted in karen sparck jones and peter willett  (eds.), ``readings in information retrieval'', morgan kaufmann, san francisco,  us, 1997, pp.\ 518--526.}, abstract = {
an experimental comparison of naive bayesian and keyword-based anti-spam filtering with personal e-mail messages}, booktitle = {proceedings of sigir-00, 23rd acm international conference on research and development in information retrieval}, editor = {nicholas j. belkin and peter ingwersen and mun-kew leong}, publisher = {acm press, new york, us}, address = {athens, gr}, year = {2000}, pages = {160--167}, url = {http://doi.acm.org/10.1145/345508.345569}, abstract = {
"using the essence of texts to improve document classification", booktitle = "recent advances in natural language processing", year = "2005", month = "september", address =
recurrent plausibility networks with local memory are developed and  examined for learning robust text routing.
{springer verlag, heidelberg, germany}, year = {1994}, address = {dublin, ireland}, pages = {282--289}, url = {\small http://www.acm.org/pubs/articles/proceedings/ir/188490/p282-hull/p282-hull.pdf}, abstract = {latent semantic indexing (lsi) is a novel approach to information retrieval that attempts to model the underlying structure of term associations by transforming the traditional representation of documents as vectors of weighted term frequencies to a new coordinate space where both documents and terms are represented as linear combinations of underlying semantic factors.
the results we obtain allow us to determine the  relative hardness of these subsets, thus establishing an indirect means for  comparing tc systems that have, or will be, tested on these different  subsets.}, } @inproceedings{debuenaga97, author = {
adaboost  produces better classifiers than rocchio when the training collection contains  a very large number of relevant documents.
in  order to select a good hypothesis language (or model) from a collection of  possible models, one has to assess the generalization performance of the  hypothesis which is returned by a learner that is bound to use that model.
the technique can  effectively rank the words in the unlabeled set according to their importance.
our  experiments show our algorithms represent a good trade-off between speed and  accuracy in most applications.}, } @inproceedings{cheong02, author = {cheong  fung, gabriel p. and jeffrey x. yu and hongjun lu}, title = {discriminative  category matching:
we review some of the  variations of naive bayes models used for text retrieval and classification,  focusing on the distributional assumptions made about word occurrences in  documents.}, } @inproceedings{lewis99, author = {lewis, david d. and daniel l.  stern and amit singhal}, title = {{\sc attics}: a software platform for on-line  text classification}, booktitle = {proceedings of sigir-99, 22nd acm  international conference on research and development in information retrieval},  editor =
our results show that the new method is highly effective and promising." } @article{bang:2006:hdc, author = {s.l. bang and j.d. yang and h.j. yang}, title = {hierarchical document categorization with k-nn and concept-based thesauri}, journal = {information processing and management}, year = {2006}, volume = {42}, number = {2}, pages = {387--406}, abstract = {in this paper, we propose a new algorithm, which incorporates the relationships of concept-based thesauri into the document categorization using the k-nn classifier (k-nn).
we propose a system that interacts with an author using an automatic text classifier to suggest controlled keywords to be used as metadata.
martha e. pollack}, publisher = {morgan kaufmann publishers, san francisco, us}, year = {1997}, pages = {764--769}, address = {nagoya, jp}, url = {http://www.cs.strath.ac.uk/~fabioc/papers/97-ijcai.pdf}, abstract = {new methods and new systems are needed to filter or to selectively distribute the increasing volume of electronic information being produced nowadays.
the {reuters corpus volume 1} -- from  yesterday's news to tomorrow's language resources}, booktitle = {proceedings of  lrec-02, 3rd international conference on language resources and evaluation},  year = {2002}, address = {las palmas, es}, pages = {827--832}, }  @inproceedings{rosso04, author = {paolo rosso and antonio molina and ferran pla  and daniel jiménez and vicent vidal}, title = {information retrieval and  text categorization with semantic indexing}, booktitle = {proceedings of  cicling-04, 5th international conference on computational linguistics and  intelligent text processing}, year = {2004}, editor = {
integrating background knowledge into text classification}, pages = {1448--1449}, url = {}, booktitle = {proceedings of ijcai-03, 18th international joint conference on artificial intelligence}, editor =
{3}, month = {march}, pages = {1265--1287}, year = {2003}, url =  {http://www.jmlr.org/papers/volume3/dhillon03a/dhillon03a.pdf}, abstract =  {high dimensionality of text can be a deterrent in applying complex learners  such as support vector machines to the task of text classification.
"2006", pages = "228--237", } @inproceedings{schuetze:2006:ptp, author = "hinrich schuetze and emre velipasaoglu and jan pedersen", title = "performance thresholding in practical text classification", booktitle = "cikm", year = "2006", pages = "662--671", } @inproceedings{shen:2006:tci, author =
differences include: different notions as to what constitutes a context; different ways of combining contexts to construct a classifier; different methods to search for a combination of contexts; and different criteria as to what contexts should be included in such a combination.
this approach clusters words into groups based on the distribution of class labels associated with each word.
it  is demonstrated that the performance of such a classifier is further enhanced  when employing the kernel derived from an appropriate hierarchic mixture model  used for partitioning a document corpus rather than the kernel associated with  a at non-hierarchic mixture model.
such systems can be deployed in various applications where instantaneous interactive learning is necessary such as on-line email or news categorization, text summarization and information filtering in general.}, } @inproceedings{ghani00, author = {rayid ghani}, title = {using error-correcting codes for text classification},
we also analyse the relationships of news  headlines and their contents of the new reuters corpus by a series of  experiments.
these theoretical findings are  supported by our experiments, which show that hyperbolic soms can successfully  be applied to text categorization and yield results comparable to other  state-of-the-art methods.
107--125}, url =  {http://www.wkap.nl/article.pdf?391242}, abstract = {
using density estimation over the raw tf*idf  values, we obtain a classification accuracy of 82\%, a number that outperforms  baseline estimates and earlier, image-based approaches, at least in the domain  of news articles, and that nears the accuracy of humans who perform the same  task with access to comparable information.}, } @inproceedings{sable01, author  = {carl sable and ken church}, title = {using bins to empirically estimate term  weights for text categorization}, booktitle = {proceedings of emnlp-01, 6th  conference on empirical methods in natural language processing}, year = {2001},  publisher =
the method outperforms all known methods when tested recognized standard benchmarks for this task."
"proceedings of the 28th annual international {acm} {sigir} conference on research and development in information retrieval", year =
{chai k. adam and hwee t. ng and hai l. chieu}, title = {bayesian online classifiers for text classification and filtering}, booktitle = {proceedings of
experiments have been conducted on a real-world document collection demonstrating the effectiveness of our approach.
we performed a number of experiments with texts from a german newspaper.
a stochastic decision list is an ordered sequence of if-then rules, and our method can be viewed as a rule-based method for text clsssification having advantages of readability and refinability of acquired knowledge.
"rochester, ny" } @article{zelikovitz:2007:ewb, author = {
{springer verlag, heidelberg, de}, address = {hong kong, cn}, note = {
dunja mladeni{\'{c}} and janez brank and marko grobelnik and natasa  mili{\'{c}}-frayling}, title = {feature selection using linear classifier  weights: interaction with classification models}, booktitle = {proceedings of  sigir-04, 27th acm international conference on research and development in  information retrieval}, editor = {
in this paper, we provide a solid basis for the application of ilp methods to these learning problems.
{267--268}, url = {http://www.acm.org/pubs/articles/proceedings/ir/312624/p267-lewis/p267-lewis.pdf}, abstract = {numerous systems for ranked retrieval on text databases have been implemented by both information retrieval researchers and in the commercial sector.
as the results, the micro averaged f1 measure for  reuters-21578 improved from 83.69 to 87.27\%.}, } @article{kehagias03, title =
i have discovered a bug in my experimental software which caused the relevance sampling results reported in the paper to be incorrect.
{kluwer academic publishers}, address =
machine learning techniques are used on data  collected from yahoo, a large text hierarchy of web documents.
text  categorization techniques are adopted to convert each process to a vector and  calculate the similarity between two program activities.
we approached the task with  careful consideration of the specialized terminology and paid special attention  to dealing with various forms of gene synonyms, so as to exhaustively locate  the occurrences of the target gene.
to this end, we propose a set of style markers including analysis-level measures that represent the way in which the input text has been analyzed and capture useful stylistic information without additional cost.
the growing problem of unsolicited bulk e-mail, also known as ``spam'', has generated a need for reliable anti-spam e-mail filters.
{seattle, us}, pages = {130--136}, url =  {http://www.cs.utah.edu/~riloff/psfiles/sigir95.ps}, abstract = {most  information retrieval systems use stopword lists and stemming algorithms.
kenneth w. church}, title = {
text classification is becoming more and more important with the rapid growth of on-line information available.
we have manually selected the most regular words that exist in each class and weighted them using an entropy weighting scheme.
we propose to exploit the natural hierarchy of topics, or taxonomy, that many corpora, such as internet directories, digital libraries, and patent databases enjoy.
proceedings of ictai-95, 7th international  conference on tools with artificial intelligence}, publisher = {ieee computer  society press, los alamitos, us}, editor = {}, address = {herndon, us}, year =
an alternative which has not been sufficiently explored is the use of word meanings, also known as senses.
{journal of experimental and theoretical artificial intelligence}, year =
document collections from the medline database and mayo patient records are  used for studies on the effectiveness of our approach, and on how much the  effectiveness depends on the choices of training data, indexing language,  word-weighting scheme, and morphological canonicalization.
the availability of vast amounts of information on the world wide web has created a big demand for automatic tools to organize and index that information.
unsolicited commercial e-mail, or "spam", floods mailboxes, causing frustration, wasting bandwidth, and exposing minors to unsuitable content.
we obtain a classification accuracy of 82\%, a number that clearly outperforms baseline estimates and competing image-based approaches and nears the accuracy of humans who perform the same task with access to comparable information.}, } @inproceedings{sahami96, author = {
} @inproceedings{tailby:2006:eca,  author =
published in the ``lecture notes in computer science'' series, number 1810}, year = {2000}, url = {http://www.lsi.upc.es/~escudero/recerca/ecml00.pdf}, abstract = {
we  propose a system that interacts with an author using an automatic text  classifier to suggest controlled keywords to be used as metadata.
in our experiments we  demonstrate significantly superior performance both over a single classifier as  well as over the use of the traditional weighted-sum voting approach.
we also outline the formal analysis of the algorithm in  the mistake bound model.
this paper focuses on the application of  mixtures of multivariate bernoulli distributions to binary data.
to advance research on this challenging but  important problem, we promote a natural, experimental framework-the daily  classification task-which can be applied to large time-based datasets, such as  reuters rcv1.
"proceedings of the thirteenth international world wide web conference ({www2004})", year = 2004, pages = "482--490", month =
managing the  hierarchical organization of data is starting to play a key role in the  knowledge management community due to the great amount of human resources  needed to create and maintain these organized repositories of information.
published  in the ``lecture notes in computer science'' series, number 1874}, address =
"large scale learning is often realistic only in a semi-supervised setting where a small set of labeled examples is available together with a large collection of unlabeled data.
the former hints a generative model of news articles, and the latter  provides data enriched environments to perform red.
we also observed that extracting meta data from related  web sites was extremely useful for improving classification accuracy in some of  those domains.
finally, we apply the ssahc algorithm  to the reuters database of documents and show that its performance is superior  to the bayes classifier and to the expectation-maximization algorithm combined  with bayes classifier.
while the experimental literature on text categorization emphasizes effectiveness comparisons, we list a variety of other characteristics of learning approaches that are equally important to consider.
we report an experiment to study the impact of  term redundancy on the performance of text classifier.
ellen m. voorhees and donna k. harman}, year = {1997}, address = {gaithersburg, us}, pages = {619--621}, url = {http://trec.nist.gov/pubs/trec6/papers/siemens.ps.gz}, abstract = {
even though most of these features are relevant, the underlying concepts can be concisely captured using only a few features, while keeping all of them has substantially detrimental effect on categorization accuracy.
{679--694}, year = {2006} }  @inproceedings{radovanovic:2006:drc, author = {milo\v{s} radovanovi\'c and  mirjana ivanovi\'c}, title = {document representations for classification of  short {w}eb-page descriptions}, booktitle = {proceedings of dawak-06, 8th  international conference on data warehousing and knowledge discovery}, year =  {2006}, pages = {544--553}, series = {lecture notes in computer science},  volume = {4081}, address = {krakow, poland}, publisher = {springer-verlag}, url  = {http://perun.im.ns.ac.yu/radovanovic/publications/2006-dawak-docrep.pdf},  abstract = {motivated by applying text categorization to sorting web search  results, this paper describes an extensive experimental study of the impact of  bag-of-words document representations on the performance of five major  classifiers -- naive bayes, svm, voted perceptron, knn and c4.5.
systems  for text retrieval, routing, categorization and other ir tasks rely heavily on  linear classifiers.
we report on experiences with the reuters newswire benchmark,  the us patent database, and web document samples from yahoo!.
{205--209}, url =  {http://dlib.computer.org/conferen/ijcnn/0619/pdf/06193581.pdf}, abstract = {we  propose to solve a text categorization task using a new metric between  documents, based on a priori semantic knowledge about words.
we treat a document as a set of phrases, which the learning algorithm must learn to classify as positive or negative examples of keyphrases.
different transformations of input data: stemming, normalization,  logtf and idf, together with dimensionality reduction, are found to have a  statistically significant improving or degrading effect on classification  performance measured by classical metrics -- accuracy, precision, recall, f$_1$  and f$_2$. the emphasis of the study is not on determining the best document  representation which corresponds to each classifier, but rather on describing  the effects of every individual transformation on classification, together with  their mutual relationships.} } @inproceedings{radovanovic:2006:ibd, author =
the application of term clustering to this representation to improve its statistical properties while retaining its desirable meaning properties is proposed.
integrating background knowledge into nearest-neighbor text classification}, pages = {1--5}, url = {}, booktitle = {proceedings of eccbr-02, 6th european conference on case-based reasoning}, editor = {
however the existing clustering, techniques are agglomerative in nature and  result in (i) suboptimal word clusters and (ii) high computational cost.
further, the categorization system can be trained on noisy ocr output, without need for the true text of any image, or for editing of ocr output.
in this paper, we adopt novel dimension reduction methods to reduce the dimension of the document vectors dramatically.
"xiaoguang qi and brian davison", title =
furthermore, detailed analysis of the retrieval performance on each individual test query is provided.}, } @inproceedings{lang95, author = {ken lang}, title = {{\sc newsweeder}: learning to filter netnews}, booktitle = {proceedings of icml-95, 12th international conference on machine learning}, editor = {armand prieditis and stuart j. russell}, address = {lake tahoe, us}, pages = {331--339}, year = {1995}, publisher =
{301--315}, url = {http://www.research.att.com/~lewis/papers/ittner95.ps},  abstract = {
morgan kaufmann publishers, san francisco, us}, url = {http://www.cs.cmu.edu/~rayid/mypapers/ecoc-icml.ps}, abstract = {this paper explores in detail the use of error correcting output coding (ecoc) for learning text classifiers.
typical text classifiers learn from example texts that are manually categorized.
madison, us},  pages =
each topic is, in turn, modeled as an infinite mixture over an underlying set  of topic probabilities.
last but not least, we describe an evaluation experiment that classifies professional nature scenery photographs to demonstrate the effectiveness and efficiency of visual keywords for automatic categorization of images in digital libraries.}, } @article{liu01, author =
a generic system for text categorization is presented which is based on statistical analysis of representative text corpora.
the ability to cheaply train text classifiers is critical to their use in information retrieval, content analysis, natural language processing, and other tasks involving data which is partly or fully textual.
intuitively, one would like each of the two kernels to contribute information that is not available to the other.
in this paper, we present pva, an adaptive personal view information agent system for tracking, learning and managing user interests in internet documents.
the  technique is then used with two randomized sample document groups drawn from  nine categories.
we compare the effectiveness of five different automatic learning algorithms for text categorization in terms of learning speed, real-time classification speed, and classification accuracy.
we apply a two-phase framework  to tackle the above difficulties.
this method might, however, degrade classification results, since the distributions it employs are not always precise enough for representing the differences between categories.
text categorization in an intelligent agent for filtering information on the web}, journal = {international journal of pattern recognition and artificial intelligence}, pages = {527--549}, year = {2001}, number = {3}, volume = {15}, url = {http://www.worldscinet.com/journals/ijprai/15/preserved-docs/1503/s021800140100099x.pdf}, abstract =
{hershey, {us}}, pages = {683--687}, url = {http://www.math.unipd.it/~fabseb60//publications/edta05.pdf}, } @incollection{sebastiani06, author = {fabrizio sebastiani}, title = {classification of text, automatic}, editor = {keith brown}, year = {2005}, volume = {14}, booktitle = {
{2001}, pages = {897--902}, url = {}, abstract = {
published in the ``lecture notes in computer science'' series, number 1696.
the system employs several knowledge sources including a letter database, word frequency statistics for german, lists of message type specific words, morphological knowledge as well as the underlying document structure.
cheng, chun-hung and jian  tang and ada wai-chee and irwin king}, title = {hierarchical classification of  documents with error control}, booktitle = {proceedings of pakdd-01, 5th  pacific-asia conferenece on knowledge discovery and data mining}, editor =
a comparison of event models for naive bayes text classification}, booktitle = {proceedings of aaai-98, workshop on learning for text categorization}, year = {1998}, url = {citeseer.nj.nec.com/mccallum98comparison.html}, } @inproceedings{meretakis00, author = {dimitris meretakis and dimitris fragoudis and hongjun lu and spiros likothanassis}, title = {scalable association-based text classification}, booktitle = {proceedings of cikm-00, 9th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, address = {mclean, us}, editor = {
text  categorization based on regularized linear classification methods}, journal =
the goal of these  methods is to discover automatically classification patterns that can be used  for general document categorization or personalized filtering of free text.
comparisons with an optimized version of the  traditional rocchio's algorithm adapted for text categorization, as well as  flat neural network classifiers are provided.
{proceedings of icapr-01, 2nd international conference on advances in pattern recognition}, publisher = {springer verlag, heidelberg, de}, note = {published in the ``lecture notes in computer science'' series, number 2013}, editor =
for example, rather than choosing one set decision threshold, they  can be used in a bayesian risk model to issue a run-time decision which  minimizes a user-specified cost function dynamically chosen at prediction time.
we present two extensions to the  algorithm that improve classification accuracy under these conditions: (1) a  weighting factor to modulate the contribution of the unlabeled data, and (2)  the use of multiple mixture components per class.
we describe the algorithm and present experimental results on  applying it to the document routing problem.
berkeley, us}, year = {1999}, pages = {281--282}, url = {http://www.acm.org/pubs/articles/proceedings/ir/312624/p281-ruiz/p281-ruiz.pdf}, abstract = {
bala and peter pachowicz}, title = {
= {sam scott and stan matwin}, title = {
this dissertation introduces a new theoretical model for text classification systems, including systems for document retrieval, automated indexing, electronic mail filtering, and similar tasks.
our experiments clearly indicate  that automatic categorization improves the retrieval performance compared with  no categorization.
"computational linguistics and  intelligent text processing (lecture notes in computer science, vol.
on integrating catalogs}, booktitle = {proceedings of www-01, 10th international conference on the world wide web}, publisher = {acm press, new york, us}, editor = {}, year = {2001}, address = {hong kong, cn}, pages = {603--612}, url = {http://doi.acm.org/10.1145/371920.372163}, abstract = {
such topics can be used as descriptors, similarly to the way librarians use for example, the library of congress cataloging system to annotate and categorize books.
yiming yang and jian zhang and bryan kisiel}, title = {a scalability analysis of classifiers in text categorization}, booktitle = {proceedings of sigir-03, 26th acm international conference on research and development in information retrieval}, editor = {jamie callan and gordon cormack and charles clarke and david hawking and alan smeaton}, publisher = {acm press, new york, us}, address = {toronto, ca}, year = {2003}, pages = {96--103}, url = {http://doi.acm.org/10.1145/860435.860455}, abstract = {real-world applications of text categorization often require a system to deal with tens of thousands of categories defined over a large taxonomy.
{micheline beaulieu and ricardo  baeza-yates and sung hyon myaeng and kalervo j{\"{a}}rvelin}, publisher =
therefore, the proposed novelty  detection approach focuses on the identification of previously unseen  query-related patterns in sentences.
{404--417}, url =  {http://www.acm.org/pubs/articles/journals/jacm/1961-8-3/p404-maron/p404-maron.pdf},  abstract = {
in this framework, the concept of multidimensional category  model is introduced for representing classes.
we verify experimentally that ssfcm both outperforms and takes less  time than the fuzzy-c-means (fcm) algorithm.
various  non-binary weighting schemes are widely used for this purpose.
on the one hand, news articles are always aroused by events; on the other hand, similar articles reporting the same event often redundantly appear on many news sources.
luc de raedt and peter a. flach},
a neural network approach to topic spotting  in text}, school = {department of computer science, university of colorado at  boulder}, address = {boulder, us}, year = {1995}, url =  {http://www.stern.nyu.edu/~aweigend/research/papers/textcategorization/wiener_thesis95.ps},  abstract = {
the method is evaluated using backpropagation neural networks, as the machine learning algorithm, that learn to assign mesh categories to a subset of medline records.
a direct computation of this feature vector would involve  a prohibitive amount of computation even for modest values of k, since the  dimension of the feature space grows exponentially with k.
our analysis  and empirical evaluation show substantial improvement in the accuracy of  catalog integration.}, } @inproceedings{aizawa00, author = {akiko aizawa},  title = {
{2001}, pages = {137--145}, url =  {http://www.cs.cmu.edu/~yiming/papers.yy/sigir01.ps.gz}, abstract =  {thresholding strategies in automated text categorization are an underexplored  area of research.
the algorithms analyze inter- and intra- document dynamics by considering how information evolves over time from article to article, as well as within individual articles.
{http://www.acm.org/pubs/articles/proceedings/ir/312624/p281-ruiz/p281-ruiz.pdf},  abstract = {
"rome, italy" } @inproceedings{demelo:2007:mtc, author = "de melo, gerard and siersdorfer, stefan", title =
different from the conventional techniques, the  proposed mfom method attempts to integrate any performance metric of interest  (e.g. accuracy, recall, precision, or f1 measure) into the design of any  classifier.
{fabrizio sebastiani}, address = {
the recent explosion of online information in digital libraries and on the  world wide web has given rise to a number of query-based search engines and  manually constructed topical hierarchies.
{2004}, pages = {59--68}, url = {}, abstract = {}, }  @inproceedings{siersdorfer05, author = {stefan siersdorfer and gerhard weikum},  title = {
{markus stumptner and dan corbett and michael j. brooks}, publisher = {springer verlag, heidelberg, de}, address = {
it is built on top of a text-categorization  paradigm where text articles are annotated with keywords organized in a  hierarchical structure.
using an already coded training database of about 50,000 stories from the dow jones press release news wire, and seeker [stanfill] (a text retrieval system that supports relevance feedback) as the underlying match engine, codes are assigned to new, unseen stories with a recall of about 80\% and precision of about 70\%.
} @inproceedings{zhang:2005:igf, author = "baoping zhang and yuxin chen and weiguo fan and edward a. fox and marcos goncalves and marco cristo and pavel calado", title =
the learning algorithm combines an adaptive phase  which instantly updates dictionary and weights during interaction and a tuning  phase which fine tunes for performance using previously seen data.
we implemented pagetypesearch system based on our approach.
david j. hand and joost n. kok and michael r. berthold}, address = {amsterdam, nl}, year = {1999}, pages = {487--497}, url = {http://www.ai.univie.ac.at/~juffi/publications/ida-99.ps.gz}, abstract = {
we propose to use adaboost to optimally combine weak hypotheses based on both types of features.
this task is easy  to understand, but the lack of straightforward training set, subjective user  intents of queries, poor information in short queries, and high noise level  make the task very challenge.
two different methods are proposed for improving lsi representations for the topic spotting task.
on  the other hand, a discriminative measure is proposed for term selection and is  combined with the plu-based likelihood ratio to determine the text category.
dunja mladeni{\'{c}}}, title = {machine  learning on non-homogeneous, distributed text data}, school = {j.\ stefan  institute, university of ljubljana}, address
instead of using a randomly selected training set, the learner has access to a pool of unlabeled instances and can request the labels for some number of them.
our  experimental results on a large dataset confirm that the use of the implicit  links is better than using explicit links in classification performance, with  an increase of more than 10.5\% in terms of the macro-f1 measurement."
we present results comparing the performance of  boostexter and a number of other text-categorization algorithms on a variety of  tasks.
{581--586}, url =  {http://link.springer.de/link/service/series/0558/papers/1910/19100673.pdf},  abstract = {recently knowledge discovery and data mining in unstructured or  semi-structured texts (text mining) has attracted lots of attention from both  commercial and research fields.
the innovative aspects of this work are the feature selection process, the automated threshold determination for classification scores, and an experimental study on real-word web documents that can be associated to any node in the hierarchy.
several measures of categorization success are described and evaluated.
our results  with the english newswire collection show a very large gain in performance as  compared to published benchmarks, while our initial results with the german  newswires appear very promising.
dunja mladeni{\'{c}} and marko  grobelnik}, title = {
the  experimental results obtained are encouraging and support the choice of a  hybrid case-based approach to text categorization.}, }  @inproceedings{geutner93, author = {petra geutner and uli bodenhausen and alex  waibel}, title = {flexibility through incremental learning: neural networks for  text categorization},
in the second stage, the enriched queries are classified through the base classifiers trained in phase i. based on the classification results obtained by the base classifiers, two ensemble classifiers based on two different strategies are proposed.
other linguistic  resources that are emerging, like lexical databases, can also be used for  classification tasks.
instead of estimating weights for individual words, as naive bayes does, words with similar features are grouped into bins, and a single weight is estimated for each bin.
our experimental results  demonstrate that the technique of refining the training set reduces from  one-third to two-thirds of the storage.
{leah s. larkey and w. bruce croft}, title = {combining classifiers in text categorization}, booktitle = {proceedings of sigir-96, 19th acm international conference on research and development in information retrieval}, editor = {hans-peter frei and donna harman and peter sch{\"{a}}uble and ross wilkinson}, publisher = {acm press, new york, us}, year = {1996}, address = {z{\"{u}}rich, ch}, pages = {289--297}, url = {http://cobar.cs.umass.edu/pubfiles/1combo.ps.gz}, abstract = {three different types of classifiers were investigated in the context of a text categorization problem in the medical domain: the automatic assignment of icd9 codes to dictated inpatient discharge summaries.
the system is used as part of a commercial news clipping and retrieval product.
based on two chinese corpora, a series of controlled experiments evaluated their learning capabilities and efficiency in mining text classification knowledge.
with the latter, we provide a theoretical sound justification for the various  parameters of the k-nearest neighbour classifier.
to this end, we propose several clustering algorithms, and report results of various evaluations on standard benchmark corpora such as the newsgroups corpus.}, } @inproceedings{wang00, author = {
using  n-gram frequency profiles provides a simple and reliable way to categorize  documents in a wide range of classification tasks.}, } @inproceedings{ceci03,  author = {michelangelo ceci and donato malerba}, title = {hierarchical  classification of html documents with {webclassii}}, booktitle = {proceedings  of ecir-03, 25th european conference on information retrieval}, publisher =
the  comparison is performed over three known datasets.
the comparison is performed over three known datasets.
by providing a formal analysis of the computational complexity of each classification method, followed by an investigation on the usage of different classifiers in a hierarchical setting of categorization, we show how the scalability of a method depends on the topology of the hierarchy and the category distributions.
this paper describes how we have applied an existing  similarity-based learning algorithm, charade, to the text categorization  problem and compares the results with those obtained using decision tree  construction algorithms.
"proposing a new term weighting scheme for text  categorization", booktitle1 =
therefore, an accurate classifier is an essential component of a hypertext database.
we introduce three basic types (namely a type for text, one for words and one for positions in texts) and three simple predicate definitions over these types which enable us to write tc and ie rules as logic programs.
a document includes informative keywords  and non-informative keywords.
this paper  explores the use of association rule mining in building a text categorization  system and proposes a new fast algorithm for building a text classifier.
published in the ``lecture notes in computer science'' series, number 2035}, pages = {53--65}, url = {http://link.springer.de/link/service/series/0558/papers/2035/20350053.pdf}, abstract =
text classification  from labeled and unlabeled documents using em}, journal = {machine learning},  year = {2000}, number = {2/3}, volume = {39}, pages = {103--134}, url =  {http://www.cs.cmu.edu/~knigam/papers/emcat-mlj99.ps}, abstract = {this paper  shows that the accuracy of learned text classifiers can be improved by  augmenting a small number of labeled training documents with a large pool of  unlabeled documents.
"isbn 0-387-23535-3",  publisher = "springer", year = 2005, pages =
works in text retrieval through internet suggest that embedding linguistic information at a suitable level within traditional quantitative approaches (e.g. sense distinctions for query expansion as in [14]) is the crucial issue able to bring the experimental stage to operational results.
we find that simple averaging strategies do indeed improve performance, but that direct averaging of probability estimates is not the correct approach.
in order to explicitly capture the optimality of word clusters in an information theoretic framework, we first derive a global criterion for feature clustering.
published in the ``lecture notes in computer science'' series, number 2291}, pages = {248--267}, url = {http://link.springer.de/link/service/series/0558/papers/2291/22910248.pdf}, abstract = {
in concrete, we have evaluated a range of machine  learning methods for the task (c4.5, naive bayes, part, support vector machines  and rocchio), made cost sensitive through several methods (threshold  optimization, instance weighting, and meta-cost).
in contrast with traditional flat  and hierarchical category models, the multi-dimensional category model  classifies each text document in a collection using multiple predefined sets of  categories, where each set corresponds to a dimension.
we demonstrate that, although this categorization problem is quite different from 'topical' text classification, certain categories of messages can nonetheless be detected with high precision (above 80%) and reasonable recall (above 50%) using existing text-classification learning methods.
we present  efficient approximate inference techniques based on variational methods and an  em algorithm for empirical bayes parameter estimation.
the wpcm uses a neural network with inputs  obtained by both the principal components and class profile-based features.
empirical results indicate that our approach outperforms the best published results on this reuters collection.
information gain is a  well-known and empirically proven method for high-dimensional feature  selection.
in addition, centralized approaches are more vulnerable to  attacks or system failures and less robust in dealing with them.
very accurate text classifiers can be learned automatically from training examples.
these recurrent connections in multiple layers encode the sequential context of word sequences.
% % % updated and  maintained by % % % % evgeniy gabrilovich % % department of computer science %
{pattern recognition letters}, pages = {1225--1231}, year = {1997}, volume = {18}, number = {11/13}, url = {}, abstract = {
"edinburgh, scotand", url = "http://www.cs.technion.ac.il/~gabr/papers/fg-tc-ijcai05.pdf",
in this paper, a systematic study aimed to understand the role of rocchio formula in selection and weighting of linguistic features will be described.}, } @inproceedings{basili01b, author = {roberto basili and alessandro moschitti}, title = {
the results show that the use of  the hierarchical structure improves text categorization performance with  respect to an equivalent flat model.
published in the ``lecture notes in computer science'' series, number 2291}, pages = {213--228}, url = {http://link.springer.de/link/service/series/0558/papers/2291/22910213.pdf}, abstract = {typical text classifiers learn from example and training documents that have been manually categorized.
we analyze the relative utility of document text, and the text in  citing documents near the citation, for classification and description.
in the recent years, unsolicited bulk email has became an increasingly important problem, with a big economic impact.
"nirmalya chowdhury and diganta saha", title =
we  conducted several experiments on a news corpus, called msdn.
{springer science}, year = {2004}, volume = {7}, number = {3-4}, pages = {347--368}, abstract = {topic detection and tracking (tdt) is a research initiative that aims at techniques to organize news documents in terms of news events.
published in the ``lecture notes in computer science'' series, number 2997}, pages = {112--126}, url = {http://springerlink.metapress.com/openurl.asp?genre=article&issn=0302-9743&volume=2997&spage=112}, abstract =
"2007", month =  "june", address =
we also introduce  decision functions for the centroid-based classification algorithm and support  vector classifiers to handle the classification problem where a document may  belong to multiple classes.
a novel model for tc, extending a well know statistical model (i.e. rocchio's formula [ittner et al., 1995]) and applied to linguistic features has been defined and experimented.
aixin  sun and ee-peng lim}, title = {web unit mining: finding and classifying  subgraphs of web pages},
"proceedings of the  eleventh acm sigkdd international conference on knowledge discovery in data  mining", year =
martha e.  pollack}, publisher =
we have conducted an extensive experimental evaluation of our technique over collections of real documents, including over one hundred web-accessible databases.
experimental  results show that winnow with thesauri attains high accuracy and that the  proposed filtering and disambiguation methods also contribute to the improved  accuracy.}, } @inproceedings{yang00, author = {yiming yang and thomas ault and  thomas pierce and charles w. lattimer}, title = {
"bremen, germany", publisher = "acm press", url = "http://doi.acm.org/10.1145/1099554.1099591", abstract =
{elsevier science publishers, amsterdam, nl}, editor = {andr{\'e} lichnerowicz}, address = {barcelona, es}, year = {1991}, pages = {606--623}, url = {http://www.darmstadt.gmd.de/~tzeras/fullpapers/gz/fuhr-etal-91.ps.gz}, abstract = {air/x is a rule-based system for indexing with terms (descriptors) from a prescribed vocabulary.
{1992}, pages = {48--63}, url =  {http://www.acm.org/pubs/articles/journals/cacm/1992-35-8/p48-creecy/p48-creecy.pdf},  } @inproceedings{cristianini01, author = {nello cristianini and john  shawe-taylor and huma lodhi}, title = {latent semantic kernels}, booktitle =
maintaining catalogues manually is  becoming increasingly difficult, due to the sheer amount of material on the  web; it is thus becoming necessary to resort to techniques for the automatic  classification of documents.
all of these methods showed significant improvement (up to 71\% reduction in weighted error rates) over the performance of the original knn algorithm on tdt benchmark collections, making knn among the top-performing systems in the recent tdt3 official evaluation.
nicosia, cy}, year = {2004}, url = {http://doi.acm.org/10.1145/967900.968026}, abstract = {
{2003}, url =  {http://www.ai.mit.edu/projects/jmlr/papers/volume3/blei03a/blei03a.pdf},  abstract = {
the system has been tested in two applications in particular, one  concerning passive invoices and the other bank documents.
however, separate databases of short system call sequences  have to be built for different programs, and learning program profiles involves  time-consuming training and testing processes.
c-evolve first finds highly accurate cluster digests (partial clusters), gets user feedback to merge and correct these digests, and then uses the classification algorithm to complete the partitioning of the data.
the mit press}, address = {cambridge, us}, year = {1994}, chapter = {21}, editor = {kenneth e. kinnear}, pages = {459--476}, url = {}, abstract = {}, } @inproceedings{matsuda98, author = {katsushi matsuda and toshikazu fukushima}, title = {task-oriented {world wide web} retrieval by document type classification}, booktitle = {proceedings of cikm-98, 7th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, editor =
we present a unified view of text categorization systems, focusing on the selection of features.
our experiments with the acm computing classification scheme, using documents from the acm digital library, indicate that gp can discover similarity functions superior to those based solely on a single type of evidence.
as www grows at an increasing speed, a classifier targeted at  hypertext has become in high demand.
the scheme for automatic text classification  proposed in the paper, is based on document indexing, where a document is  represented as a list of keywords.
we present experimental evidence that confirms this hypothesis on a set of web pages that relate to computer science departments.} } @inproceedings{gabrilovich:2004:newsjunkie, author = "gabrilovich, evgeniy and dumais, susan and horvitz, eric", title = "newsjunkie: {p}roviding personalized newsfeeds via analysis of information novelty", booktitle =
we  experimentally evaluate the quality of the lower dimensional spaces both in the  context of document categorization and improvements in retrieval performance on  a variety of different document collections.
this new synthesis of neural networks, learning and information retrieval techniques allows us to scale up to a real-world task and demonstrates a lot of potential for hybrid plausibility networks for semantic text routing agents on the internet.}, } @inproceedings{wermter99a, author = {stefan wermter and garen arevian and christo panchev}, title = {
instead of concentrating on a small set of ambiguous words, as done in most of the related previous work, all content words of the examined corpus are disambiguated.
publisher = {acm press, new york, us}, editor = {}, year = {2003}, address = {new orleans, us}, pages = {295--302}, url = {http://doi.acm.org/10.1145/956863.956920}, abstract = {
= {springer verlag, heidelberg, de}, address = {dublin, ie}, pages = {13--22}, year = {1994}, url = {http://www.acm.org/pubs/articles/proceedings/ir/188490/p13-yang/p13-yang.pdf}, abstract = {expert network (expnet) is our approach to automatic categorization and retrieval of natural language texts.
{american society for  information science, washington, us}, editor = {raymond f. vondran and anne  caputo and carol wasserman and richard a. diener}, year = {1983}, address =
new event detection is a challenging task that still offers scope for great improvement after years of effort.
an extended version appears as~\cite{cohen99}}, url  = {http://www.research.whizbang.com/~wcohen/postscript/sigir-96.ps}, abstract =  {two machine learning algorithms, ripper and sleeping experts for phrases, are  evaluated on a number of large text categorization problems.
joining statistics  with nlp for text categorization}, booktitle = {proceedings of anlp-92, 3rd  conference on applied natural language processing}, publisher = {association  for computational linguistics, morristown, us}, editor = {marcia bates and  oliviero stock}, year = {1992}, address = {trento, it}, pages = {178--185}, url  = {}, abstract = {automatic news categorization systems have produced high  accuracy, consistency, and flexibility using some natural language processing  techniques.
andrzej skowron and zbigniew w. ra{\'{s}}}, pages = {235--243}, year = {1999}, address = {warsaw, pl}, publisher = {springer verlag, heidelberg, de}, note = {
there are two  frequently used approaches to the development of intelligent agents using  machine learning techniques: a content-based and a collaborative approach.
rcut is most natural for online response but is too coarse-grained for global or local optimization.
{leah s. larkey}, title = {
one of its main drawbacks, in ir, is its computational cost.
however, the benefits  that this has brought about have somehow been limited by the fact that  different researchers have ``carved'' different subsets out of this collection,  and tested their systems on one of these subsets only; systems that have been  tested on different \textsf{reuters-21578} subsets are thus not
the study showed that the category interface is superior both in objective and subjective measures.
{proceedings of cikm-99, 8th acm international conference on information and  knowledge management}, publisher = {acm press, new york, us}, editor = {}, year  = {1999}, address = {kansas city, us}, pages = {122--130}, url =  {http://www.acm.org/pubs/articles/proceedings/cikm/319950/p122-li/p122-li.pdf},  abstract = {
we focus on the robustness of these methods in dealing with a skewed category distribution, and their performance as function of the training-set category frequency.
a chinese documents classification system following above described techniques is implemented with naive bayes, knn and hierarchical classification methods.
filtering obscene content on the web}, booktitle = {proceedings of ecdl-00, 4th european conference on research and advanced technology for digital libraries}, editor = {jos{\'e} l. borbinha and thomas baker}, publisher = {springer verlag, heidelberg, de}, note = {published in the ``lecture notes in computer science'' series, number 1923}, year = {2000}, address = {lisbon, pt}, pages = {403--406}, url = {http://link.springer.de/link/service/series/0558/papers/1923/19230403.pdf}, abstract =
the empirical evaluation indicates that the error rate (as obtained by running the naive bayes classifier on isolated pages) can be significantly reduced if contextual information is incorporated.}, } @inproceedings{frommholz01, author = {ingo frommholz}, title = {
in this paper, instead of overhauling  the classifier itself, we propose mechanisms to detect misclassification and  take appropriate actions.
in this paper we present a fast  supervised dimensionality reduction algorithm that is derived from the recently  developed cluster-based unsupervised dimensionality reduction algorithms.
tc has become  very important in the information retrieval area, where information needs have  tremendously increased with the rapid growth of textual information sources  such as the internet.
the results we have obtained significantly outperform the results achieved by previous automated survey coding approaches.}, } @article{giorgetti03a, author = {daniela giorgetti and fabrizio sebastiani}, title = {
"kules, bill and  kustanowitz, jack and scneiderman, ben", title =
we extract proper names, locations, temporal expressions and normal terms into distinct sub-vectors of the document representation.
we have used the receiver operating characteristic convex hull method for the evaluation, that best suits classification problems in which target conditions are not known, as it is the case.
we investigate a meta-model approach, called meta-learning using  document feature characteristics (mudof), for the task of automatic textual  document categorization.
the main improvements of the algorithms employed by the system concern the computation of the distance between weighted trigram vectors and further analysis of the two-pool evolutionary algorithm.
the experimental results show  that our new approaches give better results for both micro-averaged f1 and  macro-averaged f1 scores.}, } @article{lehnert94, author = {
in this framework, the concept of multidimensional category model is introduced for representing classes.
{seattle, us}, pages = {256--263}, url = {http://www.cs.cmu.edu/~yiming/papers.yy/sigir95.ps}, abstract = {
we propose a new  statistical model for the classification of structured documents and consider  its use for multimedia document classification.
"seattle,  washington", abstract =
although our automated learning approach still gives a lower accuracy, by  appropriately incorporating a set of manually chosen words to use as features,  the combined, semi-automated approach yields accuracy close to the rule-based  approach.}, } @article{nieto02, author = {salvador nieto s{\'{a}}nchez and  evangelos triantaphyllou and donald kraft}, title = {
we compare  pagetypesearch using the document type-indices with a conventional  keyword-based search system in experiments.
"274--281", booktitle = "proceedings of the 28th annual international {acm} {sigir} conference on research and development in information retrieval", year = "2005", month =
we have implemented our gis algorithm, the expnet algorithm, and some linear classifiers.
we use two different strategies, latent semantic indexing and optimal term selection, to reduce the number of features.
{575--608}, url = {}, abstract = {}, } @article{maron61, author = {m.e. maron}, title = {automatic indexing: an experimental inquiry}, year = {1961}, journal = {journal of the association for computing machinery}, volume = {8}, number = {3}, pages = {404--417}, url = {http://www.acm.org/pubs/articles/journals/jacm/1961-8-3/p404-maron/p404-maron.pdf}, abstract =
it is found that decision forest outperforms both c4.5 and knn in all cases, and that category dependent term selection yields better accuracies.
text categorization}, editor = {alessandro zanasi}, year = {2005}, booktitle = {
the authors found that topic identification performance was maintained or slightly improved using character shape codes derived from images.}, } @article{stamatatos00, author = {efstathios stamatatos and nikos fakotakis and george kokkinakis}, title = {automatic text categorization in terms of genre and author}, journal = {computational linguistics}, pages = {471--495}, year = {2000}, number = {4}, volume = {26}, url = {}, abstract = {
{proceeding of ijcai-01, 17th international joint conference on artificial intelligence}, editor = {bernhard nebel}, address = {
therefore,  it is very costly to assign a category for them because humans investigate  their contents.
turning  {{\sc yahoo!}}\ into an automatic web page classifier}, booktitle =
the best f1 value of our two solutions is 9.6% higher than the best of all other participants solutions.
a subsequence is any ordered sequence of k characters occurring in the text though not necessarily contiguously.
some of these measures outperformed traditional measures from information retrieval and information theory in certain situations."
"2007", pages = "889--894", address =
year = {2005}, volume = {60}, number = {1-3}, pages =
{feature selection on hierarchy of web documents}, journal = {decision support systems}, year = {2003}, number = {1}, volume = {35}, pages = {45--87}, url = {}, abstract = {
traditionally these tasks are carried out individually in two distinct phases: the first is the global feature selection during a corpus pre-processing and the second is the application of the feature weighting model.
"dou shen  and jian-tao sun and qiang yang and zheng chen", title =
georges gardarin and james c. french and niki pissinou and kia makki and luc bouganim},
mh boosting algorithm is applied to the  word sense disambiguation (wsd) problem.
our experimental results indicate that the naive bayes classifier and the subspace method outperform the other two classifiers on our data sets.
text categorization for multi-page documents: a hybrid naive {bayes hmm} approach}, booktitle = {proceedings of jcdl, 1st acm-ieee joint conference on digital libraries}, editor = {}, publisher = {ieee computer society press, los alamitos, us}, year = {2001}, address = {roanoke, us}, pages = {11--20}, url = {http://www.dsi.unifi.it/~paolo/ps/jcdl01-hmm-text.pdf}, abstract = {
the results showed that our method performs well for reuters text collection when enough training documents are given and the new measures have indeed considered the contributions of misclassified documents.}, } @article{sun03, author = {aixin sun and ee-peng lim and wee-keong ng}, title = {performance measurement framework for hierarchical text classification}, journal = {journal of the american society for information science and technology}, year = {2003}, volume = {54}, number = {11}, pages = {1014--1028}, url = {http://www.cais.ntu.edu.sg/~sunaixin/paper/sun_jasist03.pdf}, abstract = {hierarchical text classification or simply hierarchical classification refers to assigning a document to one or more suitable categories from a hierarchical category space.
we study the problem of classifying data in a given taxonomy when  classifications associated with multiple and/or partial paths are allowed.
{joins that generalize:
whereas most statistical approaches to text categorization derive classification knowledge based on training examples alone, aram performs supervised learning and integrates user-defined classification knowledge in the form of if-then rules.
in this paper we propose a hierarchical clustering algorithm that constructs a set of clusters having the maximum bayesian posterior probability, the probability that the given texts are classified into clusters.
{70--77}, url = {http://www.cs.huji.ac.il/~singer/papers/rankboost.ps.gz}, abstract = {rankboost is a recently proposed algorithm for learning ranking functions.
use of this data for research on text categorization requires a detailed understanding of the real world constraints under which the data was produced.
alexei vinokourov and mark girolami}, title = {a probabilistic framework for the hierarchic organisation and classification of document collections}, journal =
experiments also were conducted to compare the performance between gp techniques and other fusion techniques such as genetic algorithms (ga) and linear fusion.
we applied the technique to several standard text  collections and found that they contained a significant number of duplicate and  plagiarised documents.
we report experiments that compare its performance with that of a well-known probabilistic classifier.
this paper explores multi-label conditional random field (crf)classification models that directly parameterize label co-occurrences in multi-label classification.
there are two aspects to the key difficulties of this problem: one is that the meaning of the queries and the semantics of the predefined categories are hard to determine.
using  corpus statistics to remove redundant words in text categorization}, journal =
there are about 350 different  codes to be assigned.
{thorsten joachims}, title = {a statistical learning model of text  classification with support vector machines}, booktitle = {proceedings of  sigir-01, 24th acm international conference on research and development in  information retrieval}, editor = {w. bruce croft and david j. harper and donald  h. kraft and justin zobel}, publisher = {acm press, new york, us}, address =
after training, the incoming news  articles are classified based on their similarity to the existing newsgroup  categories.
{145--148}, }  @article{mladenic99, author = {
{proceedings of pkdd-00, 4th  european conference on principles of data mining and knowledge discovery},  editor = {
these levels  control the ability of the categories to attract documents during the  categorization process.
in this study, we adapt a simple text classifier (rocchio), using weakly supervised clustering techniques.
it has been successfully applied in analyzing patent portfolios, customer complaint letters, and even competitors' web pages.
we investigate four different methods for document  classification: the naive bayes classifier, the nearest neighbour classifier,  decision trees and a subspace method.
the experimental results show that the proposed method outperforms a direct application of a statistical learner often used for subject classification.
{proceedings of ecml-98, 10th european conference on machine learning},  publisher = {springer verlag, heidelberg, de}, note = {
www indices, like {{\sc yahoo!}}\ provide a huge  hierarchy of categories (topics) that touch every aspect of human endeavors.
a method for using the knowledge about the hierarchy to gain better  categorization results is discussed.
furthermore, the supervised lower dimensional space greatly improves the retrieval performance when compared to lsi.}, } @inproceedings{kawatani02, author = {
"linear prediction models with  graph regularization for web-page categorization", booktitle =  "proceedings of the twelfth acm sigkdd international conference on  knowledge discovery and data mining", year =
in  this paper we propose a hierarchical clustering algorithm that constructs a set  of clusters having the maximum bayesian posterior probability, the probability  that the given texts are classified into clusters.
each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities.
for the hierarchical approach, we found the same accuracy using a sequential boolean decision rule and a multiplicative decision rule.
in  addition, unlike other feature selection models --- which typically require  different feature selection parameters for categories at different hierarchical  levels --- our technique works equally well for all categories in a  hierarchical structure.
finally, the system computes a distance measure between the document's profile and each of the category profiles.
{acm press, new york, us}, editor = {henrique paques and ling liu and david  grossman}, year = {2001}, address = {atlanta, us}, pages = {105--113}, url =  {http://www.stanford.edu/~krist/papers/cikm2001.pdf}, abstract = {documents are  commonly categorized into hierarchies of topics, such as the ones maintained by  yahoo!
morgan kaufmann publishers, san francisco, us}, url =  {http://www.cs.cmu.edu/~yiming/papers.yy/ml97.ps}, abstract = {
in addition, we are able to obtain tight bounds for the complexities by using the power law to approximate category distributions over a hierarchy.
many large organizations create and maintain huge volumes of textual information online, and there is a pressing need for support of efficient and effective information retrieval, filtering, and management.
we introduce a model for learning patterns for text  categorization (the lpt-model) that does not rely on an attribute-value  representation of documents but represents documents essentially "as they  are".
"157--158", abstract = "automatic text classification is an important operational problem in digital library practice.
{david cheung and qing li and graham williams}, year = {2001}, publisher =
since the sequential approach is much more  efficient, requiring only 14\%-16\% of the comparisons used in the other  approaches, we find it to be a good choice for classifying text into large  hierarchical structures.} } @inproceedings{dumais98, author = {
we introduce an new algorithm for performing  active learning with support vector machines, i.e., an algorithm for choosing  which instances to request next.
on the  one hand, news articles are always aroused by events; on the other hand,  similar articles reporting the same event often redundantly appear on many news  sources.
upon close examination of the algorithm, we concluded that the algorithm is most successful in correctly classifying more positive documents, but may cause more negative documents to be classified incorrectly.}, } @inproceedings{taskar01, author = {benjamin taskar and eran segal and daphne koller}, title = {probabilistic classification and clustering in relational data}, booktitle =
our implementation uses an inverted file to store the trained term  structures of each newsgroup, and uses a list similar to the inverted file to  buffer the newly arrival articles, for efficient routing and updating purposes.
"2005", pages = "294--303", abstract = "a web object is defined to represent any meaningful object embedded in web pages (e.g. images, music) or pointed to by hyperlinks (e.g. downloadable files).
"wenyuan dai and gui-rong xue and qiang yang and yong yu", title =
further,  the weak performance of naive bayes can be partly explained by extreme skewness  of posterior probabilities generated by it.
our experiments on a variety of categorization tasks indicate that there is  significant potential in improving classifier performance by feature  re-weighting, beyond that achieved via membership queries alone (traditional  active learning) if we have access to an oracle that can point to the important  (most predictive) features.
this information must be  filtered in order to make an effective use of it in our model of tc.
the analysis  was conducted in two phases.
we report results in  document modeling, text classification, and collaborative filtering, comparing  to a mixture of unigrams model and the probabilistic lsi model.}, }  @article{bloedorn98, author = {eric bloedorn and ryszard s. michalski}, title =  {data-driven constructive induction}, journal = {ieee intelligent systems},  year = {1998}, number = {2}, volume = {13}, pages = {30--37}, url = {},  abstract = {an inductive learning program's ability to find an accurate  hypothesis can depend on the quality of the representation space.
one technique to implement such systems is rule induction.
our approach is to develop a trainable information extraction system that takes two inputs: an ontology defining the classes and relations of interest, and a set of training data consisting of labeled regions of hypertext representing instances of these classes and relations.
however, like most machine learning algorithms, they are generally applied using a randomly selected training set classified in advance.
{morgan kaufmann publishers, san  francisco, us}, editor = {
our results show that ridge regression seems  to be the most promising candidate for rare class problems.}, }  @inproceedings{zhdanova02, author = {anna v. zhdanova and denis v. shishkin},  title = {classification of email queries by topic:
we modify the query-by-committee (qbc) method of active learning to use the unlabeled pool for explicitly estimating document density when selecting examples for labeling.
we show that link information can be useful when the document collection has a sufficiently high link density and links are of sufficiently high quality.
we experiment with various types of descriptions for the  {{\sc yahoo!}}\ categories and the webpages to be categorized.
nashville, us}, pages =  {412--420}, publisher =
= {an application of {least squares fit} mapping to text  information retrieval}, booktitle = {proceedings of sigir-93, 16th acm  international conference on research and development in information retrieval},  editor = {
{text classification using string kernels}, journal = {journal of machine learning research}, volume = {2}, pages = {419--444}, year = {2002}, url = {http://www.ai.mit.edu/projects/jmlr/papers/volume2/lodhi02a/lodhi02a.pdf}, abstract =
published in the ``lecture notes in computer science'' series, number 2769}, year = {2003}, address = {trondheim, no}, pages = {126--139}, url = {}, abstract = {}, } @article{benkhalifa01, author = {mohammed benkhalifa and abdelhak mouradi and houssaine bouyakhf}, title = {integrating external knowledge to supplement training data in semi-supervised learning for text categorization}, journal = {information retrieval}, number = {2}, volume = {4}, pages = {91--113}, year = {2001}, url = {http://www.wkap.nl/article.pdf?351286}, abstract = {
hbc can reconstruct the original clusters more accurately than other non-probabilistic algorithms.
our experimental results using real netnews articles and newsgroups demonstrate  (1) applying feature reduction to the training set improves the routing  accuracy, efficiency, and database storage; (2) updating improves the routing  accuracy; and (3) the batch technique improves the efficiency of the updating  operation.}, } @inproceedings{huffman94, author = {stephen huffman and marc  damashek}, title = {acquaintance: a novel vector-space n-gram technique for  document categorization}, booktitle = {proceedings of trec-3, 3rd text  retrieval conference}, publisher =
specifically, we apply this to a maximum entropy classifier on a large scale  multi-class text categorization task: the online job directory flipdog with  over half a million jobs in 65 categories.}, note = {
{marti a. hearst and haym hirsh}, title = {machine learning in information access.
experiments using a number of e-mail documents generated by different authors  on a set of topics gave promising results for both aggregated and multi-topic  author categorisation.}, } @inproceedings{dhillon02, author = {inderjit dhillon  and subramanyam mallela and rahul kumar}, title =
"lan, man and tan, chew-lim and low, hwee-boon", title =
experiments also were  conducted to compare the performance between gp techniques and other fusion  techniques such as genetic algorithms (ga) and linear fusion.
it is called gaussian weighting and  it is a supervised learning algorithm that, during the training phase,  estimates two very simple and easily computable statistics which are: the  presence \emphp, how much a term \emph{t} is present in a category \emph{c};  the expressiveness \emphe, how much \emph{t} is present outside \emph{c} in the  rest of the domain.
{}, address = {taipei, tw}, url =  {http://www.his.sunderland.ac.uk/ps/coling-232.pdf}, abstract = {
{international journal on document analysis and recognition}, number = {4},  volume = {3}, pages = {232--247}, year = {2001}, url =  {http://link.springer.de/link/service/journals/10032/papers/1003004/10030232.pdf},  abstract = {
proceedings of the 1st
we describe latent dirichlet allocation (lda), a generative  probabilistic model for collections of discrete data such as text corpora.
however, once a misclassification  occurs at a high level class, it may result in a class that is far apart from  the correct one.
we  address this open challenge by using a combination of classifiers with  different performance characteristics to effectively reduce the performance  variance on average of the overall system across all classes, including those  not seen before.
we then introduce an algorithm for learning from labeled and unlabeled text based on the combination of expectation-maximization with a naive bayes classifier.
"edinburgh, scotand", url =  "http://www.cs.technion.ac.il/~gabr/papers/fg-tc-ijcai05.pdf",  abstract =
our approach is especially suitable for applications of on-line text classification.}, } @inproceedings{zhou02a, author = {shuigeng zhou and jihong guan}, title = {chinese documents classification based on n-grams},
this paper describes the proposed technique, discusses the integration of a keyword acquisition algorithm, latent semantic indexing (lsi) with rough set-based rule generate algorithm, and provides experimental results.
text categorization for multi-page documents: a hybrid naive {bayes hmm} approach}, journal = {journal of intelligent information systems}, year = {2002}, note = {special issue on automated text categorization}, volume = {18}, number = {2/3}, pages =
a standard choice  of kernel function has been the inner product between the vector-space  representation of two documents, in analogy with classical information  retrieval (ir) approaches.
furthermore, the performance of the refined centroid classifier implemented is comparable, if not better, to that of state-of-the-art support vector machine (svm)-based classifier, but offers a much lower computational cost."
we show that by using large feature vectors in combination with feature reduction, we can train linear support vector machines that achieve high classification accuracy on data that present classification challenges even for a human annotator.
categorization by context}, journal = {journal of universal computer science}, year = {1998}, number = {9}, volume = {4}, pages = {719--736}, url = {http://www.jucs.org/jucs_4_9/categorisation_by_context}, abstract = {assistance in retrieving of documents on the world wide web is provided either by search engines, through keyword based queries, or by catalogues, which organise documents into hierarchical collections.
} @article{shen:2005:q2c, author =
we provide a theoretical motivation for the  algorithm using the notion of a version space.
the wpcm uses a neural network with inputs obtained by both the principal components and class profile-based features.
in this paper schapire and singer's adaboost.
the paper describes the technique of categorisation by context, which  exploits the context perceivable from the structure of html documents to  extract useful information for classifying the documents they refer to.
malcolm p. atkinson and maria e. orlowska and patrick valduriez and stanley b. zdonik and michael l. brodie}, year = {1999}, address = {edinburgh, uk}, pages = {363--374}, url = {http://www.comp.nus.edu.sg/~wangk/pub/vldb99.ps}, abstract = {
"edinburgh, scotand", url = "http://www.ijcai.org/papers/1401.pdf", abstract =
bootstrapping for hierarchical document classification},  booktitle = {proceedings of cikm-03, 12th acm international conference on  information and knowledge management}, publisher = {acm press, new york, us},  editor = {}, year = {2003}, address = {new orleans, us}, pages = {295--302},  url = {http://doi.acm.org/10.1145/956863.956920}, abstract = {
further, we show that selecting a best  classification method using text-only features and then adding numerical  features to the problem (as might happen if numerical features are only later  added to a pre existing text-classification problem) gives performance that  rivals a more time-consuming approach of evaluating all classification methods  using the full set of both text and numerical features.}, }  @article{maderlechner97, author = {maderlechner, g. and suda, p. and bruckner,  t.}, title = {classification of documents by form and content}, journal =
volume = {36}, number = {3}, pages = {415--444}, url = {}, abstract = {document  classifiers can play an intermediate role in multilevel filtering systems.
georgios  paliouras and vangelis karkaletsis and constantine d. spyropoulos}, title =
and then, it uses the  categorized sentences for training.
this can be seen as a complementary tool for topic detection and  tracking applications.
{158}, pages = {69--88}, url = {http://dx.doi.org/10.1016/j.ins.2003.03.003}, abstract = {automatic categorization is the only viable method to deal with the scaling problem of the world wide web (www).
this is a novel approach for document  classification, where each agent evolves a parse-tree representation of a  user's particular information need.
text categorization using the semi-supervised  fuzzy c-means algorithm}, booktitle = {proceedings of nafips-99, 18th  international conference of the north american fuzzy information processing  society}, address = {new york, us}, pages = {561--565}, year = {1999}, url =  {}, abstract = {
both  textual and non-textual information associated with the projects are used in  the learning and classification phases.
in order to make boosting practical for a real learning domain  of thousands of words, several ways of accelerating the algorithm by reducing  the feature space are studied.
using the same representation of categories,  experiments show a significant improvement when the above mentioned method is  used.
there is a  wide variety of tasks for which keyphrases are useful, as we discuss in this  paper.
the paper firstly defines a criterion to identify the high-degree biased chinese bigrams.
} @article{bianchi:2006:iah, author =
the authors show how this term-frequency approach supports a range  of kdd operations, providing a general framework for knowledge discovery and  exploration in collections of unstructured text.}, } @inproceedings{dagan97,  author = {ido dagan and yael karov and dan roth}, title = {mistake-driven  learning in text categorization}, booktitle = {proceedings of emnlp-97, 2nd  conference on empirical methods in natural language processing}, publisher =
"gongde guo and hui wang and david bell and yaxin bi and kieran greer", title =
we report experiments on text classification of the cora and webkb data sets using probabilistic latent semantic analysis and probabilistic hypertext induced topic selection.
the research described in this paper combines weighted trigram analysis, clustering, and a special two-pool evolutionary algorithm, to create an adaptive information filtering system with such useful properties as domain independence, spelling error insensitivity, adaptability, and optimal use of user feedback while minimizing the amount of user feedback required to function properly.
{acm press, new york, us}, address = {tampere, fi}, year = {2002}, pages =  {207--214}, url = {http://doi.acm.org/10.1145/564376.564413}, abstract = {
furthermore, these experiments show that feature labeling takes much less (about 1/5th) time than document labeling.
this paper introduces a new criterium for term selection, which is based on the notion of uncertainty.
however, the algorithm is much more  efficient (because the learner does not have to be invoiced at all) and thus  solves model selection problems with as many as a thousand relevant attributes  and 12000 examples.}, } @inproceedings{schneider03, author = {{karl-michael}  schneider}, year = {2003}, title = {a comparison of event models for naive  bayes anti-spam e-mail filtering}, pages = {}, address = {}, editor = {},  booktitle = {proceedings of eacl-03, 11th conference of the european chapter of  the association for computational linguistics}, url =  {http://www.phil.uni-passau.de/linguistik/mitarbeiter/schneider/pub/eacl2003.pdf},  abstract = {}, } @inproceedings{schutze95, author = {hinrich sch{\"{u}}tze  and david a. hull and jan o. pedersen}, title = {
{hershey, {us}}, pages = {683--687}, url =  {http://www.math.unipd.it/~fabseb60//publications/edta05.pdf}, }  @incollection{sebastiani06, author = {fabrizio sebastiani}, title =
we report on experiences with the reuters newswire  benchmark, the us patent database, and web document samples from {{\sc  yahoo!}}\.}, } @inproceedings{chakrabarti98b, author = {soumen chakrabarti and  byron e. dom and piotr indyk}, title = {enhanced hypertext categorization using  hyperlinks}, booktitle = {proceedings of sigmod-98, acm international  conference on management of data},
measuring differentiability: unmasking pseudonymous authors}, journal = {journal of machine learning research}, volume = {8}, pages = {1261--1276}, year = {2007}, month = {june} }
in addition, in this paper, we describe a new threshold relaxation algorithm.
we report here on experiments using a committee of winnow-based learners and demonstrate that this approach can reduce the number of labeled training examples required over that used by a single winnow learner by 1-2 orders of magnitude.}, } @inproceedings{liere98, author = {ray liere and prasad tadepalli}, title = {active learning with committees: preliminary results in comparing winnow and perceptron in text categorization}, booktitle = {proceedings of conald-98, 1st
this algorithm is the  ``semi-supervised fuzzy c-means'' (ssfcm).
previously i compared the effectiveness of uncertainty sampling with that of  random sampling and relevance sampling in choosing training data for a text  categorization data set (lewis and gale, 1994).
using {wordnet} to complement training information in text  categorization}, booktitle = {proceedings of ranlp-97, 2nd international  conference on recent advances in natural language processing}, publisher = {},  editor = {ruslan milkov and nicolas nicolov and nilokai nikolov}, address =
on the use of bernoulli mixture models for text classification}, journal = {pattern recognition}, year = {2002}, volume = {35}, number = {12}, pages = {2705--2710}, url = {}, abstract = {mixture modelling of class-conditional densities is a standard pattern recognition technique.
we  tackle two different problems of {\em text categorization} (tc), namely feature  selection and classifier induction.
issues of document indexing, classifier  construction, and classifier evaluation, will be touched upon.}, }  @article{selamat04, author = {ali selamat and sigeru omatu}, title = {web page  feature selection and classification using neural networks}, journal =
since there is no need  to learn individual program profiles separately, the calculation involved is  largely reduced.
three types of term distributions, called  inter-class, intra-class and in-collection distributions, are introduced.
this paper investigates automatic  hierarchical categorisation and, specifically, the role of features in the  development of more effective categorisers.
based on the hierarchical structure, we propose a way of dividing the problem into subproblems, each representing one of the categories included in the yahoo hierarchy.
we present a theoretical argument showing that, under common assumptions, unlabeled data contain information about the target function.
"using ``annotator rationales'' to improve  machine learning for text categorization", booktitle = "proceedings  of the annual conference of the north american chapter of the association for  computational linguistics", year = "2007", month =  "april", address =
latent semantic indexing (lsi) has been successfully  used for ir purposes, as a technique for capturing semantic relations between  terms and inserting them into the similarity measure between two documents.
we evaluate the performance of nine different configurations of c4.5.
in our approach, training data are represented as the projections of training documents on each feature.
combining the results of classifiers has shown much promise in  machine learning generally.
"large scale learning is often realistic  only in a semi-supervised setting where a small set of labeled examples is  available together with a large collection of unlabeled data.
a  divisive information-theoretic feature clustering algorithm for text  classification}, journal = {journal of machine learning research}, volume =
we discuss the role of importance-weights (e.g. document frequency and  redundancy), which is not yet fully understood in the light of model complexity  and calculation cost, and we show that time consuming lemmatization or stemming  can be avoided even when classifying a highly inflectional language like  german.}, } @article{lertnattee04, author = {verayuth lertnattee and thanaruk  theeramunkong}, title = {effect of term distributions on centroid-based text  categorization}, journal = {information sciences}, year = {2004}, number = {1},  volume = {158}, pages = {89--115}, url =  {http://dx.doi.org/10.1016/j.ins.2003.07.007}, abstract = {
this paper describes how we have applied an existing similarity-based learning algorithm, charade, to the text categorization problem and compares the results with those obtained using decision tree construction algorithms.
"a hierarchical text categorization approach and its application to {frt} expansion", journal =
our approach can be seen as an extension to kleinberg's hubs and authorities algorithm that analyzes hyperlink relations among web pages.
however, the exponential growth in the volume of on-line textual information makes it nearly impossible to maintain such taxonomic organization for large, fast-changing corpora by hand.
in this paper, we propose a notion of visual keywords for  similarity matching between visual contents.
naive bayes  classifiers are recognized to be among the best for classifying text.
classifying web documents in a hierarchy of categories: a comprehensive study}, journal = {journal of intelligent information systems}, volume = {28}, number = {1}, pages =
in previous work, we developed several algorithms that use information extraction techniques to achieve high-precision text categorization.
in this paper text classification (tc) has been taken  as the ir task and the effect of linguistic capabilities of the underlying  system have been studied.
"incorporating prior knowledge with weighted margin support vector  machines", booktitle = "kdd'04", year = "2004", pages  =
{chemnitz, de}, pages = {137--142}, year = {1998}, url =  {http://www-ai.cs.uni-dortmund.de/dokumente/joachims_98a.ps.gz}, abstract =  {the paper explores the use of support vector machines (svms) for learning text  classifiers from examples.
one aspect of text mining is automatic text categorization, which assigns a text document to some predefined category according to the correlation between the document and the category.
the case of contiguous  subsequences is also considered for comparison with the subsequences kernel  with different decay factors.
the categorization,  which consists in assigning an international code of disease (icd) to the  medical document under examination, is based on well-known information  retrieval techniques.
such a system will have to be adaptive to the user changing  interest.
{acm press, new york, us}, address = {toronto, ca}, year = {2003}, pages =  {26--32}, url = {http://doi.acm.org/10.1145/860435.860443}, abstract =  {question classification is very important for question answering.
this holds for ocr texts as well as for correct ascii texts.}, } @article{kaban02, author = {
in our opinion, more sophisticated natural language processing techniques need to be developed before better text representations can be produced for classification.}, } @inproceedings{sebastiani00, author = {fabrizio sebastiani and alessandro sperduti and nicola valdambrini}, title = {
improved  estimates of the term distributions are made by differentiation of words in the  hierarchy according to their level of generality/specificity.
the first concerns an information filtering system based on an adaptation of the generalized probabilistic model of information retrieval.
the hierarchy of categories is involved in all phases of automated document classification, namely feature extraction, learning, and classification of a new document.
zelikovitz, sarah and cohen, william w. and hirsh, haym}, title = {
yang xiang and brahim chaib-draa}, address = {halifax, ca}, year = {2003}, pages = {505--509}, url = {}, abstract = {}, } @inproceedings{spitz00, author = {larry spitz and arman maghbouleh}, title = {text categorization using character shape codes}, booktitle = {proceedings of the 7th spie conference on document recognition and retrieval}, publisher = {spie, the international society for optical engineering}, editor = {daniel p. lopresti and jiangying zhou}, year = {2000}, address = {san jose, us}, pages = {174--181}, url = {}, abstract = {
in this paper, we apply lsi to the routing task, which operates under the assumption that a sample of relevant and non-relevant documents is available to use in constructing the query.
our approach to text classification uses  case-based reasoning to represent natural language contexts that can be used to  classify texts with extremely high precision.
phrase-descriptor relations has  been developed.
in this tutorial we look at the main  approaches that have been taken towards automatic text categorisation within  the general machine learning paradigm.
"proceedings of the eighth international conference on intelligent text processing and computational linguistics", year =
we showed also that ssahc helps ahc techniques to  improve their performance.}, } @inproceedings{slattery00, author = {
"proceedings of coling 2004", year =
classifier of  pagetypesearch classifies web pages into the document types by comparing their  pages with typical structural characteristics of the types.
most of these relations have been obtained by means of statistical and heuristical methods.
the documents are retrieved by considering both the predicted  relevance and its value as a training observation.
first, we train the linear svm on a subset of training data  and retain only those features that correspond to highly weighted components  (in absolute value sense) of the normal to the resulting hyperplane that  separates positive and negative examples.
it is thus unsurprising that this task has traditionally  been performed manually, by trained coders.
in experiments using a standard text retrieval test collection, small effectiveness improvements were obtained.
this paper  examines the use of inductive learning to categorize natural language documents  into predefined content categories.
{http://www.cs.utah.edu/~riloff/psfiles/single-thesis.ps}, abstract =  {knowledge-based natural language processing systems have achieved good success  with many tasks, but they often require many person-months of effort to build  an appropriate knowledge base.
aberdeen, uk}, year = {1997}, pages = {}, url = {http://www.ewic.org.uk/ewic/workshop/fetch.cfm/irr-97/moulinier/moulinier.ps}, abstract = {statistical classification techniques and machine learning methods have been applied to some information retrieval (ir) problems: routing, filtering and categorization.
the methods to create, detect, summarize, select, and code visual keywords will be detailed.
to avoid their being  overflowed by the incoming data, methods of information filtering are required.
{ieee computer society press, los alamitos, us}, editor = {nick cercone and tsau y. lin and xindong wu}, year = {2001}, address = {san jose, ca}, pages = {647--648}, url = {}, abstract = {}, } @inproceedings{soucy03, author = {pascal soucy and guy w. mineau}, title = {feature selection strategies for text categorization}, booktitle = {
the methods considered are foil, the propositional  rule-learning system ripper, and a first-order version of ripper called  flipper.
the results demonstrated that  representative sampling offers excellent learning performance with fewer  labeled documents and thus can reduce human efforts in text classification  tasks.}, } @inproceedings{xue03, author = {dejun xue and maosong sun}, title =
we show that there are in fact differences between citations and links in this context.
in this paper, we propose a news web page classification method (wpcm).
it allows an effective exploitation of the available  linguistic information that better emphasizes this latter with significant both  data compression and accuracy.
boosting is based on the idea of relying on the collective judgment of a  committee of classifiers that are trained sequentially.
"a study on automatically extracted keywords in text  categorization", booktitle =
this is because when the positive training data is too few, the boundary over-iterates and trespasses the natural gaps between positive and negative class in the feature space and thus ends up fitting tightly around the few positive training data.}, } @inproceedings{yu98, author = {
the result shows that term redundancy behaves very similar to noise and may degrade the classifier performance.
even though our experimental results do not outperform earlier approaches,  they give rise to promising perspectives.}, } @inproceedings{moulinier96a,  author = {isabelle moulinier and jean-gabriel ganascia}, title = {
by using the  hierarchically structured subject domain and classification rules, the  classifier's engine assigns an email query to the most relevant category or  categories.}, } @article{zheng04, author = {zhaohui zheng and xiaoyun wu and  rohini srihari}, title = {
madison, us}, pages = {359--367}, publisher = {
our experiments on the reuters-21578 benchmark show  that boosting is not effective in improving the performance of the base  classifiers on common categories.
next, we investigate the application of  automatic categorization to text retrieval.
these experiments have shown that {\sc adaboost.
hyperlinks pose  new problems not addressed in the extensive text classification literature.
our feature selection approach employs distributional clustering of words via the recently introduced information bottleneck method, which generates a more efficient word-cluster representation of documents.
the crucial question of the quality of automatic classification is treated at considerable length, and empirical data are introduced to support the hypothesis that classification quality improves as more information about each document is used for input to the classification program.
as the number  of unique words in the collection set is big, the principal component analysis  (pca) has been used to select the most relevant features for the  classification.
tc is the classification of documents into a predefined set of categories.
in addition to a unigram model of document representation, a bigram model is also explored.
{}, address = {pittsburgh, us}, url =
a central problem in good text classification for information filtering and retrieval (if/ir) is the high dimensionality of the data.
{l. alfonso ure{\~{n}}a-l{\'{o}}pez and manuel buenaga and jos{\'{e}} m.  g{\'{o}}mez}, title = {
furthermore, the effectiveness of word sense disambiguation for  different parts of speech (nouns and verbs) is examined empirically.}, }  @inproceedings{pang02, author = {
"tailby, ross and dean, richard and milner, ben and smith, dan", title =
we also propose a simple and effective way of combining a traditional text based classifier with a citation-link based classifier.
the paper shows how a text classifier's need for labeled training  documents can be reduced by taking advantage of a large pool of unlabeled  documents.
hamill, karen a. and zamora,  antonio}, title = {
these performance measures often assume independence between  categories and do not consider documents misclassified into categories that are  similar or not far from the correct categories in the category tree.
this model allows us to simultaneously take into account structure and content information.
it is  developed for the naive bayesian classifier applied on text data, since it  combines well with the addressed learning problems.
in addition, this paper investigates how these term distributions  contribute to weight each term in documents, e.g., a high term distribution of  a word promotes or demotes importance or classification power of that word.
however, published work on combining text categorizers suggests that, for this particular application, improvements in performance are hard to attain.
classifying text documents by associating terms with text categories}, booktitle = {proceedings of the 13th australasian conference on database technologies}, publisher = {acm press, new york, us}, year = {2002}, pages = {215--222}, address = {melbourne, au}, volume = {5}, url = {}, note =
effective algorithm for text corpus pruning is designed.
"841--846", booktitle =
in all cases we show that our approach either outperforms other methods tried for these tasks or performs comparably to the best.}, } @article{ruiz02, author = {miguel ruiz and padmini srinivasan}, title = {hierarchical text classification using neural networks}, journal = {information retrieval}, number = {1}, volume = {5}, pages = {87--118}, year = {2002}, url = {http://www.wkap.nl/article.pdf?383232}, abstract = {
explorative research using a simple voting  system is presented and discussed in the light of a probabilistic model that  was originally developed for safety critical software.
in order to  reflect the subtopic structure of a document, we propose a new passage-level or  passage-based text categorization model, which segments a test document into  several passages, assigns categories to each passage, and merges the passage  categories to the document categories.
we make available  detailed, per-category experimental results, as well as corrected versions of  the category assignments and taxonomy structures, via online appendices.}, }  @inproceedings{lewis91, author = {lewis, david d.}, title = {data extraction as  text categorization: an experiment with the {muc-3} corpus.}, booktitle =
our feature selection method strives to reduce redundancy between  features while maintaining information gain in selecting appropriate features  for text categorization.
reducing the dimensionality, or selecting a good subset of features, without sacrificing accuracy, is of great importance for neural networks to be successfully applied to the area.
our experimental results show that lb, an association-based lazy classifier can achieve a good tradeoff between high classification accuracy and scalability to large document collections and large feature sizes.}, } @article{merkl98, author = {merkl, dieter}, title = {
phase i corresponds to the training phase of machine learning research and phase ii corresponds to testing phase.
eibe frank and chang chui  and ian h. witten}, title = {
tackling the poor assumptions of naive bayes text classifiers}, booktitle = {proceedings of icml-03, 20th international conference on machine learning}, editor = {}, year = {2003}, address = {washington, dc}, pages = {}, publisher =
andrzej skowron  and zbigniew w. ra{\'{s}}}, pages = {235--243}, year = {1999}, address =
in this paper, it is proposed that a document be represented into a fuzzy set of informative keywords, instead of a crisp set of informative keywords.
discretizing continuous attributes in adaboost  for text categorization}, booktitle = {proceedings of ecir-03, 25th
to date, index-based search engines for the web have been the  primary tool by which users search for information.
we present three algorithms  that use varying amounts of extracted information to classify texts.
{proceedings of cikm-00, 9th acm international conference on information and  knowledge management}, publisher = {acm press, new york, us}, address =
integration into production lines is under execution.}, } @article{apte94, author =
{2000}, publisher = {springer verlag, heidelberg, de}, address = {kyoto, jp},  note = {
{191--208}, url = {},  abstract = {}, } @inproceedings{koehn02, author =
however, owing to the use of context-sensitive features, the  classifier is very accurate.
experimental results are encouraging overall; in particular, document classification results fulfill the requirements of high-volume application.
{acm press, new york, us}, address = {toronto, ca}, year = {2003}, pages =
pisa, it}, pages = {185--196}, year = {2004}, publisher = {springer verlag, heidelberg, de}, note = {
existing classification  techniques have limited applicability in the data sets of these natures.
our  investigation leads to conclude that association rule mining is a good and  promising strategy for efficient automatic text categorization.}, }  @inproceedings{zelikovitz00, author = {sarah zelikovitz and haym hirsh}, title  = {
{1993}, url =  {http://www.darmstadt.gmd.de/~tzeras/fullpapers/gz/tzeras-hartmann-93.ps.gz},  abstract = {
we  demonstrate that precision and recall can be significantly improved by solving  the categorization problem taking hierarchy into account.
= {springer  verlag, heidelberg, de}, note = {
"proceedings of the 14th {acm} international conference on information and knowledge management", year = "2005", month =
"proceedings of the 21st national conference on artificial intelligence", year =
abstract = "knn and svm  are two machine learning approaches to text categorization (tc) based on the  vector space model.
the crawling process is modeled as a parallel best-first search over a graph defined by the web.
we conclude by examining factors that make the sentiment classification problem more challenging.}, } @article{park04, author = {seong-bae park and byoung-tak zhang}, title = {co-trained support vector machines for large scale unstructured document classification using unlabeled data and syntactic information}, journal = {information processing and management}, year = {2004}, volume = {40}, number = {3}, pages = {421--439}, url = {}, abstract = {}, } @inproceedings{peng03, author = {fuchun peng and dale schuurmans}, title =
"proceedings of the 21st conference of the spanish society for natural language processing (sepln'2005)", year =
{proceedings of caia-90, 6th ieee conference on artificial intelligence  applications}, publisher =
tdfa obtains the axes that maximize the ratio between the document sets as to the sum of squared projections by solving a generalized eigenvalue problem.
"2006" }  @inproceedings{gliozzo:2006:ecc, author =
feature clustering is a powerful alternative to feature selection for reducing the dimensionality of text data.
the performance of the system using the  neural network classifier was generally satisfactory and, as expected, the  filtering performance varied with regard to the accuracy rates of classes.}, }  @inproceedings{moulinier96, author = {isabelle moulinier and gailius  ra{\u{s}}kinis and jean-gabriel ganascia}, title = {
though it is not a heavy requirement to rely on some existing pn dictionary (often these resources are available on the web), its coverage of a domain corpus may be rather low, in absence of manual updating.
while many existing methods achieve good levels of performance, they generally require levels of computation that prevent them from making sufficiently fast decisions in some applied setting.
{51--57}, year = {1992}, url = {http://www.acm.org/pubs/articles/proceedings/ir/133160/p51-blosseville/p51-blosseville.pdf}, abstract = {
proceedings of asai-99, 1st
senses are interpreted as groups (or clusters) of similar  contexts of the ambiguous word.
we present results from text classification experiments  that compare relevancy signatures, which use local linguistic context, with  corresponding indexing terms that do not.
these tests also have investigated finding the best indexing terms that could be used in making these classification decisions.
the benefit of the approach has been assessed via extensive cross evaluation over three corpora in two languages.
a major challenge in indexing unstructured hypertext databases is  to automatically extract meta-data that enables structured searching using  topic taxonomies, circumvents keyword ambiguity and improves the quality of  searching and profile-based routing and filtering.
finally, representing each newsgroup by k vectors (with k = 2 or 3)  using clustering yields the most significant improvement in routing accuracy,  ranging from 60\% to loo\%, while causing only slightly higher storage  requirements.}, } @inproceedings{hsu99a, author =
comparing classification schemes}, journal = {acm  transactions on information systems}, year = {2005}, volume = {23}, number =  {4}, pages = {430--462}, url = {http://doi.acm.org/10.1145/1095872.1095875},  abstract = {topical crawling is a young and creative area of research that  holds the promise of benefiting from several sophisticated data mining  techniques.
in text domains, effective feature selection is essential to make the learning task efficient and more accurate.
the system is a component  of a more extensive intelligent agent for adaptive information filtering on the  web.
the other is that there are no  training data for this classification problem.
an improved probabilistic text  categorization method is also presented.}, } @phdthesis{lewis92a, author =
this  approach is motivated by the observation that hyperbolic spaces possess a  geometry where the size of a neighborhood around a point increases  exponentially and therefore provides more freedom to map a complex information  space such as language into spatial relations.
the results of these  computational experiments on a sample of 2897 text documents from the tipster  collection indicate that the first approach has many advantages over the vsm  approach for solving this type of text document classification problem.
profile filters the  netnews and uses a scale of 11 predefined values of relevance.
the experimental results support  the claim that a custom-designed algorithm (genex), incorporating specialized  procedural domain knowledge, can generate better keyphrases than a  general-purpose algorithm (c4.5).
"kules, bill and kustanowitz, jack and scneiderman, ben", title =
% % % %  references that are *not* considered pertinent are: % % % % * publications that  discuss techniques in principle useful for % % atc (e.g. machine learning  techniques, information retrieval % % techniques) but do not explicitly discuss  their application % % to atc; % % % %
we investigate bayesian  methods for automatic document categorization and develop a new approach to  this problem.
although the corpus contains documents written in chinese, the proposed approach can be applied to documents written in any language and such documents can be transformed into a list of separated terms.}, } @inproceedings{yang01, author = {yiming yang}, title = {a study on thresholding strategies for text categorization}, booktitle = {proceedings of sigir-01, 24th acm international conference on research and development in information retrieval}, editor =
a controlled study using three classifiers, knn, llsf and word, was conducted to examine the impact of configuration variations in five versions of reuters on the observed performance of classifiers.
in spite  of these differences, both ripper and sleeping experts perform extremely well  across a wide variety of categorization problems, generally outperforming  previously applied learning methods.
{automatic document classification.
our system can be used in any application that requires fast and easily adaptable text categorization in terms of stylistically homogeneous categories.
if a category's recall exceeds its precision, the category is too strong and its level is reduced.
our algorithm automatically induces a very natural behavior, where our knowledge about one instance helps us classify related ones, which in turn help us classify others.
this could be due to the fact that when a word along with its adjoining word - a phrase - is considered towards building a category profile, it could be a good discriminator.
"sentiment classification on customer feedback  data: noisy data, large feature vectors, and the role of linguistic  analysis", booktitle =
the category that contains the largest categorical points is selected as the  category of a document.
we use support vector machine (svm) classifiers, which have been shown  to be efficient and effective for classification, but not previously explored  in the context of hierarchical classification.
a hierarchical  approach to the automatic categorization of medical documents}, booktitle =
improving linear classifier for chinese text categorization}, journal = {information processing and management}, year = {2004}, volume = {40}, number = {2}, pages = {223--237}, url = {}, abstract = {}, } @article{turney00, author = {peter d. turney}, title = {learning algorithms for keyphrase extraction}, journal = {information retrieval}, number = {4}, volume = {2}, pages = {303--336}, year = {2000}, url = {http://extractor.iit.nrc.ca/reports/ir2000.ps.z}, abstract = {many academic journals ask their authors to provide a list of about five to fifteen keywords, to appear on the first page of each article.
{tampere, fi}, year = {2002}, pages = {151--158}, url = {http://doi.acm.org/10.1145/564376.564404}, abstract = {
{association for computational linguistics}, pages = {56--63}, url  = {http://www.aclweb.org/anthology/w/w05/w05-0608} }  @inproceedings{fukumoto:2004:cna, author =
technical documentation %  required: title % optional: author, organization, address, edition, month,  year, note % %
} } @article{kazama:2004:mem, author =
the paper argues that the creation of efficient web  spiders is best framed and solved by reinforcement learning, a branch of  machine learning that concerns itself with optimal sequential decision making.
our approach also indicates a new promising way to use trust-worthy deep web knowledge to help organize dispersive information of surface web."
augmenting naive {b}ayes classifiers with statistical language models}, journal = {information retrieval}, publisher = {springer science}, year = {2004}, volume = {7}, number = {3-4}, pages = {317--345}, abstract = {
highly accurate text segmentation is also possible - the accuracy of the ppm-based chinese word segmenter is close to 99\% on chinese news text; similarly, a ppm-based method of segmenting text by language achieves an accuracy of over 99\%.}, } @inproceedings{teytaud01, author =
empirical evaluation results  indicate that the proposed technique, mice, was more effective than the  category discovery approach and was insensitive to the quality of original  categories.}, } @article{weigend99, author = {andreas s. weigend and erik d.  wiener and jan o. pedersen}, title = {
moreover, by ranking words and  phrases in the citing documents according to expected entropy loss, we are able  to accurately name clusters of web pages, even with very few positive examples.
"multilingual text classification using  ontologies", booktitle =
experimental results confirm the predictions of the  theory in the hypertext domain.}, } @inproceedings{joachims01c, author =
an evaluation of four sites was successfully carried out using this  approach.}, } @inproceedings{lewis96, author = {lewis, david d. and robert e.  schapire and james p. callan and ron papka}, title =
mh$^kr$} is based on the idea to build, at every  iteration of the learning phase, not a single classifier but a sub-committee of  the $k$ classifiers which, at that iteration, look the most promising.
text categorization and relational learning}, booktitle = {proceedings of icml-95, 12th international conference on machine learning}, editor = {armand prieditis and stuart j. russell}, address = {lake tahoe, us}, year = {1995}, pages = {124--132}, publisher = {morgan kaufmann publishers, san francisco, us}, url = {http://www.research.whizbang.com/~wcohen/postscript/ml-95-ir.ps}, abstract = {we evaluate the first order learning system foil on a series of text categorization problems.
we introduce a class of representations for classifying text data based on decision trees; (i.e., decision trees over attributes on strings) and present an algorithm for learning them inductively.
{65--72}, url = {http://www.cs.cmu.edu/~yiming/papers.yy/sigir00.ps}, abstract  = {automated tracking of events from chronologically ordered document streams  is a new challenge for statistical text classification.
"proceedings of the 19th international joint conference on  artificial intelligence", year =
the experimental results show that the proposed  pattern-based approach significantly outperforms all three baselines in terms  of precision at top ranks."
we find that our  simple corrections result in a fast algorithm that is competitive with  state-of-the-art text classification algorithms such as the support vector  machine.}, } @inproceedings{rennie99, author = {jason rennie and andrew k.  mccallum}, title = {using reinforcement learning to spider the web  efficiently}, booktitle = {proceedings of icml-99, 16th international  conference on machine learning}, editor = {ivan bratko and saso dzeroski}, year  = {1999}, address = {bled, sl}, publisher = {morgan kaufmann publishers, san  francisco, us}, pages = {335--343}, url =  {http://www.watson.org/~jrennie/papers/icml99.ps.gz}, abstract = {consider the  task of exploring the web in order to find pages of a particular kind or on a  particular topic.
since the single layers of self-organizing maps represent different aspects of the document collection at different levels of detail, the neural network shows the document collection in a form comparable to an atlas where the user may easily select the most appropriate degree of granularity depending on the actual focus of interest during the exploration of the document collection.}, } @inproceedings{meyer04, author = {
the results we obtain allow us to determine the relative difficulty of these subsets, thus establishing an indirect means for comparing tc systems that have, or will be, tested on these different subsets.}, note = {
we explore how  to organize a text database hierarchically to aid better searching and  browsing.
to  classify a database, our algorithm does not retrieve or in-spect any documents  or pages from the database, but rather just exploits the number of matches that  each query probe generates at the database in question.
by applying tdfa to the document set that belongs to a given class and a set of documents that is misclassified as belonging to that class by an existent classifier, we can obtain features that take large values in the given class but small ones in other classes, as well as features that take large values in other classes but small ones in the given class.
our approach is to develop a trainable information extraction system that takes two inputs.
the overall compression of an article with respect to different models can be compared to see which one it fits most closely.
this paper proposes topic difference factor analysis  (tdfa) as a method to extract projection axes that reflect topic differences  between two document sets.
morgan kaufmann publishers, san francisco, us}, pages = {361-370}, url = {http://www-ai.cs.uni-magdeburg.de/~scheffer/papers/icml99.ps}, abstract = {
{fuhr, norbert}, title = {
weighted trigram analysis is a quick and flexible technique for describing the contents of a document.
a linear least squares fit (llsf) technique is employed to estimate the likelihood of these associations.
{40}, number = {2}, pages = {239--255}, url = {}, abstract = {}, }  @inproceedings{wu04a, author = {xiaoyun wu and rohini srihari and zhaohui  zheng}, title = {document representation for one-class svm}, booktitle =
on investigating the root cause, we find that a large class of feature scoring methods suffers a pitfall: they can be blinded by a surplus of strongly predictive features for some classes, while largely ignoring features needed to discriminate difficult classes.
enhancing text categorization with encyclopedic  knowledge", booktitle1 =
nevertheless, hierarchical text classi- fication methods in the past have often been constructed in a proprietary manner.
"prague, czech republic" } @inproceedings{blitzer:2007:bbb, author =
in text classification most  techniques use bag-of-words to represent documents.
the method has been implemented in the context of a client-server application, named webclassii.
nashville, us}, pages = {143--151}, publisher = {morgan kaufmann publishers, san francisco, us}, url = {http://www-ai.cs.uni-dortmund.de/dokumente/joachims_97a.ps.gz}, abstract = {the rocchio relevance feedback algorithm is one of the most popular and widely applied learning methods from information retrieval.
"kardkovacs,  z. t. and tikk, d. and bansaghi, z.", title =
in this chapter, we therefore propose a specification language known as hcl (hierarchical classification language).
it  is simple to implement and has strong justifications from computational  learning theory.
{spie {}-{} the international society for optical engineering}, editor = {daniel p. lopresti and jiangying zhou}, year = {2000}, address = {san jose, us}, pages = {191--199}, url = {http://cm.bell-labs.com/who/tkh/papers/textcat.ps.gz}, abstract = {
an empirical evaluation of the system was performed by means of a confidence interval technique.
the system  does not perform a complete semantic or syntactic analyses of the input  stories.
despite this, we show for one data set that fax quality images can be  categorized with nearly the same accuracy as the original text.
usually, only about 1\%-10\% of examples belong to the selected category.
the experimental results obtained are encouraging and support the choice of a hybrid case-based approach to text categorization.}, } @inproceedings{geutner93, author = {petra geutner and uli bodenhausen and alex waibel}, title = {flexibility through incremental learning: neural networks for text categorization},
as a practical matter, we also explain how the text classification  system can be easily ported across domains.}, } @phdthesis{riloff94a, author =
the recent explosion of online information in digital libraries and on the world wide web has given rise to a number of query-based search engines and manually constructed topical hierarchies.
in this research, our experiment dealt with the classification of news wire articles using category profiles.
{221--226}, url  = {}, abstract = {
on computational cybernetics (iccc03)", address =
we investigate three approaches to automatically classifying documents by genre: traditional bag of words techniques, part-of-speech statistics, and hand-crafted shallow linguistic features.
such a system will have to be adaptive to the user  changing interest.
"michael  davy and saturnino luz", title =
comparisons with an optimized version of the traditional rocchio's algorithm adapted for text categorization, as well as flat neural network classifiers are provided.
{using text classification to predict the gene knockout behaviour of {s.\ cerevisiae}}, booktitle = {proceedings of apbc-03, 1st asia-pacific bioinformatics conference}, editor = {yi-ping p. chen}, publisher
however, most of them are useless for document categorization  because of the weakness in representing document contents.
however, we have found that recognizing singular and plural nouns, verb forms, negation, and prepositions can produce dramatically different text classification results.
we demonstrate that the idc algorithm is especially advantageous when the data exhibits high attribute noise.
the construction steps often involve human efforts and are not completely automated.
"2006"  } @inproceedings{wiebe:2006:wss, author =
stanford, us}, pages = {303--310}, publisher = {morgan kaufmann publishers,  san francisco, us}, url = {http://www.cs.cmu.edu/~rayid/mypapers/ecoc-icml.ps},  abstract =
such systems  can be deployed in various applications where instantaneous interactive  learning is necessary such as on-line email or news categorization, text  summarization and information filtering in general.}, } @inproceedings{ghani00,  author = {rayid ghani}, title = {using error-correcting codes for text  classification},
overall both algorithms are comparable and are quite effective.
a classifier for semi-structured documents}, booktitle = {proceedings of kdd-00, 6th acm international conference on knowledge discovery and data mining}, editor = {}, publisher = {acm press, new york, us}, address = {boston, us}, year = {2000}, pages = {340--344}, url = {http://doi.acm.org/10.1145/347090.347164}, abstract = {
boosting  applied to word sense disambiguation}, booktitle = {proceedings of ecml-00,  11th european conference on machine learning}, editor =
these four algorithms were tested on two  different data sets: one data set where the number of features were constrained  to the 1000 best features and another data set where the dimensionality was  over 7000.
{abraham bookstein and yves chiaramella and gerard salton and vijay v.  raghavan}, publisher = {acm press, new york, us}, address = {chicago, us},  pages = {337--346}, year = {1991}, url =  {http://www.acm.org/pubs/articles/proceedings/ir/122860/p337-rau/p337-rau.pdf},  abstract = {indexing text for accurate retrieval is a difficult and important  problem.
{1998}, url = {}, }  @inproceedings{sahami98b, author = {mehran sahami and salim yusufali and  michelle q. baldonado}, title = {sonia: a service for organizing networked  information autonomously},
this method might, however, degrade  classification results, since the distributions it employs are not always  precise enough for representing the differences between categories.
in contrast, for the reuters collection, we only achieve mediocre results.
we use different feature sets and  integrate neural network learning into the method.
an evaluation of four sites was successfully carried out using this approach.}, } @inproceedings{lewis96, author = {lewis, david d. and robert e. schapire and james p. callan and ron papka}, title =
when combined with the classification power of the svm, this method yields high performance in text categorization.
we first describe the text representation method we use in this work; we then present a feature selection method that is used to reduce the dimensionality of the feature space.
current text learning techniques for  combining labeled and unlabeled, such as em and co-training, are mostly  applicable for classification tasks with a small number of classes and do not  scale up well for large multiclass problems.
the  voting for a classification is processed on the basis of individual feature  projections.
the classification is a minimum least-squares approach based on polynomials.
published in the ``lecture notes in computer science'' series, number 2167}, url = {http://link.springer.de/link/service/series/0558/papers/2167/21670454.pdf}, abstract = {
experimental  results obtained on three real-world data sets show that we can reduce the  feature dimensionality by three orders of magnitude and lose only 2\% accuracy,  significantly better than latent semantic indexing, class-based clustering,  feature selection by mutual information, or markov-blanket-based feature  selection.
{journal of intelligent information systems}, year = {2002}, note = {special issue on automated text categorization}, volume = {18}, number = {2/3}, pages = {103--105}, url = {http://www.wkap.nl/article.pdf?391241}, } @book{joachims02a, author = {thorsten joachims}, title = {
with a smaller number of features,  ssfcm's performance is also superior to that of ssahc's.
{38--52}, address = {
{published in the ``lecture notes in computer science'' series, number 2412},  url =  {http://link.springer.de/link/service/series/0558/papers/2412/24120099.pdf},  abstract = {
we  examine more complex combination strategies but find them less successful due  to the high correlations among our filtering methods which are optimized over  the same training data and employ similar document representations.}, }  @inproceedings{hull98, author = {david a. hull}, title = {
{209--222}, url = {}, abstract = {
the second problem is that even with a representative model, the improvements given by unlabeled data do not sufficiently compensate for a paucity of labeled data.
{hirotoshi taira and masahiko haruno}, title = {
the main idea of ferrety algorithm can be generalized for mapping one taxonomy to another if training documents are available."
the paper presents an analysis of why tsvms are well  suited for text classification.
using support vector machines}, booktitle = {proceedings of icml-99, 16th  international conference on machine learning}, editor = {ivan bratko and saso  dzeroski}, year = {1999}, address = {bled, sl}, publisher =
text categorization algorithms usually represent documents as bags of words and consequently have to deal with huge numbers of features.
after investigating two similarity-based classifiers (k-nn and rocchio) and three common thresholding techniques (rcut, pcut, and scut), we describe a new learning algorithm known as the keyword association network (kan) and a new thresholding strategy (rinscut) to improve performance over existing techniques.
here, the challenges are: a) obtain a high accuracy classification model; b) consume low computational time for both model training and operation; and c) occupy low storage space.
in addition, more than 80\% of automatically  extracted terms are meaningful.
a  memory-based approach to anti-spam filtering for mailing lists}, journal =
we address these problems with a system for topical information space navigation that combines the query-based and taxonomic systems.
"andrea esuli and fabrizio sebastiani", title = "determining the semantic orientation of terms through gloss classification", pages =
we have experimented with five machine learning  algorithms: nearest neighbors (nn), naive bayes (nb), decision tree (dt),  sparse network of winnows (snow), and support vector machines (svm) using two  kinds of features: bag-of-words and bag-ofngrams.
to our knowledge, this work is the first to report performance results with the entire new reuters corpus.}, } @article{craven00, author = {mark craven and dan dipasquo and dayne freitag and andrew k. mccallum and tom m. mitchell and kamal nigam and se{\'{a}}n slattery}, title = {
moreover, a guided strategy for the ocat-based approach is presented for  deciding which document one needs to consider next while building the training  example sets.}, } @article{nigam00, author = {kamal nigam and andrew k.  mccallum and sebastian thrun and tom m. mitchell}, title = {
gliozzo, alfio and strapparava,  carlo}, title = {domain kernels for text categorization}, booktitle =
in this paper, we devised a novel approach to automatically generate categories.
today, text categorization is a necessity due to the very  large amount of text documents that we have to deal with daily.
text categorization, or the assignment of textual  documents to one or more pre-defined categories based on their content, is an  essential component of efficient management and retrieval of documents.
creating segmented databases from free text for text retrieval}, booktitle = {proceedings of sigir-91, 14th acm international conference on research and development in information retrieval}, editor = {abraham bookstein and yves chiaramella and gerard salton and vijay v. raghavan}, publisher = {acm press, new york, us}, address = {
de buenaga rodr{\'{\i}}guez,  manuel and g{\'o}mez-hidalgo, jos{\'e} mar{\'{\i}}a and d{\'{\i}}az-agudo,  bel{\'e}n}, title = {
our evaluations show that for web collections, the megadocument method clearly outperformes other classification methods.
{12}, pages = {278--295}, url = {http://www.acm.org/pubs/articles/journals/tois/1994-12-3/p278-liddy/p278-liddy.pdf}, abstract = {
adaptive information filtering is concerned with filtering information streams in changing environments.
we review a  variety of standard approaches to converting scores (and poor probability  estimates) from text classifiers to high quality estimates and introduce new  models motivated by the intuition that the empirical score distribution for the  "extremely irrelevant", "hard to discriminate", and  "obviously relevant" items are often significantly different.
in  this paper, we propose a novel text classification approach, called  discriminative category matching, which could achieve all of the stated  characteristics.
in this paper we examine text categorization methods from a perspective that considers the tradeoff between accuracy and scalability to large data sets and large feature sizes.
{kenneth e. kinnear}, pages = {459--476}, url = {}, abstract = {}, }  @inproceedings{matsuda98, author = {katsushi matsuda and toshikazu fukushima},  title = {task-oriented {world wide web} retrieval by document type  classification}, booktitle = {proceedings of cikm-98, 7th acm international  conference on information and knowledge management}, publisher = {acm press,  new york, us}, editor = {georges gardarin and james c. french and niki pissinou  and kia makki and luc bouganim}, year = {1998}, address = {bethesda, us}, pages  = {109--113}, url =  {http://www.acm.org/pubs/articles/proceedings/cikm/319950/p109-matsuda/p109-matsuda.pdf},  abstract = {this paper proposes a novel approach to accurately searching web  pages for relevant information in problem solving by specifying a web document  category instead of the user's task.
{association for computational linguistics, morristown, us}, editor  = {lillian lee and donna harman}, pages = {58--66}, address = {pittsburgh, us},  url = {http://www.cs.columbia.edu/~sable/research/emnlp01.ps}, abstract = {
as a result, they are not portable across domains.
* publications thet discuss related topics sometimes confused with % % atc; these include, in particular, text clustering (i.e. text % % classification by unsupervised learning) and text indexing; % % % % * technical reports and workshop papers.
published in the ``lecture notes in computer science'' series, number 3202}, url = {}, abstract = {}, } @inproceedings{goevert99, author = {
a text is represented as a set of cases and we classify a text as relevant if any of its cases is deemed to be relevant.
{proceedings of icml-94, 11th international conference on machine learning},  editor = {
{philip r. cohen and wolfgang wahlster}, year =
naive use of terms in the link neighborhood of a document can even  degrade accuracy.
experiments also show that the proportion of meaningful terms extracted from training data is relative to the classification accuracy in outside testing.}, } @inproceedings{lam01, author = {wai lam and kwok-yin lai}, title = {a meta-learning approach for text categorization}, booktitle = {proceedings of sigir-01, 24th acm international conference on research and development in information retrieval}, editor =
text classification from positive and  unlabeled documents}, booktitle = {proceedings of cikm-03, 12th acm  international conference on information and knowledge management}, publisher =
in this paper, we present the megadocument approach for  categorization.
to date, index-based search engines for the web have been the primary tool by which users search for information.
we report on experiences with the reuters newswire benchmark, the us patent database, and web document samples from {{\sc yahoo!}}\.}, } @inproceedings{chakrabarti98b, author = {soumen chakrabarti and byron e. dom and piotr indyk}, title = {enhanced hypertext categorization using hyperlinks}, booktitle = {proceedings of sigmod-98, acm international conference on management of data},
{}, year = {2000}, address = {den haag, nl}, pages = {145--152}, url =  {http://www.acm.org/pubs/articles/proceedings/chi/332040/p145-chen/p145-chen.pdf},  abstract = {
{information sciences}, year = {2004}, number = {1}, volume = {158}, pages =  {69--88}, url = {http://dx.doi.org/10.1016/j.ins.2003.03.003}, abstract =  {automatic categorization is the only viable method to deal with the scaling  problem of the world wide web (www).
significant features are automatically derived from training texts by selecting substrings from actual word forms and applying statistical information and general linguistic knowledge.
it was found that typical categorization approaches produce predictions which are too similar for combining them to be effective since they tend to fail on the same records.
this paper reports a controlled study with statistical significance tests on  five text categorization methods: the support vector machines (svm), a  k-nearest neighbor (knn) classifier, a neural network (nnet) approach, the  linear least-squares fit (llsf) mapping and a naive bayes (nb) classifier.
"rochester, ny" } @inproceedings{zaidan:2007:uar, author =
also the role of linguistic preprocessing seems to provide positive effects on the performance.
an  important aspect of this dissertation is that autoslog and the text  classification systems can be easily ported across domains.}, }  @inproceedings{riloff95, author = {ellen riloff}, title = {
the ultimate goal of taxsom is to create  the premise for successfully training a supervised classifier.}, }  @inproceedings{aggarwal99, author = {charu c. aggarwal and stephen c. gates and  philip s. yu}, title = {
in this paper we present an approach to text categorization in terms of genre and author for modern greek.
the impact of rule insertion is most significant  for categories with a small number of relevant documents.}, } @article{tan02,  author =
{evaluating and optmizing autonomous text classification systems}, booktitle =  {proceedings of sigir-95, 18th acm international conference on research and  development in information retrieval}, editor =
we found ig and chi most effective in our experiments.
and dmoz are extremely popular: dmoz is maintained by over 56,000 volunteers, is used as the basis of the popular google directory, and is perhaps used by millions of users each day.
{34--64}, url = {http://doi.acm.org/10.1145/595576.595579}, abstract = {
stanford, us}, pages = {655--662}, publisher = {morgan kaufmann publishers,  san francisco, us}, url = {http://www.cs.cmu.edu/~rayid/mypapers/ecoc-icml.ps},  abstract =
= {chien chin chen and chang chen,  meng and yeali sun}, title = {
words, contexts and senses are represented in  word space, a high-dimensional real-valued space in which closeness corresponds  to semantic similarity.
{66--73}, publisher = {
categorization of text is of increasing importance in information retrieval and natural language processing systems.
the proposed  architecture matches the hierarchical structure of the topic space, as opposed  to a flat model that ignores the structure.
}  @inproceedings{cui:2006:ces, author =
however, even the most successful techniques are defeated by many real-world applications that have a strong time-varying component.
the modalities and techniques to fit this objectives are still under discussion.
ssahc is (i) a clustering algorithm that (ii) uses a finite design set of labeled data to (iii) help agglomerative hierarchical clustering (ahc) algorithms partition a finite set of unlabeled data and then (iv) terminates without the capability to label other objects.
text categorization techniques are adopted to convert each process to a vector and calculate the similarity between two program activities.
the ddc method for  subject indexing is very close to operational status for a data base which  grows at the rate of two million words of text per year.}, }  @inproceedings{klinkenberg00, author = {ralf klinkenberg and thorsten  joachims}, title = {
this risk is found to be most severe when little data is available.
in this paper, we introduce a method for automating  this classi-fication process by using a small number of query probes.
an alternative approach has recently been proposed, whereby a naive  bayesian classifier is trained automatically to detect spam messages.
in order to accomplish this testing, we employ the em algorithm which helps efficiently estimate parameters in a finite mixture model.
in particular, we demonstrate (1) how variation in document length can  be tolerated by either normalizing feature weights or by using negative  weights, (2) the positive effect of applying a threshold range in training, (3)  alternatives in considering feature frequency, and (4) the benefits of  discarding features while training.
the methods considered are foil, the propositional rule-learning system ripper, and a first-order version of ripper called flipper.
our evaluations show that  for web collections, the megadocument method clearly outperformes other  classification methods.
when tested on 50 user profiles and 550 megabytes of documents, results indicate that the feature set that is the basis of the text categorization module and the algorithm that establishes the boundary of categories of potentially relevant documents accomplish their tasks with a high level of performance.
{hans-peter frei and donna harman and peter sch{\"{a}}uble and  ross wilkinson}, publisher = {acm press, new york, us}, year = {1996}, address  = {z{\"{u}}rich, ch}, pages = {298--306}, url =  {http://www.research.att.com/~lewis/papers/lewis96d.ps}, abstract = {
text classification from positive and unlabeled documents}, booktitle = {proceedings of cikm-03, 12th acm international conference on information and knowledge management}, publisher =
the study showed that the  category interface is superior both in objective and subjective measures.
optimizing and estimating effectiveness is greatly aided if classifiers that explicitly estimate the probability of class membership are used.}, } @article{lewis95a, author = {lewis, david d.}, title = {
we observed  that our new method made a significant improvement in all classifiers and both  data sets.}, } @article{ko04, author = {youngjoong ko and jinwoo park and  jungyun seo}, title = {
rijsbergen, cornelis j.}, year = {2002}, address = {glasgow, uk}, publisher =
we describe the architecture of a classification system that uses a web directory to identify the subject context that the query terms are frequently used in.
masoud mohammadian}, publisher = {ios  press}, address = {amsterdam, nl}, pages = {194--198}, year = {1999}, url = {},  abstract = {
daniel  r. tauritz and ida g. sprinkhuizen-kuyper}, title = {adaptive information  filtering algorithms}, booktitle = {proceedings of ida-99, 3rd symposium on  intelligent data analysis}, publisher = {springer verlag, heidelberg, de}, note  = {
we  describe webwatcher as a tour guide agent for the web, the learning algorithms  used by webwatcher, experimental results based on learning from thousands of  users, and lessons learned from this case study of tour guide agents.}, }  @inproceedings{joachims98, author = {
text categorization and relational learning},  booktitle =
= {benkhalifa, mohamed and bensaid, amine and mouradi, abdelhak}, title = {
in this paper, we compare learning techniques based on statistical classification to traditional methods of relevance feedback for the document routing problem.
kld method achieve substantial improvements over the tfidf performing  method.}, } @article{blei03, author = {david m. blei and andrew y. ng and  michael i. jordan}, title = {latent dirichlet allocation}, journal = {journal  of machine learning research}, volume = {3}, pages = {993--1022}, year =
and a somewhat different version of  one-class svm based on identifying ``outlier" data as representative of  the second-class.
the proposed method shows a similar degree of performance, compared with the traditional supervised learning methods.
{christian shin and david doermann and azriel rosenfeld}, title = {classification of document pages using structure-based features}, journal = {international journal on document analysis and recognition}, number = {4}, volume = {3}, pages = {232--247}, year = {2001}, url = {http://link.springer.de/link/service/journals/10032/papers/1003004/10030232.pdf}, abstract = {
the accuracy of the supervised classifier was established by comparing its performance with a baseline system that uses human classification information.
in order to  explicitly capture the optimality of word clusters in an information theoretic  framework, we first derive a global criterion for feature clustering.
"bremen, germany", publisher =
in many information retrieval and data mining applications, linear classifiers are strongly preferred because of their ease of implementation, interpretability and empirical performance.
this basic em procedure works well when the data conform to the generative  assumptions of the model.
we prove that the negative geodesic distance (ngd) on the multinomial manifold is conditionally positive definite (cpd), thus can be used as a kernel in svms.
our comparative analysis includes three learning algorithms: naive bayes,  perceptron, and support vector machines (svm) in combination with three feature  weighting methods: odds ratio, information gain, and weights from linear  models, the linear svm and perceptron.
[1], and shows that  some of the modifications included in twcnb may not be necessary to achieve  optimum performance on some datasets.
{dallas, us}, url = {}, abstract = {methods for taking into account linguistic  content into text retrieval are receiving a growing attention
text categorization is of increasing interest in both controlled vocabulary indexing and other applications.
conference on automated learning and discovery}, editor = {}, publisher = {aaai press, menlo park, us}, year = {1998}, pages = {}, address = {pittsburgh, us}, url = {http://www.rdrop.com/~lierer/conald98.ps}, abstract = {
our results show that an index can be constructed on a desktop workstation with little effect on categorisation accu-racy compared to a memory-based approach.
"the detection of new information in a document stream is an important component of many potential applications.
significant improvements in computational efficiency without losing categorization accuracy were evident in the testing results.}, } @article{yang96, author =
in our approach, first, layered lsi spaces are built  for a better representation of the hierarchically structured domain knowledge,  in order to emphasize the specific semantics and term space in each layer of  the domain knowledge.
koppel, moshe and argamon, shlomo and shimoni,  anat r.}, title = {automatically categorizing written texts by author gender},  journal = {literary and linguistic computing}, year = {2002}, number = {4},  volume = {17}, pages = {401--412}, url =  {http://www3.oup.co.uk/litlin/hdb/volume_17/issue_04/pdf/170401.pdf}, abstract  = {
upon the fact that refined statistics may have more chance to meet sparse data problem, we re-evaluate the role of the binary weighting model (bwm) in tc for further consideration.
= {everett h. brenner}, year = {1978}, address = {new york, us}, pages = {152--155}, url = {}, abstract = {}, } @article{hamill80, author = {
{chris hutchison and gaetano lanzarone}, year = {1999}, address = {varese, it}, pages = {105--119}, url = {http://www.math.unipd.it/~fabseb60/publications/thai99.pdf}, abstract = {assistance in retrieving documents on the world wide web is provided either by search engines, through keyword-based queries, or by catalogues, which organize documents into hierarchical collections.
such engines can build giant indices that let you quickly retrieve the set of all web pages containing a given word or string.
in this paper we propose a technique  for the automatic updating of a pn dictionary through the cooperation of an  inductive and a probabilistic classifier.
twenty subjects from expert or novice literary reading experience backgrounds  were, in two experiments, required to rate two parallel sets of graphically and  phonetically manipulated poems.
our experiments use reuters 21578  database and consist of binary classifications for categories selected from the  115 topics classes of the reuters collection.
the algorithms analyze inter- and intra- document dynamics by  considering how information evolves over time from article to article, as well  as within individual articles.
we find that simple averaging strategies do  indeed improve performance, but that direct averaging of probability estimates  is not the correct approach.
"like many purely data-driven machine  learning methods, support vector machine (svm) classifiers are learned  exclusively from the evidence presented in the training dataset; thus a larger  training dataset is required for better performance.
we  present an effectiveness measure based on utility, and two sampling strategies  (pooling and stratified sampling) for estimating the utility of the submitted  sets.
{ludovic denoyer and jean-no{\"{e}}l vittaut and patrick gallinari and  sylvie brunessaux and stephan brunessaux}, title = {structured multimedia  document classification}, booktitle = {proceedings of doceng-03, acm symposium  on document engineering}, publisher = {acm press, new york, us}, editor = {},  year = {2003}, address = {
our work is distinguished from previous efforts in topic spotting by  our explicit study of the effects of dialogue length on classifier performance,  and by our use of off-the-shelf speech recognition technology.
"bing liu and xiaoli li and wee sun  lee and yu, philip s.", title =
"computational linguistics and  intelligent text processing", year = "2005" }  @inproceedings{anagnostopoulos:2006:eec, author =
in particular, boosting methods such as adaboost have shown good performance applied to real text data.
in the '90s, with the booming production and availability  of on-line documents, automated text categorisation has witnessed an increased  and renewed interest.
this calls for using a feature selection method, not only to reduce the number of features but also to increase the sparsity of document vectors.
a  performance evaluation of automatic survey classifiers}, booktitle =
this task is challenging since incoming messages may contain constructions which have not been anticipated.
the algorithm, which we proposed, operates in a fully  automatic mode and requires no supervision or training data.
"valerio freschi and andrea seraghiti and alessandro bogliolo", title =
this paper evaluates whirl on a number of inductive classification  tasks using data from the world wide web.
{acm press, new york, us}, address = {toronto, ca}, year = {2003}, pages =  {182--189}, url = {http://doi.acm.org/10.1145/860435.860470}, abstract =  {term-based representations of documents have found wide-spread use in  information retrieval.
the importance of different features is reported.
yang xiang and brahim chaib-draa}, address =
proceedings of ecml-01, 12th european conference on machine learning}, editor
the proposed architecture relies on hidden  markov models whose emissions are bag-of-words according to a multinomial word  event model, as in the generative portion of the naive bayes classifier.
this approach is illustrated for the case of  indexing with a controlled vocabulary.
this property causes k-nnfp to eliminate possible adverse effects of irrelevant features on the classification accuracy.
the method is quantitative analysis of the glosses of such definitions that these terms are given in on- dictionaries, and on the use of the resulting term representations semi-supervised term classification.
we present efficient approximate inference techniques based on variational methods and an em algorithm for empirical bayes parameter estimation.
"john blitzer and mark dredze  and fernando pereira", title =
we find that using words in web pages alone often yields  suboptimal performance of classifiers, compared to exploiting additional  sources of information beyond document content.
"pappuswamy, umarani and bhembe, dumisizwe and jordan, pamela w. and vanlehn, kurt", title =
{sofus a. macskassy and haym hirsh and arunava banerjee and aynur a. dayanik}, title = {
thus the classifier has a small model size  and is very fast.
previous work used a similar clustering procedure to show that word-clusters can significantly reduce the feature space dimensionality, with only a minor change in classification accuracy.
}, abstract = {}, } @inproceedings{alkofahi01, author = {khalid al-kofahi and alex tyrrell and arun vachher and tim travers and peter jackson}, title = {
the method combines information derived from n-grams (consecutive sequences of n characters) with a simple vector-space technique that makes sorting, categorization, and retrieval feasible in a large multilingual collection of documents.
to index successfully in the defense documentation center's environment, an automated system must chose single words or phrases (dependent upon context) rapidly and economically.
an object-oriented design allows easy addition of new preprocessors, machine learning algorithms, and classifier types.}, } @article{li02, author
the accuracy of modern text classification systems rivals that of trained human professionals, thanks to a combination of information retrieval (ir) technology and machine learning (ml) technology.
here, limited labeled data provide em initializations that lead to low-probability models.
in this paper we present a systematic, comparative experimental study of the three subsets of \textsf{reuters-21578} that have been most popular among tc researchers.
combined with the classification power of an svm, this method yields high performance text categorization that can outperform other recent methods in terms of categorization accuracy and representation efficiency.
pisa, it}, year = {2003}, pages = {393--407}, url = {http://link.springer.de/link/service/series/0558/papers/2633/26330393.pdf}, abstract =
different from existing categorization methods, mudof can automatically recommend a suitable algorithm for each category based on the category-specific statistical characteristics.
subjects liked the category interface much better than the list interface, and they were 50\% faster at finding information that was organized into categories.
svm is highly efficient in learning from well organized samples of moderate size, although on relatively large and noisy data the efficiency of svm and aram are comparable.}, } @article{heaps73, author = {h.s. heaps}, title = {a theory of relevance for automatic document classification}, year = {1973}, journal = {information and control}, volume = {22}, number = {3}, pages = {268-278}, url = {}, abstract = {}, } @inproceedings{hearst91, author = {marti a. hearst}, title = {
the experimental  evaluation demonstrates that the wpcm method provides acceptable classification  accuracy with the sports news datasets.}, } @inproceedings{sevillano04, author  = {sevillano dominguez, xavier and alias pujol, francesc and socoro carrie,  joan c.}, title = {ica-based hierarchical text classification for multi-domain  text-to-speech synthesis}, booktitle = {proceedings of icassp-04, proceedings  of the 29th ieee international conference on acoustics, speech, and signal  processing}, editor = {}, publisher = {ieee computer society press, los  alamitos, us}, address = {
"montejo-raez, arturo and urena-lopez, l. alfonso and steinberger, ralf", title =
the simple method of conducting hypothesis testing over word-based distributions in categories suffers from the data sparseness problem.
these algorithms both construct classifiers that allow  the ``context'' of a word w to affect how (or even whether) the presence or  absence of w will contribute to a classification.
the mcnemar test shows that in most categories the increases are very significant.
{recently knowledge discovery and data mining in unstructured or semi-structured texts (text mining) has attracted lots of attention from both commercial and research fields.
"a supervised clustering method for text classification", booktitle =
for larger documents and datasets the paper  introduces an approximation technique that is shown to deliver good  approximations efficiently for large datasets.}, } @inproceedings{macskassy01,  author =
booktitle = {proceedings of  aaai-92, 10th conference of the american association for artificial  intelligence}, publisher =
we describe our approach to document representation that captures contextual dependencies between terms in a corpus and makes use of these dependencies to represent documents.
the motivation is that there are statistical problems associated with natural language text when it is applied as input to existing machine learning algorithms (too much noise, too many features, skewed distribution).
in this paper we present a systematic, comparative experimental  study of the three subsets of \textsf{reuters-21578} that have been most  popular among tc researchers.
only relatively  recently did detailed studies on the impact of various document representations  step into the spotlight, showing that there may be statistically significant  differences in classifier performance even among variations of the classical  bag-of-words model.
we also demonstrate some experimental results using our algorithm on the problem of classifying bibliographic data and extracting keywords in order to show the effectiveness of our approach.}, } @inproceedings{sakkis01, author = {georgios sakkis and ion androutsopoulos and georgios paliouras and vangelis karkaletsis and constantine d. spyropoulos and panagiotis stamatopoulos}, title = {stacking classifiers for anti-spam filtering of e-mail},
finally ssfcm results  in improved performance and faster execution time as more weight is given to  training documents.}, } @inproceedings{bennett02, author = {paul n. bennett and  susan t. dumais and eric horvitz}, title = {probabilistic combination of text  classifiers using reliability indicators: models and results}, booktitle =  {proceedings of sigir-02, 25th acm international conference on research and  development in information retrieval}, editor =
published in the ``lecture notes in computer science'' series, number 2035}, pages = {78--89}, url = {http://link.springer.de/link/service/series/0558/papers/2035/20350078.pdf}, abstract =
in order to address this difficulty, guthrie et.al.
in the process of document indexing, a document is represented as a set of informative keywords.
yet, for many text  classification tasks, providing labeled training documents is expensive, while  unlabeled documents are readily available in large quantities.
"siofok, hungary", year =
"hema raghavan and omid madani  and rosie jones", title =
in this paper, we develop a framework to incorporate unlabeled data in the error-correcting output coding (ecoc) setup by first decomposing multiclass problems into multiple binary problems and then using co-training to learn the individual binary classification problems.
"2004", month =  "july", address =
"this paper studies the use of di®erent sources of information for performing a text classification task.
we show that judicious  use of a hierarchy can significantly improve both the speed and effectiveness  of the categorization process.
thorsten  joachims and dayne freitag and tom m. mitchell}, title = {{\sc webwatcher}: a  tour guide for the word wide web}, booktitle = {proceedings of ijcai-97, 15th  international joint conference on artificial intelligence}, editor = {
{\em feature selection} (fs) refers to the activity of selecting, from the set of $r$ distinct features (i.e.\ words) occurring in the collection, the subset of $r'\ll r$ features that are most useful for compactly representing the meaning of the documents.
for both data sets, boosting trees and svms had acceptable test performance in terms of accuracy and speed.
{causal models and intelligent data management}, publisher = {springer verlag},  address = {heidelberg, de}, year = {1999}, pages = {151--185}, url = {},  abstract = {}, } @inproceedings{frank00, author = {
visualization is used for the presentation of the output of learning}, } @inproceedings{hamill78, author = {
the bigrams, along with unigrams, are then given as features to two different classifiers: naive bayes and maximum entropy.
"word sense and subjectivity", booktitle =  "proceedings of the 21st international conference on computational  linguistics and 44th annual meeting of the association for computational  linguistics", year =
further, we show that selecting a best classification method using text-only features and then adding numerical features to the problem (as might happen if numerical features are only later added to a pre existing text-classification problem) gives performance that rivals a more time-consuming approach of evaluating all classification methods using the full set of both text and numerical features.}, } @article{maderlechner97, author = {maderlechner, g. and suda, p. and bruckner, t.}, title = {classification of documents by form and content}, journal =
, publisher = {acm press,  new york, us}, address = {
thus, unlike some other unsupervised dimensionality-reduction techniques, such as latent semantic indexing, we are able to compress the feature space much more aggressively, while still maintaining high document classification accuracy.
however, centralized classification approaches often are limited due to constraints on knowledge and computing resources.
in this paper we investigate the use of concept-based document representations to supplement word- or phrase-based features.
on a  text categorization task, using the reuters-22,173 collection, we give  empirical evidence that feature selection is useful: first, the size of the  collection index can be drastically reduced without causing a significant loss  in categorization effectiveness.
we use the technique of genetic programming (gp), (koza and rice 1992), to evolve classifying agents.
in application, the adapted text categorizers are reliable, fast, and completely automatic.
if term occurrences are random then there will be no  correlation and the strength will be zero, but if for any subject, the term is  either always present or never present its strength will be one.
we report the results obtained using the mean reciprocal rank as a measure of overall performance, a commonly used evaluation measure for question answering tasks.
however, one of the main shortcomings of such methods is that they largely disregard lexical semantics and, as a consequence, are not sufficiently robust with respect to variations in word usage.
"aris anagnostopoulos  and andrei broder and kunal punera", title = "effective and efficient  classification on a search-engine model", booktitle = "cikm",  year =
the learning approach presented is attribute-efficient and, therefore, appropriate for domains having very large number of attributes.
we show that even on  purely numerical-valued data the results of text-classification on the derived  text-like representation outperforms the more naive numbers-as-tokens  representation and, more importantly, is competitive with mature numerical  classification methods such as c4.5 and ripper.}, } @article{macskassy03,  author =
the ecoc method scales well to large data sets with a  large number of classes.
we extract  proper names, locations, temporal expressions and normal terms into distinct  sub-vectors of the document representation.
the text categorization module described in the paper provides a  front-end filtering function for the larger dr-link text retrieval system  (liddy and myaeng 1993).
we conclude that  our techniques permit automatic categorisation using very large train-ing  collections, vocabularies, and numbers of categories.}, } @article{shin01,  author =
highly accurate text segmentation is  also possible - the accuracy of the ppm-based chinese word segmenter is close  to 99\% on chinese news text; similarly, a ppm-based method of segmenting text  by language achieves an accuracy of over 99\%.}, } @inproceedings{teytaud01,  author =
a combined use  of the projections on and the distances to the dlsi spaces introduced from the  differential document vectors improves the adaptability of the lsi (latent  semantic indexing) method by capturing unique characteristics of documents.
initial experiments on a set of 15  selected polysemous words show that the boosting approach surpasses naive bayes  and exemplar-based approaches, which represent state-of-the-art accuracy on  supervised wsd.
text retrieval conference}, publisher = {national institute of standards  and technology, gaithersburg, us}, editor =
{679--694}, year = {2006} } @inproceedings{radovanovic:2006:drc, author = {milo\v{s} radovanovi\'c and mirjana ivanovi\'c}, title = {document representations for classification of short {w}eb-page descriptions}, booktitle = {proceedings of dawak-06, 8th international conference on data warehousing and knowledge discovery}, year = {2006}, pages = {544--553}, series = {lecture notes in computer science}, volume = {4081}, address = {krakow, poland}, publisher = {springer-verlag}, url = {http://perun.im.ns.ac.yu/radovanovic/publications/2006-dawak-docrep.pdf}, abstract = {
some experiments applying different base classifiers for a multi-label classifier in the domain of high energy physics on several of these possible sources have been carried out.
even though most of these features are relevant, the underlying concepts can be  concisely captured using only a few features, while keeping all of them has  substantially detrimental effect on categorization accuracy.
the problem of automatically determining the gender of a document's author  would appear to be a more subtle problem than those of categorization by topic  or authorship attribution.
fabio crestani and mark girolami and van rijsbergen, cornelis j.}, year = {2002}, address = {glasgow, uk}, publisher = {springer verlag, heidelberg, de}, note = {published in the ``lecture notes in computer science'' series, number 2291}, pages = {353--362}, url = {http://www.cs.ucd.ie/staff/nick/home/research/download/finn-ecir2002.ps.gz}, abstract = {
in this work we present a comparative  study of digital library citations and web links, in the context of automatic  text classification.
in order to reduce dimensionality, feature selection has been introduced as a preprocessing step.
{published in the ``lecture notes in computer science'' series, number 2013},  editor =
using a machine learning approach, we build classifiers that accept an audio file of conversational human speech as input, and output an estimate of the topic being discussed.
"pumping documents through a domain and  genre classification pipeline", booktitle = "lrec'04", year =  "2004" } @inproceedings{cohen:2004:lce, author =
the proposed  approach does not require professional librarians or that the end users have  extensive training.
in  addition to being highly accurate, this method utilizes the hamming distance  from ecoc to provide high-precision results.
ludovic denoyer and hugo zaragoza and patrick gallinari}, title = {hmm-based passage models for document classification and ranking}, booktitle
text  classification is becoming more important with the proliferation of the  internet and the huge amount of data it transfers.
"40--47", url =  "http://doi.ieeecomputersociety.org/10.1109/mis.2005.49", abstract =  "machine learning has become one of the main approaches to tackling text  categorization.
the authors developed a data-driven constructive-induction method that uses multiple operators to improve the representation space.
the goal of the research  described here is to automatically create a computer understandable knowledge  base whose content mirrors that of the world wide web.
a set of new scoring measures for feature selection taken from  the machine learning domain were evaluated over two well-known collections of  documents.
{robustness of regularized linear classification methods in text  categorization}, booktitle = {proceedings of sigir-03, 26th acm international  conference on research and development in information retrieval}, editor =
{1}, pages = {1--47}, year = {2002}, url =  {http://www.math.unipd.it/~fabseb60/publications/acmcs02.pdf}, abstract = {
"kumar, ravi and punera, kunal and tomkins, andrew", title =
"viktor pekar, richard evans and  ruslan mitkov", title =
{boosting to correct the inductive bias for text classification},  booktitle = {proceedings of cikm-02, 11th acm international conference on  information and knowledge management}, publisher = {acm press, new york, us},  editor = {}, year = {2002}, address = {mclean, us}, pages = {348--355}, url =  {http://doi.acm.org/10.1145/584792.584850}, abstract = {
classifying web documents in a hierarchy of categories: a  comprehensive study}, journal = {journal of intelligent information systems},  volume = {28}, number = {1}, pages =
{stephen e. robertson and p. harding}, title = {probabilistic automatic  indexing by learning from human indexers}, year = {1984}, journal = {journal of  documentation}, volume = {40}, number = {4}, pages = {264--270}, url = {},  abstract = {}, } @inproceedings{rose02, author = {tony rose and mark stevenson  and miles whitehead}, title = {
we evaluate the inequality me models on text  categorization datasets, and demonstrate their advantages over standard me  estimation, similarly motivated gaussian map estimation of me models, and  support vector machines (svms), which are one of the state-of-the-art methods  for text categorization.} } @article{kim:2005:drt, author = {
we found that pages in some genres change rarely if at all and can be used in present-day research experiments without requiring an updated version.
domain knowledge is used to specify a prior distribution for the parameters of a logistic regression model, and labeled training data is used to produce a posterior distribution, whose mode we take as the final classifier.
{laura m. haas and ashutosh tiwary}, publisher = {acm press, new york, us}, address = {
we also explore  some variants of our gis approach.
to solve this highly nonlinear  optimization problem, we use a generalized probabilistic descent algorithm.
{chinese documents classification based on n-grams}, booktitle = {proceedings  of cicling-02, 3rd international conference on computational linguistics and  intelligent text processing},
"2007", month = "june", address =
http://www.serve.com/cmtan/meng/ig_m.pdf}, abstract = {
editor = {w. bruce croft and alistair moffat and van rijsbergen, cornelis j. and ross wilkinson and justin zobel}, publisher = {acm press, new york, us}, year = {1998}, address = {melbourne, au}, pages = {215--223}, url = {http://www.research.att.com/~schapire/cgi-bin/uncompress-papers/schapiresisi98.ps}, abstract = {
it is shown to be more robust with respect to the training set size and to improve the performance both for ranking and classification, specially for classes with few training examples.}, } @inproceedings{denoyer03, author = {ludovic denoyer and patrick gallinari}, title = {
as a global observation, knn, llsf and a neural network method had the best performance; except for a naive bayes approach, the other learning algorithms also performed relatively well.}, } @inproceedings{yavuz98, author = {yavuz, tuba and g{\"u}venir, h. altay}, title = {application of k-nearest neighbor on feature projections classifier to text categorization}, booktitle = {proceedings of iscis-98, 13th
documents are represented as feature-vectors that include word sequences instead of including only single words as commonly used when learning on text data.
text categorization in the form of topic identification is a capability of current interest.
hang li and kenji yamanishi}, title = {
{fabio crestani and mark girolami and van rijsbergen, cornelis j.}, year = {2002}, address = {glasgow, uk}, publisher = {springer verlag, heidelberg, de}, note = {
{journal of intelligent information systems}, year = {2002}, note = {special issue on automated text categorization}, volume = {18}, number = {2/3}, pages = {127--152}, url = {http://www.wkap.nl/article.pdf?391243}, abstract = {kernel methods like support vector machines have successfully been used for text categorization.
proceedings of cikm-99, 8th acm international  conference on information and knowledge management}, publisher = {acm press,  new york, us}, editor = {}, year = {1999}, address = {kansas city, us}, pages =
these connections do not depend on whether there are shared terms among the queries and documents; therefore, they are especially effective for a mapping from queries to the documents where the concepts are relevant but the terms used by article authors happen to be different from the terms of database users.
the efficiency, effectiveness, and noise tolerance of this search strategy were confirmed to be better than those of a full search, a category based search, and a cluster based search with nonprobabilistic clustering.}, } @inproceedings{iwayama95a, author
in this paper, we introduce a new information gain and divergence-based feature selection method for statistical machine learning-based text categorization without relying on more complex dependence models.
{2004}, publisher = {springer verlag, heidelberg, de}, note = {
while previous work in hierarchical classification focused on virtual category trees where documents are assigned only to the leaf categories, we propose a top-down level-based classification method that can classify documents to both leaf and internal categories.
published in the ``lecture notes in computer science'' series, number 2857}, year = {2003}, address = {manaus, br}, pages = {183--196}, url = {http://www.gia.ist.utl.pt/~acardoso/spire03.pdf}, abstract = {
however, the svm methods turned out to be quite sensitive to the choice of representation and kernel in ways which are not well understood; therefore, for the time being leaving the neural network approach as the most robust.}, } @inbook{manning99a, author = {
published in the ``lecture notes in computer science''  series, number 2175}, editor = {floriana esposito}, year = {2001}, pages =
in this article, we evaluate the retrieval performance of an  algorithm that automatically categorizes medical documents.
traditionally the categories are arranged in hierarchical manner to achieve effective searching and indexing as well as easy comprehension for human beings.
we suggest the utilization of additional resources like lexical databases to increase the amount of information that tc systems make use of, and thus, to improve their performance.
an analysis of patterns in sentences was performed with data from the trec 2002 novelty track and experiments on novelty detection were carried out on data from the trec 2003 and 2004 novelty tracks.
hierarchically classifying documents using very few words}, booktitle = {proceedings of icml-97, 14th international conference on machine learning}, editor = {douglas h. fisher}, year = {1997}, address = {nashville, us}, pages = {170--178}, publisher = {morgan kaufmann publishers, san francisco, us}, url = {http://robotics.stanford.edu/users/sahami/papers-dir/ml97-hier.ps}, abstract = {the proliferation of topic hierarchies for text documents has resulted in a need for tools that automatically classify new documents within such hierarchies.
overall, we present an algorithm, a variation of littlestone's winnow, which performs significantly better than any other algorithm tested on this task using a similar feature set.}, } @inproceedings{dalessio00, author = {stephen d'alessio and keitha murray and robert schiaffino and aaron kershenbaum}, title = {
this paper presents a text categorization system, capable of  analyzing html/text documents collected from the web.
luc de raedt and arno siebes}, publisher
in this paper, we want to relax this assumption and allow a concept instance to be represented by a subgraph of web pages or a set of web pages.
the results of a formal evaluation are discussed, and examples are given using documents in english and japanese.}, } @article{damerau04, author = {fred j. damerau and tong zhang and sholom m. weiss and nitin indurkhya}, title = {
text categorization is the procedure of assigning a category to a particular document among predefined categories.
describes an approach to document routing on the trec corpus that employs a technique for the automatic construction of classification trees.
{ieee computer society press,  los alamitos, us}, year = {2001}, address = {kyoto, jp}, pages = {254--261},  url = {}, abstract = {
how to appropriately represent that information and automatically learn statistical patterns for solving hypertext classification problems is an open question.
in this paper, we study the use of support vector machine in text categorization.
daniel p. lopresti and jiangying zhou}, year = {2000}, address = {san  jose, us}, pages = {191--199}, url =  {http://cm.bell-labs.com/who/tkh/papers/textcat.ps.gz}, abstract = {
text categorization with  the concept of fuzzy set of informative keywords}, booktitle = {proceedings of  fuzz-ieee'99, ieee international conference on fuzzy systems}, editor = {},  publisher = {ieee computer society press, los alamitos, us}, address = {
by noisy it is meant any text obtained through  an extraction process (affected by errors) from media different than digital  texts.
its severe assumptions make such efficiency possible but also adversely affect the quality of its results.
we present experimental results showing that employing our active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings.}, note = {an extended version appears as \cite{tong01}}, } @article{tong01, author = {simon tong and daphne koller}, title = {support vector machine active learning with applications to text classification}, journal = {journal of machine learning research}, volume = {2}, month = {november}, pages = {45--66}, year = {2001}, url = {http://www.ai.mit.edu/projects/jmlr/papers/volume2/tong01a/tong01a.pdf}, abstract = {support vector machines have met with significant success in numerous real-world learning tasks.
however, an open question was whether these extraction patterns were useful for tasks other than information extraction.
the results show that a significant improvement can be derived using the proposed inference model.
"hierarchical  topic segmentation of websites", booktitle =
= {interactions between  document representation and feature selection in text categorization},  booktitle = {proceedings of dexa-06, 17th international conference on database  and expert systems applications}, year = {2006}, pages = {489--498}, series =  {lecture notes in computer science}, volume = {4080}, address = {krakow,  poland}, publisher = {springer-verlag}, url =  {http://perun.im.ns.ac.yu/radovanovic/publications/2006-dexa-idf.pdf}, abstract  = {many studies in automated text categorization focus on the performance of  classifiers, with or without considering feature selection methods, but almost  as a rule taking into account just one document representation.
in an unsupervised setting, our models produced coherent clusters with a very natural interpretation, even for instance types that do not have any attributes.}, } @inproceedings{taskar02, author = {ben taskar and pieter abbeel and daphne koller}, title = {discriminative probabilistic models of relational data}, booktitle = {proceedings of uai-02, 18th conference on uncertainty in artificial intelligence}, year = {2002}, address = {edmonton, ca}, pages = {485--492}, publisher = {morgan kaufmann publishers, san francisco, us}, editor = {}, url = {}, abstract = {
= {springer verlag, heidelberg, de}, address = {freiburg, de}, year = {2001}, pages = {338--349}, note = {
feature feedback can complement traditional active learning in applications such as news filtering, e-mail classification, and personalization, where the human teacher can have significant knowledge on the relevance of features.}, } @inproceedings{forman:2006:tcd, author =
adaboost produces better classifiers than rocchio when the training collection contains a very large number of relevant documents.
this paper introduces a multistrategy learning approach to the categorization of text documents.
our new classifier misclassified 36\% of the patents, indicating  that classifying hypertext can be more difficult than classifying text.
this paper presents an extension of the rocchio formula [11] as  a feature weighting and selection model used as a basis for multilingual  information extraction.
empirical results  show that gp was able to discover better similarity functions than ga or other  fusion techniques." }
linear support vector machines (svms) are  particularly promising because they are very accurate, quick to train, and  quick to evaluate.} } @inproceedings{elyaniv01, author = {
this paper explores in detail the use of error correcting output
since information retrieval is a domain where such data sets are widespread, it  provides an ideal application area for machine learning.
we propose a novel approach for categorizing text documents based  on the use of a special kernel.
the  results of a formal evaluation are discussed, and examples are given using  documents in english and japanese.}, } @article{damerau04, author = {fred j.  damerau and tong zhang and sholom m. weiss and nitin indurkhya}, title = {
published in the ``lecture notes in computer science'' series,  number 1696.
experiments are conducted to compare our multi-agent approach with a centralized approach.
this  dissertation also describes three algorithms that use information extraction to  support high-precision text classification.
estimating  the generalization performance of a svm efficiently}, booktitle = {proceedings  of icml-00, 17th international conference on machine learning}, editor = {
in this paper, based on the  rule learning algorithm ripper (repeated incremental pruning to produce error  reduction), we propose an efficient method for hierarchical document  categorization.}, } @inproceedings{sasaki98a, author = {minoru sasaki and kenji  kita}, title = {rule-based text categorization using hierarchical categories},  booktitle = {proceedings of smc-98, ieee international conference on systems,  man, and cybernetics}, editor = {}, publisher =
the algorithm, which we proposed, operates in a fully automatic mode and requires no supervision or training data.
"wei-hao lin and alexander  hauptmann", title = "are these documents written from different  perspectives?
we analyze a few of the commonly used statistics based and machine learning algorithms for natural language disambiguation tasks and observe that they can be recast as learning linear separators in the feature space.
published in the ``lecture notes in  computer science'' series, number 2276}, year = {2002}, address = {mexico city,  mx}, pages = {405--414}, url =  {http://link.springer.de/link/service/series/0558/papers/2276/22760405.pdf},  abstract = {traditional chinese documents classifiers are based on keywords in  the documents, which need dictionaries support and efficient segmentation  procedures.
"extracting key-substring-group features for text classification", booktitle =
the algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents.
the encouraging results indicated that our approach is hignhly feasible.}, } @article{chouchoulas01, author = {
text mining concerns of discovering unknown  patterns or knowledge from a large text repository.
= {institution of electrical engineers, london, uk}, editor = {}, year = {1999}, pages = {898--903}, address = {edinburgh, uk}, url = {http://www.his.sunderland.ac.uk/ps/icann99.pdf}, abstract = {
{proceedings of muc-3, 3rd message understanding conference}, editor = {},  publisher =
links clearly contain high-quality semantic clues that are lost upon a purely term-based classifier, but exploiting link information is non-trivial because it is noisy.
"proposing a new term weighting scheme for text categorization", booktitle1 =
booktitle = {proceedings of emnlp-02, conference on empirical methods in natural language processing}, address = {philadelphia, us}, year = {2002}, publisher = {association for computational linguistics}, pages = {172--179}, } @inproceedings{sable99, author = {carl l. sable and vasileios hatzivassiloglou}, title = {text-based approaches for the categorization of images}, booktitle = {proceedings of ecdl-99, 3rd european conference on research and advanced technology for digital libraries}, editor = {
this information must be filtered in order to make an effective use of it in our model of tc.
requirements of any such system include speed and  minimal end-user effort.
an empirical comparison of text categorization methods}, booktitle = {proceedings of spire-03, 10th international symposium on string processing and information retrieval}, editor = {mario a. nascimento and edleno s. de moura and arlindo l. oliveira}, publisher = {springer verlag, heidelberg, de}, note = {
a controlled study using three classifiers,  knn, llsf and word, was conducted to examine the impact of configuration  variations in five versions of reuters on the observed performance of  classifiers.
tc has been an application for many learning approaches,  which prove effective.
"borovets, bulgaria" } @inproceedings{mihalcea:2005:uet, author =
the results confirm that our meta-model approach can exploit the advantage of  its component algorithms, and demonstrate a better performance than existing  algorithms.}, } @inproceedings{lam97, author = {wai lam and kon f. low and chao
in the context of a very high-dimensional feature space, several methodological alternatives are examined, including universal versus local dictionaries, and binary versus frequency related features.}, } @inproceedings{apte94a, author = {apt\'{e}, chidanand and damerau, fred j. and weiss, sholom m.}, title = {towards language-independent automated learning of text categorization models}, booktitle = {proceedings of sigir-94, 17th acm international conference on research and development in information retrieval}, editor = {w. bruce croft and van rijsbergen, cornelis j.}, publisher = {springer verlag, heidelberg, de}, address = {dublin, ie}, pages =
"barcelona, spain", publisher =  "association for computational linguistics", abstract =
"juho rousu and craig saunders and sandor szedmak and john shawe-taylor", title =
in training the $i$-th  classifier special emphasis is placed on the correct categorization of the  training documents which have proven harder for the previously trained  classifiers.
we describe how the tree kernel can be computed efficiently by dynamic programming.
they can be used not only to estimate the error rate, but also to estimate recall, precision, and f1.
we tested our learning method on the task of single-label classification using  the reuters-21578 benchmark.
a performance evaluation of automatic survey classifiers}, booktitle = {proceedings of icgi-98, 4th international colloquium on grammatical inference}, address = {ames, us}, editor =
using synthetically generated data we empirically  demonstrate that whenever the dc procedure is successful in recovering some of  the structure hidden in the data, the extended idc procedure can incrementally  compute a dramatically better classification, with minor additional  computational resources.
experimental results,  obtained using text from three different real-world tasks, show that the use of  unlabeled data reduces classification error by up to 30\%.}, }  @inproceedings{nigam00a, author = {kamal nigam and rayid ghani}, title =
text retrieval conference}, publisher = {national institute of standards and technology, gaithersburg, us}, editor =
statistical methods for categorization, on  the other hand, are easy to implement and require little or no human  customization.
this paper describes the design, execution and evaluation of a modest experimental study aimed at testing empirically one statistical technique for automatic indexing.}, } @inproceedings{marton05, author = {yuval marton and ning wu and lisa hellerstein}, title = {
a new evaluation methodology is offered that focuses on the needs of the data mining practitioner faced with a single dataset who seeks to choose one (or a pair of) metrics that are most likely to yield the best performance.
in particular, in which situations can  a combination of kernel be expected to perform better than its components  considered separately?
susan craw and alun d. preece}, publisher = {springer verlag, heidelberg, de}, year = {2002}, address = {aberdeen, uk}, note = {
we present three algorithms that use varying amounts of extracted information to classify texts.
{1995}, address = {gaithersburg, us}, pages =
"september",  address =
this disser- tation demonstrates that supervised learning algorithms that use a small number of labeled examples and many inexpensive unlabeled examples can create high-accuracy text classifiers.
finally, the conditions of the application as well as problems of further development are discussed.}, } @inproceedings{bigi03, author = {brigitte bigi}, title = {
we developed a user interface that organizes web search results  into hierarchical categories.
{cesa-bianchi, nicolo and gentile, claudio and zaniboni, luca}, title = {incremental algorithms for hierarchical classification}, journal = {journal of machine learning research}, volume = {7}, pages = {31--54}, year = {2006}, url = {http://jmlr.csail.mit.edu/papers/volume7/cesa-bianchi06a/cesa-bianchi06a.pdf}, abstract =
six nonjudgmental criteria are used in testing the  hypothesis for 100 keyword lists (each list representing a document) for a  series of computer runs in which the number of words per document is increased  progressively from 12 to 36.
thorsten joachims}, title = {
however, on these tasks, rocchio  runs much faster than adaboost.}, } @inproceedings{scheffer99, author = {tobias  scheffer and thorsten joachims}, title = {expected error analysis for model  selection}, booktitle = {proceedings of icml-99, 16th international conference  on machine learning}, editor = {ivan bratko and saso dzeroski}, year = {1999},  address = {bled, sl}, publisher = {morgan kaufmann publishers, san francisco,  us}, pages = {361-370}, url =  {http://www-ai.cs.uni-magdeburg.de/~scheffer/papers/icml99.ps}, abstract = {
"chen wenliang and zhu jingbo and wu honglin and yao tianshun", title =
an unoptimized version of  the algorithm was used to process the trec database in a very short time.}, }  @inproceedings{huffman95, author = {
"proceedings of eacl-06,  11th conference of the european chapter of the association for computational  linguistics", year =
the goal of these methods is to discover automatically classification patterns that can be used for general document categorization or personalized filtering of free text.
there is an increasing interest in categorizing texts using learning algorithms.
it is evident  in this study that automated word removal based on corpus statistics has a  practical and significant impact on the computational tractability of  categorization methods in large databases.}, } @inproceedings{yang96a, author =
{this paper presents empirical results for several versions of the multinomial naive bayes classifier on four text categorization problems, and a way of improving it using locally weighted learning.
we study to what extent ocr errors affect  stylistic text classification from scanned documents.
in this paper, we assess to what extent feature selection can be used without causing a loss in effectiveness.
in this paper, we examine the effects of page evolution on genre classification of web pages.
an appropriate indexing approach and the corresponding structure of  the air/phys system are described.
in the text domain there are likely to exist many gaps in the  feature space because a document is usually mapped to a sparse and high  dimensional feature space.
the result of learning is a set of independent classifiers, each used to predict the probability that a new example is a member of the corresponding category.
this paper investigates the robustness of three regularized linear classification methods (svm, ridge regression and logistic regression) under above situations.
"support vector machines (svms) have been very successful in text classification.
support vector machines (svm) is a machine-learning algorithm which has been shown to be highly effective for automatic text categorisation.
in this paper, we propose a novel text classification approach, called discriminative category matching, which could achieve all of the stated characteristics.
while the decision tree based classifier outperforms the bayesian classifier when features and training size are selected optimally for both, a carefully designed naive bayesian classifier is more robust.}, } @inproceedings{diaz98, author = {d{\'{\i}}az esteban, alberto and de buenaga rodr{\'{\i}}guez, manuel and ure{\~n}a l{\'o}pez, l. alfonso and garc{\'{\i}}a vega, manuel}, title = {integrating linguistic resources in an uniform way for text classification tasks}, booktitle = {proceedings of lrec-98, 1st international conference on language resources and evaluation}, publisher = {}, editor = {antonio rubio and natividad gallardo and rosa castro and antonio tejada}, address = {grenada, es}, pages = {1197--1204}, year = {1998}, url = {http://www.esi.uem.es/laboratorios/sinai/postscripts/lrec98.ps}, abstract = {applications based on automatic text classification tasks, like text categorization (tc), word sense disambiguation (wsd), text filtering or routing, monolingual or multilingual information retrieval, and text summarization could obtain serious improvements by integrating linguistic resources in the current methods.
we tested this method on both retrieval and indexing with a set of medline documents which has been used by other information retrieval systems for evaluations.
"proceedings of the 29th annual international acm sigir conference on research and development in information retrieval", year =
as an example of stw, we  propose a number of ``supervised variants'' of $tfidf$ weighting, obtained by  replacing the $idf$ function with the function that has been used in phase (i)  for term selection.
representative sampling for text classification using support vector machines}, booktitle = {proceedings of ecir-03, 25th european conference on information retrieval}, publisher =
however, the effect of boosting for rare  categories varies across classifiers: for svms and decision trees, we achieved  a 13-17\% performance improvement in macro-averaged f1 measure, but did not  obtain substantial improvement for the other two classifiers.
"applied research in uncertainty  modelling and analysis", editor =
on the other hand, other techniques naturally extensible to handle multi-class classification are generally not as accurate as svm.
this paper presents a novel approach for adapting the complexity of a text categorization system to the difficulty of the task.
this paper presents an application of nonlinear neural networks to topic spotting.
we conducted several experiments on a news corpus, called msdn.
% % % % everyone is also welcome to let me know either additional % % references or corrections and additions (e.g. urls, where % % they are not already present) to the existing ones.
{2002}, address = {aberdeen, uk}, note = {
all the experiments  result in a significant improvement with respect to other purely statistical  methods (e.g. [yang, 1999]), thus stressing the relevance of the available  linguistic information.
{bratko, andrej and filipic, bogdan}, title = {
{madison, us}, note = {
{kluwer academic publishers}, address = {dordrecht, nl}, url = {http://www.cais.ntu.edu.sg/~sunaixin/paper/sun_hcl.pdf}, abstract = {hierarchical text classification refers to assigning text documents to the categories in a given category tree based on their content.
exploiting structural information for semi-structured  document categorization}, journal = {information processing and management},  volume = {42}, number = {3}, pages =
however, this fraction corresponds to only one-fourth of the mistakes made by the human specialists.}, } @inproceedings{riloff92, author = {ellen riloff and wendy lehnert}, title = {classifying texts using relevancy signatures}, booktitle = {proceedings of aaai-92, 10th conference of the american association for artificial intelligence}, publisher = {aaai press, menlo park, us}, editor = {}, year = {1998}, pages = {329--334}, address = {san jose, us}, url = {}, abstract = {}, } @inproceedings{riloff93, author = {ellen riloff}, title = {using cases to represent context for text classification}, booktitle = {proceedings of cikm-93, 2nd international conference on information and knowledge management}, publisher = {acm press, new york, us}, editor = {bharat bhargava and timothy finin and yelena yesha}, year = {1993}, address = {new york, us}, pages = {105--113}, url = {http://www.cs.utah.edu/~riloff/psfiles/cikm93-w-addend.ps}, abstract = {research on text classification has typically focused on keyword searches and statistical techniques.
document retrieval, categorization, routing and filtering can all be formulated as classification problems.
an algorithm for sequential sampling during machine learning of statistical classifiers was developed and tested on a newswire text categorization task.
springer verlag,  heidelberg, de}, note = {
combining multiclass maximum entropy text classifiers with neural network  voting}, booktitle = {proceedings of portal-02, 3rd international conference on  advances in natural language processing}, year = {2002}, editor = {elisabete  ranchod and nuno j. mamede}, pages =
% % everyone is  welcome to download the bibliography as a whole and % % distribute it, provided  that it is distributed untouched.
{takao terano and huan liu and arbee l.p. chen}, pages = {408--419}, year = {2000}, publisher = {springer verlag, heidelberg, de}, address = {kyoto, jp}, note = {
a text-categorization application developed with tcs consists of the tcs run-time system and a rule base.
georges  gardarin and james c. french and niki pissinou and kia makki and luc bouganim},
instead of concentrating on a small set of ambiguous words, as done in most of  the related previous work, all content words of the examined corpus are  disambiguated.
we verify experimentally that ssfcm both outperforms and takes less time than the fuzzy-c-means (fcm) algorithm.
experimental data is presented showing widrow-hoff and eg to be more effective than the widely used rocchio algorithm on several categorization and routing tasks.}, } @misc{lewis97a, author = {lewis, david d.}, title = {reuters-21578 text categorization test collection.
there is an increasing need however for methods that can achieve higher classification accuracy while maintaining the ability to process large document collections.
this article describes our general approach, several machine learning algorithms for this task, and promising initial results with a prototype system that has created a knowledge base describing university people, courses, and research projects.}, } @article{craven01, author = {craven, mark and slattery, se{\'{a}}n}, title = {relational learning with statistical predicate invention: better models for hypertext}, journal = {machine learning}, pages = {97--119}, year = {2001}, volume = {43}, number = {1/2}, url = {http://www.wkap.nl/article.pdf?321079},
to convert the documents into vector form, we experiment with different numbers of features, which we select, based on an information gain criterion.
combining the results of classifiers has shown much promise in machine learning generally.
we further show that feature clustering  is an effective technique for building smaller class models in hierarchical  classification.
we implemented versions of the svm appropriate for one-class  classification in the context of information retrieval.
{proceedings of erk-98, the seventh electrotechnical and computer science  conference}, year = {1998}, address = {ljubljana, sl}, pages =
the method is inspired by \emph{text categorization}  (tc), the discipline concerned with labelling natural language texts with  labels from a predefined set of domains, or categories.
this paper introduces a term weighting method for text categorization based on smoothing ideas borrowed from speech recognition.
it is evident that the llsf approach uses the relevance information effectively within human decisions of categorization and retrieval, and achieves a semantic mapping of free texts to their representations in an indexing language.
{machine learning}, volume = {39}, number = {2/3}, pages = {99--101}, year =  {2000}, url = {http://www.wkap.nl/article.pdf?255754}, }  @inproceedings{cardoso03, author = {ana cardoso-cachopo and arlindo l.  oliveira}, title = {
experiments on a real-world data set show a reduction  in classification error by up to 66\% over the traditional naive bayes  classifier.
many large organizations  create and maintain huge volumes of textual information online, and there is a  pressing need for support of efficient and effective information retrieval,  filtering, and management.
scut is  potentially better for fine-tuning but risks overfitting.
"august",  address = "bonn, germany", url =  "http://www.machinelearning.org/proceedings/icml2005/papers/053_generalizedlars_keerthi.pdf",  abstract =
an experimental evaluation of ocr text representations for learning document classifiers}, journal = {international journal on document analysis and recognition}, pages = {116--122}, year = {1998}, number = {2}, volume = {1}, url = {http://link.springer.de/link/service/journals/10032/papers/8001002/80010116.ps.gz}, abstract = {
"prague, czech republic" }  @inproceedings{blitzer:2007:bbb, author =
however, while tc deals with documents represented as vectors in a space of terms, we formulate the task of term categorization as one in which terms are (dually) represented as vectors in a space of documents, and in which terms (instead of documents) are labelled with domains.}, } @inproceedings{baker98, author = {douglas baker and andrew k. mccallum}, title = {distributional clustering of words for text classification}, booktitle = {proceedings of the 21st acm international conference on research and development in information retrieval}, editor = {bruce croft and alistair moffat and van rijsbergen, cornelis j. and ross wilkinson and justin zobel}, publisher = {acm press, new york, us}, year = {1998}, address = {melbourne, au}, pages = {96--103}, url = {http://www.cs.cmu.edu/~mccallum/papers/clustering-sigir98.ps.gz}, abstract = {we describe the application of distributional clustering to document classification.
an alternative which has not been  sufficiently explored is the use of word meanings, also known as senses.
we present experimental results in  learning to classify email in this fashion, where each class corresponds to a  verb-noun pair taken from a predefined ontology describing typical 'email  speech acts'.
"multilingual text classification using ontologies", booktitle =
senses are interpreted as groups (or clusters) of similar contexts of the ambiguous word.
extensive experiments have been conducted on a real-world  document collection and satisfactory performance is obtained.}, }  @article{lai02, author = {yu-sheng lai and chung-hsien wu}, title = {meaningful  term extraction and discriminative term selection in text categorization via  unknown-word methodology}, journal = {acm transactions on asian language  information processing}, year = {2002}, number = {1}, volume = {1}, pages =
many techniques and algorithms for automatic text categorization have been devised and proposed in the literature.
to specify a user's problem solving task, we introduce the concept of document types that directly relate to the problem solving tasks; with this approach, users can easily designate problem solving tasks.
in a candidate feature set consisting of chinese character  bigrams, there exist a number of bigrams which are high-degree biased according  to character frequencies.
this process involves an activity of {\em supervised
"a comparison and semi-quantitative analysis of words and character-bigrams as features in chinese text categorization", booktitle =
dell zhang and wee sun lee},  title = {question classification using support vector machines}, booktitle =
for example, in a scientific paper domain, papers are related to each other via citation, and are also related to their authors.
extensive experiments have been conducted on the reuters-21578 and 20-newsgroups data sets.
it is therefore worthwhile to understand whether such good performance is unique to the svm design, or if it can also be achieved by other linear classification methods.
we rely on the statistical properties of the case base to determine whether similar cases are highly correlated with relevance for the domain.
two different classification  schedules are compared along with two methods of automatically classifying  documents into categories.
{1988}, pages = {9--17}, url = {}, note = {
i have since  fixed the bug and rerun the experiments.
in this paper we present a fast supervised dimensionality reduction algorithm that is derived from the recently developed cluster-based unsupervised dimensionality reduction algorithms.
this  can aid if/ir systems that rely on the acquisition of large numbers of term  weights or other measures of relevance.
the kernel is an inner product in the feature space generated by all subsequences of length k.
published in the ``lecture notes in computer science'' series, number 1846}, year = {2000}, address = {shanghai, cn}, pages = {215--226}, url = {http://link.springer.de/link/service/series/0558/papers/1846/18460215.pdf}, abstract = {
we show that our learning algorithm can be used for automatic extraction of keywords for text retrieval and automatic text categorization.
previous work used a similar clustering procedure to show that  word-clusters can significantly reduce the feature space dimensionality, with  only a minor change in classification accuracy.
there  is an increasing need however for methods that can achieve higher  classification accuracy while maintaining the ability to process large document  collections.
in this work we present a novel implementation of  this method for supervised text classification.
serge abiteboul and anne-marie vercoustre}, publisher =
we approach the problem of automatically extracting keyphrases from text as a supervised learning task.
the algorithm's predictive accuracy is competitive with other recently introduced hierarchical multi-category or multilabel classification learning algorithms."
proceedings of icdm-02, 2nd ieee international conference on data mining}, editor = {}, publisher =
experiments on a web directory show that best results are achieved when links from pages outside the directory are considered.
trento, it}, pages = {178--185}, url = {}, abstract = {automatic news categorization systems have produced high accuracy, consistency, and flexibility using some natural language processing techniques.
the paper describes the technique of categorisation by context, which exploits the context perceivable from the structure of html documents to extract useful information for classifying the documents they refer to.
{bethesda, us}, pages = {156--160}, url =  {http://www.acm.org/pubs/articles/proceedings/cikm/288627/p156-yu/p156-yu.pdf},  abstract = {much previous work on text filtering is developed for batch  filtering.
the results are compared to 1-of-n  coding (i.e.\ one svm for each text category).
representation in which each feature corresponds to a single word.
hypertext  categorization using hyperlink patterns and meta data}, booktitle =
in this paper, based on the rule learning algorithm ripper (for repeated incremental pruning to produce error reduction), we propose an efficient method for hierarchical document categorization.}, } @article{schapire00, author = {schapire, robert e. and singer, yoram}, title = {{{\sc boostexter}}: a boosting-based system for text categorization}, journal = {machine learning}, year = {2000}, number = {2/3}, volume = {39}, pages = {135-168}, url = {http://www.research.att.com/~schapire/papers/schapiresi98b.ps.z}, abstract = {this work focuses on algorithms which learn from examples to perform multiclass text and speech categorization tasks.
} @inproceedings{ginter:2005:dcu, author = "filip ginter and sampo pyysalo and tapio salakoski", title = "document classification using semantic networks with an adaptive similarity measure", booktitle =
since a  multi-dimensional model can be converted to flat and hierarchical models, three  classification strategies are possible, i.e., classifying directly based on the  multi-dimensional model and classifying with the equivalent flat or  hierarchical models.
in [3] it has been suggested that  classifiers based on generalized rocchio formula can be used to weight features  in category profiles in order to exploit the selectivity of linguistic  information techniques in text classification.
we find that both  algorithms achieve reasonable performance and allow controlled tradeoffs  between false positives and false negatives.
to perform classification, a naive bayesian classifier was designed and implemented, and a decision tree based classifier was implemented.
because the analyses of data are generally so expensive, most parts in databases remains as raw, unanalyzed primary data.
{gianni amati and fabio crestani and flavio ubaldini and stefano de nardis}, title =
approach based on  hierarchically structured subject domain}, booktitle = {proceedings of  ideal-02, 3rd international conference on intelligent data engineering and  automated learning}, editor = {hujun yin and nigel allinson and richard freeman  and john keane and simon hubbard}, publisher = {springer verlag, heidelberg,  de}, address = {manchester, uk}, year = {2002}, pages = {99--104}, note =
the changes may occur both on the transmission side (the nature of the streams can change) and on the reception side (the interests of a user can change).
text categorization in an intelligent agent for filtering  information on the web}, journal = {international journal of pattern  recognition and artificial intelligence}, pages = {527--549}, year = {2001},  number = {3}, volume = {15}, url =  {http://www.worldscinet.com/journals/ijprai/15/preserved-docs/1503/s021800140100099x.pdf},  abstract =
we describe latent dirichlet allocation (lda), a generative probabilistic model for collections of discrete data such as text corpora.
morgan kaufmann publishers, san francisco, us}, editor = {matthias jarke and michael j. carey and klaus r. dittrich and frederick h. lochovsky and pericles loucopoulos and manfred a. jeusfeld}, year = {1997}, address = {athens, gr}, pages = {446--455}, url = {http://www.vldb.org/conf/1997/p446.pdf}, note = {an extended version appears as~\cite{chakrabarti98c}}, abstract = {we explore how to organize a text database hierarchically to aid better searching and browsing.
in this paper, we describe a novel text classifier that can effectively cope with structured documents.
text categorization using adaptive context trees}, booktitle = {proceedings of cicling-01, 2nd international conference on computational linguistics and intelligent text processing}, year = {2001}, editor = {alexander gelbukh}, publisher = {springer verlag, heidelberg, de}, address = {mexico city, me}, note = {
published in the ``lecture notes in computer science'' series, number 1040}, url = {http://www.cs.utah.edu/~riloff/psfiles/ijcai-book-chapter.ps}, abstract = {
selforganizing classification on the reuters news  corpus}, booktitle = {proceedings of coling-02, the 19th international  conference on computational linguistics}, year = {2002}, editor = {}, pages =
"acm press", url =  "http://doi.acm.org/10.1145/1099554.1099713", abstract =
{edward a. fox and peter ingwersen and raya  fidel}, publisher = {acm press, new york, us}, year = {1995}, address =
in this article, we evaluate the retrieval performance of an algorithm that automatically categorizes medical documents.
moreover, the derived classifier reachs the performance  (about 85\%) of the best known models (i.e. support vector machines (svm) and  k-nearest neighbour (knn)) characterized by an higher computational complexity  for training and processing.}, } @inproceedings{basili01a, author = {
"2006", pages =  "228--237", } @inproceedings{schuetze:2006:ptp, author =  "hinrich schuetze and emre velipasaoglu and jan pedersen", title =  "performance thresholding in practical text classification",  booktitle =
however, support vector machines are so far considered special in that they have been demonstrated to achieve the state of the art performance.
in this paper, we extend the concept of linkages from  explicit hyperlinks to implicit links built between web pages.
noun homograph disambiguation using local context in large  corpora}, booktitle = {proceedings of the 7th annual conference of the  university of waterloo centre for the new oxford english dictionary}, publisher  = {}, editor = {}, year = {1991}, pages = {1--22}, address = {oxford, uk}, url  = {ftp://parcftp.xerox.com/pub/hearst/oed91.ps.gz}, abstract = {
dunja mladeni{\'{c}} and marko grobelnik}, title =
william w. cohen and haym hirsh}, year = {1994}, address = {new  brunswick, us}, pages = {148--156}, publisher =
the technique is then used with two randomized sample document groups drawn from nine categories.
three aspects are considered in the investigation: (i) a method for weighting terms based on the concept of a probability weighted amount of information, (ii) estimation of term occurrence probabilities using a probabilistic language model, and (iii) automatic extraction of terms based on pos tags automatically generated by a morphological analyzer.
diederich, joachim and kindermann,  j{\"{o}}rg and leopold, edda and paass, gerhard}, title = {authorship  attribution with support vector machines}, journal = {applied intelligence},  year = {2003}, volume = {19}, number = {1/2}, pages = {109--123}, url =  {http://ipsapp007.kluweronline.com/content/getfile/4504/36/6/abstract.htm},  abstract = {
{139--145}, url =  {http://www.acm.org/pubs/articles/proceedings/dl/313238/p139-lim/p139-lim.pdf},
in addition, nlp systems can increase the information contained  in keyword fields by separating keywords into segments, or distinct fields that  capture certain discriminating content or relations among keywords.
this paper presents a machine learning  approach to question classification.
information fusion almost always gives better results than the individual constituent feature sets, with certain combinations doing better than the others.}, } @inproceedings{davidov04, author = {dmitry davidov and evgeniy gabrilovich and shaul markovitch}, title = {parameterized generation of labeled datasets for text categorization based on a hierarchical directory}, booktitle = {proceedings of sigir-04, 27th acm international conference on research and development in information retrieval}, editor =
w. bruce croft and david j. harper and donald h. kraft and justin zobel}, publisher = {acm press, new york, us}, address = {new orleans, us}, year = {2001}, pages = {137--145}, url = {http://www.cs.cmu.edu/~yiming/papers.yy/sigir01.ps.gz}, abstract = {thresholding strategies in automated text categorization are an underexplored area of research.
experimental results  show that our system can effectively and efficiently classify chinese web  documents.}, } @inproceedings{zhou02, author = {shuigeng zhou and jihong guan},  title = {an approach to improve text classification efficiency}, booktitle =  {proceedings of adbis-02, 6th east-european conference on advances in databases  and information systems}, publisher = {springer verlag, heidelberg, de}, editor  = {yannis manolopoulos and pavol n{\'a}vrat}, year = {2002}, address =  {bratislava, sk}, pages = {65--79}, url =  {http://link.springer.de/link/service/series/0558/papers/2435/24350065.pdf},  abstract =
this note presents the corrected  results, along with additional data supporting the original claim that  uncertainty sampling has an advantage over relevance sampling in most training  situations.}, } @inproceedings{lewis95b, author = {david d. lewis}, title =  {the {trec-4} filtering track: description and analysis}, booktitle =  {proceedings of trec-4, 4th text retrieval conference}, publisher = {national  institute of standards and technology, gaithersburg, us}, editor =
for example, in hypertext  classification, the labels of linked pages are highly correlated.
experiments with syntactic phrase indexing, however, have never yielded significant improvements in text retrieval performance.
ripper, a machine learning rule induction algorithm.
these experiments suggest that stopword lists and stemming algorithms may remove or conflate many words that could be used to create more effective indexing terms.}, } @inproceedings{riloff96, author = {ellen riloff}, title = {using learned extraction patterns for text classification}, booktitle = {connectionist, statistical, and symbolic approaches to learning for natural language processing}, editor = {stefan wermter and ellen riloff and gabriele scheler}, pages = {275--289}, year = {1996}, publisher = {springer verlag, heidelberg, de}, note = {
idea group publishing},  address =
in this paper, we propose an unsupervised learning method to overcome these difficulties.
our approach is  especially suitable for applications of on-line text classification.}, }  @inproceedings{zhou02a, author = {shuigeng zhou and jihong guan}, title =
one difficulty in handling some classes of documents is the presence of different kinds of textual errors, such as spelling and grammatical errors in email, and character recognition errors in documents that come through ocr.
feature engineering for a symbolic approach to text classification}, school = {computer science department, university of ottawa}, address = {
"an  application of text categorization methods to gene ontology annotation",  pages =
"yunqing xia and angelo dalli and yorick wilks and louise guthrie", title = "{fasil} adaptive email categorization system", booktitle =
we  propose herein a new approach for automatic text categorization.
in an empirical study we compared representative sampling both with random sampling and with svm active learning.
} @inproceedings{whitelaw:2005:uag, author =  "casey whitelaw and navendu garg and shlomo argamon", title =
hamill, karen a. and zamora, antonio}, title = {
in contrast, programs that filter text streams, software that categorizes documents, agents which alert users, and many other ir systems must make decisions without human input or supervision.
among the three different combination approaches, our adaptive classifier combination method introduced here performed the best.}, } @inproceedings{li99, author = {
the pattern extraction is aimed at providing descriptions (in  the form of two logical expressions) of the two classes of positive and  negative examples.
this has encouraged interest in developing agents/softbots that can act as electronic personal assistants and can develop and adapt representations of users information needs, commonly known as profiles.
our  approach uses a natural language processing task called information extraction  as a basis for high-precision text classification.
skarmeta and amine bensaid and nadia tazi}, title = {data mining for text categorization with semi-supervised agglomerative hierarchical clustering}, journal =
{information storage and retrieval}, year = {1973}, volume = {9}, number = {4},  pages = {233--242}, url = {}, abstract = {a system of automatic indexing based  on bayes' theorem is described briefly.
our experiments show that the lower dimensional spaces computed by our algorithm consistently improve the performance of traditional algorithms such as c4.5, k-nearest-neighbor, and support vector machines (svm), by an average of 2\% to 7\%.
a central problem in information retrieval is the automated classification of text documents.
{olivier y. {de vel} and alison anderson and malcolm corney and george m. mohay}, title = {mining email content for author identification forensics}, journal = {sigmod record}, year = {2001}, volume = {30}, number = {4}, pages = {55--64}, url = {}, abstract = {
% % % %  % % a bibliography on automated text categorization %
{acm press, new york, us}, address = {pittsburgh, us}, pages = {22--34}, year =
we verify experimentally that  the integration of wordnet helps ssfcm improve its performance, effectively  addresses the classification of documents into categories with few training  documents and does not interfere with the use of training data.}, }  @article{benkhalifa01a, author = {mohammed benkhalifa and abdelhak mouradi and  houssaine bouyakhf}, title = {
this paper presents the infoclas system applying statistical  methods of information retrieval for the classification of german business  letters into corresponding message types such as order, offer, enclosure, etc.
the learning algorithm combines an adaptive phase which instantly updates dictionary and weights during interaction and a tuning phase which fine tunes for performance using previously seen data.
we use a training set of manually categorized documents to learn word-category associations, and use these associations to predict the categories of arbitrary documents.
the self-organizing map model is  used to generate two maps, namely the word cluster map and the document cluster  map, in which a neuron represents a cluster of words and documents  respectively.
mh$^kr$}, an improved boosting algorithm, and its application to text categorization.
maui, us}, pages = {}, url = {http://dlib.computer.org/conferen/hicss/0981/pdf/09817061.pdf}, abstract = {with rapid expansion of the numbers and sizes of text repositories and improvements in global connectivity, the quantity of information available online as free-format text is growing exponentially.
the  resulting system was adopted by the main italian financial news agency  providing a pay-to-view service.}, } @inproceedings{clack97, author = {chris  clack and johnny farringdon and peter lidwell and tina yu}, title = {autonomous  document classification for business}, editor = {w. lewis johnson}, publisher =
text mining applies the same analytical functions of data mining to  the domain of textual information, relying on sophisticated text analysis  techniques that distill information from free-text documents.
however, on these tasks, rocchio runs much faster than adaboost.}, } @inproceedings{scheffer99, author = {tobias scheffer and thorsten joachims}, title = {expected error analysis for model selection}, booktitle = {proceedings of icml-99, 16th international conference on machine learning}, editor = {ivan bratko and saso dzeroski}, year = {1999}, address = {bled, sl}, publisher = {
the paper shows how a text classifier's need for labeled training documents can be reduced by taking advantage of a large pool of unlabeled documents.
we conclude that, in general, more accurate  hierarchical categorisation is possible by using our simple feature selection  technique.}, } @inproceedings{wiener95, author = {erik d. wiener and jan o.  pedersen and andreas s. weigend}, title = {
by referencing various relationships in the thesaurus corresponding to the structured categories, k-nn can be prominently improved, removing the ambiguity.
morgan kaufmann publishers,  san francisco, us}, year = {1999}, pages = {674--679}, address = {stockholm,  se}, url = {http://www.iit.demokritos.gr/~paliourg/papers/ijcai99.ps.gz},  abstract = {word sense disambiguation (wsd) is the process of distinguishing  between different senses of a word.
this paper describes an approach to feature subset selection that takes into  account problem specifics and learning algorithm characteristics.
our approach is based on a new  and improved family of boosting algorithms.
a chinese documents classification system  following above described techniques is implemented with naive bayes, knn and  hierarchical classification methods.
{609--614}, year = {1999}, url = {}, abstract = {
there are two frequently used approaches to the development of intelligent agents using machine learning techniques: a content-based and a collaborative approach.
{ieee computer society press, los alamitos, us},  editor = {}, year = {1990}, address = {santa barbara, us}, pages = {320--326},  url = {}, abstract = {
it analyzes the particular properties of learning  with text data and identifies why svms are appropriate for this task.
the problem is not easy to tackle due to the semi-structured or even unstructured nature of those texts under consideration.
"dou shen and jian-tao sun and qiang yang and zheng chen", title =
an appraisal group is represented as a set of attribute values in several task-independent semantic taxonomies, based on appraisal theory.
southampton, uk}, url =  {http://www.math.unipd.it/~fabseb60//publications/tm05.pdf}, abstract = {
we compare pagetypesearch using the document type-indices with a conventional keyword-based search system in experiments.
{georges gardarin and james c. french and niki pissinou and kia makki and luc bouganim}, year = {1998}, address = {bethesda, us}, pages = {109--113}, url = {http://www.acm.org/pubs/articles/proceedings/cikm/319950/p109-matsuda/p109-matsuda.pdf}, abstract = {this paper proposes a novel approach to accurately searching web pages for relevant information in problem solving by specifying a web document category instead of the user's task.
{37--78}, year = {2007}, month =  {february} } @article{koppel:2007:mdu, author = {
in this  paper we describe and evaluate a learning model for information filtering which  is an adaptation of the generalised probabilistic model of information  retrieval.
svms have been  trained on data sets with several thousand instances, but web directories today  contain millions of instances which are valuable for mapping billions of web  pages into yahoo!-like directories.
we consider a family of models that take  into account the fact that relevant documents may contain irrelevant passages;  the originality of the model is that it does not explicitly segment documents  but rather considers all possible segmentations in its final score.
in comparison with other machine-learning techniques, results on a key benchmark from the reuters collection show a large gain in performance, from a previously reported 67\% recall/precision breakeven point to 80.5\%.
the  results show that a significant improvement can be derived using the proposed  inference model.
the proceedings of a conference % required: title, year %  optional: editor, volume or number, series, address, month, % organization,  publisher, note % %
on a text categorization task, using the reuters-22,173 collection, we give empirical evidence that feature selection is useful: first, the size of the collection index can be drastically reduced without causing a significant loss in categorization effectiveness.
finally, the naive bayesian filter is compared, in terms of performance, to a filter that uses keyword patterns, and which is part of a widely used e-mail reader.}, } @article{appiani01, author = {enrico appiani and francesca cesarini and annamaria colla and massimiliano diligenti and marco
the paper describes the kdt system  for knowledge discovery in texts.
we describe how it is applied to generate two new classification algorithms; a refined centroid classifier and a refined naïve bayes classifier.
our techniques considerably improve the robustness and accuracy of  the classification outcome, as shown in systematic experimental comparisons  with previously published methods on three different real-world datasets."
to accomplish this task, each substantive word in a text is first categorized using a feature set based on the semantic subject field codes (sfcs) assigned to individual word senses in a machine-readable dictionary.
"17", pages = "1223--1232", url = "http://doi.ieeecomputersociety.org/10.1109/tkde.2005.149", abstract =
the paper demonstrates good performance of context-group discrimination for a sample of natural and artificial ambiguous words.}, } @mastersthesis{scott98, author = {sam scott}, title = {
"pappuswamy, umarani and bhembe, dumisizwe and jordan, pamela w.  and vanlehn, kurt", title =
w. bruce croft and van rijsbergen, cornelis j.}, publisher = {springer verlag, heidelberg, de}, address
the construction steps often involve human efforts and are  not completely automated.
{mohammed benkhalifa and abdelhak mouradi and houssaine bouyakhf}, title =  {integrating external knowledge to supplement training data in semi-supervised  learning for text categorization}, journal = {information retrieval}, number =  {2}, volume = {4}, pages = {91--113}, year = {2001}, url =  {http://www.wkap.nl/article.pdf?351286}, abstract = {
this paper describes a unique example-based mapping method for  document retrieval.
finally, we propose a  natural extension of idc for (semi-supervised) transductive learning where we  are given both labeled and unlabeled examples, and present preliminary  empirical results showing the plausibility of the extended method in a  semi-supervised setting.}, } @inproceedings{escudero00, author = {gerard  escudero and llu{\'{\i}}s m{\`{a}}rquez and german rigau}, title = {
% % % % this bibliography resides at % % http://www.cs.technion.ac.il/~gabr/resources/atc/atcbibliography.bib % %
{125--132}, address = {faro, pt}, url =
phrases, word senses and syntactic relations derived by natural language processing (nlp) techniques were observed ineffective to increase retrieval accuracy.
a  simple feature selection method for text classification}, booktitle =
using hcl, a hierarchical classification method can be materialized easily with the help of a method generator system.}, } @inproceedings{sun03b, author = {aixin sun and ee-peng lim}, title = {web unit mining: finding and classifying subgraphs of web pages}, booktitle = {proceedings of cikm-03, 12th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, editor = {}, year = {2003}, address = {new orleans, us}, pages = {108--115}, url = {http://doi.acm.org/10.1145/956863.956885}, abstract = {
at their core, our algorithms employ recently developed modified finite newton techniques.
a framework for filtering news and managing distributed data}, journal = {journal of universal computer science}, year = {1997}, number = {8}, volume = {3}, pages = {1007--1021}, url = {http://www.jucs.org/jucs_3_8/a_framework_for_filtering}, abstract = {with the development and diffusion of the internet worldwide connection, a large amount of information is available to the users.
{american society for information  science, washington, us}, year = {1997}, address = {washington, us}, pages =
we found small advantages in  accuracy for hierarchical models over flat models.
we are particularly interested in domain transfer: how  well the learned classifiers generalize from the training corpus to a new  document corpus.
profile is a  filtering system for the netnews which uses this model with a scale of 11  predefined values of relevance.
latent semantic indexing (lsi) has been successfully used for ir purposes, as a technique for capturing semantic relations between terms and inserting them into the similarity measure between two documents.
these were applied to seven-class yahoo  news groups (business, entertainment, health, international, politics, sports  and technology) individually and in combination, we studied three classifier  combination approaches: simple voting, dynamic classifier selection and  adaptive classifier combination.
since there is a difference between important sentences and unimportant sentences in a document, the features from more important sentences should be considered more than other features.
"zhang, tong and popescul,  alexandrin and dom, byron", title =
such a scheme has several potential  advantages because it does not require any pre-processing of the input text.
given these inputs, the  system learns to extract information from other pages and hyperlinks on the  web.
{melbourne, au}, pages = {215--223}, url =  {http://www.research.att.com/~schapire/cgi-bin/uncompress-papers/schapiresisi98.ps},
one is  synonym-based and the other is statistics based.
experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classification hierarchies consisting of hundreds of nodes.
the determination  of category themes and their hierarchical structures were most done by human  experts.
in this paper an original classification model sensitive to document syntactic information and characterized by a novel inference method is described.
the chi(2) test, which is sometimes used for selecting terms that are highly related to a text class, is applied in a novel way when constructing a category weight vector.
{acm press, new york, us}, editor = {}, year = {2003}, address = {new orleans,  us}, pages = {108--115}, url = {http://doi.acm.org/10.1145/956863.956885},  abstract = {
we show how to train these models effectively, and how to use approximate probabilistic inference over the learned model for collective classification of multiple related entities.
the multi-classifier approach helps us leverage all  the relevant textual features and meta data, and appears to generalize to  related classification tasks.}, } @inproceedings{amati96, author = {gianni  amati and daniela d'aloisi and vittorio giannini and flavio ubaldini}, title =
this  paper looks at a simplified version of the problem: classifying online product  reviews into positive and negative classes.
we present a system  for searching and classifying u.s. patent documents, based on inquery.
{arkadi kosmynin and ian davidson}, title = {using background contextual  knowledge for documents representation}, booktitle = {proceedings of podp-96,  3rd international workshop on principles of document processing}, editor =
"feng he and xiaoqing ding", title =
idea group publishing}, address =
results obtained from evaluation show that the integration of wordnet clearly outperforms training approaches, and that an integrated technique can effectively address the classification of low frequency categories.}, } @inproceedings{delima98, author = {de lima, luciano r. and laender, alberto h. and ribeiro-neto, berthier a.}, title = {
"wei-hao lin and alexander hauptmann", title = "are these documents written from different perspectives?
we present the results of a number of experiments designed to evaluate the effectiveness and behavior of different compression-based text classification methods on english text.
"d. tikk and {gy.} biro", title = "experiment with a hierarchical text categorization method on the {wipo} patent collection", booktitle = "proc. of the 4th int. symp.
the results show that our approach outperformed the bayesian independence classifier as measured by a metric that combines precision and recall measures.}, } @inproceedings{lam98, author = {wai lam and chao y. ho}, title = {using a generalized instance set for automatic text categorization}, booktitle = {proceedings of sigir-98, 21st acm international conference on research and development in information retrieval}, editor = {
in this paper, we propose a new method of text categorization based on feature space restructuring for svms.
we report on experiments with different kernels for both of these implementations and with different representations of the data, including binary vectors, tf-idf representation and a modification called ``hadamard" representation.
{rakesh agrawal and ramakrishnan srikant}, title = {
although  text categorization is a burgeoning area of ir research, readily available test  collections in this field are surprisingly scarce.
three types of term distributions, called inter-class, intra-class and in-collection distributions, are introduced.
in an effort to deal more effectively with this large vocabulary and improve information processing, a method of focus has been developed which allows one to classify terms based on a measure of their importance in describing the content of the documents in which they occur.
it allows an effective exploitation of the available linguistic information that better emphasizes this latter with significant both data compression and accuracy.
in the research community the dominant approach to this problem  is based on machine learning techniques: a general inductive process  automatically builds a classifier by learning, from a set of preclassified  documents, the characteristics of the categories.
this  filtering process is a word sense disambiguation task.
"context and learning in novelty  detection", booktitle = "emnlp'05", year = "2005",  abstract =
one  difficulty in handling some classes of documents is the presence of different  kinds of textual errors, such as spelling and grammatical errors in email, and  character recognition errors in documents that come through ocr.
claire n{\'{e}}dellec and c{\'{e}}line rouveirol}, address =
knowledge discovery is performed by analyzing the  co-occurrence frequencies of keywords from this hierarchy in the various  documents.
with the increasing availability of lexical resources in electronic form (including lexical databases (ldbs), machine readable dictionaries, etc.), there is an interesting opportunity for the integration of them in learning-based atc.
however, the svm methods turned out to be quite sensitive to the choice of  representation and kernel in ways which are not well understood; therefore, for  the time being leaving the neural network approach as the most robust.}, }  @inbook{manning99a, author = {christopher manning and hinrich  sch{\"{u}}tze}, title = {
mh$^r$} algorithm.}, } @article{sebastiani02, author = {fabrizio sebastiani}, title = {machine learning in automated text categorization}, journal = {acm computing surveys}, volume = {34}, number = {1}, pages = {1--47}, year = {2002}, url = {http://www.math.unipd.it/~fabseb60/publications/acmcs02.pdf}, abstract = {
editor = {}, address = {kyoto, jp}, year = {1994}, pages = {1059--1063}, url = {}, abstract = {}, } @incollection{guthrie99, author = {louise guthrie and joe a. guthrie and james leistensnider}, title = {document classification and routing}, booktitle = {natural language information retrieval}, editor = {tomek strzalkowski}, year = {1999}, pages = {289--310}, publisher =
text-classification methods have thus far not easily  incorporated numerical features.
our  approach combines standard information retrieval methods with a text  categorization meta-learning scheme that determines when to even venture a  guess." } @inproceedings{dayanik:2006:cip, author =
annual meeting of the american society for information science}, publisher = {american society for information science, washington, us}, editor
"rome, italy" } @inproceedings{li:2007:sts, author =
both textual and non-textual information associated with the projects are used in the learning and classification phases.
in this  paper we experiment with two different learning techniques, one based on  na\"{\i}ve bayesian classification and the other one based on multiclass  support vector machines, and test the resulting framework on a corpus of social  surveys.
first, undirected models do not impose  the acyclicity constraint that hinders representation of many important  relational dependencies in directed models.
we augment naive bayes models with statistical n-gram  language models to address short-comings of the standard naive bayes text  classifier.
nevertheless, hierarchical  text classi- fication methods in the past have often been constructed in a  proprietary manner.
we consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative.
[stanfill] (a text retrieval system that supports relevance feedback) as the  underlying match engine, codes are assigned to new, unseen stories with a  recall of about 80\% and precision of about 70\%.
so far,  documents have been classified according to their contents manually.
the stepwise feature selection in the decision tree algorithm is particularly effective in dealing with the large feature sets common in text categorization.
title = {a new pairwise ensemble approach for text classification}, booktitle =  {proceedings of ecml-03, 14th european conference on machine learning},  publisher = {}, editor = {}, year = {2003}, address
especially the vagueness of spatial and temporal terms needs to be addressed.}  } @article{kazama:2004:mem, author = {junichi kazama and junichi tsujii}, title  = {maximum entropy models with inequality constraints: a case study on text  categorization}, journal = {machine learning}, publisher = {springer science},
there are different research questions important for the development of text-learning intelligent agents.
this paper studies the ability of symbolic learning algorithms to perform a text categorization task.
no systematic study, however, has been done on their relative merits.
we suggest that one (or a collection) of names of {{\sc yahoo!}}\  (or any other www indexer's) categories can be used to describe the content of  a document.
each group in turn is used as the basis for indexing the  other.
ran el-yaniv and oren  souroujon}, title
www indices, like {{\sc yahoo!}}\ provide a huge hierarchy of categories (topics) that touch every aspect of human endeavors.
{2003}, pages = {335--350}, url =  {http://link.springer.de/link/service/series/0558/papers/2633/26330335.pdf},  abstract = {we augment the naive bayes model with an n-gram language model to  address two shortcomings of naive bayes text classifiers.
our method is unique in  that decision lists are automatically constructed on the basis of the principle  of minimizing extended stochastic complexity (esc), and with it we are able to  construct decision lists that have fewer errors in classification.
text collections in different domains were used for evaluation.
isabelle moulinier}, title = {feature selection: a useful preprocessing step},
in this paper, we show how inverted indexes can be used for scalable training in categorisation, and propose novel heuristics for a fast, accurate, and memory efficient approach.
"emotions from text: machine learning for text-based emotion prediction", booktitle = "emnlp'05", year = "2005" } @inproceedings{schiffman:2005:cln, author =
morgan kaufmann publishers, san francisco, us}, url = {http://www.cs.technion.ac.il/~gabr/papers/fs-svm.pdf}, abstract = {
our results show simple windows are best for all test collections tested in these experiments.
"proceedings of  the 29th annual international acm sigir conference on research and development  in information retrieval", year =
in the literature, many feature types are proposed for document  classification.
their only potential drawback is their training time and memory requirement.
extensive experiments have been conducted on two common document corpora, namely the ohsumed collection and the reuters-21578 collection.
a user study compared our new category interface with the typical ranked list interface of search results.
using the reuters collection, we show that adaptive resampling techniques can improve decision-tree performance and that relatively small, pooled local dictionaries are effective.
the utilized concepts are automatically  extracted from documents via probabilistic latent semantic analysis.
a fuzzy measure of agreement between machine and manual assignment of documents to subject categories}, booktitle = {proceedings of asis-83, 46th annual meeting of the american society for information science}, publisher =
"february", address =
we attempt to explain through experiments what factors contribute to the improvement.}, } @inproceedings{ontrup01, author = {j{\"{o}}rg ontrup and helge ritter}, title =
standard term clustering strategies from information retrieval (ir),  based on cooccurence of indexing terms in documents or groups of documents,  were tested on a syntactic indexing phrase representation.
moreover, the number of irrelevant references gathered by our system is about one-thirteenth that of traditional keyword-based search systems.
{a theory of relevance for automatic document classification}, year = {1973},  journal = {information and control}, volume = {22}, number =
we provide experimental results demonstrating that the approach can significantly improve performance, and that it does not impair it.}, } @inproceedings{dagan96, author = {dagan, ido and feldman, ronen and hirsh, haym}, title = {keyword-based browsing and analysis of large document sets}, booktitle = {proceedings of sdair-96, 5th annual symposium on document analysis and information retrieval}, publisher = {}, editor = {}, year = {1996}, address = {las vegas, us}, pages = {191--207}, url = {}, abstract = {knowledge discovery in databases (kdd) focuses on the computerized exploration of large amounts of data and on the discovery of interesting patterns within them.
we  focus on three of them: what representation is used for documents, how is the  high number of features dealt with and which learning algorithm is used.
in order to classify text  documents, we must extract good features from them.
an extensive empirical  study of feature selection metrics for text classification}, journal = {journal  of machine learning research}, volume = {3}, month = {march}, pages =
if term occurrences are random then there will be no correlation and the strength will be zero, but if for any subject, the term is either always present or never present its strength will be one.
a work that is printed and bound, but without a named publisher % or  sponsoring institution % required: title % optional: author, howpublished,  address, month, year, note % %
in this paper, we therefore propose new performance measures for hierarchical classification.
in this tutorial we look at the main approaches that have been taken towards automatic text categorisation within the general machine learning paradigm.
we  describe and compare different sets of experiments.
"juho rousu and craig saunders and sandor szedmak and john  shawe-taylor", title =
{feature selection on hierarchy of  web documents}, journal = {decision support systems}, year = {2003}, number =
{proceedings of ictai-01, 13th ieee international conference on tools with  artificial intelligence}, publisher = {ieee computer society press, los  alamitos, us}, editor = {}, year = {2001}, pages = {265--272}, address =
this method, which we call uncertainty sampling, reduced by as much as 500-fold the amount of training data that would have to be manually classified to achieve a given level of effectiveness.}, } @article{lewis94b, author = {lewis, david d. and philip j. hayes}, title = {guest editors' introduction to the special issue on text categorization}, journal = {acm transactions on information systems}, volume = {12}, number = {3}, pages = {231}, year = {1994}, } @inproceedings{lewis94c, author = {lewis, david d. and jason catlett}, title = {heterogeneous uncertainty sampling for supervised learning}, booktitle = {proceedings of icml-94, 11th international conference on machine learning}, editor =
we  study the problem of mapping a search engine query to those nodes of a given  subject taxonomy that characterize its most likely meanings.
without a doubt, updating the  classification model frequently rather than using the old model for a very long  period is absolutely essential.
we explore modifications to the  document representation in a vector space-based ned system.
an experimental evaluation of ocr text representations for  learning document classifiers}, journal = {international journal on document  analysis and recognition}, pages = {116--122}, year = {1998}, number = {2},  volume = {1}, url =  {http://link.springer.de/link/service/journals/10032/papers/8001002/80010116.ps.gz},  abstract = {
thus, they do not fully make use of the weight information provided  by standard term weighting methods.
the goal of the work  described in this paper is to automatically categorize web documents in order  to enable effective retrieval of web information.
year = {1998}, address = {bethesda, us}, pages = {148--155}, url = {http://robotics.stanford.edu/users/sahami/papers-dir/cikm98.pdf}, abstract = {text categorization - the assignment of natural language texts to one or more predefined categories based on their content - is an important component in many information organization and management tasks.
{proceedings of ecml-04, 15th european conference on machine learning}, editor  = {jean-fran{\c{c}}ois boulicaut and floriana esposito and fosca giannotti and  dino pedreschi}, address =
we evaluate empirically a scheme  for combining classifiers, known as stacked generalization, in the context of  anti-spam filtering, a novel cost-sensitive application of text categorization.
european  colloquium on information retrieval research}, editor = {}, year = {2001},  address = {darmstadt, de}, publisher = {}, pages = {126--135}, url =  {http://www-connex.lip6.fr/~denoyer/publications/denoyer-final-ecir01.ps},  abstract = {
all three  algorithms achieved high precision on both test sets, with the augmented  relevancy signatures algorithm and the case-based algorithm reaching 100\%  precision with over 60\% recall on one set.
finally, the conditions of the application  as well as problems of further development are discussed.}, }  @inproceedings{bigi03, author = {brigitte bigi}, title = {using  kullback-leibler distance for text categorization}, booktitle = {proceedings of  ecir-03, 25th european conference on information retrieval}, publisher =
the classifiers and regression equations were then  applied to a new set of essays.
{franca debole and fabrizio sebastiani}, title = {an analysis of the relative  hardness of reuters-21578 subsets}, year = {2004}, booktitle = {proceedings of  lrec-04, 4th international conference on language resources and evaluation},  address = {lisbon, pt}, pages = {971--974}, url =  {http://www.math.unipd.it/~fabseb60/publications/lrec04.pdf}, }  @article{debole05, author = {franca debole and fabrizio sebastiani}, title =  {an analysis of the relative hardness of reuters-21578 subsets}, journal =
"the effect of ocr errors on stylistic text classification", booktitle =
it is evident in this study that automated word removal based on corpus statistics has a practical and significant impact on the computational tractability of categorization methods in large databases.}, } @inproceedings{yang96a, author = {yiming yang}, title = {
"2006", month = "april", address =
we improve a high-accuracy maximum entropy classifier by combining  an ensemble of classifiers with neural network voting.
the experiments were  conducted on the standard reuters data set.
} @inproceedings{soucy:2005:btw, author =  "pascal soucy and guy mineau", title =
the comparison of the examined models demonstrates that techniques from information retrieval integrated into recurrent plausibility networks performed well even under noise and for different corpora.}, } @inproceedings{wermter02, author = {stefan wermter and chihli hung}, title = {
the system has been applied to the following tasks: presorting of forms, reports and letters, index extraction for archiving and retrieval, page type classification and text column analysis of real estate register documents, in-house mail sorting and electronic distribution to departments.
and then, it uses the categorized sentences for training.
the result of  learning is a set of independent classifiers, each used to predict probability  that a new example is a member of the corresponding category.
amherst, us}, year = {1992}, url =  {http://www.research.att.com/~lewis/papers/lewis91d.ps}, abstract = {
morgan kaufmann  publishers, san francisco, us}, url =  {http://robotics.stanford.edu/users/sahami/papers-dir/ml97-hier.ps}, abstract =  {the proliferation of topic hierarchies for text documents has resulted in a  need for tools that automatically classify new documents within such  hierarchies.
these methods can greatly reduce the number of instances that an expert need  label.
oh-woog kwon and sung-hwa jung and jong-hyeok lee and geunbae lee}, title = {evaluation of category features and text structural information on a text categorization using memory based reasoning}, booktitle = {proceedings of iccpol-99, 18th international conference on computer processing of oriental languages}, editor = {}, year = {1999}, address = {tokushima, jp}, pages = {153--158}, url = {}, abstract = {}, } @inproceedings{labrou99, author = {yannis labrou and tim finin}, title = {{{\sc yahoo!}} as an ontology: using {{\sc yahoo!}}\ categories to describe documents},
in this paper, we study the properties of phrasal and clustered indexing languages on a text categorization task, enabling us to study their properties in isolation from query interpretation issues.
{yiming yang and  john w. wilbur}, title = {an analysis of statistical term strength and its use  in the indexing and retrieval of molecular biology texts}, journal = {computers  in biology and medicine}, year = {1996}, volume = {26}, number = {3}, pages =
in this paper, we suggest, for text categorization, the integration  of external wordnet lexical information to supplement training data for a  semi-supervised clustering algorithm which can learn from both training and  test documents to classify new unseen documents.
we thereby treat the problem of classifying documents as that of conducting statistical hypothesis testing over finite mixture models.
"proceedings of the twelfth  acm sigkdd international conference on knowledge discovery and data  mining", year =
by allowing this interactivity in the clustering  process, c-evolve achieves considerably higher clustering accuracy (10 to 20\%  absolute increase in our experiments) than the popular k-means and  agglomerative clustering methods.}, } @inproceedings{agrawal01, author =
the filtering  engine memorizes both user preferences and past situations.
the results we obtain allow us to determine the relative hardness of these subsets, thus establishing an indirect means for comparing tc systems that have, or will be, tested on these different subsets.}, } @inproceedings{debuenaga97, author = {
{association for computational linguistics, morristown, us}, editor = {lillian  lee and donna harman}, pages = {51--57}, address = {pittsburgh, us}, url =  {http://www.cs.cornell.edu/home/llee/emnlp/papers/takamura.pdf}, abstract = {
we show how a naive bayes classification can be enhanced to incorporate the similarity information present in source catalogs.
morgan kaufmann publishers, san  francisco, us}, url =  {http://www.cs.cmu.edu/~yiming/papers.yy/hypertext-icml01.ps.gz}, abstract =  {hypertext poses new text classification research challenges as hyperlinks,  content of linked documents, and meta data about related web sites all provide  richer sources of information for hypertext classification that are not  available in traditional text classification.
combined with the  classification power of an svm, this method yields high performance text  categorization that can outperform other recent methods in terms of  categorization accuracy and representation efficiency.
this basic em procedure works well when the data conform to the generative assumptions of the model.
"a comparative study of citations and links in document classification", booktitle =
we first describe the text representation method we use in  this work; we then present a feature selection method that is used to reduce  the dimensionality of the feature space.
"2006", pages = "539--542" } @inproceedings{fu:2005:atc, author =
more specifically, it compares  standard multinomial naive bayes to the recently proposed transformed  weight-normalized complement naive bayes classifier (twcnb)
in this paper, we introduce hyperlink ensembles, a novel type of ensemble classifier for classifying hypertext documents.
however, an open question was whether these  extraction patterns were useful for tasks other than information extraction.
this level of complexity makes  an ``off-the-shelf'' database-management and information-retrieval solution  impossible.
however, an extensive and systematic evaluation of the various  approaches has not yet been done.
while this weighting  method seems very appropriate for ir, it is not clear that it is the best  choice for tc problems.
an automatic document classification system using pattern  recognition techniques}, booktitle = {proceedings of asis-78, 41st
we demonstrate that the classifiers perform 10-15\% better than relevance feedback via rocchio expansion for the trec-2 and trec-3 routing tasks.
toward optimal feature selection.
when using a state-of-the-art classifier, knn, the average accuracy is 96.40\%, outperforming all the other systems evaluated on the same collection, including the traditional term-word by knn (88.52\%); sleeping-experts (82.22\%); sparse phrase by four-word sleeping-experts (86.34\%); and boolean combinations of words by ripper (87.54\%).
{fabio crestani and mark girolami and van rijsbergen, cornelis j.}, year =
these structures can then  be applied to individual documents to derive a posterior probability that the  document is about a particular target concept.}, } @inproceedings{toutanova01,  author =
"proceedings of the eighteenth national conference on artificial intelligence", year =
{ieee computer society press, los alamitos, us}, editor = {}, year = {2001}, pages = {265--272}, address = {dallas, us}, url = {}, abstract = {methods for taking into account linguistic content into text retrieval are receiving a growing attention
the high  number of features is reduced by taking into account the hierarchical structure  and using feature subset selection based on the method known from information  retrieval.
traditionally, the categories are arranged in hierarchical manner to achieve effective searching and indexing, as well as easy comprehension for humans.
much of the diversity resides in its technical  terminology, which has also become voluminous.
"email classification for automated service  handling", booktitle =
morgan kaufmann publishers, san francisco, us}, address = {san  diego, us}, pages = {245--255}, year = {1991}, url =  {http://www.research.att.com/~lewis/papers/lewis91c.ps}, abstract = {
while it is easy to collect the unlabeled documents, it is not so easy to manually categorize them for creating training documents.
however, many popular feature selection techniques such as information gain (ig) and $\chi^2$-test (chi) are all greedy in nature and thus may not be optimal according to some criterion.
as we show, each of these smaller problems can be solved accurately by focusing only on a very small set of features, those relevant to the task at hand.
each news web page is represented by the term-weighting scheme.
then we study the problem of combining it with a standard  bag of words kernel.
we show that even on purely numerical-valued data the results of text-classification on the derived text-like representation outperforms the more naive numbers-as-tokens representation and, more importantly, is competitive with mature numerical classification methods such as c4.5 and ripper.}, } @article{macskassy03, author =
a document includes informative keywords and non-informative keywords.
performance can be significantly improved by using active learning to select high-quality initializations, and by using alternatives to em that avoid low-probability local maxima.}, } @inproceedings{nigam98, author = {kamal nigam and andrew k. mccallum and sebastian thrun and tom m. mitchell}, title = {
"quantifying trends accurately despite classifier  error and class imbalance", booktitle =
historically, several attempts have been made to automate this process, using various stock phrases as the features on which to base the classification.
} @inproceedings{ginter:2005:dcu, author =  "filip ginter and sampo pyysalo and tapio salakoski", title =  "document classification using semantic networks with an adaptive  similarity measure", booktitle =
once the optimal number of is selected, for eac ter, the procedure is repeated.
text categorization for multiple users  based on semantic features from a machine-readable dictionary}, journal = {acm  transactions on information systems}, year = {1994}, number = {3}, volume =
it also outperforms the popular svm method in micro-averaging f1.
unlike conventional approaches to learning text classifiers, which rely primarily on empirical evidence, this model explains why and when svms perform well for text classification.
} @inproceedings{gabrilovich:2005:odp, author =  "gabrilovich, evgeniy and markovitch, shaul", title = "feature  generation for text categorization using world knowledge", pages =  "1048--1053", booktitle =
"hang cui and vibhu mittal and mayur datar", title =
abstract = {automatic categorization of multimedia documents is an important  function for a digital library system.
second, they permit straightforward application of sophisticated smoothing techniques from statistical language modeling, which allows one to obtain better parameter estimates than the standard laplace smoothing used in naive bayes classification.
categorization of text is of increasing  importance in information retrieval and natural language processing systems.
we believe this approach is effective in reducing the development time to implement classification systems involving large number of topics for the purpose of classification, message routing etc.}, } @incollection{masand94, author = {briji masand}, title = {optimising confidence of text classification by evolution of symbolic expressions}, booktitle = {advances in genetic programming}, publisher = {
we describe a procedure for  generating a hierarchy of classifiers that model the hierarchy structure.
usually, only about 1\%-10\% of examples  belong to the selected category.
automatic text classification is needed to store documents like that.
* publications thet discuss related  topics sometimes confused with % % atc; these include, in particular, text  clustering (i.e. text % % classification by unsupervised learning) and text  indexing; % % % % * technical reports and workshop papers.
in this paper, we propose an unsupervised  learning method to overcome these difficulties.
the recognition of proper nouns (pns) is considered an important task in the area of information retrieval and extraction.
feature  feedback can complement traditional active learning in applications such as  news filtering, e-mail classification, and personalization, where the human  teacher can have significant knowledge on the relevance of features.}, }  @inproceedings{forman:2006:tcd, author =
while text categorization has received much attentions by ir researchers, classification of visual data is at its infancy stage.
the hypothesis that cdm`s  performance exceeds two non-domain specific algorithms, bayesian classification  and decision tree learners, is empirically tested.}, } @article{goldberg96,  author = {goldberg, jeffrey l.}, title = {cdm: an approach to learning in text  categorization}, journal = {international journal on artificial intelligence  tools}, year = {1996}, number = {1/2}, volume = {5}, pages = {229--253}, url =  {}, abstract = {
"elena monta{\~n}{\'e}s and irene d{\'i}az and jos{\'e} ranilla and el{\'i}as f. combarro and javier fern{\'a}ndez", journal =
{ieee computer society press, los alamitos, us}, volume  = {1}, url = {http://panda.cs.binghamton.edu/~meng/pub.d/wise00.doc}, abstract  = {document categorization, as a technique to improve the retrieval of useful  documents, has been extensively investigated.
finally, we apply the ssahc algorithm to the reuters database of documents and show that its performance is superior to the bayes classifier and to the expectation-maximization algorithm combined with bayes classifier.
we  also show that, surprisingly, the addition of deep linguistic analysis features  to a set of surface level word n-gram features contributes consistently to  classification accuracy in this domain."
{morgan kaufmann  publishers, san francisco, us}, year = {1997}, pages = {764--769}, address =
recurrent neural network learning for text routing}, booktitle = {proceedings of icann-99, 9th international conference on artificial neural networks}, publisher
we showed also that ssahc helps ahc techniques to improve their performance.}, } @inproceedings{slattery00, author = {se{\'{a}}n slattery and mark craven}, title = {discovering test set regularities in relational domains}, booktitle = {proceedings of icml-00, 17th international conference on machine learning}, editor = {pat langley}, year = {2000}, address = {stanford, us}, pages = {895--902}, publisher = {morgan kaufmann publishers, san francisco, us}, url = {http://www.cs.cmu.edu/~sean/papers/icml2000.ps}, abstract = {machine learning typically involves discovering regularities in a training set, then applying these learned regularities to classify objects in a test set.
then the system computes a profile for a particular document  that is to be classified.
the system is small, fast and robust.
we investigate the potential reasons for this behavior and relate it to structural differences between the datasets.}, } @inproceedings{bel03, author = {nuria bel and cornelis h. koster and marta villegas}, title = {cross-lingual text categorization}, booktitle = {proceedings of ecdl-03, 7th european conference on research and advanced technology for digital libraries}, editor = {traugott koch and torvik s{\o}lvberg, ingeborg}, publisher = {springer verlag, heidelberg, de}, note = {
"elena monta{\~n}{\'e}s and irene d{\'i}az  and jos{\'e} ranilla and el{\'i}as f. combarro and javier fern{\'a}ndez",  journal =
we evaluate the algorithms on the basis of two test sets from the muc-4 corpus.
foundations of statistical natural language  processing}, publisher = {the mit press}, address = {cambridge, us}, year =  {1999}, chapter = {16:
we then present a fast, divisive algorithm that monotonically decreases this  objective function value, thus converging to a local minimum.
text  categorization is the procedure of assigning a category to a particular  document among predefined categories.
the authors found that topic  identification performance was maintained or slightly improved using character  shape codes derived from images.}, } @article{stamatatos00, author =
european colloquium on information retrieval research}, editor = {}, year = {2001}, address = {darmstadt, de}, publisher = {}, pages = {24--40}, url = {http://cis.paisley.ac.uk/vino-ci0/fisher_hierarchic.ps}, abstract = {this paper demonstrates that the probabilistic corpus model which emerges from the automatic or unsupervised hierarchical organisation of a document collection can be further exploited to create a kernel which boosts the performance of state-of-the-art support vector machine document classifiers.
} @inproceedings{whitelaw:2005:uag, author = "casey whitelaw and navendu garg and shlomo argamon", title = "using appraisal groups for sentiment analysis", pages = "625--631", booktitle =
{zhao xu and kai yu and volker tresp and  xiaowei xu and jizhi wang}, title
in  contrast to most ir methods, theoretical analysis provides performance  guarantees and guidance on parameter settings for these algorithms.
"quantifying trends accurately despite classifier error and class imbalance", booktitle =
newsjunkie employs  novelty-analysis algorithms that represent articles as words and named  entities.
as the result of collaborative research with friends of the earth, an environmental issues campaigning organisation, we have developed a general purpose information classification agent architecture and have applied it to the problem of document classification and routing.
"olsson, j. scott", title = "an analysis of the coupling between training set and neighborhood sizes for the knn classifier", booktitle =
"proceedings of the 29th european conference on information retrieval", year =
our experiments in a transductive classification setting  indicate that accuracy can be significantly improved by modeling relational  dependencies.
our new classifier misclassified 36\% of the patents, indicating that classifying hypertext can be more difficult than classifying text.
{acm press, new york, us}, address = {mclean, us}, editor = {arvin agah and  jamie callan and elke rundensteiner}, year = {2000}, pages = {70--77}, url =  {http://www.cs.huji.ac.il/~singer/papers/rankboost.ps.gz}, abstract =  {rankboost is a recently proposed algorithm for learning ranking functions.
"in this paper we generalize the lars feature selection method  to the linear svm model, derive an efficient algorithm for it, and empirically  demonstrate its usefulness as a feature selection tool for text  classification." } @inproceedings{ramakrishnan:2005:mha, author =
"a comparison of implicit and explicit links for web page classification", booktitle = "proceedings of the 15th international conference on world wide web", year =
one strength of reinforcement learning is that it provides a formalism for measuring the utility of actions that give benefit only in the future.
in particular, we show that in our environment, ocr errors have no effect on categorization when we use a classifier based on the naive bayes model.
madison, us}, url = {http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/www/wwkb/ilp98.ps.gz},
the bigrams, along with unigrams,  are then given as features to two different classifiers: naive bayes and  maximum entropy.
the  motivation is that there are statistical problems associated with natural  language text when it is applied as input to existing machine learning  algorithms (too much noise, too many features, skewed distribution).
effectiveness of the similarity functions discovered  through simple majority voting is better than that of content-based as well as  combination-based support vector machine classifiers.
published in the ``lecture notes  in computer science'' series, number 3238}, url =  {http://www-ai.upb.de/aisearch/ki04-frame.pdf}, abstract = {
learning algorithms for keyphrase extraction}, journal = {information  retrieval}, number = {4}, volume = {2}, pages = {303--336}, year = {2000}, url  = {http://extractor.iit.nrc.ca/reports/ir2000.ps.z}, abstract = {many academic  journals ask their authors to provide a list of about five to fifteen keywords,  to appear on the first page of each article.
we present detailed experimental  results using naive bayes and support vector machines on the 20newsgroups data  set and a 3-level hierarchy of html documents collected from the open directory  project (www.dmoz.org).}, } @inproceedings{diao00, author = {yanlei diao and  hongjun lu and dekai wu}, title = {
furthermore, the online approach offers the advantage  of continuous learning in the batch-adaptive text filtering task.}, }  @inproceedings{adami03, author = {giordano adami and paolo avesani and diego  sona}, title = {
what are sufficient conditions for applying svms to text-classification problems successfully?}, } @article{joachims02, author =
this interesting  finding of boosting on rare categories has not been reported before.}, }  @inproceedings{liu03, author = {yan liu and jaime carbonell and rong jin},
"we introduce a bayesian model, bayesanil, that is capable of  estimating uncertainties associated with the labeling process.
the result of learning is a set of independent classifiers, each used to predict probability that a new example is a member of the corresponding category.
published in the ``lecture notes in computer science'' series, number 1910}, year = {2000}, address = {lyon, fr}, pages = {581--586}, url = {http://link.springer.de/link/service/series/0558/papers/1910/19100673.pdf}, abstract =
coupled with the ability to generalize concepts using the ontology, this approach addresses the two main problems of natural language processingsynonymy and polysemy.
in this paper we show an adaptive incremental learning algorithm  that learns interactively to classify text messages (here: emails) into  categories without the need for lengthy batch training runs.
it may contain many unnecessary and irrelevant  features.
in this case the  assumptions can be made more representative in two ways: by modeling sub-topic  class structure, and by modeling super-topic hierarchical class relationships.
learning from little: comparison of  classifiers given little training}, booktitle = {proceedings of pkdd-04, 8th  european conference on principles of data mining and knowledge discovery},  editor = {jean-fran{\c{c}}ois boulicaut and floriana esposito and fosca  giannotti and dino pedreschi}, address = {
{proceeding of ijcnn-01, 12th international joint  conference on neural networks}, editor = {}, address = {washington, us}, year =
published in the ``lecture notes in computer science'' series, number  1805}, url = {http://www.cs.berkeley.edu/~diaoyl/publications/pakdd00.ps},  abstract =
we  refer to the original data as rcv1-v1, and the corrected data as rcv1-v2.
as the number of unique words in the collection set is big, the principal component analysis (pca) has been used to select the most relevant features for the classification.
it of applications, ranging from tracking usersõ products or about political candidates as expressed forums, to customer relationship management.
we  have performed extensive experiments on the use of ppm compression models for  categorization using the standard reuters-21578 dataset.
this paper explores the techniques of utilizing n-gram information to categorize chinese documents so that the classifier can shake off the burden of large dictionaries and complex segmentation processing, and subsequently be domain and time independent.
our experimental results on three real world data sets show that we achieve substantial improvements over standard naive bayes classification, while also achieving state of the art performance that competes with the best known methods in these cases.}, } @inproceedings{peng03a, author = {fuchun peng and dale schuurmans and shaojun wang}, title = {language and task independent text categorization with simple language models}, booktitle = {proceedings of hlt-03, 3rd human language technology conference}, publisher = {}, editor = {}, address = {
last but not  least, we describe an evaluation experiment that classifies professional nature  scenery photographs to demonstrate the effectiveness and efficiency of visual  keywords for automatic categorization of images in digital libraries.}, }  @article{liu01, author = {zhi-qiang liu and ya-jun zhang}, title = {a  competitive neural network approach to web-page categorization}, journal =
http://www.cs.technion.ac.il/~gabr % % % % % % originally created by % % % %  fabrizio sebastiani % % dipartimento di matematica pura e applicata % %  universita' di padova % %
the personal view maintainer synchronizes user  interests and the personal view periodically.
the other unusual features of our research  are the longevity of our agents and the fact that they undergo a continual  training process; feedback from the user enables the agent to adapt to the  user's long-term information requirements.}, } @inproceedings{cohen95, author =
it also makes use of bayesian classification techniques to  classify new documents within an existing categorization scheme.
we show that, even with an average word error rate of around 50\%, the categorization performance loss with respect to the clean version of the same documents is negligible.}, url = {ftp://ftp.idiap.ch/pub/reports/2003/rr03-61.pdf}, } @inproceedings{vinokourov01, author = {alexei vinokourov and mark girolami}, title = {document classification employing the fisher kernel derived from probabilistic hierarchic corpus representations},
the system is composed of two main modules: the central indexer (extraction and weighting of indexing terms) and the classifier (classification of business letters into given types).
we applied  the decision forest classifier and compared its accuracies to those of c4.5 and  knn classifiers, using both category dependent and category independent term  selection schemes.
dunja  mladeni{\'{c}} and marko grobelnik}, title =
the linear combination approach  makes use of limited knowledge in the training document set.
hwee t. ng and wei b. goh and kok l. low}, title = {feature selection, perceptron learning, and a usability case study for text categorization}, booktitle = {proceedings of sigir-97, 20th acm international conference on research and development in information retrieval}, editor = {nicholas j. belkin and a. desai narasimhalu and peter willett}, publisher = {acm press, new york, us}, year = {1997}, address = {philadelphia, us}, pages = {67--73}, url = {http://www.acm.org/pubs/articles/proceedings/ir/258525/p67-ng/p67-ng.pdf}, abstract = {
the em algorithm is then applied to build the classifier.
we introduce appropriate cost-sensitive measures, investigating at  the same time the effect of attribute-set size, training-corpus size,  lemmatization, and stop lists, issues that have not been explored in previous  experiments.
a dynamic probabilistic model to visualise topic evolution  in text streams}, journal = {journal of intelligent information systems}, year  = {2002}, note = {special issue on automated text categorization}, volume =
documents are represented as feature-vectors that  include word sequences instead of including only single words as commonly used  when learning on text data.
a neural network approach to topic  spotting}, booktitle = {proceedings of sdair-95, 4th annual symposium on  document analysis and information retrieval}, publisher = {}, editor = {}, year  = {1995}, address = {las vegas, us}, pages = {317--332}, url =  {http://www.stern.nyu.edu/~aweigend/research/papers/textcategorization/wiener.pedersen.weigend_sdair95.ps},  abstract =
we then discuss an alternative that masks the  misclassification based on a well known software fault tolerance technique.
we show that our learning algorithm can be used for  automatic extraction of keywords for text retrieval and automatic text  categorization.
we present  experimental results on the \textsf{reuters-21578} text categorization  collection, showing that for both algorithms the version with discretized  continuous attributes outperforms the version with traditional binary  representations.}, } @inproceedings{ng97, author =
{http://www.stern.nyu.edu/~aweigend/research/papers/textcategorization/hierarchy.ps},
our system, named sonia (service for organizing networked information autonomously), has been implemented as part of the stanford digital libraries testbed.
document retrieval,  categorization, routing and filtering can all be formulated as classification  problems.
it is important to define what constitutes good effectiveness for these  autonomous systems, tune the systems to achieve the highest possible  effectiveness, and estimate how the effectiveness changes as new data is  processed.
we compare the accuracy of our learning approach to a rule-based, expert system approach that uses a text categorization shell built by carnegie group.
as control experiment best human categorization performance was  established at 79.4\% for this task.
}  @inproceedings{dumais:2000:hcw, author = {
a document is usually written in an organized structure to present its main topic(s).
} @inproceedings{raghavan:2005:ifs, author =
experimental comparisons of the performance of  the kernel compared with a standard word feature space kernel (joachims, 1998)
our approach  includes some original ideas for handling large number of features, categories  and documents.
"australian journal of intelligent information processing  systems", volume = 8, number = 3, year = 2004, pages =  "123--131", issn = "1321-2133" } @incollection{isumabook05,  author =
we present here a transductive boosting method for text categorization in order to make use of the large amount of unlabeled data efficiently.
these individual predictions for each hyperlink are subsequently combined to a final prediction for the class of the target page.
for each category, all corresponding document texts from the training sample are concatenated to a megadocument, which is indexed using standard methods.
this paper  studies how link information can be used to improve classification results for  web collections.
in addition, pva considers the aging problem of user interests.
the method is evaluated using the umls  metathesaurus as the underlying hierarchical structure, and the ohsumed test  set of medline records.
we propose a new method of  classifying documents into categories.
we investigate two meta-model approaches for the task of automatic  textual document categorization.
an empirical evaluation  of the system was performed by means of a confidence interval technique.
the first concerns the information filtering system profile based on an adaptation of the generalized probabilistic model of information retrieval.
communications of the acm}, volume = {35}, number = {8}, year =
we show how to update such databases with new documents with high speed and accuracy.
"2006", pages = "821--826", } @inproceedings{forman:2006:qta, author =
as a first step toward automatic go annotation, we aim to assign go domain codes given a specific gene and an article in which the gene appears, which is one of the task challenges at the trec 2004 genomics track.
nashville, us}, pages = {143--151}, publisher =
sofus a. macskassy and haym hirsh}, title = {
although the  test corpus contains documents written in chinese, the proposed approach can be  applied to documents written in any language and such documents can be  transformed into a list of separated terms.} } @inproceedings{esuli:2005:dso,  author =
it is thus unsurprising that this task has traditionally been performed manually, by trained coders.
we also examine training set size, and alternative  document representations.
and the open directory project, in order to facilitate browsing and  other interactive forms of information retrieval.
knowledge discovery is performed by analyzing the co-occurrence frequencies of keywords from this hierarchy in the various documents.
mainly non-informative keywords play the roles of grammatical functions in sentences; such keywords, what are called functional keywords, reflect its contents very little, so they should be removed in the process of document indexing.
"shen, d. and pan, r. and sun, j.-t. and  pan, j. j. and wu, k. and yin, j. and yang, q.", title = "{q2c@ust}:  our winning solution to query classification in kddcup 2005", journal =  "{sigkdd} explorations", pages =
rankboost achieves comparable  performance to the state-of-the-art algorithm when combined with feature or  example selection heuristics.
the first level of the architecture predicts the probabilities  of the meta-topic groups.
"novelty detection based on sentence level patterns", pages =
subjects liked the category interface much better than the list interface, and  they were 50\% faster at finding information that was organized into  categories.
an existing approach to coping with this problem requires terms also to be arranged hierarchically.
international symposium on computer and information sciences}, editor = {u. gudukbay and t. dayar and a. gursoy and erol gelenbe}, publisher = {ios press, amsterdam, nl}, year = {1998}, address = {ankara, tr}, pages = {135--142}, url = {ftp://ftp.cs.bilkent.edu.tr/pub/tech-reports/1998/bu-ceis-9809.ps.z}, abstract = {
these search engines are, however, unsuited for a wide  range of equally important tasks.
an unoptimized version of the algorithm was used to process the trec database in a very short time.
learning to classify text from  labeled and unlabeled documents}, booktitle = {proceedings of aaai-98, 15th  conference of the american association for artificial intelligence}, publisher  = {aaai press, menlo park, us}, editor = {}, year = {1998}, pages = {792--799},  address = {madison, us}, note = {
"salvador, brazil", abstract =
it then trains a new classifier using the labels for all the documents, and iterates to convergence.
}  @inproceedings{wang:2005:woi, author =
we find that using words in web pages alone often yields suboptimal performance of classifiers, compared to exploiting additional sources of information beyond document content.
seattle, us}, pages = {273--281}, url =  {http://www.acm.org/pubs/articles/proceedings/ir/215206/p273-iwayama/p273-iwayama.pdf},  abstract =
we extend the traditional  active learning framework to include feedback on features in addition to  labeling instances, and we execute a careful study of the effects of feature  selection and human feedback on features in the setting of text categorization.
our experiments on hierarchical classification  methods based on svm classifiers and binary naive bayes classifiers showed that  svm classifiers perform better than naive bayes classifiers on reuters-21578  collection according to the extended measures.
{washington, us}, pages = {265}, url = {}, } @inproceedings{chai02, author =
however, the question remains on whether the same techniques that are used on the web can be applied to collections of documents containing citations between scientific papers.
the  existence, public availability, and widespread acceptance of a standard  benchmark for a given information retrieval (ir) task are beneficial to  research on this task, since they allow different researchers to experimentally  compare their own systems by comparing the results they have obtained on this  benchmark.
"proceedings of the 21st  national conference on artificial intelligence", year =
employing {em} in pool-based active learning for text classification},  booktitle = {proceedings of icml-98, 15th international conference on machine  learning}, editor = {jude w. shavlik}, year = {1998}, address = {madison, us},  pages = {350--358}, publisher = {morgan kaufmann publishers, san francisco,  us}, url = {http://www.cs.cmu.edu/~mccallum/papers/emactive-icml98.ps.gz},  abstract = {
{probabilistic information retrieval as combination of abstraction inductive  learning and probabilistic assumptions}, journal = {acm transactions on  information systems}, year = {1994}, number = {1}, volume = {12}, pages =
the self-organizing map model is used to generate two maps, namely the word cluster map and the document cluster map, in which a neuron represents a cluster of words and documents respectively.
reprinted in karen sparck jones and peter willett (eds.),  ``readings in information retrieval'', morgan kaufmann, san francisco, us,  1997, pp.\ 513--517.}, url =  {http://www.acm.org/pubs/articles/proceedings/ir/62437/p333-biebricher/p333-biebricher.pdf},  abstract = {since october 1985, the automatic indexing system air/phys has been  used in the input production of the physics data base of the  fachinformationszentrum karlsruhe/west germany.
we discuss ways of using  self-organizing maps for document classification.
to avoid their being overflowed by the incoming data, methods of information filtering are required.
we consider the problem of assigning level numbers (weights) to hierarchically organized categories during the process of text categorization.
"effects of web document evolution on genre classification", pages = "632--639", booktitle = "proceedings of the 14th {acm} international conference on information and knowledge management", year = "2005", month =
this tight packaging of word pairs could bring in some semantic value.
in  this paper, we propose a new algorithm, which incorporates the relationships of  concept-based thesauri into the document categorization using the k-nn  classifier (k-nn).
it is based on a hybrid case-based architecture, where two multilayer  perceptrons are integrated into a case-based reasoner.
previous researches on advanced representations for document retrieval have shown that statistical state-of-the-art models are not improved by a variety of different linguistic representations.
in this paper, we propose a practical method for enhancing both the speed and the quality of hypertext categorization using hyperlinks.
we also demonstrate some experimental results using our  algorithm on the problem of classifying bibliographic data and extracting  keywords in order to show the effectiveness of our approach.}, }  @inproceedings{sakkis01, author = {georgios sakkis and ion androutsopoulos and  georgios paliouras and vangelis karkaletsis and constantine d. spyropoulos and  panagiotis stamatopoulos}, title = {stacking classifiers for anti-spam  filtering of e-mail}, booktitle = {proceedings of emnlp-01, 6th conference on  empirical methods in natural language processing}, year = {2001}, publisher =
even though the learning ability and computational complexity of training in support vector machines may be independent of the dimension of the feature space, reducing computational complexity is an essential issue to efficiently handle a large number of terms in practical applications of text classification.
{morgan kaufmann  publishers, san francisco, us}, url =  {http://www.research.whizbang.com/~wcohen/postscript/ml-95-ir.ps}, abstract =  {we evaluate the first order learning system foil on a series of text  categorization problems.
the best results were obtained  using the m-estimate as search heuristics combined with the  likelihood-ratio-statics for pruning.
the proposed method shows a similar degree  of performance, compared with the traditional supervised learning methods.
we investigate a meta-model approach, called meta-learning using document feature characteristics (mudof), for the task of automatic textual document categorization.
compared with traditional document-level  categorization, two additional steps, passage splitting and category merging,  are required in this model.
several sets of  controlled experiments on the reuters-21578 corpus are conducted to investigate  the robustness of these methods.
a novel model for tc, extending a well know  statistical model (i.e. rocchio's formula [ittner et al., 1995]) and applied to  linguistic features has been defined and experimented.
turning {{\sc yahoo!}}\ into an automatic web page classifier}, booktitle = {proceedings of ecai-98, 13th
{yiming yang and thomas ault and thomas pierce}, title
the exponential growth of the internet has  led to a great deal of interest in developing useful and efficient tools and  software to assist users in searching the web.
we show that our method is especially useful for text  classification tasks involving a large number of categories and outperforms  other semi-supervised learning techniques such as em and co-training.
we report experiments on text classification of the  cora and webkb data sets using probabilistic latent semantic analysis and  probabilistic hypertext induced topic selection.
"proceedings of the 19th international  joint conference on artificial intelligence", year =
a preliminary experimentation proved that the logic approach is able to capture the semantics underlying some kind of sentences, even if the assessment of the efficiency of such a method, as well as a comparison with other related approaches, has still to be carried out.}, } @article{field75, author = {b.j. field}, title = {towards automatic indexing: automatic assignment of controlled-language indexing and classification from free indexing}, year = {1975}, journal = {journal of documentation}, volume = {31}, number = {4}, pages = {246--265}, url = {}, abstract = {}, } @inproceedings{finn02, author = {aidan finn and nicholas kushmerick and barry smyth}, title = {genre classification and domain transfer for information filtering}, booktitle =
genre  or style, on the other hand, is a different and important property of text, and  automatic text genre classification is becoming important for classification  and retrieval purposes as well as for some natural language processing  research.
in this paper, we introduce a new information  gain and divergence-based feature selection method for statistical machine  learning-based text categorization without relying on more complex dependence  models.
within this process, bootstrapping a taxonomy with examples represents a critical factor for the effective exploitation of any supervised learning model.
this ontology is used to determine the similarity of two terms in the given group.
while the former is a  realization of the well-known \textsc{adaboost} algorithm specifically aimed at  multi-label text categorization, the latter is a generalization of the former  based on the idea of learning a committee of classifier sub-committees.
in this paper we experiment with two different learning techniques, one based on na\"{\i}ve bayesian classification and the other one based on multiclass support vector machines, and test the resulting framework on a corpus of social surveys.
this paper presents a text categorization system, capable of analyzing html/text documents collected from the web.
comparing classification schemes}, journal = {acm transactions on information systems}, year = {2005}, volume = {23}, number = {4}, pages = {430--462}, url = {http://doi.acm.org/10.1145/1095872.1095875}, abstract = {topical crawling is a young and creative area of research that holds the promise of benefiting from several sophisticated data mining techniques.
the accuracy of classification achieved with our method appears better than or comparable to those of existing rule-based methods.}, } @inproceedings{liao02, author = {yihua liao and v. rao vemuri}, title = {
in this categorization process recall is considered more important than precision.
text categorization presents unique challenges due to the large  number of attributes present in the data set, large number of training samples,  attribute dependency, and multi-modality of categories.
web genre refers to the type of the page  characterized by features such as style, form or presentation layout, and  meta-content; web genre can be used to tune spider crawling re-visits and  inform relevance judgments for search engines.
since a multi-dimensional model can be converted to flat and hierarchical models, three classification strategies are possible, i.e., classifying directly based on the multi-dimensional model and classifying with the equivalent flat or hierarchical models.
"shen, d. and pan, r. and sun, j.-t. and pan, j. j. and wu, k. and yin, j. and yang, q.", title = "{q2c@ust}: our winning solution to query classification in kddcup 2005", journal = "{sigkdd} explorations", pages =
published in the ``lecture notes in computer science'' series, number 1398}, editor = {claire n{\'{e}}dellec and c{\'{e}}line rouveirol}, address = {chemnitz, de}, pages = {4--15}, year = {1998}, url = {http://www.research.att.com/~lewis/papers/lewis98b.ps}, abstract = {
it also shows that support vector machines can, in fact, sometimes very significantly outperform both methods.
the motivation for the algorithms stems from recent advances in online learning algorithms.
this margin widened in tasks with high class skew, which is rampant in text classification problems and is particularly challenging for induction algorithms.
this paper  presents two approaches.
{http://www.acm.org/pubs/articles/journals/tois/1994-12-3/p233-apte/p233-apte.pdf},
the interpretation shows the  strengths and weaknesses of using thesaurus knowledge and gives hints for  future research.}, } @article{junker98, author = {markus junker and rainer  hoch}, title = {
text categorization: a  symbolic approach}, booktitle = {proceedings of sdair-96, 5th annual symposium  on document analysis and information retrieval}, publisher = {}, editor = {},  address = {las vegas, us}, year = {1996}, pages = {87--99}, url =  {http://www-poleia.lip6.fr/~moulinie/sdair.ps.gz}, abstract = {recent research  in machine learning has been concerned with scaling-up to large data sets.
three  dictionaries produced by autoslog for different domains performed well in the  author`s text classification experiments.}, } @incollection{riloff99, author =
an application to the xml categorization}, booktitle = {proceedings of mldm-03, 3rd international conference on machine learning and data mining in pattern recognition}, editor = {
{text classification using stochastic keyword generation}, booktitle =
more empirical evidence is required to determine the suitable linguistic levels for modeling each ir subtask (e.g. information zoning, parsing, feature selection for indexing,...) and the corresponding use of this information.
extensive experimental evidence has been derived on real test data and also from well-established academic test sets.
each node's vocabulary is filtered and its words assigned weights with respect to the specific category.
document feature characteristics, derived from the  training document set, capture some inherent category-specific properties of a  particular category.
applying the multiple cause mixture model to text categorization}, booktitle = {proceedings of icml-96, 13th international conference on machine learning}, editor = {lorenza saitta}, year = {1996}, address = {bari, it}, pages = {435--443}, publisher = {morgan kaufmann publishers, san francisco, us}, url = {http://robotics.stanford.edu/users/sahami/papers-dir/ml96-mcmm.ps}, abstract = {the paper introduces the use of the multiple cause mixture model for automatic text category assignment.
an improved boosting algorithm and its application to automated text categorization}, booktitle = {proceedings of cikm-00, 9th acm international conference on information and knowledge management}, address = {mclean, us}, editor = {arvin agah and jamie callan and elke rundensteiner}, publisher = {acm press, new york, us}, year = {2000}, pages =
in many settings, we also have the option of using pool-based active learning.
new cluster analysis approaches are proposed to remedy the problems found in traditional term clustering methods.}, } @inproceedings{lewis94, author = {lewis, david d. and marc ringuette}, title = {a comparison of two learning algorithms for text categorization}, booktitle = {proceedings of sdair-94, 3rd annual symposium on document analysis and information retrieval}, publisher = {}, editor = {}, year = {1994}, address = {las vegas, us}, pages = {81--93}, url = {http://www.research.att.com/~lewis/papers/lewis94b.ps}, abstract = {this paper examines the use of inductive learning to categorize natural language documents into predefined content categories.
the  number of documents in this set would be uniquely determined by the system's  category-boundary predictor, and this set is likely to contain less than 5\% of  the incoming stream of documents.}, } @inproceedings{liere97, author
these theoretical findings are supported by experiments on three test collections.
since these key words are often  phrases of two or more words, we prefer to call them keyphrases.
the personal view maintainer synchronizes user interests and the personal view periodically.
neural networks perform equally well with either set of features and can take advantage of the additional information available when both feature sets are used as input.}, } @article{schutze98, author = {hinrich sch{\"{u}}tze}, title = {automatic word sense discrimination}, journal = {computational linguistics}, year = {1998}, volume = {24}, number = {1}, pages = {97--124}, url = {}, abstract = {this paper presents context-group discrimination, a disambiguation algorithm based on clustering.
the analysis was conducted in two phases.
the packing of word pairs also filters out words occurring frequently in isolation that do not bear much weight towards characterizing that category.}, } @article{koppel02, author = {
we also devised a method to select the category term from the word cluster map.
boosting support vector machines for text  classification through parameter-free threshold relaxation}, booktitle =
there exist many approaches for performing this difficult task.
{edward a. fox and neil rowe}, publisher = {acm press, new york, us}, year = {1999}, address = {
"august",  address = "bonn, germany", url =  "http://www.machinelearning.org/proceedings/icml2005/papers/086_handlingapproximate_ramakrishanetal.pdf",  abstract =
new event  detection is a challenging task that still offers scope for great improvement  after years of effort.
"2006" } @article{fumera:2006:sfb, author = {giorgio fumera and ignazio pillai and fabio roli}, title = {spam filtering based on the analysis of text information embedded into images}, journal = {journal of machine learning research}, volume = {7}, pages = {2699--2720}, year = {2006} } @article{bratko:2006:sfu, author = {bratko, andrej and filipic, bogdan and cormack, gordon v. and lynam, thomas r. and zupan, blaz}, title = {spam filtering using statistical data compression models}, journal = {
on  machine learning methods for chinese document categorization}, journal =
it is developed for the naive bayesian classifier applied on text data, since it combines well with the addressed learning problems.
it also shows that  support vector machines can, in fact, sometimes very significantly outperform  both methods.
} @inproceedings{boese:2005:ewd, author = "boese, elizabeth sugar and howe, adele e.", title =
while it is well-known when the combination of two kernels is again a valid kernel, it is an open question if the resulting kernel will perform well.
by contrast,  content-based methods use information about an item itself to make suggestions.
"hyderabad, india" }  @inproceedings{dai:2007:tnb, author =
therefore, it is very costly to assign a category to them because a human investigates their contents.
we are using a custom  workflow management system as the base for a range of services which are  offered via a multimodal portal, using a language-based approach to extracting  information from html forms, email, and sms.
{raj d. iyer and david d. lewis and robert e. schapire and yoram singer and amit singhal}, title = {boosting for document routing}, booktitle = {proceedings of cikm-00, 9th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, address = {mclean, us}, editor = {arvin agah and jamie callan and elke rundensteiner}, year = {2000}, pages =
the specifics of our classifier is that it allows accurate categorization of short messages containing only a few words.
{proceedings of icml-01, 18th international conference on machine learning},  editor = {carla brodley and andrea danyluk}, address = {williams college, us},  year = {2001}, pages =
= {acm press, new york, us}, address = {sheffield, uk}, year = {2004}, pages = {250--257}, url = {http://www.cs.technion.ac.il/~gabr/papers/accio.pdf}, abstract = {
unfortunately, the paradigm of supervised machine learning is ill-suited to this task, as it assumes that the training examples are classified by a teacher - usually a human.
different from existing categorization methods, mudof can  automatically recommend a suitable algorithm for each category based on the  category-specific statistical characteristics.
using density estimation over the raw tf*idf values, we obtain a classification accuracy of 82\%, a number that outperforms baseline estimates and earlier, image-based approaches, at least in the domain of news articles, and that nears the accuracy of humans who perform the same task with access to comparable information.}, } @inproceedings{sable01, author = {carl sable and ken church}, title = {using bins to empirically estimate term weights for text categorization}, booktitle = {proceedings of emnlp-01, 6th conference on empirical methods in natural language processing}, year = {2001}, publisher = {association for computational linguistics, morristown, us}, editor = {lillian lee and donna harman}, pages = {58--66}, address = {pittsburgh, us}, url = {http://www.cs.columbia.edu/~sable/research/emnlp01.ps},
for larger training  sizes, accuracy becomes increasingly stable with respect to k and the risk  decreases."
springer-verlag}, url = {http://perun.im.ns.ac.yu/radovanovic/publications/2006-dexa-idf.pdf}, abstract =
we also investigated the effect of discarding terms, using either a dynamic stoplist or the winnow heuristic.}, } @inproceedings{rahal04, author = {imad rahal and william perrizo}, title = {an optimized approach for knn text categorization using p-trees}, booktitle = {proceedings of sac-04, 19th acm symposium on applied computing}, editor = {}, pages = {613--617}, address = {
for example, rather than choosing one set decision threshold, they can be used in a bayesian risk model to issue a run-time decision which minimizes a user-specified cost function dynamically chosen at prediction time.
koppel, moshe and schler, jonathan and bonchek-dokow, elisheva}, title = {
a bidimensional view of documents for text categorisation}, booktitle = {proceedings of ecir-04, 26th european conference on information retrieval research}, editor = {
in addition to motivating the task and describing the practical details of participating in the track, this document includes a detailed graphical presentation of the experimental results and provides a brief overall analysis of the performance data.}, } @inproceedings{ipeirotis01, author = {panagiotis g. ipeirotis and luis gravano and mehran sahami}, title = {
the combination of evidence from a document and citing documents can improve on either information source alone.
the back data stores the information about keywords: the frequency for each category, the number of documents for each category.
an  extended version appears as~\cite{debole04a}}, abstract = {
we present the design of a hierarchical classifier based on the divide and conquer principle.
in this paper, we introduce hyperlink  ensembles, a novel type of ensemble classifier for classifying hypertext  documents.
{359--367}, publisher = {
"proceedings of the eleventh acm sigkdd international conference on knowledge discovery in data mining", year =
published in the ``lecture notes in computer science'' series, number  1398}, editor = {claire n{\'{e}}dellec and c{\'{e}}line rouveirol}, address =
our test collection consists of the real news articles and the 519 subnewsgroups under the rec newsgroup of netnews in a period of 3 months.
text classifiers that give probability estimates are more readily applicable in a variety of scenarios.
standard term clustering strategies from information retrieval (ir), based on cooccurence of indexing terms in documents or groups of documents, were tested on a syntactic indexing phrase representation.
our approach maintains a pool of selective terms with potentially high  predictive power.
{proceedings of cicling-01, 2nd international conference on computational  linguistics and intelligent text processing}, year = {2001}, editor =
however the high performance of most existing pn classifiers heavily depends upon the availability of large dictionaries of domain-specific proper nouns, and a certain amount of manual work for rule writing or manual tagging.
(2) can these  genres be reliably identified?
we test the use of one classifier (a highly efficient probabilistic  one) to select examples for training another (the c4.5 rule induction program).
using these techniques, we can automatically build text categorization systems that benefit from domain-specific natural language processing.}, } @article{robertson84, author = {stephen e. robertson and p. harding}, title = {probabilistic automatic indexing by learning from human indexers}, year = {1984}, journal = {journal of documentation}, volume = {40}, number = {4}, pages = {264--270}, url = {}, abstract = {}, } @inproceedings{rose02, author = {tony rose and mark stevenson and miles whitehead}, title = {
mh$^kr$} is both more efficient to train and more effective than  the original {\sc adaboost.
we focus upon  the routing of case law summaries to various secondary law volumes in which  they should be cited.
an additional test submitted 250 trec queries to a search engine and successfully categorized 66\% of the top 100 using the odp and 61\% of the top 350.
this restructuring makes it possible for svms to focus on the latent semantic  space without losing information given by the original feature space.
thus, feature reduction is often performed in order to increase the efficiency and effectiveness of the classification.
science}, year = {1995}, volume = {267}, number = {5199}, pages =
finally, the interactive nature of the system results in a more correct and precise description of each document than a fully automatic system would.}, } @article{kar78, author = {gautam kar and lee j. white}, title = {a distance measure for automated document classification by sequential analysis}, journal = {information processing and management}, pages = {57--69}, year = {1978}, number = {2}, volume = {14}, url = {}, abstract = {}, } @inproceedings{karypis00, author = {george karypis and eui-hong han}, title = {fast supervised dimensionality reduction algorithm with applications to document categorization and retrieval},
similarity in word space is based on second-order  co-occurrence: two tokens (or contexts) of the ambiguous word are assigned to  the same sense cluster if the words they co-occur with in turn occur with  similar words in a training corpus.
the us patent database and yahoo  are two examples.
the problem is not easy to  tackle due to the semi-structured or even unstructured nature of those texts  under consideration.
it accommodates both single and  multiple topic assignments for each document.
the fragments it looks for are determined by a set  of knowledge-based rules.
{giorgio fumera and ignazio pillai and fabio roli}, title = {spam filtering  based on the analysis of text information embedded into images}, journal =
in this work, we investigate the usefulness of explicit control of that combination within a proposed feature selection framework.
we also propose a simple and effective way of  combining a traditional text based classifier with a citation-link based  classifier.
"553--560" }  @inproceedings{lin:2006:atd, author =
the packing of word pairs also filters out words occurring frequently in  isolation that do not bear much weight towards characterizing that category.},  } @article{koppel02, author = {
"unsupervised text classification using  kohonen’s self organizing network", booktitle = "computational  linguistics and intelligent text processing", year =
he}, title = {hierarchical classification of real life documents}, booktitle = {
hypertext poses new  research challenges for text classification.
more importantly, our investigation suggests that meta data which is  often available, or can be acquired using information extraction techniques,  can be extremely useful for improving classification accuracy.
the research project  agnet develops agents for neural text routing in the internet.
{37--78}, year = {2007}, month = {february}
the outcome of the result was quite impressive: in different experimental setups, we reached a micro-averaged f1-measure of 0.89, with a peak of 0.899.
further, the weak performance of naive bayes can be partly explained by extreme skewness of posterior probabilities generated by it.
in consequence, the relation system is  rather imperfect.
morgan kaufmann publishers, san francisco,  us}, url = {http://www.cs.cmu.edu/~mccallum/papers/hier-icml98.ps.gz}, abstract  = {when documents are organized in a large number of topic categories, the  categories are often arranged in a hierarchy.
"computational linguistics and intelligent text processing", year = "2005" } @inproceedings{pappuswamy:2005:scm, author =
"tuning jensen-renyi divergences with statistically similar examples for unsupervised document categorization via iterative denoising trees", booktitle = "proceedings of the annual conference of the north american chapter of the association for computational linguistics", year = "2007", month = "april", address =
significant features are automatically  derived from training texts by selecting substrings from actual word forms and  applying statistical information and general linguistic knowledge.
{121--140}, url =  {http://www.elsevier.nl/gej-ng/10/23/143/56/27/27/article.pdf}, abstract =  {information filtering is concerned with filtering data streams in such a way  as to leave only pertinent data (information) to be perused.
experimental results on standard benchmarks confirm the validity of  our approach, showing that adaboost achieves consistent improvements by  including additional semantic features in the learned ensemble.}, }  @inproceedings{cai04, author = {lijuan cai and thomas hofmann}, title =
neural networks allow us to model higher-order interaction  between document terms and to simultaneously predict multiple topics using  shared hidden features.
{1998}, pages = {307--318}, url =  {http://www.acm.org/pubs/articles/proceedings/mod/276304/p307-chakrabarti/p307-chakrabarti.pdf},  abstract = {
"scoring and selecting terms for text  categorization", author =
on the merits of building categorization systems by  supervised clustering}, booktitle = {proceedings of edbt-00, 7th international  conference on extending database technology}, publisher = {acm press, new york,  us}, year = {1999}, address = {konstanz, de}, pages = {352--356}, url =  {http://doi.acm.org/10.1145/312129.312279}, abstract = {
this system, named  stretch (storage and retrieval by content of imaged documents), is based on an  archiving and retrieval engine, which overcomes the bottleneck of document  profiling bypassing some limitations of existing pre-defined indexing schemes.
{}, pages = {}, address = {
computationally, expnet has an o(n log n) time complexity which is much more efficient than the cubic complexity of the llsf method.
the experiments show substantial improvements over inductive methods, especially for small training sets, cutting the number of labeled training examples down to a 20th on some tasks.
arbee l. chen and  frederick h. lochovsky}, publisher = {ieee computer society press, los  alamitos, us}, year = {1999}, address = {hsinchu, tw}, pages = {195--202}, url  = {http://dlib.computer.org/conferen/dasfaa/0084/pdf/00840195.pdf}, abstract =  {in a text categorization model using an artificial neural network as the text  classifier scalability is poor if the neural network is trained using the raw  feature space since textural data has a very high-dimension feature space.
in this paper we present a method for detecting the text genre quickly and easily following an approach originally proposed in authorship attribution studies which uses as style markers the frequencies of occurrence of the most frequent words in a training corpus (burrows, 1992).
our algorithm is computationally efficient being bounded by o(n log n) forn  samples.}, } @inproceedings{ciravegna99, author
(relevance sampling is the application of relevance feedback to producing a training sample.)
results with the yahoo!
we believe that database categorization can be a potentially effective technique for good database selection, especially in the internet environment, where
a comparison of event models for naive bayes text  classification}, booktitle = {proceedings of aaai-98, workshop on learning for  text categorization}, year = {1998}, url =  {citeseer.nj.nec.com/mccallum98comparison.html}, } @inproceedings{meretakis00,  author = {dimitris meretakis and dimitris fragoudis and hongjun lu and spiros  likothanassis}, title = {scalable association-based text classification},  booktitle = {proceedings of cikm-00, 9th acm international conference on  information and knowledge management}, publisher = {acm press, new york, us},  address = {mclean, us}, editor = {
however, a first-order representation seems to be advantageous when high-precision classifiers are desirable.}, } @inproceedings{cohen96a, author = {william w. cohen and yoram singer}, title = {context-sensitive learning methods for text categorization}, booktitle = {proceedings of sigir-96, 19th acm international conference on research and development in information retrieval}, editor =
{ieee computer society press, los alamitos, us}, year = {2000}, address = {snowbird, us}, pages = {200--209}, url = {http://dlib.computer.org/conferen/dcc/0592/pdf/05920555.pdf}, abstract = {text categorization is the assignment of natural language texts to predefined categories based on their content.
{209--228}, url = {http://trec.nist.gov/pubs/trec1/papers/17.txt}, abstract = {
although text categorization is a burgeoning area of ir research, readily available test collections in this field are surprisingly scarce.
extensive experiments reported in the paper  shows that this new weighting method improves significantly the classification  accuracy as measured on many categorization tasks."
the application of term clustering to this representation to improve its  statistical properties while retaining its desirable meaning properties is  proposed.
we also try combining classifiers based  on different representations using a majority voting technique, and this  improves performance on both test collections.
"aris anagnostopoulos and andrei broder and kunal punera", title = "effective and efficient classification on a search-engine model", booktitle = "cikm", year =
performance of rankboost is somewhat inferior to that of a  state-of-the-art routing algorithm which is, however, more complex and less  theoretically justified than rankboost.
w. bruce croft and van rijsbergen, cornelis j.}, publisher =
published in the ``lecture notes in computer science'' series,  number 1433}, } @inproceedings{vinciarelli04, author = {vinciarelli,  alessandro}, title = {
we also compare our empirical results to semi-theoretical results  and find that the two closely agree.}, } @inproceedings{ghani01, author =
in particular, this method is most similar to naive bayes; it generally performs at least as well as naive bayes, and sometimes better.}, } @inproceedings{sable02, author = {carl sable and kathleen mckeown and
the architecture is based on the metaphor of the software  agents and incorporates innovative hints from other fields: distributed  architectures, relevance feedback and active interfaces.
a patent search and classification system}, booktitle = {proceedings of dl-99, 4th acm conference on digital libraries}, editor =
amherst, us}, year = {1992}, url = {http://www.research.att.com/~lewis/papers/lewis91d.ps}, abstract = {
= {acm press, new york, us}, address = {sheffield, uk}, year = {2004}, pages = {297--304}, url = {http://doi.acm.org/10.1145/1008992.1009044}, abstract = {
in addition, aram predictive accuracy and learning efficiency can be improved by incorporating a set of rules derived from the reuters category description.
we show how to update such databases with new  documents with high speed and accuracy.
we evaluate four different measures of subject similarity, derived from the web link structure, and determine how accurate they are in predicting document categories.
it is evident that  the llsf approach uses the relevance information effectively within human  decisions of categorization and retrieval, and achieves a semantic mapping of  free texts to their representations in an indexing language.
we present an approach using the vector space  model to integrate two different kind of resources: a lexical database and  training collections, in text content analysis tasks.
{xue, dejun and sun, maosong},  title = {
since the whole system is rule-based, it can be adapted  to different subject fields by appropriate modifications of the rule bases.
"nadia ghamrawi and andrew  mccallum", title =
{proceedings of ecir-01, 23rd european colloquium on information retrieval research}, editor = {}, year = {2001}, address = {darmstadt, de}, publisher = {}, pages = {}, url = {http://ls6-www.informatik.uni-dortmund.de/bib/fulltext/ir/frommholz:01a.pdf}, abstract = {automatic categorization of web documents (e.g. html documents)
a work that is printed and bound, but without a named publisher % or sponsoring institution % required: title % optional: author, howpublished, address, month, year, note % %
the advantages of hbc are  experimentally verified from several viewpoints.
we present empirical results on two  real world data sets.
% everyone is welcome to download the bibliography as a whole and % % distribute it, provided that it is distributed untouched.
in this paper, we present an approach to the integration of lexical knowledge extracted from the ldb wordnet in learning-based atc, based on stacked generalization (sg).
the statistical approach to the analysis of document collections and retrieval therefrom has proceeded along two main lines, associative machine searching and automatic classification.
we compare, for text categorization, two partially supervised (or semi-supervised) clustering algorithms: the semi-supervised agglomerative hierarchical clustering (ssahc) algorithm (a. amar et al., 1997) and the semi-supervised fuzzy-c-means (ssfcm) algorithm (m. amine et al., 1996).
gauging similarity with  n-grams: language-independent categorization of text}, journal = {science},  year = {1995}, volume = {267}, number = {5199}, pages =
"like many purely data-driven machine learning methods, support vector machine (svm) classifiers are learned exclusively from the evidence presented in the training dataset; thus a larger training dataset is required for better performance.
the best performance was achieved by the feature selection based on a feature scoring measure known from information retrieval called odds ratio and using relatively small number of features.}, } @inproceedings{mladenic04, author = {
empirical results show that gp was able to discover better similarity functions than ga or other fusion techniques." }
"active learning with history-based  query selection for text categorisation", booktitle =
{published in the ``lecture notes in computer science'' series, number 146},  url = {}, abstract = {}, } @inproceedings{ko00, author = {youngjoong ko and  jungyun seo}, title = {automatic text categorization by unsupervised learning},  booktitle = {proceedings of coling-00, the 18th international conference on  computational linguistics}, year = {2000}, editor = {}, pages = {}, address =
= {cambridge, us}, year = {1999}, chapter = {16:
published in the ``lecture notes in computer science'' series, number 1040}, url = {http://www-poleia.lip6.fr/~moulinie/wijcai.ps.gz}, abstract = {
maintaining catalogues manually is becoming increasingly difficult due to the sheer amount of material on the web, and therefore it will be soon necessary to resort to techniques for automatic classification of documents.
pva: a self-adaptive personal view agent}, booktitle = {proceedings of kdd-01, 7th acm sigkdd international conferece on knowledge discovery and data mining}, editor = {foster provost and ramakrishnan srikant}, year = {2001}, pages = {257--262}, publisher = {acm press, new york, us}, address = {san francisco, us}, url = {http://doi.acm.org/10.1145/502512.502548}, abstract = {
the results show that the probabilistic algorithms are preferable to the heuristic rocchio classifier not only because they are more well-founded, but also because they achieve better performance.}, } @inproceedings{joachims97b, author = {thorsten joachims and dayne freitag and tom m. mitchell}, title = {{\sc webwatcher}: a tour guide for the word wide web}, booktitle = {proceedings of ijcai-97, 15th international joint conference on artificial intelligence}, editor = {martha e. pollack}, publisher = {morgan kaufmann publishers, san francisco, us}, year = {1997}, address = {nagoya, jp}, pages = {770--775}, url = {http://www.cs.cmu.edu/afs/cs/user/dayne/www/ps/ijcai97.ps.z}, abstract = {
published in the ``lecture notes in computer science'' series, number 2168}, url = {http://link.springer.de/link/service/series/0558/papers/2168/21680266.pdf}, abstract =
{spie, the international society for optical engineering}, editor = {
traditionally, supervised learning enters only phases (i)  and (iii).
we report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic lsi model.}, } @article{bloedorn98, author = {eric bloedorn and ryszard s. michalski}, title = {data-driven constructive induction}, journal = {ieee intelligent systems}, year = {1998}, number = {2}, volume = {13}, pages = {30--37}, url = {}, abstract = {an inductive learning program's ability to find an accurate hypothesis can depend on the quality of the representation space.
"supervised  learning approaches to text classification are in practice often required to  work with small and unsystematically collected training sets.
an application to the xml categorization}, booktitle =
"comparative experiments on sentiment classification  for online product reviews", booktitle =
pisa, it}, pages = {489--500}, year = {2004}, publisher = {springer verlag, heidelberg, de}, note = {
kwok l. yu and wai lam}, title = {a new on-line learning algorithm for adaptive text filtering}, booktitle = {proceedings of cikm-98, 7th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, editor = {georges gardarin and james c. french and niki pissinou and kia makki and luc bouganim}, year = {1998}, address = {bethesda, us}, pages = {156--160}, url = {http://www.acm.org/pubs/articles/proceedings/cikm/288627/p156-yu/p156-yu.pdf}, abstract = {much previous work on text filtering is developed for batch filtering.
the hierarchy of categories is  involved in all phases of automated document classification, namely feature  extraction, learning, and classification of a new document.
{acm press, new york, us}, address = {
on both corpora the algorithms we present outperform adaptations to topic-ranking of rocchio's algorithm and the perceptron algorithm.
we address the problem  of integrating documents from different sources into a master catalog.
in this paper we describe and evaluate a learning model for information filtering which is an adaptation of the generalised probabilistic model of information retrieval.
stephen huffman}, title = {acquaintance: language-independent document categorization by n-grams}, booktitle = {proceedings of trec-4, 4th text retrieval conference}, publisher =
{289--297}, url =  {http://cobar.cs.umass.edu/pubfiles/1combo.ps.gz}, abstract = {three different  types of classifiers were investigated in the context of a text categorization  problem in the medical domain: the automatic assignment of icd9 codes to  dictated inpatient discharge summaries.
text mining applies the same analytical functions of data mining to the domain of textual information, relying on sophisticated text analysis techniques that distill information from free-text documents.
hence, the creation of new directories or the  modification of existing ones require strong investments.
the paper describes the kdt system for knowledge discovery in texts.
our  experiments show that the transductive method outperforms conventional boosting  techniques that employ only labeled data.}, } @inproceedings{taira99, author =
much of previous work focused on binary document classification problems.
{537--546}, url = {}, abstract = {
hierarchical classifi- cation is a more efficient method - instead of a single classifier, we use a set of classifiers distributed over a class taxonomy, one for each internal node.
an application of the network model is described, followed by an indexing example and some experimental results about the indexing performance of the network model.}, } @article{uren02, author = {victoria s. uren and thomas r. addis}, title = {
short queries are usually submitted.
we propose a new hierarchical generative model for textual data, where words may be generated by topic specific distributions at any level in the hierarchy.
two example categorization tasks achieve recognition scores of approximately 80\% and are very robust against recognition or typing errors.}, } @inproceedings{bekkerman01, author =
in addition, they work much better at certain tasks, such as identifying major events in texts, than at others, such as determining what sort of business or product is involved in a news event.
we propose instead combining domain  knowledge with training examples in a bayesian framework.
then a document is represented as a vector of features with  different weights according to the importance of each sentence.
from this perspective, bns was the top single choice for all goals except precision, for which information gain yielded the best result most often.
ibm's intelligent miner for text provides the necessary tools to unlock the business information that is ''trapped'' in email, insurance claims, news feeds, or other document repositories.
an extended version appears as~\cite{sable00}}, year = {1999}, address = {paris, fr}, pages = {19--38}, url = {http://www.cs.columbia.edu/~sable/research/ecdl99.ps}, abstract = {
we also introduce decision functions for the centroid-based classification algorithm and support vector classifiers to handle the classification problem where a document may belong to multiple classes.
the system selects the category whose profile has the smallest distance to the document's profile.
results from experiments show that this filter has  successfully rejected a sufficient number of non-relevant documents, resulting  in an improvement of filtering performance.}, } @inproceedings{hoch94, author =  {rainer hoch}, title = {using ir techniques for text classification in document  analysis}, booktitle = {proceedings of sigir-94, 17th acm international  conference on research and development in information retrieval}, editor = {w.  bruce croft and van rijsbergen, cornelis j.}, publisher = {springer verlag,  heidelberg, de}, year = {1994}, address = {dublin, ie}, pages = {31--40}, url =  {http://www.acm.org/pubs/articles/proceedings/ir/188490/p31-hoch/p31-hoch.pdf},  abstract =
the simple tfidf classifier is chosen to train sample data and to classify other new data.
this process requires less effort than providing words with no help or manual  labeling of documents.
we have evaluated expnet in categorization and retrieval on a document collection of the medline database, and observed a performance in recall and precision comparable to the linear least squares fit (llsf) mapping method, and significantly better than other methods tested.
as the standard performance measures assume independence between categories, they have not considered the documents incorrectly classified into categories that are similar or not far from the correct ones in the category tree.
we show that, in expectation, the excess cumulative h-loss grows at most logarithmically in the length of the data sequence.
in contrast to most ir methods, theoretical analysis provides performance guarantees and guidance on parameter settings for these algorithms.
this makes it a good candidate for a general "install-and-forget" term selection mechanism.
a  probabilistic classification procedure computes indexing weights for each  relevance description.
the combination with content-based methods can further improve the results, but too much noise may be introduced, since the text of web pages is a much less reliable source of information.
= {georges gardarin and james c. french and niki pissinou and kia makki and luc bouganim}, year = {1998}, address = {bethesda, us}, pages = {132--139}, url = {http://www.acm.org/pubs/articles/proceedings/cikm/288627/p132-de_lima/p132-de_lima.pdf}, abstract = {}, } @inproceedings{denoyer01, author
published in the ``lecture notes in computer science'' series, number 1923}, year = {2000}, address = {lisbon, pt}, pages = {59--68}, url = {http://www.math.unipd.it/~fabseb60/publications/ecdl00.pdf}, abstract = {
the construction of  a text classifier usually involves (i) a phase of \emph{term selection}, in  which the most relevant terms for the classification task are identified, (ii)  a phase of \emph{term weighting}, in which document weights for the selected  terms are computed, and (iii) a phase of \emph{classifier learning}, in which a  classifier is generated from the weighted representations of the training  documents.
unfortunately, the high computational and memory requirements of lsi and its inability to compute an effective dimensionality reduction in a supervised setting limits its applicability.
= {makoto iwayama and takenobu tokunaga}, title = {hierarchical bayesian clustering for automatic text classification}, booktitle = {proceedings of ijcai-95, 14th
a major problem facing online information services is how to index and supplement large document collections with respect to a rich set of categories.
this can be considered as the effective combination of documents with no topic or class labels (unlabeled data), labeled documents, and prior domain knowledge (in the form of the known hierarchic structure), in providing enhanced document classification performance.}, } @article{vinokourov02, author = {
this simplifies the creation of knowledge-based if/ir systems, speeds up their operation, and allows easy editing of the rule bases employed.
the paper presents an analysis of why tsvms are well suited for text classification.
{kluwer academic publishers}, issn = {1386-4564}, number = {1}, volume = {6}, pages = {49--73}, year = {2003}, url = {http://www.kluweronline.com/issn/1386-4564}, abstract = {
its probabilistic interpretation  allows its predictions to be combined in a principled way with information from  other sources.
we find that on average, labeling a feature takes much less time than labeling a document.
we  describe an investigation into e-mail content mining for author identification,  or authorship attribution, for the purpose of forensic investigation.
{washington, us}, editor = {david a. evans and luis gravano and otthein herzog  and chengxiang zhai and marc ronthaler}, year = {2004}, pages = {78--87}, url =  {}, abstract = {}, } @inproceedings{calado03, author = {p{\'{a}}vel calado and  marco cristo and edleno silva de moura and nivio ziviani and berthier a.  ribeiro-neto and marcos andr{\'{e}} gon{\c{c}}alves}, title = {
in this paper, we want to relax this assumption and allow  a concept instance to be represented by a subgraph of web pages or a set of web  pages.
such a framework provides for a better  assessment of the expected performance of a categorizer if the compression rate  of the summarizer is known.}, } @inproceedings{koller97, author = {daphne  koller and mehran sahami}, title = {
in our experiments we compare the  effectiveness of the svm -based feature selection with that of more traditional  feature selection methods, such as odds ratio and information gain, in  achieving the desired tradeoff between the vector sparsity and the  classification performance.
{janez brank and marko grobelnik and natasa  mili{\'{c}}-frayling and dunja mladeni{\'{c}}}, title = {
this  architecture acquires its language model and dictionary adaptively and hence  avoids handcoding of either.
{193--216}, year = {1999}, url = {http://www.stern.nyu.edu/~aweigend/research/papers/textcategorization/hierarchy.ps}, abstract = {
traditionally, these categories as well as the correlations among them are determined bp human experts.
it employs a meta-learning phase using document  feature characteristics.
when  a probabilistic text categorization is extended to a cluster-based one, the use  of hbc offers better performance than the use of non-probabilistic  algorithms.}, } @inproceedings{iwazume96, author = {michiaki iwazume and  hideaki takeda and toyoaki nishida}, title = {ontology-based information  gathering and text categorization from the internet}, booktitle = {proceedings  of iea/aie-96, 9th international conference in industrial and engineering  applications of artificial intelligence and expert systems}, editor = {},  publisher = {}, year = {1996}, address = {fukuoka, jp}, pages = {305--314}, url  = {}, abstract = {}, } @inproceedings{iyer00, author = {raj d. iyer and david  d. lewis and robert e. schapire and yoram singer and amit singhal}, title =  {boosting for document routing}, booktitle = {proceedings of cikm-00, 9th acm  international conference on information and knowledge management}, publisher =
we develop a novel  measure that captures feature redundancy, and use it to analyze a large  collection of datasets.
the case base of natural language  contexts is acquired automatically during sentence analysis using a training  corpus of texts and their correct relevancy classifications.
the hypothesis that cdm`s performance exceeds two non-domain specific algorithms, bayesian classification and decision tree learners, is empirically tested.}, } @article{goldberg96, author = {goldberg, jeffrey l.}, title = {cdm: an approach to learning in text categorization}, journal = {international journal on artificial intelligence tools}, year = {1996}, number = {1/2}, volume = {5}, pages = {229--253}, url = {}, abstract = {
we provide a theoretical motivation for the algorithm using the notion of a version space.
conference % = same as 'inproceedings', included  for compatibility with older versions % %
"nirmalya chowdhury and  diganta saha", title =
for larger documents and datasets the paper introduces an approximation technique that is shown to deliver good approximations efficiently for large datasets.}, } @inproceedings{macskassy01, author =
koppel, moshe and schler,  jonathan and bonchek-dokow, elisheva}, title = {
we review the results of a user study undertaken to gauge the value of the approach over legacy time-based review of newsfeeds, and also to compare the performance of alternate distance metrics that are used to estimate the dissimilarity between candidate new articles and sets of previously reviewed articles." } @inproceedings{gabrilovich04, author = {evgeniy gabrilovich and shaul markovitch}, title = {
we introduce a novel hybrid  system specifically designed for multi-page text documents.
in this paper, we extend the concept of linkages from explicit hyperlinks to implicit links built between web pages.
we  illustrate the approach using the recently available reuters corpus volume 1  (rcv1).
since information retrieval is a domain where such data sets are widespread, it provides an ideal application area for machine learning.
published in the ``lecture notes in computer  science'' series, number 2004}, pages = {423--436}, url =  {http://link.springer.de/link/service/series/0558/papers/2004/20040423.pdf},  abstract = {a new way of representing texts written in natural language is  introduced, as a conditional probability distribution at the letter level  learned with a variable length markov model called adaptive context tree model.
works in text retrieval through internet suggest  that embedding linguistic information at a suitable level within traditional  quantitative approaches (e.g. sense distinctions for query expansion as in  [14]) is the crucial issue able to bring the experimental stage to operational  results.
with consideration of these characteristics, we propose a probabilistic model to incorporate both content and time information in a unified framework.
we present an efficient algorithm for text classification using hierarchical classifiers based on a concept hierarchy.
discretizing continuous attributes in adaboost for text categorization}, booktitle = {proceedings of ecir-03, 25th
"li, jingyang and sun, maosong and  zhang, xian", title =
further, the  categorization system can be trained on noisy ocr output, without need for the  true text of any image, or for editing of ocr output.
these algorithms both construct classifiers that allow the ``context'' of a word w to affect how (or even whether) the presence or absence of w will contribute to a classification.
w.  bruce croft and david j. harper and donald h. kraft and justin zobel},  publisher = {acm press, new york, us}, address = {new orleans, us}, year =
chris clack and johnny farringdon and peter lidwell and tina yu}, title = {autonomous document classification for business}, editor = {w. lewis johnson}, publisher =
rankboost achieves comparable performance to the state-of-the-art algorithm when combined with feature or example selection heuristics.
experiments on data sets with different  properties (reuters-21578, patent abstracts and patent applications) and with  two different algorithms (winnow and rocchio) show that uc-based term selection  is not the most aggressive term selection criterium, but that its effect is  quite stable across data sets and algorithms.
text categorization using compression models},  booktitle = {proceedings of dcc-00, ieee data compression conference}, editor =
finally, the  relative performance of the different classifiers being tested gives us  insights into the strengths and limitations of our algorithms for hypertext  classification.}, } @inproceedings{ghani01a, author = {rayid ghani}, title =
the trec-4  (4th text retrieval conference) filtering track was an experiment in the  evaluation of binary text classification systems.
effectiveness of the similarity functions discovered through simple majority voting is better than that of content-based as well as combination-based support vector machine classifiers.
once again, lsi slightly improves performance.
we propose a method to solve the problem.
marti a. hearst and fredric gey and richard tong}, publisher = {acm press, new york, us}, address = {
{chris e. mellish}, publisher = {morgan  kaufmann publishers, san francisco, us}, year = {1995}, address = {montreal,  ca}, pages = {1322--1327}, url = {}, abstract = {
conventional methods such as decision trees have had  competitive, but not optimal, predictive performance.
"feature generation for text categorization using world knowledge", pages = "1048--1053", booktitle = "proceedings of the 19th international joint conference on artificial intelligence", year =
{journal of machine learning research}, volume = {7}, pages = {2699--2720},  year = {2006} } @article{bratko:2006:sfu, author = {bratko, andrej and filipic,  bogdan and cormack, gordon v. and lynam, thomas r. and zupan, blaz}, title =
grenoble, fr}, pages = {153--160}, url =  {http://doi.acm.org/10.1145/958220.958249}, abstract = {
go annotation is a major activity in most model organism database projects and annotates gene functions using a controlled vocabulary.
in particular, our new feature selection method yields considerable improvement.
a new measure of performance is proposed and some other applications of the technique indicated.}, } @inproceedings{hsu99, author = {wen-lin hsu and sheau-dong lang}, title = {classification algorithms for netnews articles}, booktitle =
a new classifier-centric measure  called blocking measure is also defined to examine the performance of subtree  classifiers in a top-down level-based hierarchical classification method.}, }  @incollection{sun03a, author = {aixin sun and ee-peng lim and wee-keong ng},  title = {hierarchical text classification methods and their specification},  booktitle = {cooperative internet computing}, editor = {alvin t. chan and  stephen c. chan and h. v. leong and vincent t. y. ng}, year = {2003}, pages =
philip  j. hayes and steven p. weinstein}, title
as the amount of data stored in storage media is increased exponentially, it becomes necessary to store documents according to their category, to access them easily.
we view this result as a confirmation of  the usefulness of classifiers that represent contextual information.}, }  @inproceedings{cohen98, author = {william w. cohen and haym hirsh}, title =
its categorizations are dependent on fragmentary recognition using pattern-matching techniques.
a re-examination of text categorization methods}, booktitle = {proceedings of sigir-99, 22nd
"in this paper, we present a general solution for the kdd cup  2005 problem.
firstly we introduce a novel kernel, whose gram matrix is the well known co-citation matrix from bibliometrics, and demonstrate on real data that it has a good performance.
to verify our  new method, we conducted experiments on two language newsgroup data sets: one  written by english and the other written by korean.
both the interest of the user and the document content change over time.
these algorithms relied on hand-coded training data, including  annotated texts and a semantic dictionary.
in a different manner from topographical techniques previously utilized for static text collections, the topography is an outcome of the coherence in time of the data stream in the proposed model.
results show that the use of metadata is almost as good as the full-text version of papers.
most importantly, this regularized estimation enables the model parameters to become sparse.
experiments with syntactic phrase indexing, however, have never yielded  significant improvements in text retrieval performance.
when  training classifiers on large collections of documents, both the time and  memory requirements connected with processing of these vectors may be  prohibitive.
text  retrieval systems typically produce a ranking of documents and let a user  decide how far down that ranking to go.
upon the fact that refined statistics  may have more chance to meet sparse data problem, we re-evaluate the role of  the binary weighting model (bwm) in tc for further consideration.
the utility of our approach is demonstrated on a set of web-pages that relate to computer science departments.}, } @inproceedings{furnkranz99, author = {johannes f{\"{u}}rnkranz}, title = {exploiting structural information for text classification on the www}, booktitle = {proceedings of ida-99, 3rd symposium on intelligent data analysis}, publisher = {springer verlag, heidelberg, de}, note = {
experiments show that feature selection using weights from linear svms yields better classification performance than other feature weighting methods when combined with the three explored learning algorithms.
we show that a careful hybrid integration of techniques from neural network architectures, learning and information retrieval can reach consistent recall and precision rates of more than 92\% on an 82,000 word corpus; this is demonstrated for 10,000 unknown news titles from the reuters newswire.
using such an architecture, the time  needed for training is reduced substantially and the user is provided with an  even more intuitive metaphor for visualization.
uncertainty-based term  selection (uc) is compared to a number of other criteria like information gain  (ig), simplified chi-square (sx), term frequency (tf) and document frequency  (df) in a text categorization setting.
much information is nowadays stored as multilingual textual data; therefore advanced classification systems are currently considered as strategic components for effective knowledge management.
more specifically, it compares standard multinomial naive bayes to the recently proposed transformed weight-normalized complement naive bayes classifier (twcnb)
we also carefully analyze the case of those documents whose  categorization is not in accordance with the one provided by the human  specialists.
{acm press, new york, us}, editor = {}, year = {2003}, address = {new orleans, us}, pages = {232--239}, url = {http://doi.acm.org/10.1145/956863.956909}, abstract = {most existing studies of text classification assume that the training data are completely labeled.
"august 29--31, 2003", pages = "33--38", isbn = "isbn 963-7154-17-5", } @inproceedings{gabrilovich:2006:wikipedia, author =
more importantly, our investigation suggests that meta data which is often available, or can be acquired using information extraction techniques, can be extremely useful for improving classification accuracy.
the proposed approach is more efficient, does not require the specification of any parameters, and similarly to the parameter-based approach, boosts the performance of baseline svms by at least 20\% for standard information retrieval measures.}, } @inproceedings{shanks03, author = {vaughan r. shanks and hugh e. williams}, title = {index construction for linear categorisation}, booktitle = {proceedings of cikm-03, 12th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, editor = {}, year = {2003}, address = {new orleans, us}, pages = {334--341}, url = {http://doi.acm.org/10.1145/956863.956926}, abstract = {
combinations of multiple classifiers did not  always improve the classification accuracy compared to the best individual  classifier.
a learning system for selective dissemination of information}, booktitle = {proceedings of ijcai-97, 15th international joint conference on artificial intelligence}, editor =
we show that the accuracy of a naive bayes classifier over text classification tasks can be significantly improved by taking advantage of the error-correcting properties of the code.
the authors show how this term-frequency approach supports a range of kdd operations, providing a general framework for knowledge discovery and exploration in collections of unstructured text.}, } @inproceedings{dagan97, author = {ido dagan and yael karov and dan roth}, title = {mistake-driven learning in text categorization}, booktitle = {proceedings of emnlp-97, 2nd conference on empirical methods in natural language processing}, publisher = {association for computational linguistics, morristown, us}, editor = {claire cardie and ralph weischedel}, year = {1997}, address = {providence, us}, pages = {55--63}, url = {http://l2r.cs.uiuc.edu/~danr/papers/categ.ps.gz}, abstract = {learning problems in the text processing domain often map the text to a space whose dimensions are the measured features of the text, e.g., its words.
pattern matching produces fairly accurate and fast  categorisation over a large number of classes, while information extraction  provides fine-grained classification for a reduced number of classes.
stefan wermter}, title = {neural network agents for learning semantic text classification}, journal = {information retrieval}, number = {2}, volume =
we then consider bayesian extensions to nb that achieve higher  accuracy by relaxing its strong independence assumptions.
{cambridge, uk}, year = {2000}, pages = {}, publisher = {}, url =
proceedings of sigir-94, 17th acm international conference on research and development in information retrieval}, editor
we describe webwatcher as a tour guide agent for the web, the learning algorithms used by webwatcher, experimental results based on learning from thousands of users, and lessons learned from this case study of tour guide agents.}, } @inproceedings{joachims98, author = {
however, an extensive and systematic evaluation of the various approaches has not yet been done.
we describe the results of extensive machine learning experiments  on large collections of reuters' english and german newswires.
however, ripper and sleeping-experts differ radically in many other respects: differences include different notions as to what constitutes a context, different ways of combining contexts to construct a classifier, different methods to search for a combination of contexts, and different criteria as to what contexts should be included in such a combination.
in this paper, we do some explorations on both directions based on the following two characteristics of news articles.
"2005" } @inproceedings{moyotlhernandez:2005:edf, author =
however, many popular feature selection techniques such as information gain  (ig) and $\chi^2$-test (chi) are all greedy in nature and thus may not be  optimal according to some criterion.
for a slim fraction of all documents (0.77\% for category coding and 1.4\% for subcategory coding), the algorithm makes assignments that are clearly incorrect.
in this paper, we present an efficient text categorization algorithm that generates bigrams selectively by looking for ones that have an especially good chance of being useful.
the experimental results show that the proposed pattern-based approach significantly outperforms all three baselines in terms of precision at top ranks."
feature selection: a useful preprocessing step}, booktitle  = {proceedings of bcsirsg-97, the 19th annual colloquium of the british  computer society information retrieval specialist group}, publisher = {springer  verlag, heidelberg, de}, series = {electronic workshops in computing}, editor =
"proceedings of the twenty-second aaai conference on artificial intelligence", year = "2007", month =
http://link.springer.de/link/service/series/0558/papers/2389/23890125.pdf},  abstract
booktitle = {proceedings of dl-98, 3rd acm  conference on digital libraries}, editor = {ian witten and rob akscyn and frank  m. shipman}, publisher = {acm press, new york, us}, year = {1998}, address =
we use techniques from statistical pattern recognition to efficiently separate the feature words or discriminants from the noise words at each node of the taxonomy.
to facilitate research in this area, the task description, data, answer set, and related information of this kdd-cup are published at the kddcup 2005 web site: http://www.acm.org/sigs/sigkdd/kdd2005/kddcup.html."
indeed we found strong correlations between the df, ig and  chi values of a term.
experiments were conducted using a large scale document collection from reuters news articles.
the investigation includes different attribute and distance-weighting schemes, and studies on the effect of the neighborhood size, the size of the attribute set, and the size of the training corpus.
hierarchically classifying documents using  very few words}, booktitle = {proceedings of icml-97, 14th international  conference on machine learning}, editor = {douglas h. fisher}, year = {1997},  address = {
published in the ``lecture notes in computer science'' series, number 1433}, } @inproceedings{vinciarelli04, author = {vinciarelli, alessandro}, title = {noisy text categorization}, booktitle = {proceedings of icpr-04, 17th international conference on pattern recognition}, year = {2004}, address = {cambridge, uk}, abstract = {
classifying news stories using memory-based  reasoning}, booktitle = {proceedings of sigir-92, 15th acm international  conference on research and development in information retrieval}, editor =
we introduce an new algorithm for performing active learning with support vector machines, i.e., an algorithm for choosing which instances to request next.
we analyze a few of the commonly used statistics based and machine  learning algorithms for natural language disambiguation tasks and observe that  they can be recast as learning linear separators in the feature space.
then, a filtering  system with the neural network integrated into it was used to filter the  medical documents and this performance was compared with the filtering results  achieved using the baseline system.
kobenhavn, dk}, pages =
booktitle = {proceedings of the 2002 acm symposium on document  engineering}, publisher = {acm press, new york, us}, editor = {}, year =  {2002}, address = {mclean, us}, pages = {111--118}, url =
this  neural model is based on significance vectors and benefits from the  presentation of document clusters.
we evaluate our approach by applying it to tasks that involve learning definitions for (i) classes of pages, (ii) particular relations that exist between pairs of pages, and (iii) locating a particular class of information in the internal structure of pages.
in this work we reproduce these  results and go further to show that when the training sample is small word  clusters can yield significant improvement in classification accuracy (up to  18\%) over the performance using the words directly.}, }  @inproceedings{soucy01, author = {pascal soucy and guy w. mineau}, title = {
we show how both algorithms  can be adapted to maximize any general utility matrix that associates cost (or  gain) for each pair of machine prediction and correct label.
our  solution obtained creativity and precision runner-up awards at the competition.
these theoretical findings are supported by  experiments on three test collections.
{morgan kaufmann publishers, san francisco, us}, year =  {1997}, address = {nagoya, jp}, pages = {770--775}, url =  {http://www.cs.cmu.edu/afs/cs/user/dayne/www/ps/ijcai97.ps.z}, abstract = {
{classifying texts using relevancy signatures},
for best accuracy, f-measure or recall, the findings reveal an outstanding new feature selection metric, "bi-normal separation" (bns).
the data-driven nature of tcs  allows it is to satisfy fully the requirements of ease of application  development, portability to other applications and maintainability.}, }  @article{he03, author = {
susan t. dumais  and john platt and david heckerman and mehran sahami}, title = {inductive  learning algorithms and representations for text categorization}, booktitle =
{jonathan furner and david harper}, address = {aberdeen, uk}, year = {1997},  pages = {}, url =  {http://www.ewic.org.uk/ewic/workshop/fetch.cfm/irr-97/moulinier/moulinier.ps},  abstract = {statistical classification techniques and machine learning methods  have been applied to some information retrieval (ir) problems: routing,  filtering and categorization.
relation selection improves foil's performance as measured by any of  recall, precision, f-measure, or error rate.
= {las vegas, us}, pages =
"a comparison  of implicit and explicit links for web page classification", booktitle =  "proceedings of the 15th international conference on world wide web",  year =
as a  consequence, a novel approach named the binary weighting model with non-binary  smoothing (bwm-nbs) is then proposed so as to overcome the drawback of bwm.
a stochastic decision list is an ordered sequence of if-then-else rules, and our method can be viewed as a rule-based method for text classification having advantages of readability and refinability of acquired knowledge.
"aaai'06", booktitle =  "proceedings of the 21st national conference on artificial  intelligence", year =
but automatic classification study also has been thriving; some of the reasons for this are discussed.
the key idea is to automatically adjust the window size so that the estimated generalization error is minimized.
here, limited labeled data provide em initializations that lead to  low-probability models.
ron bekkerman and ran el-yaniv  and naftali tishby and yoad winter}, title = {distributional word clusters
but they don't offer any of the benefits of natural language  processing, such as the ability to identify relationships and enforce  linguistic constraints.
we compare the accuracy of our learning approach to a rule-based, expert system  approach that uses a text categorization shell built by carnegie group.
the experiment results show that with only surface text features the svm outperforms the other four methods for this task.
{hans-peter frei  and donna harman and peter sch{\"{a}}uble and ross wilkinson}, publisher =
{ios press}, address = {amsterdam, nl}, pages = {124--143}, year = {1995}, url = {http://www.research.whizbang.com/~wcohen/postscript/ilp.ps},
= {ljubljana, sl}, year = {1998},  url = {http://www-ai.ijs.si/dunjamladenic/papers/phd/phdfinal.ps}, abstract =  {}, } @inproceedings{mladenic98d, author = {
our experimental comparison of eleven feature  scoring measures show that considering domain and algorithm characteristics  significantly improves the results of classification.}, } @article{moens00,  author = {marie-francine moens and jos dumortier}, title = {
an  extended set of e-mail document features including structural characteristics  and linguistic patterns were derived and, together with a support vector  machine learning algorithm, were used for mining the e-mail content.
{computer science department, carnegie mellon university}, address =
the  experiments compare the performance of a counterpropagation network against a  backpropagation neural network.
how then,  from this sea of pages, should a search engine select the correct ones-those of  most value to the user?}, } @inproceedings{chandrinos00, author = {konstantinos  v. chandrinos and ion androutsopoulos and georgios paliouras and constantine d.  spyropoulos}, title = {automatic web rating: filtering obscene content on the  web},
they were tested on a corpus of articles  from the dutch newspaper nrc, and pre-classified into four categories.
in our tests with three  categorization methods on text collections from different domains/applications,  significant numbers of words were removed without sacrificing categorization  effectiveness.
finally, the relative performance of the classifiers being  tested provided insights into their strengths and limitations for solving  classification problems involving diverse and often noisy web pages.}, }  @inproceedings{yang03, author =
in this paper, using several algorithms, we compare the categorization accuracy of classifiers based on words to that of classifiers based on senses.
our experiments using the webkb dataset showed that iwum  improves the overall classification performance and works very well on the more  structured parts of a web site.}, } @inproceedings{taghva00, author = {taghva,  kazem and nartker, thomas a. and julie borsack and steven lumos and allen  condit and ron young}, title = {
we tested this approach by applying k-nearest neighbor, rocchio and language modeling classifiers and their combination to the event tracking problem in the topic detection and tracking (tdt) domain, where new classes (events) are created constantly over time, and representative validation sets for new classes are often difficult to obtain on time.
abstract = {systems for learning text classifiers recently gained considerable interest.
noisy text categorization}, booktitle = {proceedings of  icpr-04, 17th international conference on pattern recognition}, year = {2004},  address = {cambridge, uk}, abstract = {
our feature selection approach employs distributional clustering of words via  the recently introduced information bottleneck method, which generates a more  efficient word-cluster representation of documents.
{kuo-jui wu and menc-chang chen and  yeali sun}, title = {automatic topics discovery from hyperlinked documents},  journal = {information processing and management}, year = {2004}, volume =
our work is distinguished from previous efforts in topic spotting by our explicit study of the effects of dialogue length on classifier performance, and by our use of off-the-shelf speech recognition technology.
the text classification algorithms classify texts with high accuracy by using an underlying information extraction system to represent linguistic phrases and contexts.
by referencing  various relationships in the thesaurus corresponding to the structured  categories, k-nn can be prominently improved, removing the ambiguity.
noun homograph disambiguation using local context in large corpora}, booktitle = {proceedings of the 7th annual conference of the university of waterloo centre for the new oxford english dictionary}, publisher = {}, editor = {}, year = {1991}, pages = {1--22}, address = {oxford, uk}, url = {ftp://parcftp.xerox.com/pub/hearst/oed91.ps.gz}, abstract = {
experimental evaluation on real-world data shows that the proposed approach gives good results.
there are different research  questions important for the development of text-learning intelligent agents.
published in the ``lecture notes in computer science'' series, number 1777}, pages = {365--379}, url = {http://www.almaden.ibm.com/cs/people/ragrawal/papers/athena.ps}, abstract =
mh$^kr$} is both more efficient to train and more effective than the original {\sc adaboost.
using the vector space model,  each document is represented by its original feature vector augmented with  external feature vector generated using wordnet.
in  previous research, lsi has produced a small improvement in retrieval  performance.
our test collection  consists of the real news articles and the 519 subnewsgroups under the rec  newsgroup of netnews in a period of 3 months.
{dordrecht, nl}, year = {2002}, } @inproceedings{joachims97, author = {thorsten  joachims}, title = {a probabilistic analysis of the {rocchio} algorithm with  {tfidf} for text categorization}, booktitle = {proceedings of icml-97, 14th  international conference on machine learning}, editor = {douglas h. fisher},  year = {1997}, address = {
the changes may occur both on the  transmission side (the nature of the streams can change), and on the reception  side (the interest of a user can change).
[no abstract]}, } @inproceedings{lewis98, author = {lewis, david d.}, title = {naive (bayes) at forty: the independence assumption in information retrieval.}, booktitle = {proceedings of ecml-98, 10th european conference on machine learning}, publisher = {springer verlag, heidelberg, de}, note = {
to save the storage space and computation time in  text categorization, efficient and effective algorithms for reducing the data  before analysis are highly desired.
a neural network approach to topic spotting}, booktitle = {proceedings of sdair-95, 4th annual symposium on document analysis and information retrieval}, publisher = {}, editor = {}, year = {1995}, address = {las vegas, us}, pages = {317--332}, url = {http://www.stern.nyu.edu/~aweigend/research/papers/textcategorization/wiener.pedersen.weigend_sdair95.ps}, abstract =
the axes are called topic difference factors (tdf's).
abstract = {current trend in operational text categorization is the designing of fast classification tools.
using the vector space model (vsm), each document is represented by its original feature vector augmented with external feature vector generated using wordnet.
"sentiment classification is a recent subdiscipline classification which is concerned not with the topic is about, but with the opinion it expresses.
{relational learning with statistical predicate invention: better models for  hypertext}, journal = {machine learning}, pages = {97--119}, year = {2001},  volume = {43}, number = {1/2}, url = {http://www.wkap.nl/article.pdf?321079},  abstract = {
in [3] it has been suggested that classifiers based on generalized rocchio formula can be used to weight features in category profiles in order to exploit the selectivity of linguistic information techniques in text classification.
book % = a book with an explicit publisher % required: author or editor, title, publisher, year % optional: volume or number, series, address, edition, month, note % % booklet % =
for this reason, the automatic construction of disambiguation rules is highly desirable.
"this paper reports a cross-benchmark evaluation of regularized logistic regression (lr) and incremental rocchio for adaptive filtering.
boosting is a method for supervised learning which has successfully been applied to many different domains, and that has proven one of the best performers in text categorization exercises so far.
we suggest the utilization of  additional resources like lexical databases to increase the amount of  information that tc systems make use of, and thus, to improve their  performance.
{305--310}, url = {}, abstract =  {acquaintance is the name of a novel vector-space n-gram technique for  categorizing documents.
= {}, publisher = {}, year = {1996}, address = {
while developing simpl, we also make a detailed experimental analysis of the cache performance of svms.}, } @inproceedings{chakrabarti97, author = {soumen chakrabarti and byron e. dom and rakesh agrawal and prabhakar raghavan}, title = {using taxonomy, discriminants, and signatures for navigating in text databases}, booktitle = {proceedings of vldb-97, 23rd international conference on very large data bases}, publisher =
by noisy it is meant any text obtained through an extraction process (affected by errors) from media different than digital texts.
we call  this idea \emph{supervised term weighting} (stw).
this study benchmarks the performance of twelve feature selection metrics across 229 text classification problems drawn from reuters, ohsumed, trec, etc.
combining multiple classifiers for text categorization}, booktitle = {proceedings of cikm-01, 10th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, editor = {henrique paques and ling liu and david grossman}, year = {2001}, address = {
{national institute of standards and technology, gaithersburg, us}, editor =
stanford, us}, pages = {487--494}, publisher =
the performance of our approach is promising, when tested  on the questions from the trec qa track.}, } @inproceedings{zhang03a, author =
we found that it and other existing methods failed to produce good results on an industrial text classification problem.
it is shown that both classifiers can perform filtering with reasonable accuracy.
= {william hersh and christopher buckley and t.j. leone and david hickman}, title = {{{\sc ohsumed}}: an interactive retrieval evaluation and new large text collection for research}, booktitle = {proceedings of sigir-94, 17th acm international conference on research and development in information retrieval}, editor = {
the measurements of performance evaluation are: classification rate, correctness rate, and classified correctness rate.}, } @inproceedings{joachims00, author = {thorsten joachims}, title = {
among the four dimensionality reduction techniques proposed, principal  component analysis was found to be the most effective in reducing the  dimensionality of the feature space.}, } @article{lam99a, author = {
while the majority of approaches rely on learning linear  classifiers, there is also some interest in describing document categories by  text patterns.
in this case, the label of one entity (e.g., the topic of the paper) is often correlated with the labels of related entities.
the described system can be  efficiently adapted to new domains or different languages.
in this paper we will evaluate the effectiveness of several ilp methods for text categorization, and also compare them to their propositional analogs.
{dasigi, venu and mann, reinhold c. and protopopescu, vladimir a.}, title =  {information fusion for text classification: an experimental comparison},  journal = {pattern recognition}, year = {2001}, volume = {34}, number = {12},  pages = {2413--2425}, url = {}, abstract = {this article reports on our  experiments and results on the effectiveness of different feature sets and  information fusion from some combinations of them in classifying free text  documents into a given number of categories.
% % % %  concerning urls from which to download on-line copies of the % % papers, where  possible i have included urls with unrestricted % % access (e.g. home pages of  authors).
on the other hand, we also observe that linked pages can be more harmful than helpful when the linked neighborhoods are highly ``noisy'' and that links have to be used in a careful manner.
bayesian independence classifiers and k-nearest-neighbor classifiers were trained to assign scores to manually-graded essays.
teklis is a training based statistical categorization system which incorporates shallow linguistic processing and fuzzy set methods.
this process involves an activity of {\em supervised learning}, in which information on the membership of training documents in categories is used.
the encyclopedia of  database technologies and applications}, publisher =
we present the design of a hierarchical  classifier based on the divide and conquer principle.
this paper presents a new method  for graph-based classification, with particular emphasis on hyperlinked text  documents but broader applicability.
the space has been bound by giving a feature selection interpretation of the rocchio parameters.
this work provides an important insight on which measures derived from links are more appropriate to compare web documents and how these measures can be combined with content-based algorithms to improve the effectiveness of web classification.}, } @inproceedings{caldon03, author = {
{26--41}, url = {},  abstract = {}, } @article{wu04, author =
the learning process constructs a  relationship between an index term and the words relevant and irrelevant to it,  based on the positive training set and negative training set, which are sample  documents indexed by the index term, and those not indexed by it, respectively.
{acm press, new york, us}, year = {1996}, address = {z{\"{u}}rich, ch},  pages = {279--288}, url =  {ftp://parcftp.xerox.com/pub/qca/papers/sigirfiltering96.ps}, abstract = {there  is strong empirical and theoretic evidence that combination of retrieval  methods can improve performance.
in the results of an experiment with news article  classification, precision is about 98\%.}, } @incollection{jo99b, author =
we report the results of our experiments, using various feature selection measures and varying values of $\sigma$, performed on the {\sc reuters-21578} standard tc benchmark.
w. bruce croft and david j. harper and donald h. kraft and justin zobel}, publisher = {acm press, new york, us}, address = {new orleans, us}, year = {2001}, pages = {303--309}, url = {http://portal.acm.org/citation.cfm?doid=383952.384011}, abstract =
this paper explores the use of hierarchical structure for classifying a large, heterogeneous collection of web content.
existing learning  techniques must be adapted or improved in order to effectively handle difficult  situations where the number of positive training instances per event is  extremely small, the majority of training documents are unlabelled, and most of  the events have a short duration in time.
using three hypertext datasets and three well-known learning algorithms (naive bayes, nearest neighbor, and first order inductive learner), we examine these regularities in different domains, and compare alternative ways to exploit them.
user assisted text classification and knowledge  management}, booktitle = {proceedings of cikm-03, 12th acm international  conference on information and knowledge management}, publisher = {acm press,  new york, us}, editor = {}, year = {2003}, address = {new orleans, us}, pages =
we describe a class of text  categorization problems that are characterized with many redundant features.
many approaches have been devised for mining various kinds  of knowledge from texts.
information today, medford, usa}, address = {new york, us}, year = {2000}, pages =
in particular, we develop metrics that estimate the difficulty of a dataset by examining the host directory structure.
we present an application of hidden markov models to supervised  document classification and ranking.
in addition, we carefully analyze the internal representation using  cluster analysis and output representations using a new surface error  technique.
more concretely, a text classification task aimed at improving language modelling for machine translation is considered.}, } @inproceedings{junker00, author = {markus junker and michaell sintek and matthias rinck}, title = {
experienced users can make effective use of such  engines for tasks that can be solved by searching for tightly constrained  keywords and phrases.
title = {{\sc tcs}: a shell for content-based text categorization}, booktitle =
{london, uk}, pages = {409--418}, url =  {http://www.cs.iastate.edu/~yang/papers/dawak00.ps}, abstract = {
european colloquium on information retrieval research}, editor = {}, year = {2001}, address = {darmstadt, de}, publisher = {}, pages = {126--135}, url = {http://www-connex.lip6.fr/~denoyer/publications/denoyer-final-ecir01.ps},
text collections in different domains were used for  evaluation.
in the recent years,  unsolicited bulk email has became an increasingly important problem, with a big  economic impact.
} @inproceedings{keerthi:2005:gli, author =
svms have been trained on data sets with several thousand instances, but web directories today contain millions of instances which are valuable for mapping billions of web pages into yahoo!-like directories.
{proceedings of mldm-03, 3rd international conference on machine learning and  data mining in pattern recognition}, editor = {petra perner and azriel  rosenfeld}, year = {2003}, address = {leipzig, de}, publisher = {springer  verlag, heidelberg, de}, pages = {328--342}, url =  {http://www.springerlink.com/openurl.asp?genre=article&issn=0302-9743&volume=2734&spage=328},
probe, count, and classify: categorizing hidden web databases}, booktitle = {proceedings of sigmod-01, acm international conference on management of data}, editor = {
{lewis, david d. and fan li and tony rose and yiming yang}, title = {{reuters corpus volume 1} as a text categorization test collection}, journal = {journal of machine learning research}, volume = {5}, month = {april}, pages = {361--397}, year = {2004}, url = {http://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf}, abstract = {reuters corpus volume i (rcv1) is an archive of over 800,000 manually categorized newswire stories recently made available by reuters, ltd. for research purposes.
we also demonstrate that the retrieval performance using  automatic categorization achieves the same retrieval quality as the performance  using manual categorization.
in this paper, we  focus on using user assisted text classification in conjunction with a web  portal, multiple document management systems and an ontology, to provide a  powerful solution for organizing information about a company's technology.
this paper describes a series of automatic text categorization experiments with case law documents.
the subsequences are weighted by an exponentially decaying factor of their full length in the text, hence emphasising those occurrences that are close to contiguous.
this paper describes new recurrent plausibility networks with internal recurrent hysteresis connections.
finally, a user study was performed to further investigate the causes for these results.
"vancouver, british columbia, canada" } @inproceedings{betts:2007:uie, author =
the algorithm uses the information gain metric, combined with various frequency thresholds.
{283--290}, year = {2000}, address =
this article investigates the  applicability of rs theory to the if/ir application domain and compares this  applicability with respect to various existing tc techniques.
text categorization and semantic browsing with self-organizing maps on non-euclidean spaces}, booktitle = {proceedings of pkdd-01, 5th european conference on principles and practice of knowledge discovery in databases}, editor = {
the text categorization (tc) is the automated assignment of text  documents to predefined categories based on document contents.
therefore,  the contributions of this research are in learning and generalizing neural  architectures for the robust interpretation of potentially noisy unrestricted  messages.
published in the ``lecture notes in computer science''  series, number 1040}, url =
the engine exploits a structured document representation and can activate  appropriate methods to characterise and automatically index heterogeneous  documents with variable layout.
we are given a set  of categories, organized hierarchically.
"583--600" } @article{ajiips04a, author =  "d. tikk and {gy.} biro and j. d. yang", title =
this paper  reports on a system that uses natural language text processing to derive  keywords from free text news stories, separate these keywords into segments,  and automatically build a segmented database.
"2004" } @inproceedings{xue:2004:rhd, author = "dejun xue and maosong sun", title =
our experiments are conducted in the text categorization  domain, which is characterized by a large number of features, many of which are  irrelevant.
however, a problem with iterative training techniques such as svm is that during their learning or training phase, they require the entire training collection to be held in main-memory; this is infeasible for large training collections such as dmoz or large news wire feeds.
{2000}, pages = {176--183}, url = {http://www.acm.org/pubs/articles/proceedings/ir/345508/p176-hoashi/p176-hoashi.pdf}, abstract = {document filtering is a task to retrieve documents relevant to a user's profile from a flow of documents.
we will also provide some numerical experiments to illustrate these algorithms on a number of datasets.}, } @inproceedings{zhang03, author = {
we have developed a new effective probabilistic classifier for document classification by introducing the concept of differential document vectors and dlsi (differential latent semantic indexing) spaces.
we propose  here the use of soft clustering of words, i.e., in which a word can be assigned  to several different clusters and each cluster is characterized by a specific  word probability distribution.
the baseline method combines the terms of all the articles of each newsgroup in the training set to represent the newsgroups as single vectors.
in this paper the  suitability of different document representations for automatic document  classification is compared, investigating a whole range of representations  between bag-of-words and bag-of-phrases.
"computational linguistics and intelligent text  processing", year = "2005" } @inproceedings{pappuswamy:2005:scm,  author =
{acm press, new york, us}, year = {1996}, address = {z{\"{u}}rich, ch}, pages = {307--315}, note = {an extended version appears as~\cite{cohen99}}, url = {http://www.research.whizbang.com/~wcohen/postscript/sigir-96.ps}, abstract = {two machine learning algorithms, ripper and sleeping experts for phrases, are evaluated on a number of large text categorization problems.
even better results can be obtained, when replacing the likelihood-ratio-statics by a new measure for pruning; this we call l-measure.
{391--401}, volume = {52}, url = {http://www3.interscience.wiley.com/cgi-bin/fulltext?id=76511157&placebo=ie.pdf}, abstract = {
this paper applies two statistical learning algorithms,  the linear least squares fit (llsf) mapping and a nearest neighbor classifier  named expnet, to a large collection of medline documents.
% % this is a bibliography, in bibtex format, on automated text % % categorization (atc), defined as the activity of automatically % % building, by means of machine learning techniques, automatic text % % classifiers, i.e. systems capable of assigning to a text % % document one or more thematic categories from a predefined set.
the author presents the core technology of teklis, the results on the filtering and routing tasks and a discussion of the insights gained through participation in the exercise.}, } @inproceedings{cai03, author = {lijuan cai and thomas hofmann}, title = {
tampere, fi}, year = {2002}, pages = {97--104}, url = {http://doi.acm.org/10.1145/564376.564395}, abstract = {
its aim is to disengage the user from learning complex tools and from performing tedious and repetitive actions.}, } @inproceedings{amati97, author = {gianni amati and fabio crestani and flavio ubaldini}, title = {
first, we use the system to compute profiles on training set data that represent the various categories, e.g., language samples or newsgroup content samples.
"knowing a web page by the company it  keeps", booktitle = "cikm", year =
lda is a three-level hierarchical bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics.
an experimental study in automatically categorizing medical documents}, journal = {journal of the american society for information science and technology}, year = {2001}, number = {5}, pages =
{information storage and retrieval}, year = {1973}, volume = {9}, number = {2},  pages = {79--84}, url = {}, abstract = {
making no assumptions on the mechanism generating the data instances, and assuming a linear noise model for the labels, we bound the h-loss of our on-line algorithm in terms of the h-loss of a reference classifier knowing the true parameters of the label-generating process.
{1995}, url = {http://www.research.whizbang.com/~wcohen/postscript/ilp.ps},  abstract = {
our experiments demonstrate that this new approach is able to learn more accurate classifiers than either of its constituent methods alone.}, } @inproceedings{slonim01, author = {
the accuracy of classification achieved with our method appears better than or comparable to those of existing rule-based methods.
while most work on kdd has been concerned with structured databases, there has been little work on handling the huge amount of information that is available only in unstructured textual form.
we present a method to detect automatically pornographic content on  the web.
we show how a naive bayes classification can be enhanced to  incorporate the similarity information present in source catalogs.
"gliozzo, alfio and strapparava, carlo", title =
it may contain many unnecessary and irrelevant features.
the  automatic indexing system {air/phys}.
our key insight is that many of the data sources have their own categorization, and classification accuracy can be improved by factoring in the implicit information in these source categorizations.
"ganesh ramakrishnan and chitrapura, krishna prasad and raghu krishnapuram  and pushpak bhattacharyya", title =
in this paper, we propose and evaluate several database categorization algorithms.
coding (ecoc) for learning text classifiers.
"2006", pages =  "662--671", } @inproceedings{shen:2006:tci, author =
} @inproceedings{lee00, author = {hahn-ming lee and chih-ming chen and cheng-wei hwang}, title = {a neural network document classifier with linguistic feature selection}, booktitle = {proceedings of iea/aie-00, 13th international conference on industrial and engineering applications of artificial intelligence and expert systems}, publisher = {}, editor = {}, year = {2003}, address = {new orleans, us}, pages = {555--560}, url = {}, abstract = {}, } @inproceedings{lee02, author = {yong-bae lee and sung h. myaeng}, title = {
methods of information filtering and fetching are then required.
published in the  ``lecture notes in computer science'' series, number 1810}, year = {2000}, url  = {http://www.lsi.upc.es/~escudero/recerca/ecml00.pdf}, abstract = {
however, many real-world domains are best described by relational models in which instances of multiple types are related to each other in complex ways.
phrases are represented by an abstraction called head/modifier pairs.
in contrast, software for text categorization, message filtering,  textual data mining, and related tasks is less common.
published in the ``lecture notes in computer science'' series, number 2997}, pages = {197--208}, url = {http://springerlink.metapress.com/openurl.asp?genre=article&issn=0302-9743&volume=2997&spage=197}, abstract = {high dimensionality of feature space is a main obstacle for text categorization (tc).
possible specific scenarios within this framework include the  learning of the routing of publication titles or news titles.
when such urls were not % % available, sometimes a url with  restricted access (e.g. the % % acm digital library or the ieee computing  society digital % % library, which are accessible to subscribers only) is  indicated.
finally, the system computes a distance measure  between the document's profile and each of the category profiles.
{hierarchical classification of real life documents}, booktitle = {proceedings  of the 1st siam international conference on data mining}, publisher = {},  editor = {}, year = {2001}, address = {chicago, us}, pages = {}, url =  {http://www.cs.sfu.ca/~wangk/pub/sdm2001.ps}, abstract = {}, }  @inproceedings{wang04, author = {gang wang and frederick h. lochovsky}, title =  {feature selection with conditional mutual information maximin in text  categorization}, booktitle = {proceedings of cikm-04, 13th acm international  conference on information and knowledge management}, publisher = {acm press,  new york, us}, address = {washington, us}, editor = {david a. evans and luis  gravano and otthein herzog and chengxiang zhai and marc ronthaler}, year =
we present experimental results in learning to classify email in this fashion, where each class corresponds to a verb-noun pair taken from a predefined ontology describing typical 'email speech acts'.
"2006", pages =  "210--219", abstract =
in this paper, we present an alternative framework that builds on (conditional) markov networks and addresses two limitations of the previous approach.
philip j. hayes and steven p. weinstein}, title
an improved probabilistic text categorization method is also presented.}, } @phdthesis{lewis92a, author = {lewis, david d.}, title = {representation and learning in information retrieval}, school = {department of computer science, university of massachusetts}, address = {
although the  corpus contains documents written in chinese, the proposed approach can be  applied to documents written in any language and such documents can be  transformed into a list of separated terms.}, } @inproceedings{yang01, author =
one important goal in text mining is automatic classification of electronic documents.
for very long codes, the performance is in  some cases further improved by kl-distance optimization.}, }  @inproceedings{klas00, author = {klas, claus-peter and fuhr, norbert}, title =
it outperforms existing systems by keeping most of their interesting properties (i.e. easy implementation, low complexity and high scalability).
traditionally, each component value is  assigned using the information retrieval tfidf measure.
"485--492", abstract = "automatic  classification of data items, based on training samples, can be boosted by  considering the neighborhood of data items in a graph structure (e.g.,  neighboring documents in a hyperlink environment or co-authors and their  publications for bibliographic data entries).
the effect of using  hierarchical classifiers in text categorization}, booktitle = {proceeding of  riao-00, 6th international conference ``recherche d'information assistee par  ordinateur''}, editor = {}, address = {paris, fr}, year = {2000}, pages =  {302--313}, url = {http://www.iona.edu/cs/facultypublications/riao2000new.pdf},  abstract = {
the effect of using hierarchical classifiers in text categorization}, booktitle = {proceeding of riao-00, 6th international conference ``recherche d'information assistee par ordinateur''}, editor = {}, address = {paris, fr}, year = {2000}, pages = {302--313}, url = {http://www.iona.edu/cs/facultypublications/riao2000new.pdf}, abstract = {
"joachims, thorsten",  title =
published in the ``lecture notes in computer science'' series, number 2945}, pages = {571--579}, url = {}, abstract = {}, } @inproceedings{cheng01, author = {cheng, chun-hung and jian tang and ada wai-chee and irwin king}, title = {hierarchical classification of documents with error control}, booktitle = {proceedings of pakdd-01, 5th pacific-asia conferenece on knowledge discovery and data mining}, editor = {david cheung and qing li and graham williams}, year = {2001}, publisher = {springer verlag, heidelberg, de}, address = {hong kong, cn}, note = {
this work addresses a logical approach to text categorization inside a framework aimed at full automatic paper document processing.
two problems arise from this basic approach.
meanwhile, the web object representation is constructed  by hyperlink analysis, and further pruned to remove the noises.
based on a revision of self-organizing maps, namely taxsom, the proposed model performs an unsupervised classification, exploiting the a-priori knowledge encoded in a taxonomy structure both at the terminological and topological level.
applied to trec-7 and trec-8 filtering track documents, the proposed  method obtained a significant improvement in lf1, lf2, fl and f3 measures  compared to the best results submitted by other trec entries.}, }  @inproceedings{kim04, author =
this is because when the positive training data is  too few, the boundary over-iterates and trespasses the natural gaps between  positive and negative class in the feature space and thus ends up fitting  tightly around the few positive training data.}, } @inproceedings{yu98, author  = {kwok l. yu and wai lam}, title = {a new on-line learning algorithm for  adaptive text filtering}, booktitle = {proceedings of cikm-98, 7th acm  international conference on information and knowledge management}, publisher =
"feature  selection based on the shapley value", pages =
existing learning techniques must be adapted or improved in order to effectively handle difficult situations where the number of positive training instances per event is extremely small, the majority of training documents are unlabelled, and most of the events have a short duration in time.
"automatic learning features using bootstrapping for text categorization", booktitle = "computational linguistics and intelligent text processing (lecture notes in computer science, vol. 2945)", year =
romain vinot and fran{\c{c}}ois yvon}, title  = {
the training approaches  we test are the rocchio (relevance feedback) and the widrow-hoff (machine  learning) algorithms and wordnet as the lexical database.
we are facing a new challenge due to the fact that web documents have a rich structure and are highly heterogeneous.
"kybernetika", year = 2003, volume = 39, number = 5, pages =
tampere, fi}, year = {2002}, pages = {137--144}, url = {http://doi.acm.org/10.1145/564376.564402}, abstract = {to improve performance in text categorization, it is important to extract distinctive features for each class.
analyzing the applicability and effectiveness of co-training}, booktitle =
}  @inproceedings{ghamrawi:2005:cmc, author =
multiple noise reduction strategies are proposed and  evaluated, including: an aggressive removal of ``noninformative words'' from  texts before training; the use of a truncated singular value decomposition to  cut off noisy ``latent semantic structures'' during training; the elimination  of noninfluential components in the llsf solution (a word concept association  matrix) after training.
current text learning techniques for combining labeled and unlabeled, such as em and co-training, are mostly applicable for classification tasks with a small number of classes and do not scale up well for large multiclass problems.
a typical  example is information filtering, i.e. the adaptive classification of documents  with respect to a particular user interest.
however, even in these extended formulations  the problem of tuning its parameters is still neglected.
william a. gale and kenneth w. church and david yarowsky}, title = {
the authors' initial work with this algorithm has demonstrated that probabilistic structures can be automatically acquired from a training set of documents with respect to a single target concept, or a set of related concepts.
george forman}, title
"a novel refinement approach for text  categorization", booktitle =
a  companion web page is available at % %  http://www.cs.technion.ac.il/~gabr/resources/atc/atcbib.html
{yiming yang and xin liu}, title = {
"combined  syntactic and semanitc kernels for text classification", booktitle =  "proceedings of the 29th european conference on information  retrieval", year =
we designed a special evolutionary algorithm with a two-pool strategy for this changing environment.}, } @inproceedings{tauritz99, author = {daniel r. tauritz and ida g. sprinkhuizen-kuyper}, title = {adaptive information filtering algorithms}, booktitle = {proceedings of ida-99, 3rd symposium on intelligent data analysis}, publisher = {springer verlag, heidelberg, de}, note = {
proceedings of  coling-94, 15th international conference on computational linguistics},  publisher = {}, editor = {}, address = {
this paper describes an approach to apply term distributions, in addition to tf and idf, to improve performance of centroid-based text categorization.
masoud mohammadian}, publisher = {ios press}, address = {amsterdam, nl}, pages = {194--198}, year = {1999}, url = {}, abstract = {
no prior information about document  content or language is required.
published in the ``lecture notes in computer  science'' series, number 1446}, editor = {david page}, year = {1998}, pages =
machine learning techniques developed for learning on text data are used here on the hierarchical classification structure.
it was found that categorizers consisting of the words with highest tf.idf  values scored best.}, } @inproceedings{paliouras99, author = {
this paper introduces a new criterium for term selection, which is  based on the notion of uncertainty.
specifically, we define five {\em hypertext regularities} which may (or may not) hold in a particular application domain, and whose presence (or absence) may significantly influence the optimal design of a classifier.
our approach has practical advantages for problem solving by introducing the viewpoint of tasks to achieve higher performance.}, } @inproceedings{mccallum98, author = {andrew k. mccallum and kamal nigam}, title = {
in this paper, we present an approach to the  integration of lexical knowledge extracted from the ldb wordnet in  learning-based atc, based on stacked generalization (sg).
experimental results show that winnow with thesauri attains high accuracy and that the proposed filtering and disambiguation methods also contribute to the improved accuracy.}, } @inproceedings{yang00, author = {yiming yang and thomas ault and thomas pierce and charles w. lattimer}, title = {
{international journal of digital libraries}, year = {2000}, number = {3},  volume = {3}, pages = {261--275}, url =  {http://www.cs.columbia.edu/~sable/research/ijodl00.pdf}, abstract = {
a system called  autoslog is presented which automatically constructs dictionaries for  information extraction, given an appropriate training corpus.
our approach is similar to the query by committee framework,  where disagreement among the committee members on the predicted label for the  input part of the example is used to signal the need for knowing the actual  value of the label.
"shay cohen and eytan ruppin and gideon dror", title =
the design of the new approach as  well as its justification are presented.
text classification in a hierarchical mixture model for  small training sets}, booktitle = {proceedings of cikm-01, 10th acm  international conference on information and knowledge management}, publisher =
an experiment has been carried out to measure the performance of our proposed hierarchical classification method.
moreover, the estimators developed here address the special performance measures needed for evaluating text classifiers.
text categorization with the concept of fuzzy set of informative keywords}, booktitle = {proceedings of fuzz-ieee'99, ieee international conference on fuzzy systems}, editor = {}, publisher = {ieee computer society press, los alamitos, us}, address = {
{acm press, new york, us}, address = {toronto, ca}, year = {2003}, pages = {190--197}, url = {http://doi.acm.org/10.1145/860435.860471}, abstract = {real-world applications often require the classification of documents under situations of small number of features, mis-labeled documents and rare positive examples.
stephen huffman}, title = {acquaintance:  language-independent document categorization by n-grams}, booktitle =
moreover, the number of irrelevant references  gathered by our system is about one-thirteenth that of traditional  keyword-based search systems.
"proceedings of the 29th annual  international acm sigir conference on research and development in information  retrieval", year =
this procedure involves a complex  hierarchical taxonomy, within which we classify documents into 114 classes and  451 subclasses.
this paper studies the effects of boosting in the context of different classification methods for text categorization, including decision trees, naive bayes, support vector machines (svms) and a rocchio-style classifier.
for very long codes, the performance is in some cases further improved by kl-distance optimization.}, } @inproceedings{klas00, author = {klas, claus-peter and fuhr, norbert}, title = {a new effective approach for categorizing web documents}, booktitle = {proceedings of bcsirsg-00, the 22nd annual colloquium of the british computer society information retrieval specialist group}, editor = {}, address = {cambridge, uk}, year = {2000}, pages = {}, publisher = {}, url = {http://ls6-www.informatik.uni-dortmund.de/bib/fulltext/ir/klas_fuhr:00.ps.gz}, abstract = {categorization of web documents poses a new challenge for automatic classification methods.
the background of rs theory is presented, with an illustrative example to  demonstrate the operation of the rs-based dimensionality reduction.
in  previous work, we developed several algorithms that use information extraction  techniques to achieve high-precision text categorization.
seattle, us}, year = {1998}, pages = {307--318}, url = {http://www.acm.org/pubs/articles/proceedings/mod/276304/p307-chakrabarti/p307-chakrabarti.pdf}, abstract = {
text categorization using adaptive context trees}, booktitle =
{309--320}, note = {
booktitle = {proceedings of spire-03, 10th international symposium on string  processing and information retrieval}, editor = {mario a. nascimento and edleno  s. de moura and arlindo l. oliveira}, publisher = {springer verlag, heidelberg,  de}, note = {
our results confirm, quantify, and extend previous research using web structure  in these areas, introducing new methods for classification and description of  pages.}, } @inproceedings{godbole04, author = {shantanu godbole and abhay  harpale and sunita sarawagi and soumen chakrabarti}, title = {document  classification through interactive supervision of document and term labels},  booktitle = {proceedings of pkdd-04, 8th european conference on principles of  data mining and knowledge discovery}, editor = {jean-fran{\c{c}}ois boulicaut  and floriana esposito and fosca giannotti and dino pedreschi}, address = {
in this paper, a new novelty detection approach based on the identification of sentence level patterns is proposed.
the empirical evaluation  indicates that the error rate (as obtained by running the naive bayes  classifier on isolated pages) can be significantly reduced if contextual  information is incorporated.}, } @inproceedings{frommholz01, author = {ingo  frommholz}, title = {
mastersthesis % = a master's thesis % required: author, title,  school, year % optional: type, address, month, note % %
the results showed that the proposed model was able to  achieve high categorization effectiveness as measured by precision and recall.
the msdn corpus is collected from an online news website maintained by the min-sheng daily news, taiwan.
the architecture is based on the metaphor of the software agents and incorporates innovative hints from other fields: distributed architectures, relevance feedback and active interfaces.
experiments in the  terrorism domain suggest that increasing the amount of linguistic context can  improve performance.
experimental  results confirm improved performance, breaking through the plateau previously  reached in the field."
this approach has the advantage of being able to recommend previously unrated items to users with unique interests and to provide explanations for its recommendations.
proceeding of ijcai-01, 17th  international joint conference on artificial intelligence}, editor = {bernhard  nebel}, address = {
while maintaining its other advantages." } @inproceedings{couto:2006:csc, author =
abis compares  documents with the past situations and finds the similarity scores on the basis  of a memory-based reasoning approach.}, } @article{amati99, author = {
classification experiments usually grab a snapshot (temporally and spatially) of the web for a corpus.
we show that an old corpus  can be used for training when testing on new web pages, with only a marginal  drop in accuracy rates on genre classification.
this level of performance should be satisfactory for a wide variety of applications.}, } @inproceedings{tzeras93, author = {tzeras, konstadinos and hartmann, stephan}, title = {automatic indexing based on bayesian inference networks}, booktitle = {proceedings of sigir-93, 16th acm international conference on research and development in information retrieval}, editor =
we show that a good hierarchical machine learning-based categoriser can be developed using small numbers of features from pre-categorised training documents.
arvin agah and jamie callan and elke  rundensteiner}, year = {2000}, pages = {373--374}, url =  {http://www.cs.ust.hk/~meretaks/papers/mfll-cikm2000.pdf}, abstract = {naive  bayes (nb) classifier has long been considered a core methodology in text  classification mainly due to its simplicity and computational efficiency.
we built  these profiles by selecting feature words and phrases from the training  documents.
this work presents a system for the  categorization of noisy texts.
{81--86}, address = {bari, it}, url =  {http://link.springer.de/link/service/series/0558/papers/2175/21750081.pdf},  abstract =
furthermore, the use of shape coding is particularly advantageous over ocr in  the processing of page images of poor quality.
booktitle = {proceedings of  ecir-01, 23rd
empirical results on the reuters-22173 collection are also discussed.}, } @article{kwon03, author = {
our experimental results on a large dataset confirm that the use of the implicit links is better than using explicit links in classification performance, with an increase of more than 10.5\% in terms of the macro-f1 measurement."
"we demonstrate the value of using context in a new-information detection system that achieved the highest precision scores at the text retrieval conference's novelty track in 2004.
we approach the problem of automatically extracting keyphrases from text  as a supervised learning task.
subjective human evaluation of the keyphrases  generated by extractor suggests that about 80\% of the keyphrases are  acceptable to human readers.
lda  is a three-level hierarchical bayesian model, in which each item of a  collection is modeled as a finite mixture over an underlying set of topics.
booktitle = {proceedings of  sigir-02, 25th acm international conference on research and development in  information retrieval}, editor =
unrestricted  potentially faulty text messages arrive at a certain delivery point (e.g. email  address or world wide web address).
in text categorization (tc) based on the vector space model,  feature weighting is vital for the categorization effectiveness.
using a bayesian network model, we combine  these measures with the results obtained by traditional content-based  classifiers.
in particular, we evaluate the vector and latent semantic analysis (lsa) methods, a classifier based on support vector machines (svm) and the k-nearest neighbor variations of the vector and lsa models.
in this work we study three mistake-driven learning algorithms for a  typical task of this nature - text categorization.
i have since fixed the bug and rerun the experiments.
we discuss the role of structural information for classification and describe experiments on a small collection of class labeled structured documents.
abis minimizes user's  effort in selecting the huge amount of available documents.
it combines a well known feature  selection criterion, the information gain, and a new algorithm that selects and  adds a feature to a bag-of-words if it does not occur too often with the  features already in a small set composed of the best features selected so far  for their high information gain.
the method is inspired by \emph{text categorization} (tc), the discipline concerned with labelling natural language texts with labels from a predefined set of domains, or categories.
the experiments reported in this paper deal with the relationship between specific formal textual features, i.e. graphic and phonetic information, and the reader's literary educational background in the categorization of poetic texts.
to address this limitation, we propose the second meta-model approach, called meta-learning using document feature characteristics (mudof), which employs a meta-learning phase using document feature characteristics.
conversely,  a category's level is increased to strengthen it if its precision exceeds its  recall.
{dordrecht, nl}, url =  {http://www.cs.utah.edu/~riloff/psfiles/nlp-ir-chapter.ps}, abstract = {
the {reuters corpus volume 1} -- from yesterday's news to tomorrow's language resources}, booktitle = {proceedings of lrec-02, 3rd international conference on language resources and evaluation}, year = {2002}, address = {las palmas, es}, pages = {827--832}, } @inproceedings{rosso04, author = {paolo rosso and antonio molina and ferran pla and daniel jiménez and vicent vidal}, title = {information retrieval and text categorization with semantic indexing}, booktitle = {proceedings of cicling-04, 5th international conference on computational linguistics and intelligent text processing}, year = {2004}, editor = {
arbee l. chen and frederick h. lochovsky}, publisher = {ieee computer society press, los alamitos, us}, year = {1999}, address = {hsinchu, tw}, pages = {195--202}, url = {http://dlib.computer.org/conferen/dasfaa/0084/pdf/00840195.pdf}, abstract = {in a text categorization model using an artificial neural network as the text classifier scalability is poor if the neural network is trained using the raw feature space since textural data has a very high-dimension feature space.
adelaide, au}, year = {2003}, pages = {211--214}, url = {}, abstract = {}, } @article{carbonell00, author = {jaime carbonell and william w. cohen and yiming yang}, title = {guest editors' introduction to the special issue on machine learning and information retrieval}, journal = {machine learning}, volume = {39}, number = {2/3}, pages = {99--101}, year = {2000}, url = {http://www.wkap.nl/article.pdf?255754}, } @inproceedings{cardoso03, author = {
web genre refers to the type of the page characterized by features such as style, form or presentation layout, and meta-content; web genre can be used to tune spider crawling re-visits and inform relevance judgments for search engines.
{18}, number = {2/3}, pages = {
these distributions are useful to increase classification accuracy by exploiting information of (1) term distribution among classes, (2) term distribution within a class and (3) term distribution in the whole collection of training data.
via giovanni battista belzoni, 7 - 35131 padova,  italy % % http://www.math.unipd.it/~fabseb60/ %
feature  clustering is a powerful alternative to feature selection for reducing the  dimensionality of text data.
in contrast to prior work along these lines, our approach employs a number of novel techniques: dynamically inferring the link/class pattern in the graph in the run of the iterative relaxation labeling, judicious pruning of edges from the neighborhood graph based on node dissimilarities and node degrees, weighting the influence of edges based on a distance metric between the classification labels of interest and weighting edges by content similarity measures.
we experimented with pre-classified samples from {{\sc yahoo!}}\ and the us patent database.
text categorization experiments demonstrates the ability of this representation to catch information about the semantic content of the text.}, } @inproceedings{viechnicki98, author = {peter viechnicki}, title = {
published in the ``lecture notes in computer science'' series, number 3202}, url = {}, abstract = {}, } @incollection{forsyth99, author = {richard s. forsyth}, title = {new directions in text categorization}, editor = {alex gammerman}, booktitle = {causal models and intelligent data management}, publisher = {springer verlag}, address = {heidelberg, de}, year = {1999}, pages = {151--185}, url = {}, abstract = {}, } @inproceedings{frank00, author = {
alternative methods  are also tested on these data collections for comparison.
{saarbr{\"{u}}cken, de}, url =  {http://nlp3.korea.ac.kr/proceeding/coling2000/coling/ps/066.ps}, abstract =  {the goal of text categorization is to classify documents into a certain number  of pre-defined categories.
in contrast, mi had relatively poor performance due to its bias towards favoring rare terms, and its sensitivity to probability estimation errors.}, } @inproceedings{yang99, author =
in this paper we describe and evaluate a learning model for  information filtering which is an adaptation of the generalized probabilistic  model of information retrieval.
our experiments use reuters 21578 database  and consist of binary classifications for categories selected from the 89  topics classes of the reuters collection.
"improving naive bayes text classifier using smoothing methods", booktitle =
"acm press", url =  "http://doi.acm.org/10.1145/1099554.1099687", abstract =
{acm transactions on information systems}, year = {1994}, number = {3}, volume  = {12}, pages = {233--251}, url =
differences include: different notions as to what constitutes a  context; different ways of combining contexts to construct a classifier;  different methods to search for a combination of contexts; and different  criteria as to what contexts should be included in such a combination.
the exponential growth of the internet has led to a great deal of interest in developing useful and efficient tools and software to assist users in searching the web.
the  mfom learning framework is evaluated on the reuters-21578 task with lsi-based  feature extraction and a binary tree classifier.
roberto  basili and alessandro moschitti and maria t. pazienza}, title = {
although much research has been done on text categorization, this algorithm is novel in that it is unsupervised, i.e., it does not require pre-labeled training examples, and it can assign multiple category labels to documents.
booktitle = {proceedings of ismis-99, 11th international  symposium on methodologies for intelligent systems}, editor = {
it is natural to assume that the documents are on the multinomial manifold, which is the simplex of multinomial models furnished with the riemannian structure induced by the fisher information metric.
in this paper, based on the rule  learning algorithm ripper (for repeated incremental pruning to produce error  reduction), we propose an efficient method for hierarchical document  categorization.}, } @article{schapire00, author = {schapire, robert e. and  singer, yoram}, title = {{{\sc boostexter}}: a boosting-based system for text  categorization}, journal = {machine learning}, year = {2000}, number = {2/3},  volume = {39}, pages = {135-168}, url =  {http://www.research.att.com/~schapire/papers/schapiresi98b.ps.z}, abstract =  {this work focuses on algorithms which learn from examples to perform  multiclass text and speech categorization tasks.
the  normalization to the relative word frequency, the principal component analysis  (k-l transformation) and the power transformation were applied to the feature  vectors, which were classified by the euclidean distance, the linear  discriminant function, the projection distance, the modified projection  distance and the svm.}, } @inproceedings{yan:2005:ocfs, author =
applying an existing machine learning algorithm to text categorization}, booktitle = {connectionist, statistical, and symbolic approaches to learning for natural language processing}, editor = {stefan wermter and ellen riloff and gabriele scheler}, pages = {343--354}, year = {1996}, publisher = {springer verlag, heidelberg, de}, note = {
= {santa barbara, us}, pages = {67--78}, url = {http://doi.acm.org/10.1145/375663.375671}, abstract = {
the hierarchical structure of categories may be generated by recursively applying the same method.
in a batch mode, the  programs to accomplish this indexing would require no more than fifteen minutes  of cpu time per week.}, } @article{klingbiel73a, author = {paul h. klingbiel},  title = {a technique for machine-aided indexing}, journal = {information  storage and retrieval}, year = {1973}, volume = {9}, number = {9}, pages =
then, two schemes called s-br1 and s-br2 are proposed to deal with these bigrams: the former directly eliminates them from the feature set whereas the latter replaces them with the corresponding significant characters involved.
text categorization}, pages =
to control overfitting in me estimation, we propose the use of box-type inequality constraints, where equality can be violated up to certain predefined levels that reflect this uncertainty.
specifically, we show that by adjusting the category levels in a principled way, that precision can be significantly improved, from 84\% to 91\%, on the much-studied reuters-21578 corpus organized in a three-level hierarchy of categories.}, } @article{damashek95, author = {marc damashek}, title = {
the method is evaluated  using backpropagation neural networks, as the machine learning algorithm, that  learn to assign mesh categories to a subset of medline records.
it produces inferior results because it is insensitive to subtle differences between articles that belong to a category and those that do not.
seattle, us}, year = {2001}, pages = {897--902}, url = {}, abstract = {
a generic  system for text categorization is presented which is based on statistical  analysis of representative text corpora.
in order to obtain class labels for training samples, we conducted a study where subjects ranked document pages with respect to their resemblance to representative page images.
proceedings of sigir-00, 23rd acm international conference on research and development in information retrieval}, editor = {nicholas j. belkin and peter ingwersen and mun-kew leong}, publisher
"salvador,  brazil", abstract =
in this paper we propose simple, heuristic solutions to some of the problems with naive bayes classifiers, addressing both systemic issues as well as problems that arise because text is not actually generated according to a multinomial model.
researchers have used benchmark data, such as the reuters-21578 test collection, to measure advances in automated text categorization.
our approach has been tested on the standard  reuters text categorization collection.
for example, we have previously shown  how foil, a relational learner, can learn to classify web pages by discovering  training set regularities in the words occurring on target pages, and on other  pages related by hyperlinks.
we use image features such as percentages of text and non-text (graphics,  images, tables, and rulings) content regions, column structures, relative point  sizes of fonts, density of content area, and statistics of features of  connected components which can be derived without class knowledge.
grammatical errors do not exceed five per cent of the output, so human screening is satisfactorily low.
however, like most machine learning  algorithms, they are generally applied using a randomly selected training set  classified in advance.
three different cost scenarios are identified, and suitable  cost-sensitive evaluation functions are employed.
edmund  s. yu and elizabeth d. liddy}, title = {feature selection in text  categorization using the baldwin effect networks}, booktitle = {proceedings of  ijcnn-99, 10th international joint conference on neural networks}, editor = {},  publisher =
when parameters tuned on an early benchmark tdt corpus were evaluated on a later tdt benchmark corpus with no overlapping events, we observed a 38-65\% reduction in tracking cost (a weighted combination of errors) by the combined system over the individual methods evaluated under the same conditions, strongly suggesting the robustness of this approach as a solution for improving cross-class performance consistency of statistical classifiers when standard cross-validation fails due to the lack of representative validation sets.}, } @inproceedings{yang00b, author = {hsin-chang yang and chung-hong lee}, title = {automatic category generation for text documents by self-organizing maps}, booktitle = {proceedings of ijcnn-00, 11th international joint conference on neural networks}, publisher =
while hand-crafting rules for both tasks has a long tradition, learning approaches used to gain much interest in the past.
an evaluation of statistical approaches to medline indexing}, booktitle = {proceedings of amia-96, fall symposium of the american medical informatics association}, editor = {james j. cimino}, publisher =
we  present an algorithm for learning a value function that maps hyperlinks to  future discounted reward using a naive bayes text classifier.
as a consequence, these algorithms cannot take full advantage of the ``weighted'' representations (consisting of vectors of continuous attributes) that are customary in information retrieval tasks, and that provide a much more significant rendition of the document's content than binary representations.
the focus this year is on adaptive filtering, where the system begins with only the topic statement and must interactively adjust a filtering profile constructed from that topic in response to on-line feedback.
the first concerns an information filtering system  based on an adaptation of the generalized probabilistic model of information  retrieval.
published in the ``lecture notes in computer science'' series, number 1925}, url = {}, abstract =
"a model for handling approximate, noisy or incomplete labeling in text classification", booktitle =
some computational experiments have investigated the effectiveness of the ocat-based approach and compared it to the well-known vector space model (vsm).
a theoretical analysis and experiments show that the new method can effectively estimate the performance of svm text classifiers in an efficient way.}, } @inproceedings{joachims01b, author = {thorsten joachims and nello cristianini and john shawe-taylor}, title = {composite kernels for hypertext categorisation}, booktitle = {proceedings of icml-01, 18th international conference on machine learning}, editor = {carla brodley and andrea danyluk}, address = {williams college, us}, year = {2001}, pages = {250--257}, publisher = {morgan kaufmann publishers, san francisco, us}, url = {http://www.cs.cornell.edu/people/tj/publications/joachims_etal_01a.pdf}, abstract = {kernels are problem-specific functions that act as an interface between the learning system and the data.
support  vector machines (svms) have been recognized as one of the most successful  classification methods for many applications including text classification.
a standard  approach is to classify each entity independently, ignoring the correlations  between them.
published in the ``lecture notes in computer science'' series, number 2035}, pages = {66--77}, url = {http://link.springer.de/link/service/series/0558/papers/2035/20350066.pdf}, abstract =
machine learning for text classification is the cornerstone of document categorization, news filtering, document routing, and personalization.
denotes the task of automatically finding relevant categories for a (new) document which is to be inserted into a web catalogue like yahoo!.
given a labeled  or partially labeled training corpus of text documents, the model estimates the  joint distribution of training documents and class labels by using a  generalization of the expectation maximization algorithm.
"2007", month =  "april", address =
after identifying the weakness and strength of each technique, we propose a new technique known as the generalized instance set (gis) algorithm by unifying the strengths of lnn and linear classifiers and adapting to characteristics of text categorization problems.
in the process of document indexing, a  document is represented as a set of informative keywords.
kluwer academic publishers}, address =
little words can  make a big difference for text classification},
william  a. gale and kenneth w. church and david yarowsky}, title = {
again, boosting compares  favourably to the other benchmark algorithms.}, } @article{fall03, author = {c.  j. fall and a. t{\"o}rcsv{\'a}ri and k. benzineb and g. karetka}, title =
published in the ``lecture notes in computer science'' series, number 1398}, editor = {claire n{\'{e}}dellec and c{\'{e}}line rouveirol}, address = {chemnitz, de}, pages = {95--100}, year = {1998}, url = {http://www-ai.ijs.si/dunjamladenic/papers/pww/pwwecml98.ps.gz}, abstract = {this paper describes several known and some new methods for feature subset selection on large text data.
from these, we extract  vocabulary, words that appear with high frequency within a given category,  characterizing each subject area.
boosting trees for anti-spam email filtering}, year = {2001}, editor = {}, booktitle = {proceedings of ranlp-01, 4th international conference on recent advances in natural language processing}, address = {tzigov chark, bg}, pages = {}, url = {http://www.lsi.upc.es/~carreras/pub/boospam.ps}, } @inproceedings{cavnar94, author =
the  feature quantity, a quantitative representation of specificity introduced in  this paper, is based on an information theoretic perspective of co-occurrence  events between terms and documents.
the results suggest that the spatial and the temporal similarity measures need to be improved.
we find that both algorithms achieve reasonable performance and allow controlled tradeoffs between false positives and false negatives.
in this paper we present an approach to discovering additional regularities in the test set, and show that in relational domains such test set regularities can be used to improve classification accuracy beyond that achieved using the training set alone.
we demonstrate that when learning from labeled and  unlabeled data, algorithms explicitly leveraging a natural independent split of  the features outperform algorithms that do not.
a feature mining based  approach for the classification of text documents into disjoint classes},  journal = {information processing and management}, year = {2002}, volume =
this structure can be expressed as a sequence of subtopic text blocks, or passages.
our  results on a collection of scanned journals from the making of america project
our working hypothesis is that it is often easier to classify a hypertext page using information provided on pages that point to it instead of using information that is provided on the page itself.
this is a problem in which there are large amounts of data available, but the rules for classification are not explicitly available.
our approach has been tested on the standard reuters text categorization collection.
text categorization as an information  retrieval task}, journal = {the south african computer journal}, year = {1999},  pages = {4--15}, volume = {21}, url = {}, abstract = {
the indexing process determines whether an index term is assigned to a certain  document, based on the relationship constructed by the learning process, and  the text found in the document.
machine learning techniques are used on data collected from yahoo, a large text hierarchy of web documents.
our findings  suggest that the best results occur when using the very brief descriptions of  the {{\sc yahoo!}}\ categorized entries; these brief descriptions are provided  either by the entries' submitters or by the {{\sc yahoo!}}\ human indexers and  accompany most {{\sc yahoo!}}\-indexed entries.}, } @inproceedings{lai01,  author = {kwok-yin lai and wai lam}, title = {meta-learning models for  automatic textual document categorization}, booktitle = {proceedings of  pakdd-01, 5th pacific-asia conferenece on knowledge discovery and data mining},  editor = {david cheung and qing li and graham williams}, year = {2001},  publisher = {springer verlag, heidelberg, de}, address = {hong kong, cn}, note  = {
these questions are addressed in an overview of the existing approaches to text classification.
this paper addresses the problem with respect to a set of popular algorithms in text categorization, including support vector machines, k-nearest neighbor, ridge regression, linear least square fit and logistic regression.
a hierarchical model  for clustering and categorising documents}, booktitle = {proceedings of  ecir-02, 24th european colloquium on information retrieval research}, editor =
in this paper, we develop a  framework to incorporate unlabeled data in the error-correcting output coding  (ecoc) setup by first decomposing multiclass problems into multiple binary  problems and then using co-training to learn the individual binary  classification problems.
we apply a two-phase framework to tackle the above difficulties.
extensive experiments have been conducted on a real-world document collection and satisfactory performance is obtained.}, } @article{lai02, author = {yu-sheng lai and chung-hsien wu}, title = {meaningful term extraction and discriminative term selection in text categorization via unknown-word methodology}, journal = {acm transactions on asian language information processing}, year = {2002}, number = {1}, volume = {1}, pages = {34--64}, url = {http://doi.acm.org/10.1145/595576.595579}, abstract = {
the changes may occur both on the  transmission side (the nature of the streams can change) and on the reception  side (the interests of a user can change).
experimental results on these data sets confirm that waknn consistently outperforms other existing classification algorithms.}, } @article{hanauer96, author = {david hanauer}, title = {
we compare these methods in terms of their loss functions and score distributions, and establish the connection between their optimization problems and generalization error bounds.
the proposed architecture relies on hidden markov models whose emissions are bag-of-words according to a multinomial word event model, as in the generative portion of the naive bayes classifier.
first, unlabeled data can hurt performance in domains where the generative modeling assumptions are too strongly violated.
= {new brunswick, us}, pages = {148--156}, publisher = {
k-nn is one of the most popular document categorization  methods because it shows relatively good performance in spite of its  simplicity.
{morgan kaufmann publishers, san francisco, us}, url = {}, abstract = {}, }  @inproceedings{li03b, author = {tao li and shenghuo zhu and mitsunori ogihara},  title = {efficient multi-way text categorization via generalized discriminant  analysis}, booktitle = {proceedings of cikm-03, 12th acm international  conference on information and knowledge management}, publisher = {acm press,  new york, us}, editor = {}, year = {2003}, address = {new orleans, us}, pages =
from these web subgraphs, web  units are constructed and classified into semantic concepts (or categories) in  an iterative manner.
specifically, we apply this to a maximum entropy classifier on a large scale multi-class text categorization task: the online job directory flipdog with over half a million jobs in 65 categories.}, note = {
prior to text categorization, a feature generator analyzes the documents and maps them onto appropriate ontology concepts, which in turn induce a set of generated features that augment the standard bag of words.
in this paper we propose instead that learning from training data should also affect phase (ii), i.e.\ that information on the membership of training documents to categories be used to determine term weights.
the focus this year is on adaptive filtering, where the system begins  with only the topic statement and must interactively adjust a filtering profile  constructed from that topic in response to on-line feedback.
the main goal of this  research is to build neural networks and to train them in assigning mesh  phrases based on term frequency of single words from title and abstract.
improving text retrieval for the routing problem
"training linear {svm}s in linear time", booktitle = "proceedings of the twelfth acm sigkdd international conference on knowledge discovery and data mining", year = "2006", pages = "217--226", } @inproceedings{kumar:2006:hts, author =
"2007", month = "february", address =
{cornelis h. koster and mark seutter}, title = {taming wild phrases}, booktitle  = {proceedings of ecir-03, 25th european conference on information retrieval},  publisher = {springer verlag, heidelberg, de}, editor = {fabrizio sebastiani},  address = {pisa, it}, year = {2003}, pages = {161--176}, url =  {http://www.cs.kun.nl/~kees/peking/ecir03.pdf}, abstract = {
we also investigate codes with optimized kl distance between the text categories which are merged in the code-words.
the relevance values are interpreted as subjective  probabilities and hence are mapped into the real interval [0; 1].
{g{\'o}mez-hidalgo, jos{\'e} m.}, title = {
this paper extends the mistake-driven learner winnow to better utilize thesauri for text categorization.
this has resulted in high accuracy,  shorter customization time, and good prospects for the application of the  statistical methods to problems in lexical acquisition.}, } @article{jacobs93,  author = {paul s. jacobs}, title = {using statistical methods to improve  knowledge-based news categorization}, journal = {ieee expert}, year = {1993},  number = {2}, volume = {8}, pages = {13--23}, url = {}, abstract = {}, }  @inproceedings{jo99, author = {taeho c. jo}, title = {
our experiments  show that our system has low overhead and achieves high classification  ac-curacy across a variety of databases.}, } @inproceedings{ittner95, author =
the former refer to certain representations of documents and queries and use additional independence assumptions, whereas the latter map documents and queries onto feature vectors which form the input to certain classification procedures or regression methods.
we show how to learn such models efficiently from data.
to convert the documents into vector form, we experiment with  different numbers of features, which we select, based on an information gain  criterion.
two  example categorization tasks achieve recognition scores of approximately 80\%  and are very robust against recognition or typing errors.}, }  @inproceedings{bekkerman01, author =
once the system has learned this information, a gaussian function is shaped for each term of a category, in order to assign the term a weight that estimates the level of its importance for that particular category.
extending whirl with background knowledge for  improved text classification}, journal = {information retrieval}, volume =
in this paper, we apply lsi to the routing task, which operates  under the assumption that a sample of relevant and non-relevant documents is  available to use in constructing the query.
we applied the technique to several standard text collections and found that they contained a significant number of duplicate and plagiarised documents.
{springer science}, year = {2004}, volume = {7}, number = {3-4}, pages =
experiments  with knn and svm classifiers on the ohsumed corpus are reported on, as concrete  examples.}, } @inproceedings{yang93, author =
then an optimal  matching between the web object and the domain knowledge is performed, in order  to pick out the structure attributes of the web object from the knowledge.
"we present and study the contribution-selection algorithm (csa), a novel algorithm for feature selection.
the rocchio classifier, its probabilistic variant, and a naive  bayes classifier are compared on six text categorization tasks.
text categorization experiments supported a number of predictions of the concept learning model about properties of phrasal representations, including dimensionality properties not previously measured for text representations.
fabrizio sebastiani % % dipartimento di matematica pura e applicata % % universita' di padova % %
tampere, fi}, year = {2002}, pages = {97--104}, url =  {http://doi.acm.org/10.1145/564376.564395}, abstract = {this paper explores the  use of bayesian online classifiers to classify text documents.
the indexing process determines whether an index term is assigned to a certain document, based on the relationship constructed by the learning process, and the text found in the document.
text categorization is the task of classifying text into one of  several predefined categories.
we also observe that the  deviation formula and discrimination formula using document frequency ratios  also work as expected.
"filtering obfuscated email spam by means of phonetic string matching", booktitle =
"bonn, germany", url = "http://www.machinelearning.org/proceedings/icml2005/papers/094_hierarchical_rousuetal.pdf", abstract =
however, ripper and sleeping experts differ radically in many  other respects.
extensive tests of the  model suggest its application as a viable and robust tool for large scale text  classification and filtering, as well as a basic module for more complex  scenarios.}, } @article{bayer98, author = {thomas bayer and ulrich kressel and  heike mogg-schneider and ingrid renz}, title = {categorizing paper documents.
text  categorization: the assignment of subject descriptors to magazine articles},  journal = {information processing and management}, pages = {841--861}, year =
we provide an approach for automatically building the implicit links between web pages using web query logs, together with a thorough comparison between the uses of implicit and explicit links in web page classification.
{pittsburgh, us}, pages = {200--209}, url =  {http://robotics.stanford.edu/users/sahami/papers-dir/dl98-sonia.ps}, abstract  = {
our comparative analysis includes three learning algorithms: naive bayes, perceptron, and support vector machines (svm) in combination with three feature weighting methods: odds ratio, information gain, and weights from linear models, the linear svm and perceptron.
svm is highly efficient in learning from well organized samples of  moderate size, although on relatively large and noisy data the efficiency of  svm and aram are comparable.}, } @article{heaps73, author = {h.s. heaps}, title  =
our approach consists of three main parts: i.) a central knowledge filter ii.)
an appraisal group is represented as a  set of attribute values in several task-independent semantic taxonomies, based  on appraisal theory.
in this paper, we compare a number of known linear classification methods as well as some variants in the framework of regularized linear systems.
a  learning system for selective dissemination of information}, booktitle =
we learn a hierarchical classifier that is  guided by a layered semantic hierarchy of answer types, and eventually  classifies questions into finegrained classes.
one important goal in text mining  is automatic classification of electronic documents.
meanwhile, the web object representation is constructed by hyperlink analysis, and further pruned to remove the noises.
support vector machines provide the best accuracy on test data.}, } @article{skarmeta00, author = {
% % this is a  bibliography, in bibtex format, on automated text % % categorization (atc),
in this paper, we measure the importance of sentences using text summarization techniques.
we present empirical studies (controlled experiments on boolean decision trees and a large-scale text categorization problem) which show that the model selection algorithm leads to error rates which are often as low as those obtained by 10-fold cross validation (sometimes even lower).
in the text domain there are likely to exist many gaps in the feature space because a document is usually mapped to a sparse and high dimensional feature space.
our approach has the advantage of a very fast training phase, and the rules of the classifier generated are easy to understand and manually tuneable.
we prove that the negative geodesic  distance (ngd) on the multinomial manifold is conditionally positive definite  (cpd), thus can be used as a kernel in svms.
"forman, george", title = "tackling concept drift by temporal inductive transfer", booktitle = "proceedings of the 29th annual international acm sigir conference on research and development in information retrieval", year = "2006", pages =
{1/3}, pages = {61--77}, url = {}, abstract = {
the system also worked reasonably well for classifying articles from a number of different computer-oriented newsgroups according to subject, achieving as high as an 80\% correct classification rate.
the  mit press}, address = {cambridge, us}, year = {1994}, chapter = {21}, editor =
this is achieved by means of a data mining approach, called  one clause at a time (ocat), which is based on mathematical logic.
our classifier based on these improvements performes significantly  better on pre-classified samples from the web and the us patent database than  the usual classifiers.}, } @inproceedings{yu03a, author = {hwanjo yu and  chengxiang zhai and jiawei han}, title = {
{129--141},  publisher = {springer verlag, heidelberg, de}, note = {
for the  application of the air/phys system a large-scale dictionary containing more  than 600000 word-descriptor relations resp.
this is particularly useful when labeling text is a labor-intensive job and when there is a large amount of information available about a particular problem on the world wide web.
{51--57}, year = {1992}, url =  {http://www.acm.org/pubs/articles/proceedings/ir/133160/p51-blosseville/p51-blosseville.pdf},  abstract = {
furthermore, we focus on the  fact that document collections lend themselves naturally to a hierarchical  structure defined by the subject matter of the documents.
this  analysis determined, for example, that ig and chi-squared have correlated  failures for precision, and that ig paired with bns is a better choice.}, }  @article{forman03, author = {
moreover, the performance of these greedy methods may be deteriorated when the reserved data dimension is extremely low.
categorizing web documents in hierarchical catalogues}, booktitle =
use of this data for research on text categorization requires a  detailed understanding of the real world constraints under which the data was  produced.
the hypernym relation in wordnet supplements the neural model in classification.
experimental comparison of the described measures is given on real-world  data collected from the web.
in our method not only words but also semantic categories  given by the thesaurus are used as features in a classifier.
{automatic text categorization based on hierarchical rules}, booktitle =
we identify document genre is an important factor in retrieving useful documents and focus on the novel document genre dimension of subjectivity.
{1998}, url = {}, } @inproceedings{sahami98b, author = {mehran sahami and salim yusufali and michelle q. baldonado}, title = {sonia: a service for organizing networked information autonomously},
this approach allows us to tune the combination system on available but less-representative validation data and obtain smaller performance degradation of this system on the evaluation data than using a single-method classifier alone.
the results  show that the probabilistic algorithms are preferable to the heuristic rocchio  classifier not only because they are more well-founded, but also because they  achieve better performance.}, } @inproceedings{joachims97b, author = {
{nicholas j. belkin and peter ingwersen and annelise mark pejtersen}, publisher  = {acm press, new york, us}, address = {kobenhavn, dk}, pages = {59--65}, year  = {1992}, url =  {http://www.acm.org/pubs/articles/proceedings/ir/133160/p59-masand/p59-masand.pdf},  abstract = {
very accurate text classifiers can be learned  automatically from training examples.
david j. hand and joost n. kok and michael r. berthold}, address =  {amsterdam, nl}, year = {1999}, pages = {513--524}, url =  {http://link.springer.de/link/service/series/0558/papers/1642/16420513.pdf},  abstract = {
we describe an experiment in applying text classification algorithms to dutch texts.
the former approach has been favored  because of the tendency of people in the computer field to strive for new  methods of dealing with the literature -- methods which do not resemble those  of traditional libraries.
we apply the support vector machine  (svm) to this problem, as it is able to cope with half a million of inputs it  requires no feature selection and can process the frequency vector of all words  of a text.
in this paper we describe how the lsi approach can be implemented in a kernel-defined feature space.
experiments with knn and svm classifiers on the ohsumed corpus are reported on, as concrete examples.}, } @inproceedings{yang93, author =
{mitsuru ishizuka and abdul sattar}, publisher = {springer verlag, heidelberg,  de}, address = {tokyo, jp}, year = {2002}, pages = {444--453}, note =
"a hierarchical  text categorization approach and its application to {frt} expansion",  journal =
our approach to wsd is also based on the integration of two linguistic  resources: a training collection (semcor and reuters-21578) and a lexical  database (wordnet 1.6).}, } @inproceedings{vert01, author = {jean-philippe  vert}, title = {
finally, the interactive nature of the system results  in a more correct and precise description of each document than a fully  automatic system would.}, } @article{kar78, author = {gautam kar and lee j.  white}, title = {a distance measure for automated document classification by  sequential analysis}, journal = {information processing and management}, pages  = {57--69}, year = {1978}, number = {2}, volume = {14}, url = {},
it is argued that for a large class of automatic categorization algorithms, extraction-based document categorization can be viewed as a particular form of feature selection performed on the full text of the document and, in this context, its impact can be compared with state-of-the-art feature selection techniques especially devised to provide good categorization performance.
published in the ``lecture notes in computer science'' series, number 2276}, year = {2002}, address = {mexico city, mx}, pages = {405--414}, url = {http://link.springer.de/link/service/series/0558/papers/2276/22760405.pdf}, abstract = {traditional chinese documents classifiers are based on keywords in the documents, which need dictionaries support and efficient segmentation procedures.
this result suggests that useful task-tracking tools could be  constructed based on automatic classification into this taxonomy."
{57--72}, url = {http://link.springer.de/link/service/series/0558/papers/2633/26330057.pdf}, abstract = {
{yiming yang and christopher g. chute}, title
using a bayesian network model, we combine these measures with the results obtained by traditional content-based classifiers.
these levels control the ability of the categories to attract documents during the categorization process.
we show that former approaches in probabilistic information  retrieval are based on one or two of the three concepts abstraction, inductive  learning and probabilistic assumptions, and we propose a new approach which  combines all three concepts.
{automated categorization in the {international patent classification}},  journal = {sigir forum}, year = {2003}, pages = {10--25}, volume = {37}, number  = {1}, url = {http://www.acm.org/sigir/forum/s2003/cjf_manuscript_sigir.pdf},  abstract = {
the first approach is the linear combination approach.
{xue, dejun and sun, maosong}, title = {
second order features for maximising text  classification performance}, booktitle = {proceedings of ecml-01, 12th european  conference on machine learning}, editor = {
published in the ``lecture notes in computer  science'' series, number 1910}, year = {2000}, address = {lyon, fr}, pages =
this paper introduces a new type of self-organizing map (som) for text categorization and semantic browsing.
and dmoz are extremely popular: dmoz is  maintained by over 56,000 volunteers, is used as the basis of the popular  google directory, and is perhaps used by millions of users each day.
to discover the best fusion framework, we apply genetic programming (gp) techniques.
this paper explores multi-label  conditional random field (crf)classification models that directly parameterize  label co-occurrences in multi-label classification.
based on its performance on the classification of 800,000 example queries recorded from msn search, the system received the runner-up award for query categorization performance of the kdd cup 2005." } @article{li:2005:kddcup2005report, author =
the automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last ten years, due to the increased availability of documents in digital form and the ensuing need to organize them.
the availability of  vast amounts of information on the world wide web has created a big demand for  automatic tools to organize and index that information.
and a somewhat different version of one-class svm based on identifying ``outlier" data as representative of the second-class.
the corresponding classifier parameters are learned by optimizing  an overall objective function of interest.
"schneider, karl-michael", title =
another reformulation of the method leads to an  algorithm that can be applied to supervised multi-class categorization.
{proceedings of cikm-03, 12th acm international conference on information and  knowledge management}, publisher = {acm press, new york, us}, editor = {}, year  = {2003}, address = {new orleans, us}, pages =
the engine exploits a structured document representation and can activate appropriate methods to characterise and automatically index heterogeneous documents with variable layout.
in order to  obtain class labels for training samples, we conducted a study where subjects  ranked document pages with respect to their resemblance to representative page  images.
this task is challenging since incoming  messages may contain constructions which have not been anticipated.
however, in the web space, hyperlinks  are usually sparse, noisy and thus in many situations can only provide limited  help in classification.
{http://www.jucs.org/jucs_3_8/a_framework_for_filtering}, abstract = {
we employ a linear least squares fit (llsf) technique to compute such connections from a collection of queries and documents where the relevance is assigned by humans, and then use these connections in the retrieval of documents where the relevance is unknown.
as an example, we propose a number of  ``supervised variants'' of $tfidf$ weighting, obtained by replacing the $idf$  function with the function that has been used in phase (i) for term selection.
text classification, the grouping of texts into several clusters, has been used as a means of improving both the efficiency and the effectiveness of text retrieval/categorization.
for this specific medical categorization problem, new query formulation and weighting methods used in the k-nearest-neighbor classifier improved performance.}, } @inproceedings{larkey98, author =
the optimized rocchio algorithm achieves a performance comparable with that of the hierarchical neural networks.}, } @inproceedings{ruiz97, author = {miguel e. ruiz and padmini srinivasan}, title = {automatic text categorization using neural networks}, booktitle = {proceedings of the 8th asis/sigcr workshop on classification research}, editor = {efthimis efthimiadis}, publisher =
previously i compared the effectiveness of uncertainty sampling with that of random sampling and relevance sampling in choosing training data for a text categorization data set (lewis and gale, 1994).
the main idea  is to consider a possible double aspect of the importance of a word: the local  importance in a category, and the global importance in the rest of the  categories.
this system, named stretch (storage and retrieval by content of imaged documents), is based on an archiving and retrieval engine, which overcomes the bottleneck of document profiling bypassing some limitations of existing pre-defined indexing schemes.
pisa, it}, year = {2003}, pages = {408--419},  url =  {http://link.springer.de/link/service/series/0558/papers/2633/26330408.pdf},  abstract = {
unlike other machine learning techniques, it allows easy incorporation of new  documents into an existing trained system.
text categorization can be viewed as a process of category search, in which one or more categories for a test document are searched for by using given training documents with known categories.
however, these tools are quickly becoming inadequate as query results grow incomprehensibly large and manual classification in topic hierarchies creates an immense bottleneck.
published in the ``lecture notes in computer science'' series, number 2035},  pages = {78--89}, url =
experimental results show that our system  can achieve satisfactory performance, which is comparable with other  traditional classifiers.}, } @inproceedings{zhou03, author = {shuigeng zhou and  tok wang ling and jihong guan and jiangtao hu and aoying zhou}, title = {fast  text classification: a training-corpus pruning based approach}, booktitle =  {proceedings of dasfaa-03, 8th ieee international conference on database  advanced systems for advanced application}, editor = {}, publisher = {ieee  computer society press, los alamitos, us}, year = {2003}, address = {kyoto,  jp}, pages = {127--136}, url = {}, abstract = {}, } @inproceedings{zu03, author  = {guowei zu and wataru ohyama and tetsushi wakabayashi and fumitaka kimura},  title = {accuracy improvement of automatic text classification based on feature  transformation}, booktitle = {proceedings of doceng-03, acm symposium on  document engineering}, publisher = {acm press, new york, us},
although the test corpus contains documents written in chinese, the proposed approach can be applied to documents written in any language and such documents can be transformed into a list of separated terms.} } @inproceedings{esuli:2005:dso, author =
yiming yang and jian zhang and bryan kisiel},  title = {a scalability analysis of classifiers in text categorization},  booktitle = {proceedings of sigir-03, 26th acm international conference on  research and development in information retrieval}, editor = {jamie callan and  gordon cormack and charles clarke and david hawking and alan smeaton},  publisher = {acm press, new york, us}, address = {toronto, ca}, year = {2003},  pages = {96--103}, url = {http://doi.acm.org/10.1145/860435.860455}, abstract =  {real-world applications of text categorization often require a system to deal  with tens of thousands of categories defined over a large taxonomy.
we obtained some encouraging results on two-category situations, and the results on the general problem seem reasonably impressive---in one case outstanding.
text categorization is the problem of automatically assigning predefined categories to natural language texts.
{acm press, new york, us}, address = {toronto, ca}, year = {2003}, pages = {182--189}, url = {http://doi.acm.org/10.1145/860435.860470}, abstract = {term-based representations of documents have found wide-spread use in information retrieval.
youngjoong ko and jungyun seo}, title = {
we discuss two learning algorithms for text filtering: modified  rocchio and a boosting algorithm called adaboost.
{seattle, us}, pages = {130--136}, url = {http://www.cs.utah.edu/~riloff/psfiles/sigir95.ps}, abstract = {most information retrieval systems use stopword lists and stemming algorithms.
{proceedings of coling-00, the 18th international conference on computational linguistics}, year = {2000}, editor = {}, pages = {}, address =
categorization: generating domain-specific role relationships}, booktitle =  {natural language information retrieval}, editor = {tomek strzalkowski}, year =
for the comparison, we run a series of  experiments using a digital library of computer science papers and a web  directory.
newsjunkie employs novelty-analysis algorithms that represent articles as words and named entities.
{laura m. haas and ashutosh  tiwary}, publisher = {acm press, new york, us}, address = {
text  categorization is an important research area and has been receiving much  attention due to the growth of the on-line information and of internet.
published in the ``lecture notes in computer science''  series, number 2769}, year = {2003}, address = {trondheim, no}, pages =
the approach is also employed in  deleted interpolation, a technique for smoothing n-grams in language modeling  for speech recognition.
improving rocchio with weakly supervised clustering}, booktitle =
historically, several attempts have been made to automate this  process, using various stock phrases as the features on which to base the  classification.
the proposed approach is more  efficient, does not require the specification of any parameters, and similarly  to the parameter-based approach, boosts the performance of baseline svms by at  least 20\% for standard information retrieval measures.}, }  @inproceedings{shanks03, author = {vaughan r. shanks and hugh e. williams},  title = {index construction for linear categorisation}, booktitle =  {proceedings of cikm-03, 12th acm international conference on information and  knowledge management}, publisher = {acm press, new york, us}, editor = {}, year  = {2003}, address = {new orleans, us}, pages = {334--341}, url =  {http://doi.acm.org/10.1145/956863.956926}, abstract = {categorisation is a  useful method for organising documents into subcollections that can be browsed  or searched to more accurately and quickly meet information needs.
{391--401}, volume = {52}, url =  {http://www3.interscience.wiley.com/cgi-bin/fulltext?id=76511157&placebo=ie.pdf},  abstract = {
since the whole system is rule-based, it can be adapted to different subject fields by appropriate modifications of the rule bases.
we present a theoretical  argument showing that, under common assumptions, unlabeled data contain  information about the target function.
we show that link information can be useful when  the document collection has a sufficiently high link density and links are of  sufficiently high quality.
in many  supervised learning tasks, the entities to be labeled are related to each other  in complex ways and their labels are not independent.
the proceedings of a conference % required: title, year % optional: editor, volume or number, series, address, month, % organization, publisher, note % %
these text messages are scanned and then distributed to one of several expert agents according to a certain task criterium.
our  approach views the task as one of information integration using whirl, a tool  that combines database functionalities with techniques from the information  retrieval literature.}, } @inproceedings{zelikovitz01, author = {
{wei li and b. lee and f. krausz and k. sahin}, title = {
"2007", month = "april", address = "rome, italy" } @inproceedings{bloehdorn:2007:css, author = "stephan bloehdorn and alessandro moschitti", title =
in particular, we present an extensive experimental  comparison of our approach with other methods on several well studied lexical  disambiguation tasks such as context-sensitive spelling correction,  prepositional phrase attachment and part of speech tagging.
{gianni amati  and daniela d'aloisi and vittorio giannini and flavio ubaldini}
we show how both algorithms can be adapted to maximize any general utility matrix that associates cost (or gain) for each pair of machine prediction and correct label.
in consequence, the relation system is rather imperfect.
{hans-peter frei and donna harman and peter sch{\"{a}}uble and ross wilkinson}, publisher =
an alternative approach has recently been proposed, whereby a naive bayesian classifier is trained automatically to detect spam messages.
bootstrapping for hierarchical document classification}, booktitle = {proceedings of cikm-03, 12th acm international conference on information and knowledge management},
the system is small,  fast and robust.
"acm press", url =  "http://doi.acm.org/10.1145/1099554.1099688", abstract =
in  this report, we try to prove that a previous filtering of the words used by svm  in the classification can improve the overall performance.
{211--214}, year = {1999}, url = {}, abstract = {a scheme of automatic document  classification is presented.
in contrast, mi had  relatively poor performance due to its bias towards favoring rare terms, and  its sensitivity to probability estimation errors.}, } @inproceedings{yang99,  author =
parametric and qualitative descriptors of user's interest must be generated.
text categorization  for multi-page documents: a hybrid naive {bayes hmm} approach}, booktitle =  {proceedings of jcdl, 1st acm-ieee joint conference on digital libraries},  editor = {}, publisher = {ieee computer society press, los alamitos, us}, year  = {2001}, address = {roanoke, us}, pages = {11--20}, url =  {http://www.dsi.unifi.it/~paolo/ps/jcdl01-hmm-text.pdf}, abstract = {
the estimates can be  used in standard classification models to reduce error rates.
{goldberg, jeffrey l.}, title =
experimental results, obtained using text from three different real-world  tasks, show that the use of unlabeled data reduces classification error by up  to 33\%.}, } @inproceedings{oh00, author = {hyo-jung oh and sung hyon myaeng  and mann-ho lee}, title = {a practical hypertext categorization method using  links and incrementally available class information}, booktitle = {proceedings  of sigir-00, 23rd acm international conference on research and development in  information retrieval}, editor = {nicholas j. belkin and peter ingwersen and  mun-kew leong}, publisher = {acm press, new york, us}, address = {athens, gr},  year = {2000}, pages = {264--271}, url =  {http://www.acm.org/pubs/articles/proceedings/ir/345508/p264-oh/p264-oh.pdf},  abstract =
"617--624", booktitle = "proceedings of the 14th {acm} international conference on information and knowledge management", year =
the research described in this paper combines weighted trigram  analysis, clustering, and a special two-pool evolutionary algorithm, to create  an adaptive information filtering system with such useful properties as domain  independence, spelling error insensitivity, adaptability, and optimal use of  user feedback while minimizing the amount of user feedback required to function  properly.
the document collection was trained by a self-organizing map to form two feature maps.
in contrast with traditional flat and hierarchical category models, the multi-dimensional category model classifies each text document in a collection using multiple predefined sets of categories, where each set corresponds to a dimension.
we describe an automatic system that starts with a small sample of the corpus in which topics have been assigned by hand, and then updates the database with new documents as the corpus grows, assigning topics to these new documents with high speed and accuracy.
it is used here for classifying xml documents.
we use this to build an argument for a data driven approach which merely searches for a good linear separator in the feature space, without further assumptions on the domain or a specific problem.
given these inputs, the system learns to extract  information from other pages and hyperlinks on the web.
recently, commercial web sites have started to manually organize web-accessible databases into yahoo!-like hierarchical classification schemes.
when the data  streams are produced in a changing environment the filtering has to adapt too  in order to remain effective.
feature selection for text categorization on  imbalanced data}, journal = {sigkdd explorations}, year = {2004}, number = {1},  volume = {6}, pages = {80--89}, url =  {http://doi.acm.org/10.1145/1007730.1007741}, abstract = {
experimental results are encouraging overall; in particular, document  classification results fulfill the requirements of high-volume application.
the determination of categories and their hierarchical structures were most done by human experts.
hynet is described for the first time in this paper.
naively  using terms in neighboring documents increased the error to 38\%; our hypertext  classifier reduced it to 21\%.
rakesh agrawal and paul e. stolorz and gregory  piatetsky-shapiro}, publisher = {aaai press, menlo park, us}, year = {1998},  address = {new york, us}, pages = {169--173}, url =  {http://www.research.whizbang.com/~wcohen/postscript/kdd-98.ps}, abstract =  {whirl is an extension of relational databases that can perform ``soft joins''  based on the similarity of textual identifiers; these soft joins extend the  traditional operation of joining tables based on the equivalence of atomic  values.
{acm press, new york, us}, address = {pittsburgh, us}, pages = {281--290}, year = {1993}, note = {an extended version appears as~\cite{yang94}}, url = {http://www.acm.org/pubs/articles/proceedings/ir/160688/p281-yang/p281-yang.pdf}, abstract =
in order to determine whether  information within a sentence has been seen in material read previously, our  system integrates information about the context of the sentence with novel  words and named entities within the sentence, and uses a specialized learning  algorithm to tune the system parameters."
we present evidence that this new algorithm leads to better test set precision and recall on three binary web classification tasks where the test set web pages are taken from different web sites than the training set.}, } @inproceedings{slattery98, author =
the  structure of the web is increasingly being used to improve organization,  search, and analysis of information on the web.
georg gottlob and toby walsh}, publisher
= {morgan  kaufmann publishers, san francisco, us}, year = {2003}, address = {acapulco,  mx}, } @article{zhang01, author = {tong zhang and frank j. oles}, title = {
"lan, man and tan, chew-lim and low,  hwee-boon", title =
george karypis and eui-hong han},  title = {fast supervised dimensionality reduction algorithm with applications  to document categorization and retrieval}, booktitle = {proceedings of cikm-00,  9th acm international conference on information and knowledge management},  publisher = {acm press, new york, us}, address = {mclean, us}, editor = {arvin  agah and jamie callan and elke rundensteiner}, year = {2000}, pages = {12--19},  url = {ftp://ftp.cs.umn.edu/dept/users/kumar/cikm-ci.ps}, abstract = {retrieval  techniques based on dimensionality reduction, such as latent semantic indexing  (lsi), have been shown to improve the quality of the information being  retrieved by capturing the latent meaning of the words present in the  documents.
after posting a  query, the user is offered an opportunity to refine the results by browsing  through a category tree derived from the dmoz open directory topic hierarchy.
we demonstrate that when learning from labeled and unlabeled data, algorithms explicitly leveraging a natural independent split of the features outperform algorithms that do not.
the proposed learning model is the core of  a prototype information filtering system called profile.}, }  @inproceedings{androutsopoulos00, author = {ion androutsopoulos and john  koutsias and konstandinos v. chandrinos and constantine d. spyropoulos}, title  = {
"gliozzo, alfio and strapparava,  carlo", title =
{bruce croft and alistair moffat and van rijsbergen, cornelis j. and ross  wilkinson and justin zobel}, publisher = {acm press, new york, us}, year =
converting numerical classification into text  classification}, journal = {artificial intelligence}, volume = {143}, number =
we benchmark several widely used supervised learning methods on rcv1-v2, illustrating the collection's properties, suggesting new directions for research, and providing baseline results for future studies.
"knowing a web page by the company it keeps", booktitle = "cikm", year =
an extended version appears as~\cite{sebastiani02}},
rather than just  throwing phrases and keywords together, we shall start with pure hm pairs and  gradually add more keywords to the document representation.
"a probabilistic model for retrospective  news event detection", pages = "106--113", booktitle =  "proceedings of the 28th annual international {acm} {sigir} conference on  research and development in information retrieval", year =  "2005", month =
the current implementation of cats may be considered a baseline, on top of which many enhancements are possible.
{227--247}, url = {http://www.wkap.nl/article.pdf?391243}, abstract = {most of the text categorization algorithms in the literature represent documents as collections of words.
in  this paper, we present an efficient text categorization algorithm that  generates bigrams selectively by looking for ones that have an especially good  chance of being useful.
this model  generalizes the multinomial naive bayes and it is derived from a more general  model for different access tasks.
{cambridge university press}, editor =
the first  concerns the information filtering system profile based on an adaptation of the  generalized probabilistic model of information retrieval.
the accuracy of the system is only slightly lower than that of human categorizers.}, } @inproceedings{hayes90, author = {
in the course of investigating this idea, we address the problem of  automatic categorization of webpages in the {{\sc yahoo!}}\ directory.
other linguistic resources that are emerging, like lexical databases, can also be used for classification tasks.
{nagoya, jp}, url = {http://www.cs.strath.ac.uk/~fabioc/papers/97-ijcai.pdf},  abstract = {new methods and new systems are needed to filter or to selectively  distribute the increasing volume of electronic information being produced  nowadays.
this survey discusses the main approaches to text categorization that fall within the machine learning paradigm.
experiments on a web directory show that best results are achieved  when links from pages outside the directory are considered.
= {santa barbara,  us}, pages = {67--78}, url = {http://doi.acm.org/10.1145/375663.375671},  abstract = {
the results showed that the proposed model was able to achieve high categorization effectiveness as measured by precision and recall.
the categorization algorithm used is a supervised learning procedure that uses a linear classifier based on the category levels.
it consists of the following components:  layout analysis, pre-classification, ocr interface, fuzzy string matching, text  categorization, lexical, syntactical and semantic analysis.
to discover the best fusion framework, we apply  genetic programming (gp) techniques.
results of other issues related to building an  effective personal e-mail classifier are presented and discussed.
the system has a cooperative and supportive role: it understands the user's needs and learns from his behavior.
in particular, it addresses the  following questions: why can support vector machines handle the large feature  spaces in text classification effectively?
the approach makes use of  the classification and regression trees (cart) algorithm that has seen  application in various areas of machine learning.
in order to verify our methods, we test a large body of tagged japanese newspaper articles created by rwcp.
available as technical report ss-96-05}, url = {},  abstract = {}, } @inproceedings{hersh94, author
an unoptimized version of the algorithm was used to process the trec database in a very short time.}, } @inproceedings{huffman95, author = {
we address the problem of integrating documents from different sources into a master catalog.
mehran  sahami and marti a. hearst and eric saund}, title = {
we address the problem  of evaluating the effectiveness of summarization techniques for the task of  document categorization.
but they don't offer any of the benefits of natural language processing, such as the ability to identify relationships and enforce linguistic constraints.
the naive bayes classifier, currently experiencing a renaissance in machine learning, has long been a core technique in information retrieval.
communications of the acm}, volume = {35}, number = {8}, year = {1992}, pages = {48--63}, url = {http://www.acm.org/pubs/articles/journals/cacm/1992-35-8/p48-creecy/p48-creecy.pdf}, } @inproceedings{cristianini01, author = {nello cristianini and john shawe-taylor and huma lodhi}, title = {latent semantic kernels}, booktitle = {proceedings of icml-01, 18th international conference on machine learning}, editor = {carla brodley and andrea danyluk}, address = {williams college, us}, year = {2001}, pages = {66--73}, publisher =
in these experiments we show that, after only one epoch of training,  our algorithm performs much better than perceptron-based hierarchical  classifiers, and reasonably close to a hierarchical support vector machine.} }  @article{diaz04, author = {irene d{\'{\i}}az and jos{\'{e}} ranilla and elena  monta{\~{n}}es and javier fern{\'{a}}ndez and el{\'{\i}}as f. combarro}, title  = {
thus, unlike some other  unsupervised dimensionality-reduction techniques, such as latent semantic  indexing, we are able to compress the feature space much more aggressively,  while still maintaining high document classification accuracy.
{published in the ``lecture notes in computer science'' series, number 2417},  url =  {http://link.springer.de/link/service/series/0558/papers/2417/24170444.pdf},  abstract = {two main research areas in statistical text categorization are  similarity-based learning algorithms and associated thresholding strategies.
extensive experiments demonstrate the efficiency and effectiveness of the proposed approach.}, } @inproceedings{li91, author = {wei li and b. lee and f. krausz and k. sahin}, title = {
"overcoming the brittleness  bottleneck using {w}ikipedia:
"2006" } @inproceedings{wiebe:2006:wss, author =
springer verlag, heidelberg, de}, note =
text categorization with  many redundant features: using aggressive feature selection to make {svm}s  competitive with {c4.5}}, booktitle = {proceedings of icml-04, 21st  international conference on machine learning}, editor = {carla e. brodley},  year = {2004}, address = {banff, ca}, pages = {}, publisher =
a new measure of performance is proposed and some other  applications of the technique indicated.}, } @inproceedings{hsu99, author =
significant improvements in computational efficiency without losing  categorization accuracy were evident in the testing results.}, }  @article{yang96, author =
"wang, muyuan and li, zhiwei and  lu, lie and ma, wei-ying and zhang, naiyao", title = "web object  indexing using domain knowledge", booktitle =
this  paper introduces a term weighting method for text categorization based on  smoothing ideas borrowed from speech recognition.
our approach acts as a feature  selection technique that is an alternative to applying the techniques from  machine learning and numerical taxonomy.}, } @inproceedings{koster03, author =
pat langley}, year = {2000}, address = {
{}, address = {grenoble, fr}, year = {1985}, pages = {207--216}, url = {},  abstract = {}, } @inproceedings{fuhr91a, author =
however, in addition to relying on labeled training data, we improve classification accuracy by also using unlabeled data and other forms of available ``background" text in the classification process.
text categorization (tc) is the automated assignment of text  documents to predefined categories based on document contents.
however, even in these extended formulations the problem of tuning its parameters is still neglected.
in this paper, we explore an efficient extension of the standard support vector machine (svm) approach, called svmc (support vector mapping convergence)
in text classification most techniques use bag-of-words to represent documents.
extensive experiments have been  conducted on two common document corpora, namely the ohsumed collection and the  reuters-21578 collection.
our approach is based on iterative  relaxation labeling and can be combined with either bayesian or svm classifiers  on the feature spaces of the given data items.
in this paper, we propose an efficient optimal feature selection algorithm by optimizing the objective function of orthogonal centroid (oc) subspace learning algorithm in a discrete solution space, called orthogonal centroid feature selection (ocfs).
our experiments use reuters 21578 database and consist of binary classifications for categories selected from the 115 topics classes of the reuters collection.
it enables drawing on results from statistics  and machine learning in explaining the effectiveness of alternate  representations of text, and specifies desirable characteristics of text  representations.
alexei  vinokourov and mark girolami}, title = {a probabilistic framework for the  hierarchic organisation and classification of document collections}, journal =
we also examine training set size, and alternative document representations.
this task has several applications, including automated indexing of scientific articles according to predefined thesauri of technical terms, filing patents into patent directories, selective dissemination of information to information consumers, automated population of hierarchical catalogues of web resources, spam filtering, identification of document genre, authorship attribution, survey coding, and even automated essay grading.
we present our methodology, which seems to be  insensitive to the language of the document collections, and discuss issues  related to the differences in results that we have obtained for the two  collections.}, } @article{attardi98, author = {attardi, giuseppe and di marco,  sergio and salvi, davide}, title = {
here, special kinds of web  catalogues, those whose category scheme is hierarchically ordered, are  regarded.
using a  relatively large amount of real personal e-mail data, a comprehensive  comparative study was conducted using the two classifiers.
raising  high-degree overlapped character bigrams into trigrams for dimensionality  reduction in chinese text categorization}, booktitle = {proceedings of  cicling-04, 5th international conference on computational linguistics and  intelligent text processing}, year = {2004}, editor = {alexander f. gelbukh},  publisher = {springer verlag, heidelberg, de}, address = {seoul, ko}, note =
we apply the support vector machine (svm) to this problem, as it is able to cope with half a million of inputs it requires no feature selection and can process the frequency vector of all words of a text.
our methodology makes use of a  well-known corpus of transcribed and topic-labeled speech (the switchboard  corpus), and involves an interesting double use of the boostexter learning  algorithm.
by assuming that documents are created by a  parametric generative model, expectation-maximization (em) finds local maximum  a posteriori models and classifiers from all the data|labeled and unlabeled.
first, we use the system to compute profiles on training set data  that represent the various categories, e.g., language samples or newsgroup  content samples.
user assisted text classification and knowledge management}, booktitle = {proceedings of cikm-03, 12th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, editor = {}, year = {2003}, address = {new orleans, us}, pages = {524--527}, url = {http://doi.acm.org/10.1145/956863.956964}, abstract = {
these tests also have investigated finding the best indexing terms that could  be used in making these classification decisions.
this  paper compares three commonly applied text classifiers in the light of  semi-supervised learning, namely a linear support vector machine, a  similarity-based tfidf and a naive bayes classifier.
a new approach, based on the k-nearest neighbor (knn) classifier,  is used to classify program behavior as normal or intrusive.
we show how to do this for binary text classification systems,  emphasizing that different goals for the system lead to different optimal  behaviors.
a central problem in good text  classification for information filtering and retrieval (if/ir) is the high  dimensionality of the data.
pisa, it}, year = {2003}, pages = {335--350}, url = {http://link.springer.de/link/service/series/0558/papers/2633/26330335.pdf}, abstract = {
we  introduce a class of representations for classifying text data based on  decision trees; (i.e., decision trees over attributes on strings) and present  an algorithm for learning them inductively.
"2006" }  @inproceedings{li:2006:csq, author =
this paper  presents work that uses latent semantic indexing (lsi) for text classification.
the idea  is to identify sub-topics of the original classes which can help improve the  categorization process.
however, support vector machines are so far considered special in  that they have been demonstrated to achieve the state of the art performance.
{supervised learning techniques for text classification often require a large number of labeled examples to learn accurately.
for precision  alone, however, information gain (ig) was superior.
its aim is to disengage the user from learning complex tools  and from performing tedious and repetitive actions.}, } @inproceedings{amati97,  author = {gianni amati and fabio crestani and flavio ubaldini}, title = {
in order to classify text documents, we must extract good features from them.
through our experiments on the reuters-21578 news database, we showed that aram performed reasonably well in mining categorization knowledge from sparse and high dimensional document feature space.
"text classification with kernels on the multinomial manifold", pages = "266-273", booktitle = "proceedings of the 28th annual international {acm} {sigir} conference on research and development in information retrieval", year = "2005", month =
pat langley}, year = {2000}, address  = {
experimental  evaluation on real-world data shows that the proposed approach gives good  results.
mh boosting algorithm is applied to the word sense disambiguation (wsd) problem.
however, even this algorithm is  aided by an initial prefiltering of features, confirming the results found by  almuallim and dietterich on artificial data sets.
in the context of a very high-dimensional feature space,  several methodological alternatives are examined, including universal versus  local dictionaries, and binary versus frequency related features.}, }  @inproceedings{apte94a, author = {apt\'{e}, chidanand and damerau, fred j. and  weiss, sholom m.}, title = {towards language-independent automated learning of  text categorization models}, booktitle = {proceedings of sigir-94, 17th acm  international conference on research and development in information retrieval},  editor = {w. bruce croft and van rijsbergen, cornelis j.}, publisher =
the system has a  cooperative and supportive role: it understands the user's needs and learns  from his behavior.
in contrast to conventional measures for pruning, the l-measure takes into account properties of the search space.}, } @inproceedings{junker97, author = {markus junker and andreas abecker}, title = {
the hierarchical structure is  initially used to train different second-level classifiers.
representative sampling for text  classification using support vector machines}, booktitle = {proceedings of  ecir-03, 25th european conference on information retrieval}, publisher =
{morgan kaufmann  publishers, san francisco, us}, pages = {379--388}, url = {}, abstract = {most  research in text classification to date has used a ``bag of words''
this restructuring makes it possible for svms to focus on the latent semantic space without losing information given by the original feature space.
the results indicate that in each case, the refined classifiers achieve significant performance improvement over the base classifiers used.
rather than just throwing phrases and keywords together, we shall start with pure hm pairs and gradually add more keywords to the document representation.
in this paper we experiment with two different learning  techniques, one based on na\"{\i}ve bayesian classification and the other  one based on multiclass support vector machines, and test the resulting  framework on a corpus of social surveys.
{springer verlag, heidelberg, germany}, year = {1994}, address = {dublin,  ireland}, pages = {282--289}, url = {
dunja  mladeni{\'{c}}}, title = {feature subset selection in text learning}, booktitle  =
the alternative  to supervised learning is usually viewed to be building classifiers by hand,  using a domain expert's understanding of which features of the text are related  to the class of interest.
an  extended version appears as~\cite{sebastiani02}}, abstract = {
as a classifier, we adopted a variant of k-nearest neighbor (knn) with supervised term weighting schemes to improve the performance, making our method among the top-performing systems in the trec official evaluation.
we classify movie reviews using features based upon these taxonomies combined with standard ``bag-of-words'' features, and report state-of-the-art accuracy of 90.2%.
then we describe a larger more difficult newswire  classification task from information retrieval.
learning to classify text from labeled and unlabeled documents}, booktitle = {proceedings of aaai-98, 15th conference of the american association for artificial intelligence}, publisher = {aaai press, menlo park, us}, editor = {}, year = {1998}, pages = {792--799}, address = {
our results show that the identification of hypertext regularities in the  data and the selection of appropriate representations for hypertext in  particular domains are crucial, but seldom obvious, in real-world problems.
author detection with svms on full word forms was remarkably robust even if the  author wrote about different topics.}, } @inproceedings{dinunzio03, author =
it is demonstrated that the probabilistic corpus model which emerges from the automatic or unsupervised hierarchical organisation of a document collection can be further exploited to create a kernel which boosts the performance of state-of-the-art support vector machine document classifiers.
specifically, we view the  expansion of such lexicons as a process of learning previously unknown  associations between terms and \emph{domains}.
{sameer singh and nabeel a. murshed and walter kropatsch}, address =
we study the problem of classifying data in a given taxonomy when classifications associated with multiple and/or partial paths are allowed.
the representations are  evaluated using the ripper learning algorithm on the reuters-21578 and digitrad  test corpora.
"an knn model-based approach and its application in text categorization",
in  this paper, we study the use of support vector machine in text categorization.
robert korfhage and edie rasmussen and peter willett}, publisher =  {acm press, new york, us}, address = {pittsburgh, us}, pages = {281--290}, year  = {1993}, note = {an extended version appears as~\cite{yang94}}, url =  {http://www.acm.org/pubs/articles/proceedings/ir/160688/p281-yang/p281-yang.pdf},  abstract = {
after defining our  notion of ``text mining'', we focus on the differences between text and data  mining and describe in some more detail the unique technologies that are key to  successful text mining.}, } @article{doyle65, author = {lauren b. doyle}, title  = {is automatic classification a reasonable application of statistical analysis  of text?}, journal = {journal of the acm}, volume = {12}, number = {4}, year =
{18}, number = {11/13}, url = {}, abstract = {this paper presents a modular  software system, which classifies a large variety of office documents according  to layout form and textual content.
finally, we analyze the experimental performance of these models over the  outputs of two text classifiers.
we also present a new interactive clustering algorithm,  c-evolve, for topic discovery.
our substantial experimental results show that with several dimension reduction methods that are designed particularly for clustered data, higher efficiency for both training and testing can be achieved without sacrificing prediction accuracy of text classification even when the dimension of the input space is significantly reduced.} } @article{yang:2005:act, author = {hsin-chang yang and chung-hong lee}, title = {automatic category theme identification and hierarchy generation for chinese text categorization}, journal = {journal of intelligent information systems}, year = {2005}, volume = {25}, number = {1}, pages = {47--67}, abstract = {recently research on text mining has attracted lots of attention from both industrial and academic fields.
"2006", pages = "685--686",  abstract =
such methods can provide automatic indexing and keyword assignment  capabilities that are at least as accurate as human indexers in many  applications.
= {fabio ciravegna and alberto  lavelli and nadia mana and johannes matiasek and luca gilardoni and silvia  mazza and william j. black and fabio rinaldi}, title = {facile: classifying  texts integrating pattern matching and information extraction}, booktitle =  {proceedings of ijcai-99, 16th international joint conference on artificial  intelligence}, editor = {thomas dean}, publisher = {morgan kaufmann publishers,  san francisco, us}, year = {1999}, pages = {890--895}, address = {stockholm,  se}, url =  {http://ecate.itc.it:1024/lavelli/lavelli-papers/ijcai99/ijcai99.ps.gz},  abstract = {successfully managing information means being able to find relevant  new information and to correctly integrate it with pre-existing knowledge.
"bremen, germany", publisher = "acm press", url = "http://doi.acm.org/10.1145/1099554.1099713", abstract =
we review a variety of standard approaches to converting scores (and poor probability estimates) from text classifiers to high quality estimates and introduce new models motivated by the intuition that the empirical score distribution for the "extremely irrelevant", "hard to discriminate", and "obviously relevant" items are often significantly different.
two ways to respond to this challenge are (1) using a representation of the content of web documents that captures these two characteristics and (2) using more effective classifiers.
} @inproceedings{tan:2005:nra, author = "songbo tan and xueqi cheng and moustafa m. ghanem and bin wang and hongbo xu", title =
extensive experiments have been conducted  on the reuters-21578 and 20-newsgroups data sets.
{morgan kaufmann publishers, san francisco, us}, url = {}, abstract = {}, } @inproceedings{li03a, author = {
editor = {w. bruce croft and david j. harper and donald h. kraft and justin zobel}, publisher = {acm press, new york, us}, address = {new orleans, us}, year = {2001}, pages = {128--136}, url = {http://www.cs.cornell.edu/people/tj/publications/joachims_01a.pdf}, abstract = {this paper develops a theoretical learning model of text classification for support vector machines (svms).
"edinburgh, scotand", url =  "http://www.ijcai.org/papers/1401.pdf", abstract =
in this paper, we devised a novel  approach to automatically generate categories.
in this paper we use this as a way to learn on problems that involve a combination of text and numbers.
the paper describes how despite this fact the inner product can be efficiently evaluated by a dynamic programming technique.
it then uses these words to extract a set of documents for each class from a set of unlabeled documents to form the initial training set.
empirical comparison with several other existing feature selection methods shows that the backward elimination variant of csa leads to the most accurate classification results on an array of datasets."
{2001}, url =  {http://www.ai.mit.edu/projects/jmlr/papers/volume2/tong01a/tong01a.pdf},  abstract = {support vector machines have met with significant success in  numerous real-world learning tasks.
feature  reduction by this method remarkably outperforms information gain based feature  selection.} } @inproceedings{kules:2006:cws, author =
"in this paper, we describe our ensemble-search based approach,  q2c@ust (http://webproject1.cs.ust.hk/q2c/), for the query classification task  for the kddcup 2005.
evaluating the performance of a two-level implementation on the reuters-22173 testbed of newswire articles shows the most significant improvement for rare classes.}, } @article{weiss99, author = {sholom m. weiss and chidanand apt\'{e} and fred j. damerau and david e. johnson and frank j. oles and thilo goetz and thomas hampp}, title = {maximizing text-mining performance}, journal = {ieee intelligent systems}, year = {1999}, number = {4}, volume = {14}, pages = {63--69}, url = {http://www.research.ibm.com/dar/papers/pdf/ieee99_mtmp.pdf}, abstract = {
springer  verlag, heidelberg, de}, note = {
cross validation over 4 different corpora in two languages allowed us to gather an overwhelming evidence that complex nominals, proper nouns and word senses are not adequate to improve tc accuracy.}, } @article{mostafa00, author = {javed mostafa and wai lam}, title = {automatic classification using supervised learning in a medical document filtering application}, journal = {information processing and management}, year = {2000}, volume = {36}, number = {3}, pages = {415--444}, url = {}, abstract = {document classifiers can play an intermediate role in multilevel filtering systems.
{174--181}, url = {}, abstract = {
the paper shows that the accuracy of a naive bayes text  classifier can be significantly improved by taking advantage of a hierarchy of  classes.
the method harnesses reliability indicators---variables that  provide a valuable signal about the performance of classifiers in different  situations.
this paper examines the relationship between the idf transform and several widely used feature selection methods, in the context of naive bayes and support vector machines classifiers, on datasets extracted from the dmoz ontology of web-page descriptions.
feature selection in svm text  categorization}, booktitle = {proceedings of aaai-99, 16th conference of the  american association for artificial intelligence}, publisher =
we further  show that feature clustering is an effective technique for building smaller  class models in hierarchical classification.
the design goals for tcs are discussed, and other approaches to text categorization in the light of these goals are examined.
{sang-bum kim and hae-chang rim}, title =
"june", address =  "prague, czech republic" } @inproceedings{kim:2007:cap, author =
nashville, us}, pages = {170--178}, publisher =
we investigate bayesian methods for automatic document categorization and develop a new approach to this problem.
techreport % = a report published by a school or other  institution, % usually numbered within a series % required: author, title,  institution, year % optional: type, number, address, month, note % %
the results indicate that the use of hierarchical structures improves performance significantly.}, } @article{sable00, author = {carl l. sable and vasileios hatzivassiloglou}, title = {text-based approaches for non-topical image categorization}, journal = {international journal of digital libraries}, year = {2000}, number = {3}, volume = {3}, pages = {261--275}, url = {http://www.cs.columbia.edu/~sable/research/ijodl00.pdf}, abstract = {
wen-lin hsu and sheau-dong  lang}, title = {feature reduction and database maintenance in netnews  classification}, booktitle = {proceedings of ideas-99, 1999 international  database engineering and applications symposium}, publisher = {ieee computer  society press, los alamitos, us}, editor = {}, year = {1999}, address =
published in the ``lecture notes in computer science'' series, number 2431}, url = {http://springerlink.metapress.com/openurl.asp?genre=article&issn=0302-9743&volume=2431&spage=150}, abstract = {good feature selection is essential for text classification to make it tractable for machine learning, and to improve classification performance.
a statistical learning approach to assigning controlled index terms is presented.
however, we have found that recognizing singular and plural nouns, verb forms,  negation, and prepositions can produce dramatically different text  classification results.
specifically, a query is preprocessed and  represented with patterns that include both query words and required answer  types.
in our learning experiments, for each of the subproblems, naive bayesian classifier was used on text data.
norbert g{\"{o}}vert and mounia lalmas and norbert fuhr}, title  = {a probabilistic description-oriented approach for categorising web  documents}, booktitle = {proceedings of cikm-99, 8th acm international  conference on information and knowledge management}, publisher = {acm press,  new york, us}, editor = {}, year = {1999}, address = {kansas city, us}, pages =
in this paper, we first perform the document categorization by using k-nn and then employ the relationships to reduce the ambiguity.
to the extraction of opinions from text is the of the orientation of subjective  terms contained i.e. the determination of whether a term that opinionated  content has a positive or a negative connotation.
we  also explore the use of different kinds of codes, namely error-correcting  codes, random codes, and domain and data-specific codes and give experimental  results for each of them.
we conclude that even the most careful term selection cannot overcome the differences in document frequency between phrases and words, and propose the use of term clustering to make phrases more cooperative.}, } @article{krier02, author = {marc krier and francesco zacc{\`a}}, title = {automatic categorization applications at the european patent office}, journal = {world patent information}, year = {2002}, volume = {24}, number = {}, pages = {187--196}, url = {}, abstract = {}, } @inproceedings{krishnapuram03, author = {raghu krishnapuram and krishna chitrapura and sachindra joshi}, title = {classification of text documents based on minimum system entropy},
{7}, pages = {578--592}, url = {http://dx.doi.org/10.1002/asi.10409}, abstract  = {
our results do not show a dominant algorithm nor method for making  algorithms cost-sensitive, but are the best reported on the test collection  used, and approach real-world hand-crafted classifiers accuracy.}, }  @inproceedings{goodman90, author = {marc goodman}, title = {{\sc prism}: a  case-based telex classifier}, booktitle = {proceedings of iaai-90, 2nd  conference on innovative applications of artificial intelligence}, publisher =
some of these measures outperformed traditional measures from  information retrieval and information theory in certain situations."
"training linear {svm}s in linear time", booktitle =  "proceedings of the twelfth acm sigkdd international conference on  knowledge discovery and data mining", year =
actually, this weighting method does not leverage the information implicitly contained in the categorization task to represent documents.
in text categorization (tc) based on the vector space model, feature weighting is vital for the categorization effectiveness.
in this  paper, we compare learning techniques based on statistical classification to  traditional methods of relevance feedback for the document routing problem.
the contents of many valuable web-accessible databases are only accessible through search interfaces and are hence in-visible to traditional web ``crawlers''.
these maps were  then analyzed to obtain the category themes and their structure.
"proceedings of the 29th  annual international acm sigir conference on research and development in  information retrieval", year =
{published in the ``lecture notes in computer science'' series, number 2945},  pages = {559--570}, url = {}, abstract = {}, } @inproceedings{guthrie94, author  = {louise guthrie and elbert walker and joe a. guthrie}, title = {document  classification by machine: theory and practice}, booktitle =
"jingyang li and maosong sun",  title =
we further show that feature clustering is an effective technique for building smaller class models in hierarchical classification.
we propose a feature selection method based on linear support vector  machines (svms).
we show that a careful hybrid integration of techniques  from neural network architectures, learning and information retrieval can reach  consistent recall and precision rates of more than 92\% on an 82,000 word  corpus; this is demonstrated for 10,000 unknown news titles from the reuters  newswire.
"rome, italy" } @inproceedings{bloehdorn:2007:css, author =  "stephan bloehdorn and alessandro moschitti", title =
in our experiments we show that, whenever an existing pn dictionary allows the identification of 50\% of the proper nouns within a corpus, our technique allows, without additional manual effort, the successful recognition of about 90\% of the remaining 50\%.}, } @inproceedings{peters02, author = {c. peters and cornelis h. koster}, title = {uncertainty-based noise reduction and term selection in text categorization}, booktitle = {proceedings of ecir-02, 24th european colloquium on information retrieval research}, editor = {fabio crestani and mark girolami and van rijsbergen, cornelis j.}, year = {2002}, address = {glasgow, uk}, publisher = {springer verlag, heidelberg, de}, note = {
adaptive information filtering using  evolutionary computation}, journal = {information sciences}, year = {2000},  volume = {122}, number = {2/4}, pages =
european conference on information retrieval}, publisher = {springer verlag}, editor = {fabrizio sebastiani}, address = {pisa, it}, year = {2003}, pages =
in our tests with three categorization methods on text collections from different domains/applications, significant numbers of words were removed without sacrificing categorization effectiveness.
many of its applications have great  commercial value.
in this paper we describe and evaluate a learning model for information filtering which is an adaptation of the generalized probabilistic model of information retrieval.
in particular, a topic of any breadth will typically contain several thousand or million relevant web pages.
{1994}, number = {1}, volume = {7}, pages = {49--80}, url = {}, abstract = {},  } @article{leopold02, author = {
our results show that svm, knn and llsf significantly outperform nnet and nb when the number of positive training instances per category are small (less than ten), and that all the methods perform comparably when the categories are sufficiently common (over 300 instances).}, } @article{yang99a, author = {yiming yang}, title = {an evaluation of statistical approaches to text categorization}, journal = {information retrieval}, year = {1999}, pages = {69--90}, volume = {1}, number = {1/2}, url = {http://www.cs.cmu.edu/~yiming/papers.yy/irj99.ps}, abstract = {this paper focuses on a comparative evaluation of a wide-range of text categorization methods, including previously published results on the reuters corpus and new results of additional experiments.
"raising high-degree  overlapped character bigrams into trigrams for dimensionality reduction in  chinese text categorization", booktitle = "computational linguistics  and intelligent text processing (lecture notes in computer science, vol.
in this paper an original classification  model sensitive to document syntactic information and characterized by a novel  inference method is described.
while most  of the existing investigations of an automated genre classification are based  on news articles corpora, the idea here is applied to arbitrary web pages.
in our approach, training data are  represented as the projections of training documents on each feature.
we also show that, surprisingly, the addition of deep linguistic analysis features to a set of surface level word n-gram features contributes consistently to classification accuracy in this domain."
existing classification techniques have limited applicability in the data sets of these natures.
text classification by a neural network}, booktitle = {proceedings of the 23rd annual summer computer simulation conference}, editor = {}, publisher = {}, address = {baltimore, us}, pages = {313--318}, year = {1991}, url = {}, abstract = {when banks process their free-form telex traffic, the first task is the classification of the telexes.
these four algorithms were tested on two different data sets: one data set where the number of features were constrained to the 1000 best features and another data set where the dimensionality was over 7000.
acm international conference on research and development in information retrieval}, editor =
"guillaume cleuziou and celine poudat", title =
in addition, topic  hierarchies can be utilized to overcome the sparseness problem in text  categorization with a large number of categories, which is the main focus of  this paper.
in the results of an experiment with news article classification, precision is about 98\%.}, } @incollection{jo99b, author = {taeho c. jo}, title = {news articles classification based on representative keywords of categories}, booktitle = {computational intelligence for modelling, control and automation}, editor = {
wang, wenxian and meng, weiyi and yu, clement}, title = {concept hierarchy based text database categorization in a metasearch engine environment}, booktitle = {proceedings of wise-00, 1st international conference on web information systems engineering}, editor = {li, qing and ozsoyoglu, z. meral and wagner, roland and kambayashi, yahiko and zhang, yanchun}, pages =
we define for each document category a finite  mixture model, which is a linear combination of the probability distributions  of the clusters.
an appropriate indexing approach and the corresponding structure of the air/phys system are described.
this level of complexity makes an ``off-the-shelf'' database-management and information-retrieval solution impossible.
text categorization (tc) is the automated assignment of text documents to predefined categories based on document contents.
"proceedings of the 28th annual  international {acm} {sigir} conference on research and development in  information retrieval", year =
phase i corresponds to the training phase of  machine learning research and phase ii corresponds to testing phase.
"zhang, tong and popescul, alexandrin and dom, byron", title =
"this paper shows how citation-based information and structural content (e.g., title, abstract) can be combined to improve classification of text documents into predefined categories.
the ultimate goal of taxsom is to create the premise for successfully training a supervised classifier.}, } @inproceedings{aggarwal99, author = {charu c. aggarwal and stephen c. gates and philip s. yu}, title = {
the paper examines some alternative ways to represent text based on syntactic and semantic relationships between words (phrases, synonyms and hypernyms).
we formulate the problem of automated survey coding as a \emph{text categorization} problem, i.e.\ as the problem of learning, by means of supervised machine learning techniques, a model of the association between answers and codes from a training set of pre-coded answers, and applying the resulting model to the classification of new answers.
k-nearest-neighbour, relevance  feedback, and bayesian independence classifiers were applied individually and  in combination.
our approach includes some original ideas for handling large number of features, categories and documents.
at each node, this classifier can ignore the large number of ``noise'' words in a document.
this ontology  is used to determine the similarity of two terms in the given group.
based on its  performance on the classification of 800,000 example queries recorded from msn  search, the system received the runner-up award for query categorization  performance of the kdd cup 2005." } @article{li:2005:kddcup2005report,  author =
a series of experiments indicates that the use of senses does not result in any significant categorization improvement.}, } @inproceedings{kessler97, author = {brett kessler and geoff nunberg and hinrich sch{\"{u}}tze}, title = {automatic detection of text genre}, booktitle = {proceedings of acl-97, 35th annual meeting of the association for computational linguistics}, publisher = {morgan kaufmann publishers, san francisco, us}, editor =
we  report the results of systematic experimentation of this method performed on  the standard {\sf reuters-21578} benchmark.
{42--49}, url = {http://www.cs.cmu.edu/~yiming/papers.yy/sigir99.ps}, abstract = {this paper reports a controlled study with statistical significance tests on five text categorization methods: the support vector machines (svm), a k-nearest neighbor (knn) classifier, a neural network (nnet) approach, the linear least-squares fit (llsf) mapping and a naive bayes (nb) classifier.
"kdd cup-2005 report: facing a great challenge", journal =  "{sigkdd} explorations", pages =
overall, we present an algorithm, a  variation of littlestone's winnow, which performs significantly better than any  other algorithm tested on this task using a similar feature set.}, }  @inproceedings{dalessio00, author = {stephen d'alessio and keitha murray and  robert schiaffino and aaron kershenbaum}, title = {
our contribution is to propose robust statistical models and  a relaxation labeling technique for better classification by exploiting link  information in a small neighborhood around documents.
morgan kaufmann publishers, san francisco, us}, url = {}, abstract = {}, } @inproceedings{zhang03b, author = {jian zhang and yiming yang}, title = {robustness of regularized linear classification methods in text categorization}, booktitle =
this paper evaluates whirl on a number of inductive classification tasks using data from the world wide web.
"proceedings of the 28th european conference  on ir research (ecir)", year =
we tested our new prototype system on the reuters-21578 text categorization test collection.}, } @inproceedings{teahan00, author = {
various relaxation approaches have  been proposed to counter this problem including: asymmetric svm learning  algorithms (soft svms with asymmetric misclassification costs); uneven margin  based learning; and thresholding.
adelaide, au}, year = {2002}, pages =
grenoble, fr}, year = {1985}, pages = {207--216}, url = {}, abstract = {}, } @inproceedings{fuhr91a, author = {fuhr, norbert and hartmann, stephan and knorz, gerhard and lustig, gerhard and schwantner, michael and tzeras, konstadinos}, title = {{air/x} -- a rule-based multistage indexing system for large subject fields}, booktitle = {proceedings of riao-91, 3rd international conference ``recherche d'information assistee par ordinateur''}, publisher =
we present simpl, a nearly linear-time  classification algorithm which mimics the strengths of svms while avoiding the  training bottleneck.
firstly we introduce a novel kernel, whose gram matrix is the well known  co-citation matrix from bibliometrics, and demonstrate on real data that it has  a good performance.
"proceedings of the 6th acm/ieee-cs joint conference on digital libraries", year =
"retrospective news event detection (red) is  defined as the discovery of previously unidentified events in historical news  corpus.
cross validation over 4 different corpora in two languages  allowed us to gather an overwhelming evidence that complex nominals, proper  nouns and word senses are not adequate to improve tc accuracy.}, }  @article{mostafa00, author = {javed mostafa and wai lam}, title = {automatic  classification using supervised learning in a medical document filtering  application}, journal = {information processing and management}, year = {2000},
we show that, in expectation,  the excess cumulative h-loss grows at most logarithmically in the length of the  data sequence.
our approach views the task as one of information integration using whirl, a tool that combines database functionalities with techniques from the information retrieval literature.}, } @inproceedings{zelikovitz01, author = {sarah zelikovitz and haym hirsh}, title = {using lsi for text classification in the presence of background text}, booktitle = {proceedings of cikm-01, 10th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, editor = {henrique paques and ling liu and david grossman}, year = {2001}, address = {atlanta, us}, pages = {113--118}, url = {ftp://ftp.cs.rutgers.edu/pub/zelikovi/lsi01.ps}, abstract = {
in the domain of terrorism, autoslog created a dictionary using a training corpus and five person-hours of effort that achieved 98\% of the performance of a hand-crafted dictionary that took approximately 1500 person-hours to build.
experimental results for the  pure theoretical model as well as for heuristic variants are given.
a fuzzy measure of  agreement between machine and manual assignment of documents to subject  categories}, booktitle = {proceedings of asis-83, 46th annual meeting of the  american society for information science}, publisher =
we report the results of applying  a variety of machine learning algorithms to the automated categorization of  english-language patent documents.
"proceedings of the thirteenth  international world wide web conference ({www2004})", year = 2004, pages =  "482--490", month =
put another way, genre classification  is orthogonal to a classification based on the documents' contents.
in this paper, we show how an operator-based view of rule induction enables the easy integration of a thesaurus as background knowledge.
"granada, spain", url =  "http://www.jrc.cec.eu.int/langtech/documents/0509_sepln-05_montejo-et-al.pdf",  abstract =
to perform classification, a naive bayesian classifier was  designed and implemented, and a decision tree based classifier was implemented.
well in text categorization?}, booktitle =
{chinese text categorization based on the binary weighting model with  non-binary smoothing}, booktitle = {proceedings of ecir-03, 25th
{springer verlag}, editor = {fabrizio sebastiani}, address = {
{charles k. nicholas and derick wood}, year = {1996}, address = {palo alto,  ca}, pages = {123--133}, publisher = {springer verlag, heidelberg, de}, note =
published in the ``lecture notes in computer science'' series, number 2175}, editor = {floriana esposito}, year = {2001}, pages = {81--86}, address = {bari, it}, url = {http://link.springer.de/link/service/series/0558/papers/2175/21750081.pdf}, abstract =
a scheme of automatic document classification is presented.
in addition to a  unigram model of document representation, a bigram model is also explored.
as control experiment best human categorization performance was established at 79.4\% for this task.
"2004" }  @inproceedings{wenliang:2004:alf, author =
this reduced feature space is then used to train a classifier over a larger training set because more documents now fit into the same amount of memory.
drawing on interviews with reuters personnel and access to reuters  documentation, we describe the coding policy and quality control procedures  used in producing the rcv1 data, the intended semantics of the hierarchical  category taxonomies, and the corrections necessary to remove errorful data.
nevertheless, it is shown that automated text  categorization techniques can exploit combinations of simple lexical and  syntactic features to infer the gender of the author of an unseen formal  written document with approximately 80 per cent accuracy.
we present experiments on the  classification of multilingual pornographic html pages using text and image  data.
in this paper we demonstrate this pitfall hurts performance even for a  relatively uniform text classification task.
the training approaches we test are the rocchio (relevance feedback) and the widrow-hoff (machine learning) algorithms and wordnet as the lexical database.
a belief networks-based generative model for structured documents.
pcut copes better  with rare categories and exhibits a smoother trade-off in recall versus  precision, but is not suitable for online decision making.
we devise an algorithm that interleaves labeling features and documents which significantly accelerates standard active learning in our simulation experiments.
by contrast, content-based methods use information about an item itself to make suggestions.
to the extraction of opinions from text is the of the orientation of subjective terms contained i.e. the determination of whether a term that opinionated content has a positive or a negative connotation.
we have conducted an  extensive experimental evaluation of our technique over collections of real  documents, including over one hundred web-accessible databases.
after training, the incoming news articles are classified based on their  similarity to the existing newsgroup categories.
as an example of stw, we propose a number of ``supervised variants'' of $tfidf$ weighting, obtained by replacing the $idf$ function with the function that has been used in phase (i) for term selection.
aleksander kolcz and vidya prabakarmurthi and jugal k. kalita}, title  = {string match and text extraction: summarization as feature selection for  text categorization}, booktitle = {proceedings of cikm-01, 10th acm  international conference on information and knowledge management}, publisher =
{philipp koehn}, title =
{ieee computer society press, los alamitos, us}, editor = {amari, shun-ichi and giles, c. lee and gori, marco and piuri, vincenzo}, year = {2000}, address = {como, it}, volume = {3}, pages = {581--586}, url = {http://dlib.computer.org/conferen/ijcnn/0619/pdf/06193581.pdf}, abstract = {one important task for text data mining is automatic text categorization, which assigns a text document to some predefined category according to their correlations.
this means that the  category of potentially relevant documents for most profiles would contain at  least 80\% of all documents later determined to be relevant to the profile.
{world scientific, singapore, sn}, address = {iizuka,  jp}, year = {1998}, pages = {935--938}, url =  {http://www-a2k.is.tokushima-u.ac.jp/member/sasaki/frame_home/papers/iizuka98.ps},  abstract = {document categorization, which is defined as the classification of  text documents into one of several fixed classes or categories, has become  important with the explosive growth of the world wide web.
susan  craw and alun d. preece}, publisher
the extension comprises the computation of the codes by a simulated annealing algorithm and optimization of kullback-leibler (kl) category distances within the code-words.
we verify experimentally that the integration of wordnet helps ssfcm improve its performance, effectively addresses the classification of documents into categories with few training documents and does not interfere with the use of training data.}, } @article{benkhalifa01a, author = {mohammed benkhalifa and abdelhak mouradi and houssaine bouyakhf}, title = {
it is concluded that, while there is no significant  difference in the predictive efficiency between the bayesian and the factor  score methods, automatic document classification is enhanced by the use of a  factor-analytically-derived classification schedule.
using k-nearest neighbor (knn) as the classifier and five  evaluation benchmark collections as the testbets, three common thresholding  methods were investigated, including rank-based thresholding (rcut),  proportion-based assignments (pcut) and score-based local optimization (scut);  in addition, new variants of these methods are proposed to overcome significant  problems in the existing approaches.
this method can be applied in a post-processing step and therefore be combined with other known (non-hierarchical) categorization approaches.}, } @inproceedings{fuhr84, author = {fuhr, norbert and knorz, gerhard}, title = {retrieval test evaluation of a rule-based automated indexing {(air/phys)}}, booktitle = {proceedings of sigir-84, 7th acm international conference on research and development in information retrieval}, year = {1984}, publisher =
a review of these approaches is presented  here.
in previous research, a  text document is commonly represented by the term frequency and the inverted  document frequency of each feature.
much  information is nowadays stored as multilingual textual data; therefore advanced  classification systems are currently considered as strategic components for  effective knowledge management.
one of our main results is the identification of a single classifier with good performance (relative to our classifier space) across all subdialogue lengths.}, } @inproceedings{nardiello03, author = {pio nardiello and fabrizio sebastiani and alessandro sperduti}, title = {
stuttgart, de}, pages = {162--167}, url = {}, abstract = {}, }  @inproceedings{iwayama95, author = {makoto iwayama and takenobu tokunaga},  title = {cluster-based text categorization: a comparison of category search  strategies}, booktitle = {proceedings of sigir-95, 18th acm international  conference on research and development in information retrieval}, editor =
moreover, the estimators developed here address  the special performance measures needed for evaluating text classifiers.
{125--132}, address = {faro, pt}, url = {http://link.springer.de/link/service/series/0558/papers/2389/23890125.pdf}, abstract =
"university of maryland, usa", year = "september 21--24, 2003" } @inproceedings{iccc03, author =
users can search for  pants or classify patent text.
for  example, text can be classified with a very high degree of accuracy by  authorship, language, dialect and genre.
the open and modularized  system architecture makes our classifier be extendible.
the application of a logic-based approach to text document classification is critical when one wishes to be able to justify why a particular document has been assigned to one class versus the other class.
the system has been  applied to the following tasks: presorting of forms, reports and letters, index  extraction for archiving and retrieval, page type classification and text  column analysis of real estate register documents, in-house mail sorting and  electronic distribution to departments.
nevertheless, tc provides many challenges to machine learning.
explorative research using a simple voting system is presented and discussed in the light of a probabilistic model that was originally developed for safety critical software.
the second problem is that even with a representative model, the improvements  given by unlabeled data do not sufficiently compensate for a paucity of labeled  data.
in the context of this model, we compare two approaches to dimensionality reduction in representation: one based on term selection and another based on latent semantic indexing (lsi).
the result is a generalized naive bayes classifier which allows for a local markov dependence among observations; a model we refer to as the c hain a ugmented n aive bayes (can) bayes classifier.
with a smaller number of features, ssfcm's performance is also superior to that of ssahc's.
using insights gained from examining the way humans make fast decisions when  classifying text documents, two new text classification algorithms are  developed based on sequential sampling processes.
one way to achieve this aim is by  applying machine learning techniques to training data containing the various  senses of the ambiguous words.
to advance research on this challenging but important problem, we promote a natural, experimental framework-the daily classification task-which can be applied to large time-based datasets, such as reuters rcv1.
we find that even a relatively high level of errors in the ocred documents does not substantially affect stylistic classification accuracy."
we show how these networks can support text routing of noisy newswire titles according to different given categories.
with nearly perfect reliability the svm was able to reject other authors and detected the target author in 60-80\% of the cases.
this paper presents a modular software system, which classifies a large variety of office documents according to layout form and textual content.
"1265--1270", abstract = "evaluating text fragments for positive and negative subjective expressions and their strength can be important in applications such as single- or multi- document summarization, document ranking, data mining, etc.
this  word-cluster representation is computed using the recently introduced  information bottleneck method, which generates a compact and efficient  representation of documents.
we also try combining classifiers based on different representations using a majority voting technique, and this improves performance on both test collections.
we also devised a method to select the  category term from the word cluster map.
{proceedings of sdair-96, 5th annual symposium on  document analysis and information retrieval}, publisher = {}, editor = {}, year  = {1996}, address = {las vegas, us}, pages = {191--207}, url = {}, abstract =  {knowledge discovery in databases (kdd) focuses on the computerized exploration  of large amounts of data and on the discovery of interesting patterns within  them.
they use credible knowledge resources, including a us government organizational hierarchy, a thematic hierarchy from the open directory project (odp) web directory, and personal browse histories, to add valuable metadata to search results.
on the two other sets (reuters-21578 and webkb) the word-based representation slightly outperforms the word-cluster representation.
we also present  preliminary results showing how this model could classify documents with dtds  not represented in the training set.}, } @inproceedings{denoyer03a, author =
"100--110", volume = 7,  number = 2, year = 2005, url =  "http://www.acm.org/sigs/sigkdd/explorations/issues/7-2-2005-12/kddcup2005report_shen.pdf",  abstract =
"this paper describes an application of ir and text categorization methods  to a highly practical problem in biomedicine, specifically, gene ontology (go)  annotation.
most of them are irrelevant and others introduce noise which could mislead the classifiers.
our results indicate  that features based on latent semantic indexing are more effective for  techniques such as linear discriminant analysis and logistic regression, which  have no way to protect against overfitting.
our results indicate that the use of n-grams is an attractive technique which can even compare to techniques relying on a morphological analysis.
most of these methods are usually awkward and  sometimes intractable in high-dimensional feature spaces.
we describe a method for classifying news stories using memory based reasoning (mbr) a k-nearest neighbor method), that does not require manual topic definitions.
this approach is motivated by the observation that hyperbolic spaces possess a geometry where the size of a neighborhood around a point increases exponentially and therefore provides more freedom to map a complex information space such as language into spatial relations.
an example-based mapping method for text categorization and retrieval}, journal = {acm transactions on information systems}, year = {1994}, number = {3}, volume = {12}, pages = {252--277}, url = {http://www.acm.org/pubs/articles/journals/tois/1994-12-3/p252-yang/p252-yang.pdf}, abstract = {a unified model for text categorization and text retrieval is introduced.
conference of the  american association for artificial intelligence}, editor = {}, publisher =
{1}, volume = {35}, pages = {45--87}, url = {}, abstract = {
the analysis demonstrates that one of these  models is theoretically attractive (introducing few new parameters while  increasing flexibility), computationally efficient, and empirically  preferable.}, } @article{bennett05, author = {paul n. bennett and susan t.  dumais and eric horvitz}, title = {
for our experiments we decided on using the text corpus reuters-21578.
the  algorithm iteratively estimates the usefulness of features and selects them  accordingly, using either forward selection or backward elimination.
we show that such unlabeled background knowledge can greatly decrease error rates, particularly if the number of examples or the size of the strings in the training set is small.
claire n{\'{e}}dellec and c{\'{e}}line rouveirol}, address = {chemnitz, de}, pages = {137--142}, year = {1998}, url = {http://www-ai.cs.uni-dortmund.de/dokumente/joachims_98a.ps.gz}, abstract = {the paper explores the use of support vector machines (svms) for learning text classifiers from examples.
technique is employed to estimate the likelihood of these associations.
experiments with simulated  concept drift scenarios based on real-world text data compare the new method  with other window management approaches.
however, the three machine  learning methods we employed (naive bayes, maximum entropy classification, and  support vector machines) do not perform as well on sentiment classification as  on traditional topic-based categorization.
the best results were obtained using the m-estimate as search heuristics combined with the likelihood-ratio-statics for pruning.
the results suggest that information extraction techniques can support high-precision text classification and, in general, using more extracted information improves performance.
we describe a new family of topic-ranking algorithms for multi-labeled documents.
ssahc is (i) a  clustering algorithm that (ii) uses a finite design set of labeled data to  (iii) help agglomerative hierarchical clustering (ahc) algorithms partition a  finite set of unlabeled data and then (iv) terminates without the capability to  label other objects.
stretch offers ease of use  and application programming and the ability to dynamically adapt to new types  of documents.
recent studies have estimated the size of this ''hidden web'' to be 500 billion pages, while the size of the ``crawlable'' web is only an es-timated two billion pages.
survey coding is a difficult task, since the code that should be attributed to a respondent based on the answer she has given is a matter of subjective judgment, and thus requires expertise.
also, different derivations from the  normal recall and precision performance indicators are discussed and compared.
the technique is completely language-independent, highly garble-resistant, and computationally simple.
moreover, the derived classifier reachs the performance (about 85\%) of the best known models (i.e. support vector machines (svm) and k-nearest neighbour (knn)) characterized by an higher computational complexity for training and processing.}, } @inproceedings{basili01a, author = {roberto basili and alessandro moschitti and maria t. pazienza}, title = {
the encyclopedia of language and linguistics}, publisher = {elsevier science publishers}, address = {amsterdam, nl}, pages = {}, url = {http://www.math.unipd.it/~fabseb60//publications/ell06.pdf}, edition = {second}, } @inproceedings{sebastiani99, author = {fabrizio sebastiani}, title = {a tutorial on automated text categorisation}, booktitle = {
{proceedings of the ninth conference on computational natural language learning  (conll-2005)}, month = {june}, year = {2005}, address = {
the first level of the architecture predicts the probabilities of the meta-topic groups.
with nearly perfect reliability the svm was able to reject other  authors and detected the target author in 60-80\% of the cases.
booktitle = {proceedings of icml-00, 17th international conference on machine learning}, editor = {pat langley}, year = {2000}, address = {stanford, us}, pages =
{1997}, pages = {201--208}, url =  {http://www.acm.org/pubs/articles/proceedings/ai/267658/p201-clack/p201-clack.pdf},  abstract = {
specifically, we show  that by adjusting the category levels in a principled way, that precision can  be significantly improved, from 84\% to 91\%, on the much-studied reuters-21578  corpus organized in a three-level hierarchy of categories.}, }  @article{damashek95, author = {marc damashek}, title = {
one of its main drawbacks, in ir, is its computational  cost.
= a part of a book with its own title % required: author, title, booktitle, publisher, year % optional: editor, volume or number, series, type, chapter, pages, % address, edition, month, note %
however the high  performance of most existing pn classifiers heavily depends upon the  availability of large dictionaries of domain-specific proper nouns, and a  certain amount of manual work for rule writing or manual tagging.
we study the use of support vector machines (svms) in classifying email as spam or nonspam by comparing it to three other classification algorithms: ripper, rocchio, and boosting decision trees.
eliminating high-degree biased character bigrams for dimensionality reduction in chinese text categorization}, booktitle = {proceedings of ecir-04, 26th european conference on information retrieval research}, editor = {sharon mcdonald and john tait}, year = {2004}, address = {sunderland, uk}, publisher = {springer verlag, heidelberg, de}, note = {
} @inproceedings{wang:2005:woi, author =
a method for  disambiguating word senses in a large corpus}, journal = {computers and the  humanities}, year = {1993}, number = {5}, volume = {26}, pages = {415--439},  url = {http://www.research.att.com/~kwc/published_1993_sense.ps}, abstract =  {}, } @inproceedings{gao03, author = {sheng gao and wen wu and chin-hui lee and  tat-seng chua}, title = {
w. bruce croft  and david j. harper and donald h. kraft and justin zobel}, publisher = {acm  press, new york, us}, address = {new orleans, us}, year = {2001}, pages =
experimental results have shown that the statistical learning approach and the  learning feedback technique are practical means to automatic indexing of  controlled index terms.}, } @inproceedings{lewis00, author = {lewis, david d.},  title = {machine learning for text categorization: background and  characteristics}, booktitle = {proceedings of the 21st annual national online  meeting}, editor = {williams, martha e.}, publisher =
the system includes  a unique phrase help facility, which helps users find and add phrases and terms  related to those in their query.}, } @inproceedings{lee00, author = {hahn-ming  lee and chih-ming chen and cheng-wei hwang}, title = {a neural network document  classifier with linguistic feature selection}, booktitle = {proceedings of  iea/aie-00, 13th international conference on industrial and engineering  applications of artificial intelligence and expert systems}, publisher = {},  editor = {}, year = {2003}, address = {new orleans, us}, pages = {555--560},  url = {}, abstract = {}, } @inproceedings{lee02, author = {yong-bae lee and  sung h. myaeng}, title = {
this paper reports a system that hierarchically classifies chinese web documents without dictionary support and segmentation procedure.
text  categorization of low quality images}, booktitle = {proceedings of sdair-95,  4th annual symposium on document analysis and information retrieval}, publisher  = {}, editor = {}, year = {1995}, address
an algorithm for  sequential sampling during machine learning of statistical classifiers was  developed and tested on a newswire text categorization task.
this (semi-supervised) learning paradigm falls somewhere between the fully supervised and the fully unsupervised learning schemes, in the sense that it exploits both class information contained in labeled data (training documents) and structure information possessed by unlabeled data (test documents) in order to produce better partitions for test documents.
when  parameters tuned on an early benchmark tdt corpus were evaluated on a later tdt  benchmark corpus with no overlapping events, we observed a 38-65\% reduction in  tracking cost (a weighted combination of errors) by the combined system over  the individual methods evaluated under the same conditions, strongly suggesting  the robustness of this approach as a solution for improving cross-class  performance consistency of statistical classifiers when standard  cross-validation fails due to the lack of representative validation sets.}, }  @inproceedings{yang00b, author = {hsin-chang yang and chung-hong lee}, title =  {automatic category generation for text documents by self-organizing maps},  booktitle = {proceedings of ijcnn-00, 11th international joint conference on  neural networks}, publisher =
}  @article{kyber03, author = "d. tikk and j. d. yang and s. l. bang",  title =
in this paper, a bayesian inference network model for automatic indexing with index terms (descriptors) from a prescribed vocabulary is presented.
} @inproceedings{alm:2005:eft, author =
specifically, we view the expansion of such lexicons as a process of learning previously unknown associations between terms and \emph{domains}.
systems for text retrieval, routing, categorization and other ir tasks rely heavily on linear classifiers.
as a result, they are not portable across  domains.
"we present a principled methodology for filtering news stories by  formal measures of information novelty, and show how the techniques can be used  to custom-tailor newsfeeds based on information that a user has already  reviewed.
empirical results on the reuters-22173 collection are also  discussed.}, } @article{kwon03, author = {
"in this paper, we use a blog corpus to demonstrate that we can often identify the author of an anonymous text even where there are many thousands of candidate authors.
in addition, the way in which learning with redundancy influences categorization performance is also studied.}, } @inproceedings{moulinier97, author = {
comparing efficiency, knn was notably more costly in terms of time and memory than the other two methods.
existing classification schemes which ignore the hierarchical  structure and treat the topics as separate classes are often inadequate in text  classification where the there is a large number of classes and a huge number  of relevant features needed to distinguish between them.
{stefan wermter and garen arevian and christo panchev}, title = {
} @inproceedings{li:2005:pmr, author =
{journal of intelligent information systems}, year = {2002}, note = {special issue on automated text categorization}, volume = {18}, number = {2/3}, pages = {153--172}, url = {http://www.wkap.nl/article.pdf?391244}, abstract = {
the experimental  results on the validation dataset help confirm our conjectures on the  performance of the q2c@ust system.
% % % entry types % % article % = an article from a journal or magazine % required: author, title, journal, year % optional: volume, number, pages, month, note %
experimental results on three real world data sets from usenet, yahoo, and corporate web pages show improved performance, with a reduction in error up to 29\% over the traditional flat classifier.}, } @inproceedings{mccallum98c, author = {andrew mccallum and k. nigam}, title = {
a standard choice of kernel function has been the inner product  between the vector-space representation of two documents, in analogy with  classical information retrieval (ir) approaches.
{association for computational linguistics, morristown, us}, editor = {}, pages  = {79--86}, address = {philadelphia, us}, url =  {http://acl.ldc.upenn.edu/acl2002/emnlp/pdfs/emnlp219.pdf}, abstract = {
we develop a novel measure that captures feature redundancy, and use it to analyze a large collection of datasets.
then,  two schemes called s-br1 and s-br2 are proposed to deal with these bigrams: the  former directly eliminates them from the feature set whereas the latter  replaces them with the corresponding significant characters involved.
"experiments with multilabel text classifier on the {r}euters collection", booktitle = "proc. of the ieee int.
published in the ``lecture notes in computer science'' series, number  2168}, url =  {http://www.techfak.uni-bielefeld.de/ags/ni/publications/papers/ontrupritter2001-tca.pdf},  abstract =
in this way,  it allows users to navigate the results of a query at a more topical level  rather than having to examine each document text separately.}, }  @article{sakakibara96, author = {yasubumi sakakibara and kazuo misue and  takeshi koshiba}, title = {a machine learning approach to knowledge  acquisitions from text databases}, year = {1996}, journal = {
then an optimal matching between the web object and the domain knowledge is performed, in order to pick out the structure attributes of the web object from the knowledge.
"d. tikk and {gy.} biro and j. d. yang", title = "experiments with a hierarchical text categorization method on {wipo} patent collections", booktitle =
{nick cercone and tsau y. lin and xindong wu}, year = {2001}, address = {san  jose, ca}, pages = {647--648}, url = {}, abstract = {}, }  @inproceedings{soucy03, author = {pascal soucy and guy w. mineau}, title =  {feature selection strategies for text categorization}, booktitle = {
a comparative study on  statistical machine learning algorithms and thresholding strategies for  automatic text categorization}, booktitle = {proceedings of pricai-02, 7th  pacific rim international conference on artificial intelligence}, editor =
we investigate how best to resolve the training problems related to  the attribution of multiple classification codes to each patent document.}, }  @inproceedings{fangmeyer68, author = {hermann fangmeyer and gerhard lustig},  title = {
text categorization is useful for indexing documents for information retrieval, filtering parts for document understanding, and summarizing contents of documents of special interests.
this technique used in the learning process modifies the relationship between an index term and its relevant and irrelevant words to improve the learning performance and, thus, the indexing performance.
we have empirically demonstrated that rule-based methods like ours result in high classification accuracy when the categories to which texts are to be assigned are relatively specific ones and when the texts tend to be short.
then active learning is combined with expectation-maximization in order to ``fill in'' the class labels of those documents that remain unlabeled.
our results indicate that features based on latent semantic indexing are more effective for techniques such as linear discriminant analysis and logistic regression, which have no way to protect against overfitting.
{1}, year = {2003}, pages = {51--77}, url = {}, abstract = {}, }  @inproceedings{macskassy03a, author =
approximately 55 percent of the documents were automatedly and correctly classified.}, } @inproceedings{brank02a, author
for text  categorization (tc) are available fewer and less definitive studies on the use  of advanced document representations as it is a relatively new research area  (compared to document retrieval).
this task arises in the construction of search engines and web knowledge bases.
the experimental results show that the proposed methods of incorporating prior knowledge is effective."
based on this consideration, the authors have built a  neural network classification system, which has three subsystems: a  user-maintainable feature definition subsystem, a feature extraction subsystem,  and a neural network subsystem.
{published in the ``lecture notes in computer science'' series, number 1923},  year = {2000}, address = {lisbon, pt}, pages = {403--406}, url =  {http://link.springer.de/link/service/series/0558/papers/1923/19230403.pdf},  abstract = {
we  present a unified view of text categorization systems, focusing on the  selection of features.
text  categorization is typically formulated as a concept learning problem where each  instance is a single isolated document.
we find  that term selection and our modified lsi representations lead to similar topic  spotting performance, and that this performance is equal to or better than  other published results on the same corpus.}, } @mastersthesis{wiener95a,  author = {erik d. wiener}, title = {
however, these tools are quickly  becoming inadequate as query results grow incomprehensibly large and manual  classification in topic hierarchies creates an immense bottleneck.
"474--483", }  @inproceedings{hulth:2006:sae, author =
proceedings of ecir-02, 24th european colloquium on information retrieval research}, editor =
a number of methods for  feature reduction and feature selection in text classification and information  retrieval systems are compared.
a  major knowledge-engineering bottleneck for information extraction systems is  the process of constructing an appropriate dictionary of extraction patterns.
with the former, we provide an enhanced document representation that incorporates the structural and heterogeneous nature of web documents.
nevertheless, tc provides many challenges to machine  learning.
the high number of features is reduced by feature subset  selection and additionally by using `stop-list', pruning low-frequency features  and using a short description of each document given in the hierarchy instead  of using the document itself.
however, svms had significantly less training time.}
we report the results of systematic experimentation of this method performed on the standard {\sf reuters-21578} benchmark.
the experiments of the categorization of news articles show that the proposed schemes of text categorization outperform the schemes with crisp sets.}, } @incollection{jo99a, author = {taeho c. jo}, title = {news article classification based on categorical points from keywords in backdata}, booktitle = {computational intelligence for modelling, control and automation}, editor = {
the  approach is compared to other attempts at homograph disambiguation using both  machine readable dictionaries and unrestricted text and the use of training  instances is determined to be a crucial difference.}, } @proceedings{hearst96a,  editor =
we focus our discussion on the ability to discriminate between authors for the case of both aggregated e-mail topics as well as across different email topics.
we also investigate the usability of our automated learning approach by actually developing a system that categorizes texts into a tree of categories.
in this way, it allows users to navigate the results of a query at a more topical level rather than having to examine each document text separately.}, } @article{sakakibara96, author = {yasubumi sakakibara and kazuo misue and takeshi koshiba}, title = {
here the probabilistic model forms a guideline for the definition of the feature vector.
in this work we study three mistake-driven learning algorithms for a typical task of this nature - text categorization.
} @inproceedings{radovanovic:2006:ccm, author = {milo\v{s} radovanovi\'c and mirjana ivanovi\'c}, title = {cat{s}: a classification-powered meta-search engine}, booktitle = {
text classifiers that  give probability estimates are more readily applicable in a variety of  scenarios.
{pattern recognition letters}, pages = {1225--1231}, year = {1997}, volume =
while this weighting method seems very appropriate for ir, it is not clear that it is the best choice for tc problems.
"information retrieval and text categorization  with semantic indexing", booktitle = "computational linguistics and  intelligent text processing (lecture notes in computer science, vol.
we propose a method that incorporates simple semantics into tdt by splitting the term space into groups of terms that have the meaning of the same type.
the research described in this paper  details the progress made in a prototype adaptive information filtering system  based on weighted trigram analysis and evolutionary computation.
the traditional  method of building a single classifier to do all the classification work would  incur a high overhead.
text categorization with support vector machines: how to represent texts in input space?}, journal = {machine learning}, year = {2002}, volume = {46}, number = {1/3}, pages = {423--444}, url = {http://www.wkap.nl/article.pdf?380516}, abstract = {
previous researches have  investigated the use of $n$-grams (or some variant of them) in the context of  specific learning algorithms, and thus have not obtained general answers on  their usefulness for tc.
the proxy logs the  user's activities and extracts the user's interests without user intervention.
results show that for hierarchical techniques it is better to use hierarchical training sets.}, } @inproceedings{cerny83, author = {barbara a. cerny and anna okseniuk and j. dennis lawrence}, title = {
a study of  approaches to hypertext categorization}, journal = {journal of intelligent  information systems}, year = {2002}, note = {special issue on automated text  categorization}, volume = {18}, number = {2/3}, pages = {219--241}, url =  {http://www.wkap.nl/article.pdf?391248}, abstract = {
we report the results of applying a variety of machine learning algorithms to the automated categorization of english-language patent documents.
we therefore propose the category-similarity measures and distance-based measures to consider the degree of misclassification in measuring the classification performance.
finally, it shows how the performance of multinomial naive
experimental results indicate that the mfom classifier gives improved f1 and enhanced robustness over the conventional one.
in this paper we examine text categorization methods from a  perspective that considers the tradeoff between accuracy and scalability to  large data sets and large feature sizes.
the use of reduced feature sets allows us to utilize more complex (probabilistic) models, without encountering many of the standard computational and robustness difficulties.}, } @inproceedings{kongovi02, author = {madhusudhan kongovi and juan carlos guzman and venu dasigi}, title = {
text classification in a hierarchical mixture model for small training sets}, booktitle = {proceedings of cikm-01, 10th acm international conference on information and knowledge management}, publisher =
the baseline method combines the terms of all the articles of each  newsgroup in the training set to represent the newsgroups as single vectors.
{journal of the american society for information science}, year = {1996},  volume = {47}, number = {5}, pages = {357--369}, url =  {http://www3.interscience.wiley.com/cgi-bin/fulltext?id=57757&placebo=ie.pdf},  abstract = {
therefore, the contributions of this research are in learning and generalizing neural architectures for the robust interpretation of potentially noisy unrestricted messages.
a neural network approach to topic spotting in text}, school = {department of computer science, university of colorado at boulder}, address = {boulder, us}, year = {1995}, url = {http://www.stern.nyu.edu/~aweigend/research/papers/textcategorization/wiener_thesis95.ps}, abstract = {
new  directions in text categorization}, editor
we describe a content-based book recommending system that  utilizes information extraction and a machine-learning algorithm for text  categorization.
"betts, tom and milosavljevic, maria and oberlander, jon", title =
this paper looks at a simplified version of the problem: classifying online product reviews into positive and negative classes.
it is possible to create additional hierarchy among the categories.
computationally, expnet has an o(n log n) time complexity which is much more  efficient than the cubic complexity of the llsf method.
we demonstrate the effectiveness of our categorization approach using two real-world document collections from the medline database.
text mining: finding nuggets in mountains of textual data}, booktitle = {proceedings of kdd-99, 5th acm international conference on knowledge discovery and data mining}, publisher = {acm press, new york, us}, editor = {}, year = {1999}, address = {san diego, us}, pages = {398--401}, url = {http://www.acm.org/pubs/articles/proceedings/ai/312129/p398-dorre/p398-dorre.pdf}, abstract = {
the results show that the method outperforms svm at multi-class categorization, and interestingly, that results correlate strongly with compression-based methods.}, } @inproceedings{kim00, author = {yu-hwan kim and shang-yoon hahn and byoung-tak zhang}, title = {text filtering by boosting naive bayes classifiers}, booktitle = {proceedings of sigir-00, 23rd acm international conference on research and development in information retrieval}, editor = {nicholas j. belkin and peter ingwersen and mun-kew leong}, publisher = {acm press, new york, us}, address = {athens, gr}, year = {2000}, pages = {168--175}, url = {http://www.acm.org/pubs/articles/proceedings/ir/345508/p168-kim/p168-kim.pdf}, abstract = {several machine learning algorithms have recently been used for text categorization and filtering.
experiments were conducted using a  large scale document collection from reuters news articles.
the growing number of digital libraries imposes a review of the available data from those databases.
"david vogel and steffen bickel and peter haider and rolf schimpfky and peter siemen and steve bridges and tobias scheffer", title =
}  @inproceedings{gliozzo:2005:dkt, author = {
such a semantic  mapping leads to a significant improvement in categorization and retrieval,  compared to alternative approaches.}, } @inproceedings{yang94a, author =
{automatic adaptation of proper noun dictionaries through cooperation of machine learning and probabilistic methods}, booktitle = {proceedings of sigir-00, 23rd acm international conference on research and development in information retrieval}, editor = {nicholas j. belkin and peter ingwersen and mun-kew leong}, publisher = {acm press, new york, us}, address = {athens, gr}, year = {2000}, pages = {128--135}, url = {http://www.acm.org/pubs/articles/proceedings/ir/345508/p128-petasis/p128-petasis.pdf}, abstract = {
{acm press, new york, us}, address = {toronto, ca}, year = {2003}, pages =  {104--110}, url = {http://doi.acm.org/10.1145/860435.860456}, abstract = {
pisa, it}, year = {2003}, pages = {305--319}, url = {http://link.springer.de/link/service/series/0558/papers/2633/26330305.pdf}, abstract = {
"this paper reports a cross-benchmark evaluation of regularized logistic  regression (lr) and incremental rocchio for adaptive filtering.
this is expensive, requires a degree of  sophistication about linguistics and classification, and makes it difficult to  use combinations of weak predictors.
"n. o. attok-okine and b. m.
yves chiaramella}, publisher =
we also  describe and evaluate a hybrid term selection technique, first applying uc to  eliminate noisy terms and then using another criterium to select the best  terms.}, } @inproceedings{ragas98, author =
"we consider the relationship between training set size and the parameter k for the k-nearest neighbors (knn) classifier.
we introduce a probabilistic method for combining classifiers that  considers the context-sensitive reliabilities of contributing classifiers.
when user interests change, in pva, not only the contents, but also the structure of the user profile are modified to adapt to the changes.
we tested our learning method on the task of single-label classification using the reuters-21578 benchmark.
given the large number (> 13,000) of closely related  categories, this is a challenging task that is unlikely to succumb to a single  algorithmic solution.
in  the second phase, to clarify the impact of this performance on filtering,  different types of user profiles were created by grouping subsets of classes  based on their individual classification accuracy rates.
from research to application}, booktitle = {proceedings of sigir-88, 11th acm international conference on research and development in information retrieval}, editor = {
the example application, categorization of e-mail messages, is described.
text categorization is of increasing interest in both  controlled vocabulary indexing and other applications.
our problem domain consists of hyperlinks given in a form of  small-documents represented with word vectors.
the proposed method divides the  documents into sentences, and categorizes each sentence using keyword lists of  each category and sentence similarity measure.
the indexing strategy  first automatically classifies the document, thus avoiding pre-sorting, then  locates and reads the information pertaining to the specific document class.
experimental results show that  modulating the structure of user profile does increase the accuracy of  personalization systems.}, } @article{chen02, author = {
the document collection on which this comparison takes place is a subset of the annotated brown corpus semantic concordance.
using the results evaluated on the other versions of reuters which exclude the unlabelled documents, the performance of twelve methods are compared directly or indirectly.
in these applications, several classes of documents are involved.
the rapid expansion of multimedia digital collections brings to the fore the need for classifying not only text documents but their embedded non-textual parts as well.
{aaai  press, menlo park, us}, editor = {}, year = {1999}, pages = {93--98}, address =
approximately 55 percent  of the documents were automatedly and correctly classified.}, }  @inproceedings{brank02a, author =
this hybrid approach of neural selforganization and symbolic  hypernym relationships is successful to achieve good classification rates on  100,000 full-text news articles.
stanford, us}, pages = {1167--1182}, publisher = {morgan kaufmann  publishers, san francisco, us}, url =  {http://www.cs.cmu.edu/~yiming/papers.yy/icml00.ps.gz}, abstract = {
{marti a. hearst and fredric gey and richard tong},  publisher = {acm press, new york, us}, address = {berkeley, us}, year = {1999},  pages = {281--282}, url =
we verify experimentally that the integration of wordnet helps ssahc improve its performance, effectively addresses the classification of documents into categories with few training documents.
"dayanik, aynur  and lewis, david d. and madigan, david and menkov, vladimir and genkin,  alexander", title =
our new approach is based on a bayesian network induction which does not rely on some major assumptions found in a previous method using the bayesian independence classifier approach.
we  also discuss a new technique to help the classifier distinguish better among  closely related clusters.
these generative models do not capture all the intricacies of text; however on some domains this technique substan- tially improves classification accuracy, especially when labeled data are sparse.
year = {1998}, address = {bethesda, us}, pages = {148--155}, url =  {http://robotics.stanford.edu/users/sahami/papers-dir/cikm98.pdf}, abstract =  {text categorization - the assignment of natural language texts to one or more  predefined categories based on their content - is an important component in  many information organization and management tasks.
the statistical approach to the analysis of document collections  and retrieval therefrom has proceeded along two main lines, associative machine  searching and automatic classification.
this paper  describes an accurate, relatively inexpensive method for the disambiguation of  noun homographs using large text corpora.
integrating background knowledge into nearest-neighbor text  classification}, pages = {1--5}, url = {}, booktitle = {proceedings of  eccbr-02, 6th european conference on case-based reasoning}, editor = {
kld method achieve substantial improvements over the tfidf performing method.}, } @article{blei03, author = {david m. blei and andrew y. ng and michael i. jordan}, title = {latent dirichlet allocation}, journal = {journal of machine learning research}, volume = {3}, pages = {993--1022}, year = {2003}, url = {http://www.ai.mit.edu/projects/jmlr/papers/volume3/blei03a/blei03a.pdf}, abstract =
this suggests that df thresholding, the simplest method with the lowest cost in computation, can be reliably used instead of ig or chi when the computation of these measures are too expensive.
most previous studies found that the majority of these features are relevant for classification, and that the performance of text categorization with support vector machines peaks when no feature selection is performed.
the main goal of this research is to build neural networks and to train them in assigning mesh phrases based on term frequency of single words from title and abstract.
proceedings of www-02, international conference on the world wide web},
we present an approach using the vector space model to integrate two different kind of resources: a lexical database and training collections, in text content analysis tasks.
in the  traditional setting, text categorization is formulated as a concept learning  problem where each instance is a single isolated document.
the experimental results on the validation dataset help confirm our conjectures on the performance of the q2c@ust system.
an hybrid  approach to optimize feature selection process in text classification},  booktitle = {proceedings of ai*ia-01, 7th congress of the italian association  for artificial intelligence}, publisher = {springer verlag, heidelberg, de},
the bin-based method is intended for tasks where there is insufficient training data to estimate a separate weight for each word.
a sequential algorithm for training text classifiers: corrigendum and additional data}, journal = {sigir forum}, year = {1995}, pages = {13--19}, volume = {29}, number = {2}, url = {http://www.research.att.com/~lewis/papers/lewis95g.ps}, abstract = {
published in the ``lecture notes in computer  science'' series, number 1910}, url =  {http://link.springer.de/link/service/series/0558/papers/1910/19100490.pdf},  abstract = {supervised learning algorithms usually require large amounts of  training data to learn reasonably accurate classifiers.
to  evaluate classifiers in our multipath framework, we define a new hierarchical  loss function, the h-loss, capturing the intuition that whenever a  classification mistake is made on a node of the taxonomy, then no loss should  be charged for any additional mistake occurring in the subtree of that node.
the results of this experiment were subsequently used to create a new large medical test collection, which was used in experiments with the smart retrieval system to obtain baseline performance data as well as compare smart with the other searchers.}, } @inproceedings{hoashi00, author = {keiichiro hoashi and kazunori matsumoto and naomi inoue and kazuo hashimoto}, title = {document filtering methods using non-relevant information profile}, booktitle = {proceedings of sigir-00, 23rd acm international conference on research and development in information retrieval},
the essential formula is cue validity borrowed from  cognitive psychology, and used to select from all possible single word-based  features the `best` predictors of a given category.
put another way, genre classification is orthogonal to a classification based on the documents' contents.
an analysis of  patterns in sentences was performed with data from the trec 2002 novelty track  and experiments on novelty detection were carried out on data from the trec  2003 and 2004 novelty tracks.
character shape coding is a computationally efficient, extraordinarily robust means of providing access to the character content of document images.
unpublished % = a document with an author and title, but not formally published  % required: author, title, note % optional: month, year %  %
% % % % everyone is also welcome to let me  know either additional % % references or corrections and additions (e.g. urls,  where % % they are not already present) to the existing ones.
this is a novel approach for document classification, where each agent evolves a parse-tree representation of a user's particular information need.
{edward a. fox and neil rowe},  publisher = {acm press, new york, us}, year = {1999}, address = {berkeley, us},  pages =
a divisive information-theoretic feature clustering algorithm for text classification}, journal = {journal of machine learning research}, volume = {3}, month = {march}, pages = {1265--1287}, year = {2003}, url = {http://www.jmlr.org/papers/volume3/dhillon03a/dhillon03a.pdf}, abstract = {high dimensionality of text can be a deterrent in applying complex learners such as support vector machines to the task of text classification.
in this paper we show an adaptive incremental learning algorithm that learns interactively to classify text messages (here: emails) into categories without the need for lengthy batch training runs.
in  this paper, we present a weight adjusted k-nearest neighbor (waknn)  classification that learns feature weights based on a greedy hill climbing  technique.
we describe the results of extensive machine learning experiments on large collections of reuters' english and german newswires.
traditional techniques for this purpose can generally be classified into feature extraction and feature selection.
the methods to create, detect,  summarize, select, and code visual keywords will be detailed.
"2004" }  @inproceedings{kim:2004:rcr, author =
the results indicate that the  use of hierarchical structures improves performance significantly.}, }  @article{sable00, author = {carl l. sable and vasileios hatzivassiloglou},  title = {text-based approaches for non-topical image categorization}, journal =
this paper describes some key aspects of the system (including html parsing,  classification and displaying of results), outlines the text categorization  experiments performed in order to choose the right parameters for  classification, and puts the system into the context of related work on  (meta-)search engines.
the outcome of the result was quite impressive: in  different experimental setups, we reached a micro-averaged f1-measure of 0.89,  with a peak of 0.899.
this paper  presents a method that exploits the hierarchical structure of an indexing  vocabulary to guide the development and training of machine learning methods  for automatic text categorization.
{americal association for artificial intelligence}, address = {
zhao xu and kai yu and volker tresp and xiaowei xu and jizhi wang}, title = {
in general, only references specific to atc are considered % % pertinent to this bibliography; in particular, references that % % *are* considered pertinent are: % % % % * publications that discuss novel atc methods, novel % % experimentation of previously known methods, or resources for % % atc experimentation; % % % % * publications that discuss applications of atc (e.g. % % automated indexing for boolean ir systems, filtering, etc.).
scut is potentially better for fine-tuning but risks overfitting.
in application, the  adapted text categorizers are reliable, fast, and completely automatic.
we will also provide  some numerical experiments to illustrate these algorithms on a number of  datasets.}, } @inproceedings{zhang03, author = {
we introduce an algorithm for learning from labeled and unlabeled documents based on the combination of expectation-maximization (em) and a naive bayes classifier.
a large collection of automatically generated datasets are made available for other researchers to use.}, } @inproceedings{debole03, author = {franca debole and fabrizio sebastiani}, title = {supervised term weighting for automated text categorization}, year = {2003}, booktitle = {proceedings of sac-03, 18th acm symposium on applied computing}, address = {melbourne, us}, publisher = {acm press, new york, us}, pages = {784--788}, url = {http://www.math.unipd.it/~fabseb60/publications/sac03b.pdf}, note = {
using a commercial medline product based on the vector  space model, these physicians searched just as effectively as more experienced  searchers using boolean searching.
the use of a controlled vocabulary allows for a more consistent description of corporate documents, and promotes easier access by people across the company.
one way to  reduce the amount of labeled data required is to develop algorithms that can  learn effectively from a small number of labeled examples augmented with a  large number of unlabeled examples.
the algorithm was  evaluated on a large database of email messages that fall into five subjective  categories.
the module evaluates a large incoming stream of  documents to determine which documents are sufficiently similar to a profile at  the broad subject level to warrant more refined representation and matching.
in this paper we present an approach to discovering additional  regularities in the test set, and show that in relational domains such test set  regularities can be used to improve classification accuracy beyond that  achieved using the training set alone.
analysis and empirical evidence suggest that the evaluation  results on some versions of reuters were significantly affected by the  inclusion of a large portion of unlabelled documents, making those results  difficult to interpret and leading to considerable confusions in the  literature.
in this paper we propose a new  information-theoretic divisive algorithm for feature/word clustering and apply  it to text classification.
the accuracy of the system is only slightly lower  than that of human categorizers.}, } @inproceedings{hayes90, author = {
"rome, italy" }  @inproceedings{li:2007:sts, author =
machine learning techniques developed for learning on text  data are used here on the hierarchical classification structure.
text categorization through multistrategy learning and visualization}, booktitle = {proceedings of cicling-01, 2nd international conference on computational linguistics and intelligent text processing}, year = {2001}, editor = {alexander gelbukh}, publisher = {springer verlag, heidelberg, de}, address = {mexico city, me}, note = {
book % = a book with an  explicit publisher % required: author or editor, title, publisher, year %  optional: volume or number, series, address, edition, month, note % % booklet %  =
we have delevoped experimental systems for tc and wsd.
{2000}, pages = {160--167}, url = {http://doi.acm.org/10.1145/345508.345569},  abstract = {
the logic representation of sentences required by the adopted learning algorithm is obtained by detecting structure in raw text trough a parser.
the training approaches we test are  the rocchio (relevance feedback) and the widrow-hoff (machine learning)
in particular, evaluations on ocr documents  are very rare.
mehran sahami and marti a. hearst and eric saund}, title
recently, commercial web sites have started to  manually organize web-accessible databases into yahoo!-like hierarchical  classification schemes.
to use traditional feature-vector-  based learning methods, one could treat the presence or ab-sence of a word as a  boolean feature and use these binary-valued features together with the  numerical features.
{nicholas j. belkin and peter ingwersen and annelise mark pejtersen}, publisher = {acm press, new york, us}, address = {kobenhavn, dk}, pages = {59--65}, year = {1992}, url = {http://www.acm.org/pubs/articles/proceedings/ir/133160/p59-masand/p59-masand.pdf}, abstract = {
this model gives new representations of both news articles and news events.
we present an efficient  algorithm for text classification using hierarchical classifiers based on a  concept hierarchy.
the technique of periodical updates  improves the routing accuracy ranging from 20\% to 100\% but incurs runtime  overhead.
the bases  of the cdm are research results about the way that humans learn categories and  concepts vis-a-vis contrasting concepts.
ibm's intelligent  miner for text provides the necessary tools to unlock the business information  that is ''trapped'' in email, insurance claims, news feeds, or other document  repositories.
we propose a new method of text classification using stochastic decision lists.
our  empirical results show that the proposed approach, text categorization using  feature projections (tcfp), outperforms k-nn, rocchio, and naive bayes.
we describe the algorithm and present experimental results on applying it to the document routing problem.
the case base of natural language contexts is acquired automatically during sentence analysis using a training corpus of texts and their correct relevancy classifications.
in spite of these differences, both ripper and sleeping-experts perform extremely well across a wide variety of categorization problems, generally outperforming previously applied learning methods.
thus, the classifier has a small model size and is very fast.
the combination of text classifiers using reliability indicators}, journal = {information retrieval}, number = {1}, volume = {8}, pages = {67--100}, year = {2005}, url = {http://www.kluweronline.com/issn/1386-4564}, abstract = {
{sofus a. macskassy and haym hirsh and arunava banerjee and aynur a.  dayanik}, title = {
published in the ``lecture notes in computer science'' series, number 1874}, address = {london, uk}, pages = {409--418}, url = {http://www.cs.iastate.edu/~yang/papers/dawak00.ps}, abstract = {
a new evaluation methodology is offered that focuses on the needs of the data mining practitioner who seeks to choose one or two metrics to try that are mostly likely to have the best performance for the single dataset at hand.
in particular, a topic of any breadth will  typically contain several thousand or million relevant web pages.
we describe a text categorization task and an experiment using documents from the reuters and ohsumed collections.
moreover, dimension reduction, which is usually imperative, now becomes optional.
the paper describes the novel technique of categorization by context, which instead extracts useful information for classifying a document from the context where a url referring to it appears.
managing the hierarchical organization of data is starting to play a key role in the knowledge management community due to the great amount of human resources needed to create and maintain these organized repositories of information.
this paper describes an approach to apply term  distributions, in addition to tf and idf, to improve performance of  centroid-based text categorization.
unrestricted potentially faulty text messages arrive at a certain delivery point (e.g. email address or world wide web address).
the bases of the cdm are research  results about the way that humans learn categories and concepts vis-a-vis  contrasting concepts.
proceeding  of cscsi-03, 16th conference of the canadian society for computational studies  of intelligence}, editor
one way to reduce the amount of labeled data required is to develop algorithms that can learn effectively from a small number of labeled examples augmented with a large number of unlabeled examples.
we report on computational experience using this procedure.
"traditionally, text classifiers are built from labeled training examples.
in this paper, we propose an efficient optimal feature selection algorithm by  optimizing the objective function of orthogonal centroid (oc) subspace learning  algorithm in a discrete solution space, called orthogonal centroid feature  selection (ocfs).
in the work presented here, the decision tree  learning algorithm c4.5 is applied on a corpus of financial news articles.
they may not perform effectively in adaptive text filtering which is a more realistic problem.
however, most  of existing boosting algorithms are based on classifiers that use binary-valued  features.
the results show that the use of the hierarchical structure improves  text categorization performance significantly.}, } @inproceedings{ruiz99a,  author = {miguel e. ruiz and padmini srinivasan}, title = {
the  analysis gives theoretical insight into the heuristics used in the rocchio  algorithm, particularly the word weighting scheme and the similarity metric.
for many learning tasks where data is collected over an extended period of time, its underlying distribution is likely to change.
{ramon l{\'{o}}pez de  m{\'{a}}ntaras and enric plaza}, address = {barcelona, es}, pages =
in this paper we present a learning system for information filtering and selective information dissemination.
we discovered that the knowledge about relevance among  queries and documents can be used to obtain empirical connections between query  terms and the canonical concepts which are used for indexing the content of  documents.
this  advantage is achieved by executing morphological and semantic analyses of an  incoming text.
we present evidence that this new algorithm leads to  better test set precision and recall on three binary web classification tasks  where the test set web pages are taken from different web sites than the  training set.}, } @inproceedings{slattery98, author =
moreover, the frequencies of occurrence of the most common punctuation marks play an important role in terms of accurate text categorization as well as when dealing with training data of limited size.}, } @inproceedings{sun01, author = {aixin sun and ee-peng lim}, title = {hierarchical text classification and evaluation}, booktitle = {proceedings of icdm-01, ieee international conference on data mining}, publisher = {ieee computer society press, los alamitos, us}, editor = {nick cercone and tsau y. lin and xindong wu}, year = {2001}, address = {san jose, ca}, pages = {521--528}, url = {http://www.cais.ntu.edu.sg:8000/~sunaixin/paper/sun_icdm01.pdf}, abstract = {hierarchical classification refers to assigning of one or more suitable categories from a hierarchical category space to a document.
hein ragas and cornelis h. koster}, title = {four text classification algorithms compared on a dutch corpus}, booktitle = {proceedings of sigir-98, 21st acm international conference on research and development in information retrieval}, editor = {
using text categorization techniques for intrusion detection}, booktitle =
go annotation is a major activity in most model organism database  projects and annotates gene functions using a controlled vocabulary.
whirl is also fast-up to 500 times faster than c4.5 on some benchmark problems.
this  dissertation addresses the knowledge-engineering bottleneck for a natural  language processing task called ``information extraction''.
this application is described in more detail along with experimental results.}, } @inproceedings{fuhr91b, author = {norbert fuhr and ulrich pfeifer}, title = {
a filtering system should be able to adapt to such concept changes.
we also present results with algorithms other than co-training in this framework and show that co-training is uniquely suited to work well within ecoc.}, } @inproceedings{giorgetti03, author = {daniela giorgetti and fabrizio sebastiani}, title = {multiclass text categorization for automated survey coding}, year = {2003}, address = {melbourne, us}, booktitle = {proceedings of sac-03, 18th acm symposium on applied computing}, publisher = {acm press, new york, us}, pages = {798--802}, url = {http://www.math.unipd.it/~fabseb60/publications/sac03a.pdf}, abstract = {\emph{survey coding} is the task of assigning a symbolic code from a predefined set of such codes to the answer given in response to an open-ended question in a questionnaire (aka \emph{survey}).
the experiment results show  that with only surface text features the svm outperforms the other four methods  for this task.
"edinburgh, scotand", url =  "http://www.ijcai.org/papers/0763.pdf", abstract =
{12}, pages = {278--295}, url =  {http://www.acm.org/pubs/articles/journals/tois/1994-12-3/p278-liddy/p278-liddy.pdf},  abstract = {
we develop a  framework to incorporate unlabeled data in the error-correcting output coding  (ecoc) setup by decomposing multiclass problems into multiple binary problems  and then use co-training to learn the individual binary classification  problems.
"2004", pages = "65--72", abstract =
specifically, we apply the  information bottleneck method to find word-clusters that preserve the  information about document categories and use these clusters as features for  classification.
chicago, us}, pages = {}, url = {http://www.cs.sfu.ca/~wangk/pub/sdm2001.ps}, abstract = {}, } @inproceedings{wang04, author
issues of document indexing, classifier construction, and classifier evaluation, will be touched upon.}, } @article{selamat04, author = {ali selamat and sigeru omatu}, title = {web page feature selection and classification using neural networks}, journal = {information sciences}, year = {2004}, number = {1}, volume =
we investigate the potential reasons for this behavior and relate it to  structural differences between the datasets.}, } @inproceedings{bel03, author =
the power of word clusters for text  classification}, booktitle = {proceedings of ecir-01, 23rd european colloquium  on information retrieval research}, editor = {}, year = {2001}, address =
} @inproceedings{dumais:2000:hcw, author = {
ottawa, ca}, year = {1998}, url =  {http://ai.iit.nrc.ca/ii_public/classification/thesis.pdf}, abstract = {}, }  @inproceedings{scott99, author
mining the web's link structure}, journal = {ieee computer}, year = {1999}, number = {8}, volume = {32}, pages = {60--67}, url = {http://dlib.computer.org/co/books/co1999/pdf/r8060.pdf}, abstract = {the web is a hypertext body of approximately 300 million pages that continues to grow at roughly a million pages per day.
"117--122", volume = 7, number = 2, year = 2005, url = "http://www.acm.org/sigs/sigkdd/explorations/issues/7-2-2005-12/kddcup2005report_scheffer.pdf", abstract =
the model is based on the concept of `uncertainty sampling', a technique that allows for relevance feedback both on relevant and nonrelevant documents.
luc de raedt and peter a. flach}, publisher = {springer verlag, heidelberg, de}, address = {freiburg, de}, year = {2001}, pages = {454--465}, note = {
these results reach  most of the state-of-the-art techniques of machine learning applied to text  categorization, demonstrating that this new weighting scheme does perform well  on this particular task.}, } @inproceedings{dinunzio04, author = {giorgio m.  {di nunzio}}, title = {a bidimensional view of documents for text  categorisation}, booktitle = {proceedings of ecir-04, 26th european conference  on information retrieval research}, editor = {sharon mcdonald and john tait},  year = {2004}, address = {sunderland, uk}, publisher = {springer verlag,  heidelberg, de}, note = {
automatic classification is traditionally performed by extracting the information for representing a document (``indexing'') from the document itself.
"117--122", volume = 7,  number = 2, year = 2005, url =  "http://www.acm.org/sigs/sigkdd/explorations/issues/7-2-2005-12/kddcup2005report_scheffer.pdf",  abstract =
we show that our algorithm minimizes the "within-cluster  jensen-shannon divergence" while simultaneously maximizing the  "between-cluster jensen-shannon divergence".
"in this paper we present a novel strategy, dragpushing, for improving the performance of text classifiers.
an article from a journal or magazine % required: author, title, journal, year  % optional: volume, number, pages, month, note %
booktitle = {proceedings  of ecml-01, 12th european conference on machine learning}, editor =
we evaluate the algorithms on the reuters-21578 corpus and the new  corpus released by reuters in 2000.
with large number  of categories organized as a tree, hierarchical text classification helps users  to find information more quickly and accurately.
text categorization for  multi-page documents: a hybrid naive {bayes hmm} approach}, journal = {journal  of intelligent information systems}, year = {2002}, note = {special issue on  automated text categorization}, volume = {18}, number = {2/3}, pages =
{louise guthrie and joe a. guthrie and james leistensnider}, title = {document  classification and routing}, booktitle = {natural language information  retrieval}, editor = {tomek strzalkowski}, year = {1999}, pages = {289--310},  publisher =
year = {1998}, address = {bethesda, us}, pages = {132--139}, url =  {http://www.acm.org/pubs/articles/proceedings/cikm/288627/p132-de_lima/p132-de_lima.pdf},  abstract = {}, } @inproceedings{denoyer01, author = {ludovic denoyer and hugo  zaragoza and patrick gallinari}, title = {hmm-based passage models for document  classification and ranking}, booktitle = {proceedings of ecir-01, 23rd
{boosting and {rocchio} applied to text filtering},  booktitle = {proceedings of sigir-98, 21st acm international conference on  research and development in information retrieval}, editor = {w. bruce croft  and alistair moffat and van rijsbergen, cornelis j. and ross wilkinson and  justin zobel}, publisher = {acm press, new york, us}, year = {1998}, address =
the classifiers worked very well.
we also found that passages have different degrees of contribution to the main topic(s), depending on their location in the test document.}, } @inproceedings{kindermann01, author = {j{\"{o}}rg kindermann and gerhard paa{{\ss}} and edda leopold}, title = {error correcting codes with optimized kullback-leibler distances for text categorization}, booktitle = {
genre  classification of web pages}, booktitle = {proceedings of ki-04, 27th german  conference on artificial intelligence}, publisher = {}, editor = {biundo,  susanne and fr{\"{u}}hwirth, thom and palm, g{\"{u}}nther}, address =
{montreal, ca}, year = {1997}, pages = {513--530}, note = {an extended version  appears as~\cite{amati99}}, url =  {http://www.cs.strath.ac.uk/~fabioc/papers/97-riao.pdf}, abstract = {
employing the thesaurus entails structuring categories into hierarchies, since their structure needs to be conformed to that of the thesaurus for capturing relationships between categories.
based on a revision of self-organizing maps, namely  taxsom, the proposed model performs an unsupervised classification, exploiting  the a-priori knowledge encoded in a taxonomy structure both at the  terminological and topological level.
the us patent database and yahoo are two examples.
for the first time, we apply the scheme to text categorization with support vector machines (svms) on several large text corpora with more than 100 categories.
"forman, george", title =
its categorizations are dependent on fragmentary recognition using  pattern-matching techniques.
we propose a feature selection method based on linear support vector machines (svms).
"100--110", volume = 7, number = 2, year = 2005, url = "http://www.acm.org/sigs/sigkdd/explorations/issues/7-2-2005-12/kddcup2005report_shen.pdf", abstract =
}  @inproceedings{wu:2004:ipk, author =
"edinburgh, scotand", url =  "http://www.ijcai.org/papers/0304.pdf",
{xavier carreras and llu\'{\i}s m\'arquez}, title = {
in this paper, we  present a hierarchical text classifier based on independent component analysis  (ica), which is capable of (i) organizing the contents of the corpus in a  hierarchical manner and (ii) classifying the texts to be synthesized according  to the learned structure.
we suggest that one (or a collection) of names of {{\sc yahoo!}}\ (or any other www indexer's) categories can be used to describe the content of a document.
"2006", pages = "477--484", abstract =
this can be thought of as  automatic feature selection, which is expected to improve generalization  performance further.
this paper introduces a class of predictive self-organizing neural  networks known as adaptive resonance associative map (aram) for classification  of free-text documents.
{yiming yang and christopher g.  chute}, title
{springer verlag, heidelberg, de}, note = {
moreover, the frequencies of occurrence of the most common  punctuation marks play an important role in terms of accurate text  categorization as well as when dealing with training data of limited size.}, }  @inproceedings{sun01, author = {aixin sun and ee-peng lim}, title =
these constraints may include a semantic classification of the sought after answer and may even suggest using different strategies when looking for and verifying a candidate answer.
we find that term selection and our modified lsi representations lead to similar topic spotting performance, and that this performance is equal to or better than other published results on the same corpus.}, } @article{wong96, author = {jacqueline w. wong and wing-kay kan and gilbert h. young}, title = {{{\sc action}}: automatic classification for full-text documents}, journal = {sigir forum}, year = {1996}, volume = {30}, number = {1}, pages = {26--41}, url = {}, abstract = {}, } @article{wu04, author = {kuo-jui wu and menc-chang chen and yeali sun}, title = {automatic topics discovery from hyperlinked documents}, journal = {information processing and management}, year = {2004}, volume = {40}, number = {2}, pages = {239--255}, url = {}, abstract = {}, } @inproceedings{wu04a, author = {xiaoyun wu and rohini srihari and zhaohui zheng},
"enhancement of dtp feature  selection method for text categorization", booktitle = "computational  linguistics and intelligent text processing", year =
"exploiting category information and document information to  improve term weighting for text categorization", booktitle =  "proceedings of the eighth international conference on intelligent text  processing and computational linguistics", year =
we report the results of our experiments, using various  feature selection measures and varying values of $\sigma$, performed on the  {\sc reuters-21578} standard tc benchmark.
in the context of this model, we compare two approaches  to dimensionality reduction in representation: one based on term selection and  another based on latent semantic indexing (lsi).
"yunqing xia and angelo dalli and  yorick wilks and louise guthrie", title = "{fasil} adaptive email  categorization system", booktitle =
{http://www.acm.org/pubs/articles/journals/jacm/1963-10-2/p151-borko/p151-borko.pdf},  } @article{borko64, author = {harold borko and myrna bernick}, title =
{127--152}, url = {http://www.wkap.nl/article.pdf?391243}, abstract = {kernel  methods like support vector machines have successfully been used for text  categorization.
"2007", month = "april", address = "rome, italy" } @inproceedings{davy:2007:alh, author =
this paper presents work that uses latent semantic indexing (lsi) for text classification.
measuring the similarity of two  documents is conducted by comparing a pair of their corresponding sub-vectors  at a time.
this paper we present a new method for determining orientation of subjective terms.
we propose a new statistical model for the classification of structured documents and consider its use for multimedia document classification.
text genre detection usign common word frequencies}, booktitle = {proceedings of coling-00, the 18th international conference on computational linguistics}, year = {2000}, editor = {}, pages = {808--814}, address = {saarbr{\"{u}}cken, de}, url = {http://acl.ldc.upenn.edu/c/c00/c00-2117.pdf}, abstract = {
edmonton, ca}, year = {2003}, pages = {}, url = {}, abstract = {}, } @inproceedings{petasis00, author = {georgios petasis and alessandro cucchiarelli and paola velardi and georgios paliouras and vangelis karkaletsis and constantine d. spyropoulos}, title =
while most work on classification either concentrates on structured data or free text, the work in this paper deals with both of them.
we also observe that dimensionality reduction techniques eliminate a large number of ocr errors and improve categorization results.}, } @inproceedings{taira01, author =
text categorization in the form of topic  identification is a capability of current interest.
we use different feature sets and integrate neural network learning into the method.
"zhang, dell and lee, wee sun", title =
actually, this weighting method does not leverage the  information implicitly contained in the categorization task to represent  documents.
and does not interfere with the use of training data.}, } @inproceedings{benkhalifa99, author
it has been successfully applied in analyzing patent portfolios,  customer complaint letters, and even competitors' web pages.
we also present two performance optimizations of waknn that improve the computational performance by a few orders of magnitude, but do not compromise on the classification quality.
moreover, bwm-nbs exhibits the strong stability in categorization  performance.}, } @inproceedings{xue04, author =
we also show that less aggressive clustering sometimes results in improved classification accuracy over classification without clustering.}, } @inproceedings{bao01, author = {yongguang bao and satoshi aoyama and xiaoyong du and kazutaka yamada and naohiro ishii}, title = {a rough set-based hybrid method to text categorization}, booktitle = {proceedings of wise-01, 2nd international conference on web information systems engineering}, editor = {m. tamer {\"o}zsu and hans-j{\"{o}}rg schek and katsumi tanaka and yanchun zhang and yahiko kambayashi}, publisher = {ieee computer society press, los alamitos, us}, year = {2001}, address = {kyoto, jp}, pages = {254--261}, url = {}, abstract = {
morgan kaufmann publishers, san francisco, us}, url = {http://www.cs.cmu.edu/~mccallum/papers/hier-icml98.ps.gz}, abstract = {when documents are organized in a large number of topic categories, the categories are often arranged in a hierarchy.
most such methods are character-based, and thus have the potential to automatically capture non-word features of a document, such as punctuation, word-stems, and features spanning more than one word.
domain knowledge is  used to specify a prior distribution for the parameters of a logistic  regression model, and labeled training data is used to produce a posterior  distribution, whose mode we take as the final classifier.
text categorization using the semi-supervised fuzzy c-means algorithm}, booktitle = {proceedings of nafips-99, 18th international conference of the north american fuzzy information processing society}, address = {new york, us}, pages = {561--565}, year = {1999}, url = {}, abstract = {
the alternative to supervised learning is usually viewed to be building classifiers by hand, using a domain expert's understanding of which features of the text are related to the class of interest.
in this  paper, we therefore propose new performance measures for hierarchical  classification.
the layout of a document  contains a significant amount of information that can be used to classify it by  type in the absence of domain-specific models.
{sam scott}, title = {
for best accuracy, f-measure or recall, the findings reveal an outstanding new  feature selection metric, "bi-normal separation" (bns).
henri prade}, year = {1998}, pages = {473--474}, address = {brighton, uk}, url = {http://www-ai.ijs.si/dunjamladenic/papers/pww/pwwecai98yr.ps.gz}, abstract = {the paper describes an approach to automatic web-page classification based on the yahoo hierarchy.
we tested the two filtering methods on svms as well as a decision tree algorithm, c4.5.
we have developed a new effective probabilistic classifier for  document classification by introducing the concept of differential document  vectors and dlsi (differential latent semantic indexing) spaces.
whereas most statistical approaches to text  categorization derive classification knowledge based on training examples  alone, aram performs supervised learning and integrates user-defined  classification knowledge in the form of if-then rules.
detecting concept drift with support vector machines}, booktitle = {proceedings of icml-00, 17th international conference on machine learning}, editor = {
we find  that term selection and our modified lsi representations lead to similar topic  spotting performance, and that this performance is equal to or better than  other published results on the same corpus.}, } @article{wong96, author =
"new york,  ny", publisher =
initial experiments on a set of 15 selected polysemous words show that the boosting approach surpasses naive bayes and exemplar-based approaches, which represent state-of-the-art accuracy on supervised wsd.
the best of all connectionist  architectures presented here achieves near human performance (79.1\%).
{1965}, pages = {473--489}, url =  {http://www.acm.org/pubs/articles/journals/jacm/1965-12-4/p473-doyle/p473-doyle.pdf},  abstract = {
it was found that  typical categorization approaches produce predictions which are too similar for  combining them to be effective since they tend to fail on the same records.
without any computation-intensive resampling, the new  estimators are computationally much more efficient than cross-validation or  bootstrapping.
we also discuss a new technique to help the classifier distinguish better among closely related clusters.
european  conference on information retrieval}, publisher = {springer verlag}, editor =
our method combines techniques from language engineering and image analysis within a machine-learning framework.
pisa, it}, pages = {161--172}, year = {2004}, publisher = {springer verlag, heidelberg, de}, note = {
an  effective information filtering system is one that provides the exact  information that fulfills user's interests with the minimum effort by the user  to describe it.
converting numerical classification into text classification}, journal = {artificial intelligence}, volume = {143}, number = {1}, year = {2003}, pages = {51--77}, url = {}, abstract = {}, } @inproceedings{macskassy03a, author
classification problems are first formulated as optimization  via discriminant analysis.
we have delevoped  experimental systems for tc and wsd.
most of these methods are usually awkward and sometimes intractable in high-dimensional feature spaces.
preliminary results show improved accuracy, as well as reduced cost, resulting from these automated techniques.}, } @inproceedings{rennie03, author = {jason rennie and lawrence shih and jaime teevan and david karger}, title =
{}, url = {http://www.math.unipd.it/~fabseb60//publications/ell06.pdf}, edition  = {second}, } @inproceedings{sebastiani99, author
the results are analyzed from multiple goal perspectives-accuracy, f-measure, precision, and recall-since each is appropriate in different situations.
feature selection for text categorization on imbalanced data}, journal = {sigkdd explorations}, year = {2004}, number = {1}, volume = {6}, pages = {80--89}, url = {http://doi.acm.org/10.1145/1007730.1007741}, abstract = {
conference of the american association for artificial intelligence}, editor = {}, publisher =
our techniques considerably improve the robustness and accuracy of the classification outcome, as shown in systematic experimental comparisons with previously published methods on three different real-world datasets.
in the first stage, the queries are enriched such that for each query, its related web pages together with their category information are collected through the use of search engines.
we describe a content-based book recommending system that utilizes information extraction and a machine-learning algorithm for text categorization.
it  also suggests improvements which lead to a probabilistic variant of the rocchio  classifier.
as a means of evaluating representation quality, a text  retrieval test collection introduces a number of confounding factors.
{159--194}, abstract =  {data sparseness or overfitting is a serious problem in natural language  processing employing machine learning methods.
"exploiting comparable corpora and bilingual  dictionaries for cross-language text categorization", booktitle =  "proceedings of the 21st international conference on computational  linguistics and 44th annual meeting of the association for computational  linguistics", year = "2006", pages =
for example, text can be classified with a very high degree of accuracy by authorship, language, dialect and genre.
various performance  issues regarding the training set size and the significance of the proposed  style markers are discussed.
{1059--1063}, url = {}, abstract = {}, } @incollection{guthrie99, author =
international journal of intelligent systems}, year = {2000}, number =
since there is a difference between  important sentences and unimportant sentences in a document, the features from  more important sentences should be considered more than other features.
furthermore, these experiments show that feature labeling  takes much less (about 1/5th) time than document labeling.
categorical points to each category are computed by summing the frequency of each keyword from back data, or the number of documents from it.
koppel, moshe and argamon, shlomo and shimoni, anat r.}, title = {automatically categorizing written texts by author gender}, journal = {literary and linguistic computing}, year = {2002}, number = {4}, volume = {17}, pages = {401--412}, url = {http://www3.oup.co.uk/litlin/hdb/volume_17/issue_04/pdf/170401.pdf}, abstract = {
research on machine learning for text categorization, already advancing at a rapid pace, could be further accelerated if better test collections were available.}, } @article{lewis04, author =
fan li and yiming yang}, title = {a loss function analysis for classification methods in text categorization},
(the uncertainty  sampling and random sampling results in that paper were correct.)
{2}, month = {december}, pages = {139--154}, year = {2001}, url =  {http://www.ai.mit.edu/projects/jmlr/papers/volume2/manevitz01a/manevitz01a.pdf},  abstract = {
"constructing informative prior distributions  from domain knowledge in text classification", booktitle =  "proceedings of the 29th annual international acm sigir conference on  research and development in information retrieval", year =  "2006", pages =
this paper presents an extensive empirical evaluation of memory-based learning in the context of anti-spam filtering, a novel cost-sensitive application of text categorization that attempts to identify automatically unsolicited commercial messages that flood mailboxes.
experimental results have shown that the statistical learning approach and the learning feedback technique are practical means to automatic indexing of controlled index terms.}, } @inproceedings{lewis00, author = {lewis, david d.}, title = {machine learning for text categorization: background and characteristics}, booktitle = {proceedings of the 21st annual national online meeting}, editor = {williams, martha e.}, publisher = {
the two main factors that characterize a text are its content and its style, and both can be used as a means of categorization.
our algorithm is computationally efficient being bounded by o(n log n)
we demonstrate via a novel visualization that the recurrent themes subtype is present in rcv1.
"256--264", }  @inproceedings{zhang:2006:lpm, author =
the system could potentially scale up  to an operational size of 10 million words of text per year - the equivalent of  a dozen bibles or a third of the encyclopedia britannica.
when tested on 50  user profiles and 550 megabytes of documents, results indicate that the feature  set that is the basis of the text categorization module and the algorithm that  establishes the boundary of categories of potentially relevant documents  accomplish their tasks with a high level of performance.
in  this paper, using several algorithms, we compare the categorization accuracy of  classifiers based on words to that of classifiers based on senses.
hierachically classifying  chinese web documents without dictionary support and segmentation procedure},  booktitle = {proceedings of waim-00, 1st international conference on web-age  information management}, publisher = {springer verlag, heidelberg, de}, editor  = {hongjun lu and aoying zhou}, note = {
"borovets, bulgaria" } @inproceedings{mihalcea:2005:uet,  author =
{rio de janeiro, br}, year = {2001}, pages = {137--146}, url =  {http://link.springer.de/link/service/series/0558/papers/2013/20130137.pdf},
results obtained from evaluation show that the integration of wordnet can outperform approaches based only on training.}, } @article{diederich03, author = {
{applied intelligence}, year = {2003}, volume = {18}, number = {3}, pages =
proceedings of ecir-03, 25th european conference on information retrieval}, publisher =
the results show that our new approach outperforms the latest lnn approach and linear classifiers in all experiments.}, } @inproceedings{lam99, author = {savio l. lam and dik l. lee}, title = {feature reduction for neural network based text categorization}, booktitle = {proceedings of dasfaa-99, 6th ieee international conference on database advanced systems for advanced application}, editor = {
the background of rs theory is presented, with an illustrative example to demonstrate the operation of the rs-based dimensionality reduction.
traditionally, each component value is assigned using the information retrieval tfidf measure.
springer verlag, heidelberg, de}, note = {
our approach can be seen as an  extension to kleinberg's hubs and authorities algorithm that analyzes hyperlink  relations among web pages.
integration of phonetic and graphic features in poetic text categorization judgements}, journal = {poetics}, year = {1996}, volume = {23}, number = {5}, pages = {363--380}, url = {}, abstract = {
our approach integrates wordnet information with two training  approaches through the vector space model.
"763--768" }  @article{raghavan:2006:alf, author = {raghavan, hema and madani, omid and  jones, rosie}, title = {active learning with feedback on features and  instances}, journal = {journal of machine learning research}, volume = {7},  pages = {1655--1686}, year = {2006}, abstract = {
four kinds of classifiers were used in our experiments: naive bayes, rocchio, k-nn, and svm.
relevant phrases and contexts are acquired automatically using a training corpus.
we describe a methodology and system (named accio) for automatically acquiring labeled datasets for text categorization from the world wide web, by capitalizing on the body of knowledge encoded in the structure of existing hierarchical directories such as the open directory.
second, smoothing techniques from statistical language modeling can be used to recover better estimates than the laplace smoothing techniques usually used in naive bayes classification.
in many supervised learning tasks, the entities to be labeled are related to each other in complex ways and their labels are not independent.
in addition, pva considers the aging problem  of user interests.
an efficient approach to generating word sequences  is proposed.
text classification using esc-based stochastic decision lists}, booktitle = {proceedings of cikm-99, 8th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, editor = {}, year = {1999}, address = {kansas city, us}, pages = {122--130}, url = {http://www.acm.org/pubs/articles/proceedings/cikm/319950/p122-li/p122-li.pdf}, abstract =
this is achieved by means of a data mining approach, called one clause at a time (ocat), which is based on mathematical logic.
the main idea of ferrety algorithm can be generalized for mapping one taxonomy  to another if training documents are available."
published in the ``lecture notes in computer science'' series, number 2416}, } @inproceedings{zelikovitz03, author = {sarah zelikovitz and haym hirsh}, title = {
"the kdd-cup 2005 competition was held in conjunction with the  eleventh acm sigkdd international conference on knowledge discovery and data  mining.
in this paper, we introduce can  models and apply them to various text classification problems.
philip j.  hayes and peggy m. andersen and irene b. nirenburg and linda m. schmandt},
uncertainty-based term selection (uc) is compared to a number of other criteria like information gain (ig), simplified chi-square (sx), term frequency (tf) and document frequency (df) in a text categorization setting.
we augment the naive bayes model with an n-gram language model to address two shortcomings of naive bayes text classifiers.
finally, the relative performance of the different classifiers being tested gives us insights into the strengths and limitations of our algorithms for hypertext classification.}, } @inproceedings{ghani01a, author = {rayid ghani}, title = {combining labeled and unlabeled data for text classification with a large number of categories}, booktitle = {proceedings of the ieee international conference on data mining}, editor = {nick cercone and tsau young lin and xindong wu}, address = {san jose, us}, year = {2001}, pages = {597--598}, publisher =
"252--259", abstract = "machine learning is the mainstay for text classification.
a probabilistic model of dictionary-based automatic indexing}, booktitle = {proceedings of riao-85, 1st international conference ``recherche d'information assistee par ordinateur''}, publisher = {}, editor = {}, address = {
text  mining and its applications to intelligence, crm and knowledge management},  pages = {109--129}, publisher = {wit press}, address = {
most of all, tcfp is about one hundred times faster than k-nn.
the advantages of this  approach over the knowledge engineering approach (consisting in the manual  definition of a classifier by domain experts) are a very good effectiveness,  considerable savings in terms of expert manpower, and straightforward  portability to different domains.
a text-categorization application developed with tcs  consists of the tcs run-time system and a rule base.
the algorithms we present are simple to implement and are time and memory efficient.
using these, we build a multi-level classifier.
{orlando, us}, url = {http://www.his.sunderland.ac.uk/ps/aaai99.pdf}, abstract  = {
{jamie callan and gordon cormack and charles clarke and david hawking and alan  smeaton}, publisher = {acm press, new york, us}, address = {toronto, ca}, year  = {2003}, pages =
labeling is usually done manually by human experts  (or the users), which is a labor intensive and time consuming process.
"acm press", url =  "http://doi.acm.org/10.1145/1099554.1099715", abstract =
text categorization  using transductive boosting}, booktitle = {proceedings of ecml-01, 12th  european conference on machine learning}, editor = {
madison, us}, url =  {http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/www/wwkb/ilp98.ps.gz},
we introduce a probabilistic method for combining classifiers that considers the context-sensitive reliabilities of contributing classifiers.
these recurrent connections in multiple layers encode  the sequential context of word sequences.
morgan kaufmann  publishers, san francisco, us}, url =  {http://www.ai.mit.edu/~jrennie/papers/icml03-nb.pdf}, abstract = {naive bayes  is often used as a baseline in text classification because it is fast and easy  to implement.
"edinburgh, scotand", url = "http://www.ijcai.org/papers/0763.pdf", abstract =
we provide background, present procedures for building metaclassifiers that take into consideration both reliability indicators and classifier outputs, and review a set of comparative studies undertaken to evaluate the methodology.}, } @inproceedings{bickel04, author = {steffen bickel and tobias scheffer}, title = {learning from message pairs for automatic email answering}, booktitle = {proceedings of ecml-04, 15th european conference on machine learning}, editor = {jean-fran{\c{c}}ois boulicaut and floriana esposito and fosca giannotti and dino pedreschi}, address = {
however, ripper and sleeping experts differ radically in many other respects.
our first set of experiments applies the c4.5 decision tree induction algorithm to this learning task.
{journal of the american society for information science and technology}, year  = {2004}, volume = {56}, number = {6}, pages = {584--596}, url =  {http://www.math.unipd.it/~fabseb60/publications/jasist05.pdf}, abstract = {
in comparison against a recently proposed technique that appears to  be the only one of the kind, we obtained up to 18.5\% of improvement in  effectiveness while reducing the processing time dramatically.
{van rijsbergen, cornelis j.}, pages = {391--408}, address =  {cambridge, uk}, url = {}, abstract = {}, } @inproceedings{fuhr85, author =
thus, there is the problem of determining what information is relevant to the user and how this decision can be taken by a supporting system.
"d. tikk and j. d. yang and s. l. bang", title =
" } @inproceedings{stein:2006:eoe, author =
our fully implemented and recently deployed system shows that a superior classification engine for this task can be constructed from a combination of classifiers.
we report on experiences with the reuters newswire benchmark, the us patent database, and web document samples from yahoo!.
the use of a vector space classifier and training method robust to large feature sets, combined with discarding of low frequency ocr output strings are the key to our approach.}, } @inproceedings{iwayama94, author = {makoto iwayama and takenobu tokunaga}, title = {a probabilistic model for text categorization: based on a single random variable with multiple values}, booktitle = {proceedings of anlp-94, 4th conference on applied natural language processing}, publisher = {association for computational linguistics, morristown, us}, editor = {}, year = {1994}, address = {
the user profile is a vector of weighted terms which are learned  from the relevance assessment values given by the user on the training set.
we examine several variations  to a tf*idf-based approach for this task, empirically analyze their effects,  and evaluate our system on a large collection of images from current news  newsgroups.
we show on three text categorization data sets that this approach can rescue what would otherwise be disastrously bad training situations, producing much more effective classifiers."
the interpretation shows the strengths and weaknesses of using thesaurus knowledge and gives hints for future research.}, } @article{junker98, author = {markus junker and rainer hoch}, title = {
in  this paper, we propose to select relevant features by means of a family of  linear filtering measures which are simpler than the usual measures applied for  this purpose.
instead of  estimating weights for individual words, as naive bayes does, words with  similar features are grouped into bins, and a single weight is estimated for  each bin.
preliminary experimental results are provided to compare the proposed database categorization algorithms.}, } @inproceedings{wang01, author = {ke wang and senquiang zhou and yu
"d. tikk and {gy.} biro", title =
{cesa-bianchi, nicolo and gentile, claudio and zaniboni, luca}, title  = {incremental algorithms for hierarchical classification}, journal = {journal  of machine learning research}, volume = {7}, pages = {31--54}, year = {2006},  url =  {http://jmlr.csail.mit.edu/papers/volume7/cesa-bianchi06a/cesa-bianchi06a.pdf},  abstract =
"this paper describes an application of ir and text categorization methods to a highly practical problem in biomedicine, specifically, gene ontology (go) annotation.
how is this related to the  statistical properties of text?
c-evolve first finds highly accurate cluster  digests (partial clusters), gets user feedback to merge and correct these  digests, and then uses the classification algorithm to complete the  partitioning of the data.
one problem with this approach is that the classifier best suited for an  application may be too expensive to train or use during the selection of  instances.
published in the ``lecture notes in computer science'' series, number 2168}, url = {http://www.techfak.uni-bielefeld.de/ags/ni/publications/papers/ontrupritter2001-tca.pdf}, abstract =
we present two extensions to the algorithm that improve classification accuracy under these conditions: (1) a weighting factor to modulate the contribution of the unlabeled data, and (2) the use of multiple mixture components per class.
{american society for information science, washington, us}, editor = {raymond f. vondran and anne caputo and carol wasserman and richard a. diener}, year = {1983}, address =
"san jose, ca", publisher =
employing {em} in pool-based active learning for text classification}, booktitle = {proceedings of icml-98, 15th international conference on machine learning}, editor = {jude w. shavlik}, year = {1998}, address = {madison, us}, pages = {350--358}, publisher =
this paper presents the results of the application of an instance-based  learning algorithm k-nearest neighbor method on feature projections (k-nnfp) to  text categorization and compares it with k-nearest neighbor classifier (k-nn).
based on this understanding, we present solutions inspired by round-robin scheduling that avoid this pitfall, without resorting to costly wrapper methods.
seattle, us}, year = {2001}, pages = {870--878}, url = {http://robotics.stanford.edu/~btaskar/pubs/ijcai01.ps}, abstract = {supervised and unsupervised learning methods have traditionally focused on data consisting of independent instances of a single type.
"we address the problem dealing with a  large collection of data, and investigate the use of automatically constructing  category hierarchy from a given set of categories to improve classification of  large corpora.
for more than a half  of testing examples a correct category is among the 3 categories with the  highest predicted probability.}, } @inproceedings{mladenic98b, author = {
results on a real-world text datasets show that these learners may substantially benefit from using a large amount of unlabeled documents in addition to some labeled documents.}, } @inproceedings{larkey96, author =
an experimental study in  automatically categorizing medical documents}, journal = {journal of the  american society for information science and technology}, year = {2001}, number  = {5}, pages =
this paper investigates the robustness of three  regularized linear classification methods (svm, ridge regression and logistic  regression) under above situations.
however, full-length documents available today in large quantities  pose renewed interests in text classification.
in earlier work we described an approach for converting numerical features into bags of tokens so that text classification methods can be applied to numerical classification problems, and showed that the resulting learning methods are competitive with traditional numerical classification methods.
in particular, whirl generally achieves lower generalization error than c4.5, ripper, and several nearest-neighbor methods.
"2007", pages =  "889--894", address =
{schapire, robert e. and singer, yoram and singhal, amit}, title =
in this paper we propose instead that learning from the training  data should also affect phase (ii), i.e.\ that information on the membership of  training documents to categories be used to determine term weights.
"university of maryland, usa", year =  "september 21--24, 2003" } @inproceedings{iccc03, author = "d.  tikk and {gy.} biro", title =
{ieee computer society press, los alamitos, us}, editor = {ralph h. sprague}, year = {2001}, address = {
our experiments show that the transductive method outperforms conventional boosting techniques that employ only labeled data.}, } @inproceedings{taira99, author = {hirotoshi taira and masahiko haruno}, title = {feature selection in svm text categorization},
the paper studies noise reduction for computational efficiency improvements in a statistical learning method for text categorization, the linear least squares fit (llsf) mapping.
and does not interfere with the use of training data.}, }  @inproceedings{benkhalifa99, author
positive terms are considered relevant to the informative need of the user, negative ones irrelevant.
we compare different search heuristics and pruning methods known from various symbolic rule learners on a set of representative text categorization problems.
we distinguish model-oriented and description-oriented approaches  in probabilistic information retrieval.
"we address the problem dealing with a large collection of data, and investigate the use of automatically constructing category hierarchy from a given set of categories to improve classification of large corpora.
this can be seen as a complementary tool for topic detection and tracking applications.
sentiment classification using machine learning techniques}, booktitle = {proceedings of emnlp-02, 7th conference on empirical methods in natural language processing}, year = {2002}, publisher = {association for computational linguistics, morristown, us}, editor = {}, pages = {79--86}, address = {philadelphia, us}, url = {http://acl.ldc.upenn.edu/acl2002/emnlp/pdfs/emnlp219.pdf}, abstract = {
combining model-oriented and description-oriented approaches for probabilistic indexing}, booktitle = {proceedings of sigir-91, 14th acm international conference on research and development in information retrieval}, editor = {abraham bookstein and yves chiaramella and gerard salton and vijay v. raghavan}, publisher = {acm press, new york, us}, address = {
the result shows that  term redundancy behaves very similar to noise and may degrade the classifier  performance.
{150--162}, year = {2002}, publisher =
we evaluate our approach by applying it to tasks that involve learning  definitions for (i) classes of pages, (ii) particular relations that exist  between pairs of pages, and (iii) locating a particular class of information in  the internal structure of pages.
"aaai  press", abstract =
"exploiting comparable corpora and bilingual dictionaries for cross-language text categorization", booktitle = "proceedings of the 21st international conference on computational linguistics and 44th annual meeting of the association for computational linguistics", year = "2006", pages =
we demonstrate via a novel visualization that the recurrent themes subtype is  present in rcv1.
exploiting hierarchy in text categorization}, journal = {information retrieval}, number = {3}, volume = {1}, pages =
the described system can be efficiently adapted to new domains or different languages.
text genre classification with genre-revealing and subject-revealing features}, booktitle = {proceedings of sigir-02, 25th acm international conference on research and development in information retrieval}, editor = {micheline beaulieu and ricardo baeza-yates and sung hyon myaeng and kalervo j{\"{a}}rvelin}, publisher = {acm press, new york, us}, address =
= {donna k. harman}, year = {1994},  address = {gaithersburg, us}, pages =
published in the ``lecture notes in computer science'' series,  number 2997}, pages = {181--196}, url =  {http://springerlink.metapress.com/openurl.asp?genre=article&issn=0302-9743&volume=2997&spage=181},  abstract =
previous research on automated text categorization has mixed machine learning  and knowledge engineering methods, making it difficult to draw conclusions  about the performance of particular methods.
misc % = use this type  when nothing else fits % required: none % optional: author, title,  howpublished, month, year, note % % phdthesis % =
{http://link.springer.de/link/service/series/0558/papers/2035/20350078.pdf},
the importance of text mining stems from the availability of huge volumes of text databases holding a wealth of valuable information that needs to be mined.
how weak categorizers based upon different principles strengthen performance}, journal = {the computer journal}, year = {2002}, volume = {45}, number = {5}, pages = {511--524}, url = {http://www3.oup.co.uk/computer_journal/hdb/volume_45/issue_05/pdf/450511.pdf}, abstract = {
document collections from the medline database and mayo patient records are used for studies on the effectiveness of our approach, and on how much the effectiveness depends on the choices of training data, indexing language, word-weighting scheme, and morphological canonicalization.
the  ability to cheaply train text classifiers is critical to their use in  information retrieval, content analysis, natural language processing, and other  tasks involving data which is partly or fully textual.
the proposed performance measures consist of category similarity measures and distance based measures that consider the contributions of misclassified documents.
{477--494}, url = {}, abstract = {subject indexing of text can, in principle,  be accomplished in many ways.
the learning algorithm is described and the effectiveness of the system is evaluated in a true information filtering style.}, } @inproceedings{amati97a, author =
the paper demonstrates that the addition of automatically selected word-pairs substantially increases the accuracy of text classification which is contrary to most previously reported research.
"proceedings of the 14th {acm} international conference on  information and knowledge management", year =
the simplicity of the  model, the high recall precision rates, and the efficient computation together  make expnet preferable as a practical solution for real world applications.}, }  @inproceedings{yang95, author = {
using such an architecture, the time needed for training is reduced substantially and the user is provided with an even more intuitive metaphor for visualization.
a number of feature  selection metrics have been explored in text categorization, among which  information gain (ig), chi-square (chi), correlation coefficient (cc) and odds  ratios (or) are considered most effective.
on their own, the new representations are not found to produce significant performance improvements.
we describe the email  classification experiments we have carried out and discuss the development of  customer services based on automatic email classification."
the rocchio classifier, its probabilistic variant, and a naive bayes classifier are compared on six text categorization tasks.
combining labeled and  unlabeled data for multiclass text categorization}, booktitle = {proceedings of  icml-02, 19th international conference on machine learning}, editor = {}, year  = {2002}, address = {sydney, au}, pages = {}, publisher = {morgan kaufmann  publishers, san francisco, us}, url =  {http://www.accenture.com/xdoc/en/services/technology/publications/ghani-icml02.pdf},  abstract = {supervised learning techniques for text classification often  require a large number of labeled examples to learn accurately.
we show that the benefit of using a first-order representation in this  domain is relatively modest; in particular, the performance difference between  flipper and foil and their propositional counterparts is quite small, compared  to the differences between foil and flipper.
in this paper, we discuss cost-sensitive text categorization methods for ube filtering.
\small  http://www.acm.org/pubs/articles/proceedings/ir/188490/p282-hull/p282-hull.pdf},  abstract = {latent semantic indexing (lsi) is a novel approach to information  retrieval that attempts to model the underlying structure of term associations  by transforming the traditional representation of documents as vectors of  weighted term frequencies to a new coordinate space where both documents and  terms are represented as linear combinations of underlying semantic factors.
pat langley}, year = {2000}, address = {stanford, us}, pages = {1167--1182}, publisher = {morgan kaufmann publishers, san francisco, us}, url = {http://www.cs.cmu.edu/~yiming/papers.yy/icml00.ps.gz}, abstract = {
"information retrieval and text categorization with semantic indexing", booktitle =
therefore, the indexing system needs some fault-tolerating  features.
{donna k. harman}, year = {1992}, address = {gaithersburg, us}, pages =
although our automated learning approach still gives a lower accuracy, by appropriately incorporating a set of manually chosen words to use as features, the combined, semi-automated approach yields accuracy close to the rule-based approach.}, } @article{nieto02, author = {salvador nieto s{\'{a}}nchez and evangelos triantaphyllou and donald kraft}, title = {a feature mining based approach for the classification of text documents into disjoint classes}, journal = {information processing and management}, year = {2002}, volume = {38}, number = {4}, pages = {583--604}, url = {}, abstract = {
we show that former approaches in probabilistic information retrieval are based on one or two of the three concepts abstraction, inductive learning and probabilistic assumptions, and we propose a new approach which combines all three concepts.
this has resulted in high accuracy, shorter customization time, and good prospects for the application of the statistical methods to problems in lexical acquisition.}, } @article{jacobs93, author = {paul s. jacobs}, title = {using statistical methods to improve knowledge-based news categorization}, journal = {ieee expert}, year = {1993}, number = {2}, volume = {8}, pages = {13--23}, url = {}, abstract = {}, } @inproceedings{jo99, author = {taeho c. jo}, title = {
this novel combination of svm with word-cluster representation is compared with svm-based categorization using the simpler bag-of-words (bow) representation.
using the vector space model (vsm),  each document is represented by its original feature vector augmented with  external feature vector generated using wordnet.
feature selection using one-sided metrics  selects the features most indicative of membership only, while feature  selection using two-sided metrics implicitly combines the features most  indicative of membership (e.g. positive features) and
in this paper, we combine a probabilistic model for the  darmstadt indexing approach with logistic regression.
in order to select a good hypothesis language (or model) from a collection of possible models, one has to assess the generalization performance of the hypothesis which is returned by a learner that is bound to use that model.
we provide experimental  results on a webpage classification task, showing that accuracy can be  significantly improved by modeling relational dependencies.}, }  @article{tauritz00, author = {daniel r. tauritz and joost n. kok and ida g.  sprinkhuizen-kuyper}, title = {
a unified model for text categorization and text retrieval is  introduced.
a companion web page is available at % % http://www.cs.technion.ac.il/~gabr/resources/atc/atcbib.html %
springer verlag,  heidelberg, de}, address = {hong kong, cn}, note = {
we explore modifications to the document representation in a vector space-based ned system.
experimental results obtained on three real-world data sets show that we can reduce the feature dimensionality by three orders of magnitude and lose only 2\% accuracy, significantly better than latent semantic indexing, class-based clustering, feature selection by mutual information, or markov-blanket-based feature selection.
} @inproceedings{zhu:2005:mlc, author =
"proceedings of the 29th annual international acm sigir conference on research and development in information retrieval", year = "2006", pages =
the essential formula is cue validity  borrowed from cognitive psychology, and used to select from all possible single  word based features, the best predictors of a given category.
this  paper investigates three algorithms that potentially could automate this  categorization process: 1) a nearest neighbor-like algorithm, 2) c4.5rules, a  machine learning decision tree algorithm; and 3) ripper, a machine learning  rule induction algorithm.
experimental results on standard benchmarks confirm the validity of our approach, showing that adaboost achieves consistent improvements by including additional semantic features in the learned ensemble.}, } @inproceedings{cai04, author = {lijuan cai and thomas hofmann}, title = {hierarchical document categorization with support vector machines},
weighted trigram analysis is a quick  and flexible technique for describing the contents of a document.
technology from machine learning (ml) will offer efficient tools for the  intelligent analyses of the data using generalization ability.
this level of performance should be satisfactory  for a wide variety of applications.}, } @inproceedings{tzeras93, author =
the technique is completely language-independent,  highly garble-resistant, and computationally simple.
we examine several variations to a tf*idf-based approach for this task, empirically analyze their effects, and evaluate our system on a large collection of images from current news newsgroups.
positive terms are considered relevant to the informative need of the user,  negative ones irrelevant.
{giorgio m. {di nunzio} and alessandro micarelli}, title = {does a new simple  gaussian weighting approach perform
text  categorization is the process of assigning categories or labels to documents  based entirely on their contents.
in contrast to this approach we use the  frequencies of occurrence of the most frequent words of the entire written  language.
the classification is a minimum  least-squares approach based on polynomials.
we have performed extensive experiments on the use of ppm compression models for categorization using the standard reuters-21578 dataset.
{classification of text, automatic}, editor = {keith brown}, year = {2005},  volume = {14}, booktitle = {
"common  approaches to multi-label classification learn independent classifiers for each  category, and employ ranking or thresholding schemes for classification.
the results show that the use of the hierarchical structure improves text categorization performance with respect to an equivalent flat model.
the combination of these techniques significantly influences the overall performance of text categorization.
non-membership (e.g.  negative features) by ignoring the signs of features.
the traditional method of building a single classifier to do all the classification work would incur a high overhead.
we are using a custom workflow management system as the base for a range of services which are offered via a multimodal portal, using a language-based approach to extracting information from html forms, email, and sms.
combining multiple  classifiers for text categorization}, booktitle = {proceedings of cikm-01, 10th  acm international conference on information and knowledge management},  publisher = {acm press, new york, us}, editor = {henrique paques and ling liu  and david grossman}, year = {2001}, address = {atlanta, us}, pages = {97--104},  url = {http://doi.acm.org/10.1145/502585.502603}, abstract = {a major problem  facing online information services is how to index and supplement large  document collections with respect to a rich set of categories.
"an application of text categorization methods to gene ontology annotation", pages =
our approach to classification is based on "visual similarity" of layout structure and is implemented by building a supervised classifier, given examples of each class.
thus the classifier has a small model size and is very fast.
in  this paper, we report on a set of experiments that explore the utility of  making use of the structural information of www documents.
based on the proposed representation, we present an approach to the problem of learning rules for tc and ie in terms of ilp.
in this paper we  investigate the use of concept-based document representations to supplement  word- or phrase-based features.
"techniques for  improving the performance of naive bayes for text classification",  booktitle =
we also show that features  found to be useful in one corpus do not transfer well to other corpora with  different genres." } @article{vogel:2005:kddcup2005, author =
the estimates can be used in standard classification models to reduce error rates.
recently  there has been significant interest in supervised learning algorithms that  combine labeled and unlabeled data for text learning tasks.
= {melbourne, au}, pages = {81--89}, url = {http://www.acm.org/pubs/articles/proceedings/ir/290941/p81-lam/p81-lam.pdf}, abstract = {
in comparison with other machine-learning  techniques, results on a key benchmark from the reuters collection show a large  gain in performance, from a previously reported 67\% recall/precision breakeven  point to 80.5\%.
from these, we extract vocabulary, words that appear with high frequency within a given category, characterizing each subject area.
{168--175}, url =  {http://www.acm.org/pubs/articles/proceedings/ir/345508/p168-kim/p168-kim.pdf},  abstract = {several machine learning algorithms have recently been used for  text categorization and filtering.
whirl is also fast-up to 500  times faster than c4.5 on some benchmark problems.
"in  this paper we present a novel strategy, dragpushing, for improving the  performance of text classifiers.
to select the proper number of , we use assures the degree of our disappointment in any differences between the true distribution over inputs and the learner's prediction.
previously, documents have been classified  according to their contents manually.
learning for text categorization and information extraction with ilp}, booktitle = {proceedings of the 1st workshop on learning language in logic}, editor = {cussens, james and saso dzeroski}, year = {2000}, address = {bled, sl}, pages = {247--258}, publisher = {springer verlag, heidelberg, de}, note = {
we are given a set of categories, organized hierarchically.
specifically, a query is preprocessed and represented with patterns that include both query words and required answer types.
"multi-labelled classification using maximum entropy method", pages =
various performance issues regarding the training set size and the significance of the proposed style markers are discussed.
in this paper we present empirical results on the performance of a bayesian classifier and a decision tree learning algorithm on two text categorization data sets.
a preliminary  experimentation proved that the logic approach is able to capture the semantics  underlying some kind of sentences, even if the assessment of the efficiency of  such a method, as well as a comparison with other related approaches, has still  to be carried out.}, } @article{field75, author = {b.j. field}, title =  {towards automatic indexing: automatic assignment of controlled-language  indexing and classification from free indexing}, year = {1975}, journal =
the number of common keywords between  keywords from the document itself and representative keywords from back data  classifies documents.
it then uses these words  to extract a set of documents for each class from a set of unlabeled documents  to form the initial training set.
see also~\cite{lewis95a}}, url = {http://www.research.att.com/~lewis/papers/lewis94c.ps}, abstract = {
{learning from labeled and unlabeled documents: a comparative study on  semi-supervised text classification}, booktitle =
learning to construct knowledge bases from the world wide web}, journal = {artificial intelligence}, volume = {118}, number = {1/2}, year = {2000}, pages = {69--113}, url = {http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/www/wwkb/overview-aij99.ps.gz},
an evaluation of statistical approaches to medline  indexing}, booktitle = {proceedings of amia-96, fall symposium of the american  medical informatics association}, editor = {james j. cimino}, publisher =
text categorization}, editor = {laura c. rivero and jorge h. doorn and  viviana e. ferraggine}, year = {2005}, booktitle = {
we examine more complex combination strategies but find them less successful due to the high correlations among our filtering methods which are optimized over the same training data and employ similar document representations.}, } @inproceedings{hull98, author = {david a. hull}, title = {
"janyce wiebe and rada  mihalcea", title =
after posting a query, the user is offered an opportunity to refine the results by browsing through a category tree derived from the dmoz open directory topic hierarchy.
this  model uses a divide and conquer principle to define smaller categorization  problems based on a predefined hierarchical structure.
distribution 1.0}, year = {1997}, note = {available as {\tt http://www.daviddlewis.com/resources/testcollections/reuters21578/readme.txt}}, url = {http://www.daviddlewis.com/resources/testcollections/reuters21578/readme.txt}, abstract =
as a by-product, we can compute for each  document a set of terms that occur significantly more often in it than in the  classes to which it belongs.
{computational linguistics}, pages = {471--495}, year = {2000}, number = {4},  volume = {26}, url = {}, abstract = {
"a model for handling  approximate, noisy or incomplete labeling in text classification",  booktitle =
we present experiments on the classification of multilingual pornographic html pages using text and image data.
we show accurate results on a large collection of free-form questions used in trec 10.}, } @inproceedings{li03, author = {cong li and ji-rong wen and hang li}, title = {text classification using stochastic keyword generation}, booktitle = {proceedings of icml-03, 20th international conference on machine learning}, editor = {}, year = {2003}, address = {washington, dc}, pages = {}, publisher =
however, the three machine learning methods we employed (naive bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization.
in this paper, we  describe a comparative study on techniques of feature transformation and  classification to improve the accuracy of automatic text classification.
our classifier based on these improvements performes significantly better on pre-classified samples from the web and the us patent database than the usual classifiers.}, } @inproceedings{yu03a, author = {hwanjo yu and chengxiang zhai and jiawei han}, title = {
results with an algorithm extended by  thesaurus knowledge are presented and interpreted.
non-textual information is processed by a symbolic learning  technique.
this analysis determined, for example, that ig and chi-squared have correlated failures for precision, and that ig paired with bns is a better choice.}, } @article{forman03, author = {
improving performance of text categorization by combining filtering and support vector machines}, journal = {journal of the american society for information science and technology}, year = {2004}, volume = {55}, number = {7}, pages = {578--592}, url = {http://dx.doi.org/10.1002/asi.10409}, abstract = {
possible specific scenarios within this framework include the learning of the routing of publication titles or news titles.
for indirect comparisons, knn, llsf and word were used as baselines, since they were evaluated on all versions of reuters that exclude the unlabelled documents.
"when search results against digital  libraries and web resources have limited metadata, augmenting them with  meaningful and stable category information can enable better overviews and  support user exploration.
we explore how to organize a text database hierarchically to aid better searching and browsing.
"2004" } @inproceedings{kim:2004:rcr, author =
paris, fr}, pages = {19--38}, url =
{7}, volume = {15}, pages =
the experimental result on  the fbis document corpus shows that the atf algorithm outperforms the pure eg  (exponentiated-gradient) algorithm.}, } @inproceedings{yu99, author = {
survey coding  is a difficult task, since the code that should be attributed to a respondent  based on the answer she has given is a matter of subjective judgment, and thus  requires expertise.
feature engineering for text classification}, booktitle = {proceedings of icml-99, 16th international conference on machine learning}, editor = {ivan bratko and saso dzeroski}, year = {1999}, address = {bled, sl}, publisher = {morgan kaufmann publishers, san francisco, us}, pages = {379--388}, url = {}, abstract = {most research in text classification to date has used a ``bag of words'' representation in which each feature corresponds to a single word.
evaluating text categorization in the presence  of ocr errors}, booktitle = {proceedings of the 8th spie conference on document  recognition and retrieval}, editor = {paul b. kantor and daniel p. lopresti and  jiangying zhou}, year = {2000}, address = {san jose, us}, pages = {68--74},  publisher = {spie, the international society for optical engineering,  washington, us}, url = {}, abstract = {
using lsi for text classification in the  presence of background text}, booktitle = {proceedings of cikm-01, 10th acm  international conference on information and knowledge management}, publisher =
there are also several obvious directions for improving the system's classification performance in those cases where it did not do as well.
in this paper, we explore correlations among categories with maximum entropy method and derive a classification algorithm for multi-labelled documents.
we discuss applications where our system can improve searching and filtering capabilities.}, } @article{chakrabarti99, author = {soumen chakrabarti and byron e. dom and s. ravi kumar and prabhakar raghavan and sridhar rajagopalan and andrew tomkins and david gibson and jon kleinberg}, title = {
the contents of many valuable web-accessible databases are only  accessible through search interfaces and are hence in-visible to traditional  web ``crawlers''.
given a visual content, the occurrences of visual keywords are detected, summarized spatially, and coded via singular value decomposition to arrive at a concise coded description.
{miguel e. ruiz and padmini srinivasan}, title = {hierarchical neural  networks for text categorization}, booktitle = {proceedings of sigir-99, 22nd
the former refer to certain  representations of documents and queries and use additional independence  assumptions, whereas the latter map documents and queries onto feature vectors  which form the input to certain classification procedures or regression  methods.
the analysis demonstrates that one of these models is theoretically attractive (introducing few new parameters while increasing flexibility), computationally efficient, and empirically preferable.}, } @article{bennett05, author = {paul n. bennett and susan t. dumais and eric horvitz}, title = {
here, a probabilistic  analysis of this algorithm is presented in a text categorization framework.
second, undirected models are well suited for discriminative training, where we optimize the conditional likelihood of the labels given the features, which generally improves classification accuracy.
the system is a component of a more extensive intelligent agent for adaptive information filtering on the web.
feature generation is accomplished through contextual analysis of document  text, implicitly performing word sense disambiguation.
kristina toutanova and francine chen and kris popat and thomas  hofmann}, title = {
i have  discovered a bug in my experimental software which caused the relevance  sampling results reported in the paper to be incorrect.
hyunsoo kim and  peg howland and haesun park}, title = {dimension reduction in text  classification with support vector machines}, journal = {journal of machine  learning research}, year = {2005}, volume = {6}, pages = {37--53}, url =  {http://www.jmlr.org/papers/volume6/kim05a/kim05a.pdf}, abstract = {
aaai press, menlo park, us}, editor = {}, year = {1999}, address = {orlando, us}, pages = {480--486}, url = {}, abstract = {investigates the effect of prior feature selection in support vector machine (svm) text categorization.
our experiments on a variety of categorization tasks indicate that there is significant potential in improving classifier performance by feature re-weighting, beyond that achieved via membership queries alone (traditional active learning) if we have access to an oracle that can point to the important (most predictive) features.
from research to application}, booktitle  = {proceedings of sigir-88, 11th acm international conference on research and  development in information retrieval}, editor = {
we describe the  architecture of a classification system that uses a web directory to identify  the subject context that the query terms are frequently used in.
this paper presents a machine learning approach to question classification.
the paper is concerned with categorization of electronic document images.
"hulth, anette and megyesi, beata b.", title =
"a study on automatically extracted keywords in text categorization", booktitle =
a subsequence is any ordered  sequence of k characters occurring in the text though not necessarily  contiguously.
booktitle = {proceedings of dl-98, 3rd acm conference on digital libraries}, editor = {ian witten and rob akscyn and frank m. shipman}, publisher = {acm press, new york, us}, year = {1998}, address = {pittsburgh, us}, pages = {200--209}, url = {http://robotics.stanford.edu/users/sahami/papers-dir/dl98-sonia.ps}, abstract = {
= {youngjoong ko and jinwoo park and jungyun seo}, title  = {using the feature projection technique based on a normalized voting method  for text classification}, journal = {information processing and management},  year = {2004}, volume = {40}, number = {2}, pages =
the results show  multi-agent classification can achieve promising classification results while  maintaining its other advantages."
its main feature is to bind the searching space so that optimal  parameters can be selected quickly.
we show that optimal effectiveness occurs when using  only a small proportion of the indexing terms available, and that effectiveness  peaks at a higher feature set size and lower effectiveness level for a  syntactic phrase indexing than for word-based indexing.
they applied it to two real-world problems.}, } @inproceedings{blosseville92, author = {m.j. blosseville and georges hebrail and m.g. montell and n. penot}, title = {automatic document classification: natural langage processing and expert system techniques used together}, booktitle = {proceedings of sigir-92, 15th acm international conference on research and development in information retrieval}, editor = {nicholas j. belkin and peter ingwersen and annelise mark pejtersen}, publisher = {acm press, new york, us}, address = {
{ieee computer society press, los alamitos, us}, year = {1999},  address = {washington, dc}, pages = {2924--2927}, url = {}, abstract = {
the mfom learning framework is evaluated on the reuters-21578 task with lsi-based feature extraction and a binary tree classifier.
we present experimental evidence that  confirms this hypothesis on a set of web pages that relate to computer science  departments.} } @inproceedings{gabrilovich:2004:newsjunkie, author =  "gabrilovich, evgeniy and dumais, susan and horvitz, eric", title =  "newsjunkie: {p}roviding personalized newsfeeds via analysis of  information novelty", booktitle =
sonia uses a multi-tier approach to extracting relevant terms from documents as  well as statistical clustering methods to determine potential topics within a  document collection.
we investigate the use of such  information for representing web sites, and the effectiveness of different  classifiers (naive bayes, nearest neighbor, and {\sc foil}) in exploiting those  representations.
while it is easy  to collect the unlabeled documents, it is not so easy to manually categorize  them for creating training documents.
= {benkhalifa, mohamed and bensaid, amine  and mouradi, abdelhak}, title = {
patents  are distributed through hundreds of collections, divided up by general area.
{575--608}, url = {},  abstract = {}, } @article{maron61, author = {m.e. maron}, title = {automatic  indexing: an experimental inquiry}, year = {1961}, journal = {journal of the  association for computing machinery}, volume = {8}, number = {3}, pages =
{90--95}, url = {http://cobar.cs.umass.edu/pubfiles/ir-121.ps}, abstract =  {several standard text-categorization techniques were applied to the problem of  automated essay grading.
they are accurate, robust, and quick to apply to test instances.
our results show that the new method is highly effective  and promising." } @article{bang:2006:hdc, author = {s.l. bang and j.d.  yang and h.j. yang}, title = {hierarchical document categorization with k-nn  and concept-based thesauri}, journal = {information processing and management},  year = {2006}, volume = {42}, number = {2}, pages = {387--406}, abstract = {
the co-training setting applies to datasets that have a natural separation of their features into two disjoint sets.
}  @article{montanesdrcf05, title =
booktitle = {proceedings of trec-1, 1st
machine learning community has in part addressed this problem by developing  hierarchical supervised classifiers that help maintainers to categorize new  resources within given hierarchies.
experimental results show that it  achieves nearly perfect performance on a set of hard cases.}, }  @inproceedings{chen00, author = {hao chen and susan t. dumais}, title =
the first set of  experiments tests a recurrent neural network for the task of library title  classification.
such a  knowledge base would enable much more effective retrieval of web information,  and promote new uses of the web to support knowledge-based inference and  problem solving.
our experiments show our algorithms represent a good trade-off between speed and accuracy in most applications.}, } @inproceedings{cheong02, author = {cheong fung, gabriel p. and jeffrey x. yu and hongjun lu}, title = {discriminative category matching: efficient text classification for huge document collections}, booktitle =
yiming yang}, title = {noise reduction in a statistical approach to text categorization}, booktitle = {proceedings of sigir-95, 18th acm international conference on research and development in information retrieval}, editor =
integrating background knowledge  into text classification}, pages = {1448--1449}, url = {}, booktitle =
it accommodates both single and multiple topic assignments for each document.
in this paper, we  present pva, an adaptive personal view information agent system to track, learn  and manage, user's interests in internet documents.
conventional classifiers are mostly based on the vector space model of document, which views a document simply as an n-dimensional vector of terms.
the text classification algorithms  classify texts with high accuracy by using an underlying information extraction  system to represent linguistic phrases and contexts.
empirical results are given on a number of dataset,  showing that our feature selection method is more effective than koller and  sahami’s method [koller, d., & sahami, m. (1996).
"geneva, switzerland",  publisher =
bo pang and lillian lee and shivakumar vaithyanathan}, title =
text  categorization is an interesting area for evaluating and quantifying the impact  of linguistic information.
david  j. hand and joost n. kok and michael r. berthold}, address = {amsterdam, nl},  year = {1999}, pages = {487--497}, url =  {http://www.ai.univie.ac.at/~juffi/publications/ida-99.ps.gz}, abstract = {
within this process, bootstrapping a  taxonomy with examples represents a critical factor for the effective  exploitation of any supervised learning model.
machine learning community has in part addressed this problem by developing hierarchical supervised classifiers that help maintainers to categorize new resources within given hierarchies.
we tested this approach by applying k-nearest  neighbor, rocchio and language modeling classifiers and their combination to  the event tracking problem in the topic detection and tracking (tdt) domain,  where new classes (events) are created constantly over time, and representative  validation sets for new classes are often difficult to obtain on time.
we show that an old corpus can be used for training when testing on new web pages, with only a marginal drop in accuracy rates on genre classification.
using a database  of 20,569 documents, we verify that the algorithm attains levels of average  precision in the 70-80\% range for category coding and in the 60-70\% range for  subcategory coding.
in training the $i$-th classifier special emphasis is placed on the correct categorization of the training documents which have proven harder for the previously trained classifiers.
"proceedings of the eleventh acm sigkdd international conference on knowledge discovery in data mining", year = "2005", pages =
finally, the obtained structure attributes are used to re-organize and index  the web objects.
our experimental  results show that lb, an association-based lazy classifier can achieve a good  tradeoff between high classification accuracy and scalability to large document  collections and large feature sizes.}, } @article{merkl98, author = {merkl,  dieter}, title = {
before performing text classification, back data should be constructed.
mainly non-informative keywords play the roles of  grammatical functions in sentences; such keywords, what are called functional  keywords, reflect its contents very little, so they should be removed in the  process of document indexing.
a novel  application of evolutionary computation is its use in adaptive information  filtering for optimizing various parameters, notably the weights associated  with trigrams.
{markus stumptner and dan corbett and michael j. brooks}, publisher = {springer  verlag, heidelberg, de}, address = {
we show that for problems plagued with numerous redundant features the performance of c4.5 is significantly superior to that of svm, while aggressive feature selection allows svm to beat c4.5 by a narrow margin.}, } @inproceedings{galavotti00, author = {luigi galavotti and fabrizio sebastiani and maria simi}, title = {experiments on the use of feature selection and negative evidence in automated text categorization}, booktitle = {proceedings of ecdl-00, 4th european conference on research and advanced technology for digital libraries}, editor = {jos{\'e} l. borbinha and thomas baker}, publisher = {springer verlag, heidelberg, de}, note = {
it also can be used for creating training documents.}, }  @inproceedings{ko02, author = {youngjoong ko and jinwoo park and jungyun seo},  title = {automatic text categorization using the importance of sentences},  booktitle =
when categorizing magazine articles with broad subject descriptors, we study three aspects of text classification: (1) effective selection of feature words and proper names that reflect the main topics of the text; (2) learning algorithms; and (3) improvement of the quality of the learned classifier by selection of examples.
{jacqueline w. wong and wing-kay kan and gilbert h. young}, title = {{{\sc  action}}: automatic classification for full-text documents}, journal = {sigir  forum}, year = {1996}, volume = {30}, number = {1}, pages =
the system is used as part of a  commercial news clipping and retrieval product.
for the first time, we apply the scheme to  text categorization with support vector machines (svms) on several large text  corpora with more than 100 categories.
the user profile is a vector of weighted terms which are learned from the relevance assessment values given by the user on the training set.
text categorization using  feature projections}, booktitle = {proceedings of coling-02, the 19th  international conference on computational linguistics}, year = {2002}, editor =
a new evaluation methodology  is offered that focuses on the needs of the data mining practitioner faced with  a single dataset who seeks to choose one (or a pair of) metrics that are most  likely to yield the best performance.
we also present results  suggesting that traditional term clustering methods are unlikely to provide  significantly improved text representations.
the algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents; it then trains a new classifier using the labels for all the documents, and iterates to convergence.
{ulm, de}, year = {2004}, pages = {}, note = {
in contrast to ranking  systems, binary text classification systems may need to produce result sets of  any size, requiring that sampling be used to estimate their effectiveness.
here we only highlight some key techniques used in submitted solutions.
% % % entry types % % article % =
{portland, us}, pages = {24--27}, url =  {http://werner.ira.uka.de/papers/speech/1993/wcnn_93_petra_geutner.ps.gz},  abstract = {
however, many real-world domains are  best described by relational models in which instances of multiple types are  related to each other in complex ways.
in this paper we describe extensive experiments for semantic text routing based on classified library titles and newswire titles.
in this paper we describe experiments  that investigate the effects of ocr errors on text categorization.
ray  liere and prasad tadepalli}, title = {active learning with committees for text  categorization}, booktitle = {proceedings of aaai-97, 14th
this article describes  our general approach, several machine learning algorithms for this task, and  promising initial results with a prototype system that has created a knowledge  base describing university people, courses, and research projects.}, }  @article{craven01, author = {craven, mark and slattery, se{\'{a}}n}, title =
{leah s. larkey}, title = {automatic essay grading using text categorization techniques}, booktitle = {proceedings of sigir-98, 21st acm international conference on research and development in information retrieval}, editor =
= {proceedings of ecir-01, 23rd
one key difficulty with text classification learning algorithms is that they require many hand-labeled examples to learn accurately.
using four subsets of the reuters text  categorization test collection and a full-text test collection of which  documents are varying from tens of kilobytes to hundreds, we evaluate the  proposed model, especially the effectiveness of various passage types and the  importance of passage location in category merging.
{\em classifier induction} refers instead to the problem of automatically  building a text classifier by learning from a set of documents pre-classified  under the categories of interest.
we provide a theoretical motivation for the  algorithm.
% % when this is the case, if you know of a url with unrestricted access % % from which the paper is also available, please let me know
}  @inproceedings{montejo-raez:2005:tcu, author =
we use a training set of manually categorized documents to learn  word-category associations, and use these associations to predict the  categories of arbitrary documents.
we provide an intuitive  modification to the em iterations by re-estimating the empirical distribution  in order to reinforce feature values in unlabeled data and to reduce the  influence of noisily labeled examples.
% % % % % % a bibliography on automated text categorization %
in this paper, we report on a set of experiments that explore the utility of making use of the structural information of www documents.
there are also several obvious directions for improving  the system's classification performance in those cases where it did not do as  well.
in this paper we investigate seven text representations based on n-grams and single words.
{3339}, publisher = {springer-verlag}, url =  {http://www.cs.waikato.ac.nz/~eibe/pubs/kibriya_et_al_cr.ps.gz}, abstract =  {this paper presents empirical results for several versions of the multinomial  naive bayes classifier on four text categorization problems, and a way of  improving it using locally weighted learning.
while it is well-known when the  combination of two kernels is again a valid kernel, it is an open question if  the resulting kernel will perform well.
despite being chosen by this heterogeneous approach, the uncertainty samples yielded classifiers with lower error rates than random samples ten times larger.}, } @inproceedings{lewis95, author = {lewis, david d.}, title = {evaluating and optmizing autonomous text classification systems}, booktitle = {proceedings of sigir-95, 18th acm international conference on research and development in information retrieval}, editor = {edward a. fox and peter ingwersen and raya fidel}, publisher = {acm press, new york, us}, year = {1995}, address = {
it is shown  that both classifiers can perform filtering with reasonable accuracy.
{edward a. fox and neil rowe}, publisher = {acm press, new york, us}, year =  {1999}, address = {berkeley, us}, pages = {179--187}, url =  {http://cobar.cs.umass.edu/pubfiles/ir-162.ps}, abstract = {
in our method, independent components of document vectors are extracted using ica and concatenated with the original vectors.
text categorization of low quality images}, booktitle = {proceedings of sdair-95, 4th annual symposium on document analysis and information retrieval}, publisher = {}, editor = {}, year = {1995}, address = {las vegas, us}, pages = {301--315}, url = {http://www.research.att.com/~lewis/papers/ittner95.ps}, abstract = {categorization of text images into content-oriented classes would be a useful capability in a variety of document handling systems.
the corpus contains 44,675 articles with over 35 million words.
the advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert manpower, and straightforward portability to different domains.
we use telltale as our classifier; telltale uses n-grams to compute the similarity between documents.
{darmstadt, de}, publisher = {}, pages = {}, url =  {http://www.cs.huji.ac.il/labs/learning/papers/irsg3.eps.gz}, abstract = {
we describe an experience integrating different innovative ai technologies such as hierarchical pattern matching and information extraction to provide flexible multilingual classification adaptable to user needs.
in the context of text modeling, the topic  probabilities provide an explicit representation of a document.
this paper proposes a multi-dimensional framework for classifying text documents.
"context and learning in novelty detection", booktitle = "emnlp'05", year = "2005", abstract =
the performance of our approach is promising, when tested on the questions from the trec qa track.}, } @inproceedings{zhang03a, author = {jian zhang and rong jin and yiming yang and alex hauptmann}, title = {modified logistic regression: an approximation to svm and its applications in large-scale text categorization}, booktitle = {proceedings of icml-03, 20th international conference on machine learning}, editor = {}, year = {2003}, address = {washington, dc}, pages = {}, publisher =
this note presents the corrected results, along with additional data supporting the original claim that uncertainty sampling has an advantage over relevance sampling in most training situations.}, } @inproceedings{lewis95b, author = {david d. lewis}, title = {the {trec-4} filtering track: description and analysis}, booktitle = {proceedings of trec-4, 4th text retrieval conference}, publisher = {national institute of standards and technology, gaithersburg, us}, editor = {donna k. harman and ellen m. voorhees}, year = {1995}, address = {gaithersburg, us}, pages = {165--180}, url = {http://www.research.att.com/~lewis/papers/lewis96b.ps}, abstract = {the trec-4 (4th text retrieval conference) filtering track was an experiment in the evaluation of binary text classification systems.
we propose a  novel fs technique, based on a simplified variant of the $\chi^2$ statistics.
in all cases we  show that our approach either outperforms other methods tried for these tasks  or performs comparably to the best.}, } @article{ruiz02, author = {miguel ruiz  and padmini srinivasan}, title = {hierarchical text classification using neural  networks}, journal = {information retrieval}, number = {1}, volume = {5}, pages  = {87--118}, year = {2002}, url = {http://www.wkap.nl/article.pdf?383232},
"interactive feature selection", pages  =
one problem with this approach is that the classifier best suited for an application may be too expensive to train or use during the selection of instances.
in particular, we present an extensive experimental comparison of our approach with other methods on several well studied lexical disambiguation tasks such as context-sensitive spelling correction, prepositional phrase attachment and part of speech tagging.
"a probabilistic model for retrospective news event detection", pages = "106--113", booktitle = "proceedings of the 28th annual international {acm} {sigir} conference on research and development in information retrieval", year = "2005", month =
what are sufficient conditions for applying  svms to text-classification problems successfully?}, } @article{joachims02,  author =
"couto, thierson and cristo, marco and goncalves, marcos andre and  calado, pavel and ziviani, nivio and moura, edleno and ribeiro-neto,  berthier", title =
given the large number (> 13,000) of closely related categories, this is a challenging task that is unlikely to succumb to a single algorithmic solution.
@inproceedings{li:2005:ndb, author =
berkeley, us}, pages = {179--187}, url = {http://cobar.cs.umass.edu/pubfiles/ir-162.ps}, abstract = {
{}, pages = {808--814}, address = {saarbr{\"{u}}cken, de}, url =  {http://acl.ldc.upenn.edu/c/c00/c00-2117.pdf}, abstract = {
{tapio elomaa and heikki mannila and h. toivonen}, address = {helsinki, fi}, pages =
published in the ``lecture notes in computer science'' series, number 1642}, editor = {
experiments show that the  models outperform their single-label counterparts on standard text corpora.
"categorizing web pages as a preprocessing  step for information extraction", booktitle = "lrec'04", year =  "2004" } @inproceedings{hahn:2004:pdt, author =
this set of relevant features varies widely throughout the hierarchy, so that, while the overall relevant feature set may be large, each classifier only examines a small subset.
the neural network is simulated on a vax  computer with a fast learning algorithm, and is combined with some  non-statistical knowledge from the feature definition system.
we describe the design and implementation of our  system, stressing how to exploit standard, efficient relational operations like  sorts and joins.
the use of naive bayes allows the boosting algorithm to utilize term frequency information while maintaining probabilistically accurate confidence ratio.
"couto, thierson and cristo, marco and goncalves, marcos andre and calado, pavel and ziviani, nivio and moura, edleno and ribeiro-neto, berthier", title =
annual  meeting of the american society for information science}, publisher =
{hierarchical document categorization with support vector machines}, booktitle  = {proceedings of cikm-04, 13th acm international conference on information and  knowledge management}, publisher = {acm press, new york, us}, address =
we propose a novel variant, based on the exploitation of negative evidence, of the well-known $k$-nn method.
our results indicate that the use  of n-grams is an attractive technique which can even compare to techniques  relying on a morphological analysis.
{milo\v{s} radovanovi\'c and mirjana ivanovi\'c}, title
in many real-world domains, supervised learning requires a large number of  training examples.
however, owing to the use of context-sensitive features, the classifier is very accurate.
text categorization based on regularized linear classification methods}, journal = {information retrieval}, number = {1}, volume = {4}, pages = {5--31}, year = {2001}, url = {http://www.wkap.nl/article.pdf?335913}, abstract = {
{taeho c. jo}, title = {news articles classification based on representative  keywords of categories}, booktitle = {computational intelligence for modelling,  control and automation}, editor = {
{published in the ``lecture notes in computer science'' series, number 2945},  pages = {596--600}, url = {}, abstract = {}, } @inproceedings{roth98, author =  {dan roth}, title = {learning to resolve natural language ambiguities: a  unified approach}, booktitle = {proceedings of aaai-98, 15th conference of the  american association for artificial intelligence}, publisher = {aaai press,  menlo park, us}, editor = {}, year = {1998}, pages = {806--813}, address =
the efficiency of these three classifications is  investigated on two data sets.
= {dublin, ie}, pages = {192--201}, year = {1994}, url = {http://www.acm.org/pubs/articles/proceedings/ir/188490/p192-hersh/p192-hersh.pdf}, abstract = {a series of information retrieval experiments was carried out with a computer installed in a medical practice setting for relatively inexperienced physician end-users.
"2007", month =  "february", address =
despite its simplicity, results of experiments on web pages and tv closed captions demonstrate high classification accuracy.
the filtering engine memorizes both user preferences and past situations.
at each node, this classifier can ignore the large  number of noise words in a document.
performances of all three classifiers degrade from the reuters  collection to the ohsumed collection, but decision forest remains to be  superior.}, } @inproceedings{chen01, author
published in the ``lecture  notes in computer science'' series, number 2389}, } @inproceedings{kolcz01,  author = {
in this paper we describe how the lsi approach can be implemented in a  kernel-defined feature space.
= {feature subset selection in text learning}, booktitle = {proceedings of ecml-98, 10th european conference on machine learning}, publisher = {springer verlag, heidelberg, de}, note = {
as more information becomes available on-line, intelligent information retrieval will be crucial in order to navigate the information highway efficiently and effectively.
our approach also indicates a new promising way to use  trust-worthy deep web knowledge to help organize dispersive information of  surface web."
about 70\% of the web-documents are assigned to their true genre; note in this connection that no genre classification benchmark for web pages has been published so far.}, } @article{mladenic03, author = {
such topics can be used as descriptors, similarly to the way librarians use for  example, the library of congress cataloging system to annotate and categorize  books.
"karakos, damianos and eisner,  jason and khudanpur, sanjeev and priebe, carey e.", title =
we  implemented pagetypesearch system based on our approach.
"scalable term selection for text categorization", booktitle =
"coling", pages = "841--847", url = "http://acl.ldc.upenn.edu/coling2004/main/pdf/121-637.pdf", abstract =
on one of these datasets (the 20 newsgroups) the method based on word clusters significantly outperforms the word-based representation in terms of categorization accuracy or representation efficiency.
stanford, us}, year = {1996}, note = {
"283--302" } @inproceedings{isuma03, author =
finally, representing each newsgroup by k vectors (with k = 2 or 3) using clustering yields the most significant improvement in routing accuracy, ranging from 60\% to loo\%, while causing only slightly higher storage requirements.}, } @inproceedings{hsu99a, author = {
we  consider the problem of assigning level numbers (weights) to hierarchically  organized categories during the process of text categorization.
categorisation is a useful method for organising documents into subcollections that can be browsed or searched to more accurately and quickly meet information needs.
"the  utility of information extraction in the classification of books",  booktitle =
in order to determine whether information within a sentence has been  seen in material read previously, our system integrates information about the  context of the sentence with novel words and named entities within the  sentence, and uses a specialized learning algorithm to tune the system  parameters."
"kdd cup-2005 report: facing a great challenge", journal = "{sigkdd} explorations", pages =
classification is a function that matches a new object with one of  the predefined classes.
these algorithms relied on hand-coded training data, including annotated texts and a semantic dictionary.
the  automated categorization (or classification) of texts into predefined  categories has witnessed a booming interest in the last ten years, due to the  increased availability of documents in digital form and the ensuing need to  organize them.
{edward a. fox and peter ingwersen and raya fidel}, publisher = {acm press, new  york, us}, year = {1995}, address = {
naive bayes classifiers are recognized to be among the best for classifying text.
text categorization is the task of classifying natural language documents  into a set of predefined categories.
the results we have obtained  significantly outperform the results achieved by previous automated survey  coding approaches.}, } @article{giorgetti03a, author = {daniela giorgetti and  fabrizio sebastiani}, title = {automating survey coding by multiclass text  categorization techniques}, journal = {journal of the american society for  information science and technology}, year = {2003}, volume = {54}, number =
most of these relations have been obtained by means of  statistical and heuristical methods.
the optimized rocchio algorithm achieves a  performance comparable with that of the hierarchical neural networks.}, }  @inproceedings{ruiz97, author = {miguel e. ruiz and padmini srinivasan}, title  = {automatic text categorization using neural networks}, booktitle =  {proceedings of the 8th asis/sigcr workshop on classification research}, editor  = {efthimis efthimiadis}, publisher =
our experiments on a variety of text categorization tasks indicate that there is significant potential in improving classifier performance by feature reweighting, beyond that achieved via selective sampling alone (standard active learning) if we have access to an oracle that can point to the important (most predictive) features.
year = {2001}, url =  {http://link.springer.de/link/service/series/0558/papers/2167/21670454.pdf},  abstract = {the paper demonstrates that the addition of automatically selected  word-pairs substantially increases the accuracy of text classification which is  contrary to most previously reported research.
in reality, however, many information  retrieval problems can be more accurately described as learning a binary  classifier from a set of incompletely labeled examples, where we typically have  a small number of labeled positive examples and a very large number of  unlabeled examples.
learning from little: comparison of classifiers given little training}, booktitle = {proceedings of pkdd-04, 8th european conference on principles of data mining and knowledge discovery}, editor = {jean-fran{\c{c}}ois boulicaut and floriana esposito and fosca giannotti and dino pedreschi}, address = {
in phase  i, two kinds of classifiers are developed as the base classifiers.
to remedy the drawback, we employ concept-based thesauri in the  categorization.
using support vector machines}, booktitle = {proceedings of icml-99, 16th international conference on machine learning}, editor = {ivan bratko and saso dzeroski}, year = {1999}, address = {bled, sl}, publisher = {morgan kaufmann publishers, san francisco, us}, pages = {200--209}, url = {http://www-ai.cs.uni-dortmund.de/dokumente/joachims_99c.ps.gz}, abstract = {this paper introduces transductive support vector machines (tsvms) for text classification.
"acm press", url = "http://www.cs.technion.ac.il/~gabr/papers/newsjunkie.pdf", abstract =
in the first phase, a multilayer feed-forward neural network was trained to classify medical documents in the area of cell biology.
since it does not require complicated parameterization, it is simpler  to use and more robust than comparable heuristics.
we discuss a series of experiments  with different machine learning algorithms in order to experimentally evaluate  various trade-offs, using approximately 100k product reviews from the  web."
unfortunately, the high computational and memory requirements of lsi  and its inability to compute an effective dimensionality reduction in a  supervised setting limits its applicability.
an optimized approach for knn text categorization using  p-trees}, booktitle = {proceedings of sac-04, 19th acm symposium on applied  computing}, editor = {}, pages = {613--617}, address = {nicosia, cy}, year =
= {acm press, new york, us}, address = {athens, gr}, year = {2000}, pages =
we present empirical studies  (controlled experiments on boolean decision trees and a large-scale text  categorization problem) which show that the model selection algorithm leads to  error rates which are often as low as those obtained by 10-fold cross  validation (sometimes even lower).
"beyond {tfidf} weighting for text categorization in the vector space model", pages = "1130--1135", booktitle =
we extend the traditional active learning framework to include feedback on features in addition to labeling instances, and we execute a careful study of the effects of feature selection and human feedback on features in the setting of text categorization.
in contrast to previous stylometric  approaches, we attempt to take full advantage of existing natural language  processing (nlp) tools.
these methods share the similarity by finding  hyperplanes that approximately separate a class of document vectors from its  complement.
learning to classify text using support vector machines}, publisher = {kluwer academic publishers}, address = {dordrecht, nl}, year = {2002}, } @inproceedings{joachims97, author = {thorsten joachims}, title = {
"hierarchical text categorization using fuzzy relational thesaurus", journal =
{70--77}, publisher = {acm  press, new york, us}, url = {http://doi.acm.org/10.1145/383535.383543},  abstract =
however, one of the main shortcomings of such methods is  that they largely disregard lexical semantics and, as a consequence, are not  sufficiently robust with respect to variations in word usage.
although most research on mixture models has concentrated on mixtures for  continuous data, emerging pattern recognition applications demand extending  research efforts to other data types.
"emotions from text: machine  learning for text-based emotion prediction", booktitle =  "emnlp'05", year = "2005" }  @inproceedings{schiffman:2005:cln, author =
finally, we analyze the experimental performance of these models over the outputs of two text classifiers.
integrating {wordnet} knowledge to supplement training data in semi-supervised agglomerative hierarchical clustering for text categorization}, journal = {international journal of intelligent systems}, pages = {929--947}, year = {2001}, volume = {16}, number = {8}, url = {http://www3.interscience.wiley.com/cgi-bin/fulltext?id=84503376&placebo=ie.pdf}, abstract = {
in our method not only words but also semantic categories given by the thesaurus are used as features in a classifier.
european colloquium on information retrieval research}, editor = {}, year = {2001}, address = {darmstadt, de}, publisher = {}, pages = {}, url = {http://www.cs.huji.ac.il/labs/learning/papers/irsg3.eps.gz}, abstract = {
extensive experimentation on representative classifiers, rocchio and svm, as well as a careful analysis of the literature have been carried out to study how some nlp techniques used for indexing impact tc.
unfortunately, the  paradigm of supervised machine learning is ill-suited to this task, as it  assumes that the training examples are classified by a teacher - usually a  human.
{leah s. larkey}, title = {automatic essay  grading using text categorization techniques}, booktitle = {proceedings of  sigir-98, 21st acm international conference on research and development in  information retrieval}, editor = {w. bruce croft and alistair moffat and van
in this paper, we study such a problem of performing text classification without labeled negative data tc-won).
latent semantic indexing (lsi) has been successfully used for ir purposes as a technique for capturing semantic relations between terms and inserting them into the similarity measure between two documents.
the experimental results show that modulating the structure of the user profile increases the accuracy of a personalization system.}, } @article{chen03, author = {chen l. and tokuda n. and nagai a.}, title = {a new differential lsi space-based probabilistic document classifier}, journal = {information processing letters}, pages = {203--212}, year = {2003}, volume = {88}, number = {5}, doi = {http://dx.doi.org/10.1016/j.ipl.2003.09.002}, abstract = {
"hang cui and vibhu mittal and mayur  datar", title =
pva: a self-adaptive personal view agent},  booktitle = {proceedings of kdd-01, 7th acm sigkdd international conferece on  knowledge discovery and data mining}, editor = {foster provost and ramakrishnan  srikant}, year = {2001}, pages = {257--262}, publisher = {acm press, new york,  us}, address = {san francisco, us}, url =  {http://doi.acm.org/10.1145/502512.502548}, abstract = {
this situation occurs, for instance, in declassifying documents that have been previously considered important to national security and thus are currently being kept as secret.
{jamie callan and gordon cormack and charles clarke and david hawking and alan smeaton}, publisher =
in this paper  we propose an integration of a selforganizing map and semantic networks from  wordnet for a text classification task using the new reuters news corpus.
for precision alone, however, information gain (ig) was superior.
we have tried our representation  scheme for automatic document categorisation on the reuters' test set of  documents.
the fixed number of regular words from each class will be used as a feature vectors together with the reduced principal components from the pca.
while document  categories can be a set of unstructured category labels, some document  categories are hierarchically structured.
however, the benefits that this has brought about have somehow been limited by the fact that different researchers have ``carved'' different subsets out of this collection, and tested their systems on one of these subsets only; systems that have been tested on different \textsf{reuters-21578} subsets are thus not readily comparable.
in  particular, our new feature selection method yields considerable improvement.
such a semantic mapping leads to a significant improvement in categorization and retrieval, compared to alternative approaches.}, } @inproceedings{yang94a, author = {yiming yang}, title = {expert network: effective and efficient learning from human decisions in text categorisation and retrieval}, booktitle = {
feature selection  using support vector machines}, booktitle = {proceedings of the 3rd  international conference on data mining methods and databases for engineering,  finance, and other fields}, year = {2002}, pages = {}, address = {bologna, it},  url = {http://www.brank.org/msr/fsnormal/bologna/bologna-paper-4.pdf}, abstract  =
survey coding has several applications, especially in the social  sciences, ranging from the simple classification of respondents to the  extraction of statistics on political opinions, health and lifestyle habits,  customer satisfaction, brand fidelity, and patient satisfaction.
"context and learning in novelty  detection", booktitle = "coling'04", pages =  "487--493", year =
text classification using esc-based stochastic decision lists}, journal = {information processing and management}, pages = {343--361}, year = {2002}, number = {3}, volume = {38}, url = {}, abstract = {
a document is usually written in  an organized structure to present its main topic(s).
given a labeled or partially labeled training corpus of text documents, the model estimates the joint distribution of training documents and class labels by using a generalization of the expectation maximization algorithm.
"643--650", abstract =
a  major application of air/x is the air/phys system developed for a large physics  database.
we discuss a series of experiments with different machine learning algorithms in order to experimentally evaluate various trade-offs, using approximately 100k product reviews from the web."
finally, we show empirically that this categorization  system utilizing a machine-derived taxonomy performs as well as a manual  categorization process, but at a far lower cost.}, } @inproceedings{agrawal00,  author = {rakesh agrawal and roberto j. bayardo and ramakrishnan srikant},  title = {{\sc athena}: mining-based interactive management of text databases},  booktitle = {proceedings of edbt-00, 7th international conference on extending  database technulogy}, editor = {carlo zaniolo and peter c. lockemann and marc  h. scholl and torsten grust}, year = {2000}, address = {konstanz, de},  publisher = {springer verlag, heidelberg, de}, note = {
{proceedings of cikm-98, 7th acm international conference on information and  knowledge management}, publisher = {acm press, new york, us}, editor = {
{ellen riloff and jeffrey lorenzen}, title = {extraction-based text
{1998}, volume = {24}, number = {1}, pages = {97--124}, url = {}, abstract =  {this paper presents context-group discrimination, a disambiguation algorithm  based on clustering.
in this paper, a systematic  study aimed to understand the role of rocchio formula in selection and  weighting of linguistic features will be described.}, }  @inproceedings{basili01b, author = {roberto basili and alessandro moschitti},  title = {
in this paper we explore the use of text-mining methods for the identification of the author of a text.
using these techniques, we can  automatically build text categorization systems that benefit from  domain-specific natural language processing.}, } @article{robertson84, author =
published in the ``lecture notes in computer science'' series, number 2035}, pages = {433--443}, url = {http://link.springer-ny.com/link/service/series/0558/papers/2035/20350433.pdf}, abstract = {
"text classification by labeling words", booktitle =
the approach  presented here represents a compromise between keyword-based techniques and  in-depth natural language processing.
several previous works already suggested applying this method for document clustering, gene expression data analysis, spectral analysis and more.
most such methods are  character-based, and thus have the potential to automatically capture non-word  features of a document, such as punctuation, word-stems, and features spanning  more than one word.
"traditionally, text classifiers are built from  labeled training examples.
integration of  phonetic and graphic features in poetic text categorization judgements},  journal = {poetics}, year = {1996}, volume = {23}, number = {5}, pages =  {363--380}, url = {}, abstract = {
this paper introduces a new effective model for text categorization with great corpus (more or less 1 million documents).
the open and modularized system architecture makes our classifier be extendible.
text  categorization must work reliably on all input, and thus must tolerate some  level of these kinds of problems.
experimental evaluation on real-world  data shows that the proposed approach gives good results.
ata kaban and mark girolami}, title = {
the former never consider the negative features, which are quite valuable, while the latter cannot ensure the optimal combination of the two kinds of features especially on imbalanced data.
our system, named  sonia (service for organizing networked information autonomously), has been  implemented as part of the stanford digital libraries testbed.
previous work on the categorization of document images has relied on optical character recognition (ocr) to provide the transformation between the image domain and a domain where pattern recognition techniques are more readily applied.
we achieve a precision recall break even point of 84\% which is comparable to the best known published results.
{morgan kaufmann publishers,  san francisco, us}, url =  {http://www.research.att.com/~lewis/papers/lewis94e.ps}, abstract =  {uncertainty sampling methods iteratively request class labels for training  instances whose classes are uncertain despite the previous labeled instances.
autoslog is a dictionary construction system that has been shown to substantially reduce the time required for knowledge engineering by learning extraction patterns automatically.
we introduce a new frequency-based method for selecting the most useful citations from a document collection for use in the model.}, } @inproceedings{forman02, author = {
finally ssfcm results in improved performance and faster execution time as more weight is given to training documents.}, } @inproceedings{bennett02, author = {paul n. bennett and susan t. dumais and eric horvitz}, title = {probabilistic combination of text classifiers using reliability indicators: models and results},
the method harnesses reliability indicators-variables that provide signals about the performance of classifiers in different situations.
forthcoming}, } @incollection{debole04a, author = {franca  debole and fabrizio sebastiani}, title = {supervised term weighting for  automated text categorization}, year = {2004}, booktitle = {text mining and its  applications}, editor = {spiros sirmakessis}, publisher =
this  paper presents a probabilistic mixture modeling framework for the hierarchic  organisation of document collections.
in this paper we present a hybrid text categorization method based on rough sets theory.
hierarchical classifi- cation is a more efficient method  - instead of a single classifier, we use a set of classifiers distributed over  a class taxonomy, one for each internal node.
we conduct an empirical study on several document classification tasks which confirms the value of our methods in large scale semi-supervised settings."
in assigning 124 documents to 9 categories, there were 97 cases of agreement with professional indexers.
we propose a new on-line learning algorithm, known as the atf (adaptive text filtering) algorithm, to tackle the adaptive filtering problem.
these knowledge-based categorization methods are more powerful and accurate than statistical techniques.
we present experimental results showing that employing our active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings.}, } @inproceedings{tong92, author = {richard tong and adam winkler and pamela gage}, title = {classification trees for document routing: a report on the trec experiment},
many corpora, such as internet directories,  digital libraries, and patent databases are manually organized into topic  hierarchies, also called taxonomies.
{aaai press,  menlo park, us}, editor = {}, year = {1999}, address = {orlando, us}, pages =  {480--486}, url = {}, abstract = {investigates the effect of prior feature  selection in support vector machine (svm) text categorization.
"this paper studies the use of di®erent sources of  information for performing a text classification task.
among our experiments are some specifically designed to test whether the ability to capture non-word features causes character-based text compression methods to achieve more accurate classification.}, } @inproceedings{masand92, author = {briji masand and gordon linoff and david waltz}, title = {
{http://www.cs.columbia.edu/~sable/research/ecdl99.ps}, abstract = {
{kluwer academic publishers}, address = {dordrecht, nl}, url = {},  abstract = {}, } @inproceedings{hadjarian01, author = {ali hadjarian and jerzy
thanaruk  theeramunkong and verayuth lertnattee}, title = {multi-dimensional text  classification}, booktitle = {proceedings of coling-02, the 19th international  conference on computational linguistics}, year = {2002}, editor = {}, pages =  {}, address = {taipei, tw}, url =  {http://acl.ldc.upenn.edu/coling2002/proceedings/data/area-28/co-399.pdf},  abstract =
{this paper has also been published in \emph{australian computer science communications}, 24(2), 2002.}, abstract = {automatic text categorization has always been an important application and research topic since the inception of digital documents.
a user study compared our new category interface with the  typical ranked list interface of search results.
using multinomial  naive bayes and regularized logistic regression as classifiers, our experiments  show both great potential and actual merits of explicitly combining positive  and negative features in a nearly optimal fashion according to the imbalanced  data.}, } @inproceedings{zhou00, author = {shuigeng zhou and ye fan and  jiangtao hua and fang yu and yunfa hu}, title = {
focusing on anti-spam filtering for mailing lists, a thorough investigation of the effectiveness of a memory-based anti-spam filter is performed using a publicly available corpus.
integration into production lines is under execution.}, } @article{apte94,  author =
such a system will have to be adaptive to the user changing interest.
improving short text classification using unlabeled background knowledge}, booktitle = {proceedings of icml-00, 17th international conference on machine learning}, editor = {
however, none of the  existing classification approaches could achieve all of these requirements.
experimental results show that the  improvements to active learning require less than two-thirds as many labeled  training examples as previous qbc approaches, and that the combination of em  and active learning requires only slightly more than half as many labeled  training examples to achieve the same accuracy as either the improved active  learning or em alone.}, } @inproceedings{mccallum98b, author = {andrew k.  mccallum and ronald rosenfeld and tom m. mitchell and andrew y. ng}, title =
"novelty detection based on sentence  level patterns", pages =
we propose a novel approach for categorizing text documents based on the use of a special kernel.
again, as in [14], the addition of nlp capabilities also suggested a different application of existing methods in revised forms.
last, m. and szczepaniak, p. s. and volkovich, z. and kandel,  a.}, pages = {191--200}, series = {studies in computational intelligence},  volume = {23}, publisher = {springer-verlag}, url =  {http://perun.im.ns.ac.yu/radovanovic/publications/2006-awic-cats.pdf},  abstract =
in our approach, first, layered lsi spaces are built for a better representation of the hierarchically structured domain knowledge, in order to emphasize the specific semantics and term space in each layer of the domain knowledge.
"193--200", } @inproceedings{sandler:2005:oul, author =
{yiming yang and john w. wilbur}, title = {an analysis of statistical term strength and its use in the indexing and retrieval of molecular biology texts}, journal = {
experiments using a number of e-mail documents generated by different authors on a set of topics gave promising results for both aggregated and multi-topic author categorisation.}, } @inproceedings{dhillon02, author = {inderjit dhillon and subramanyam mallela and rahul kumar}, title = {enhanced word clustering for hierarchical text classification}, booktitle = {proceedings of kdd-02, 8th acm international conference on knowledge discovery and data mining}, publisher = {acm press, new york, us}, editor = {}, year = {2002}, address = {edmonton, ca}, pages = {191--200}, url = {}, abstract = {
after defining our notion of ``text mining'', we focus on the differences between text and data mining and describe in some more detail the unique technologies that are key to successful text mining.}, } @article{doyle65, author = {lauren b. doyle}, title = {is automatic classification a reasonable application of statistical analysis of text?}, journal = {journal of the acm}, volume = {12}, number = {4}, year = {1965}, pages = {473--489}, url = {http://www.acm.org/pubs/articles/journals/jacm/1965-12-4/p473-doyle/p473-doyle.pdf}, abstract = {
experimental evidence indicates that k-nnfp is superior to k-nn in terms of classification accuracy in the presence of irrelevant features in many real world domains.}, } @inproceedings{yi00, author = {jeonghee yi and neel sundaresan}, title = {
comparing the accuracy  of our method with other techniques, we observe significant dependency of the  results on the data set.
our approach is to analyze the document cluster map to find centroids of some super-clusters.
raising high-degree overlapped character bigrams into trigrams for dimensionality reduction in chinese text categorization}, booktitle = {proceedings of cicling-04, 5th international conference on computational linguistics and intelligent text processing}, year = {2004}, editor = {alexander f. gelbukh}, publisher = {springer verlag, heidelberg, de}, address = {seoul, ko}, note = {published in the ``lecture notes in computer science'' series, number 2945}, pages = {584--595}, url = {}, abstract = {}, } @inproceedings{yamazaki97, author = {takefumi yamazaki and ido dagan}, title = {mistake-driven learning with thesaurus for text categorization}, booktitle = {proceedings of nlprs-97, the natural language processing pacific rim symposium}, editor = {}, publisher = {}, address = {phuket, th}, pages = {369--374}, year = {1997}, url = {ftp://www.links.nectec.or.th/pub/nlprs/paper/dana4r.ps.gz}, abstract = {
similar to indices for relational data,  taxonomies make search and access more efficient.
based on two chinese corpora, a series of controlled experiments evaluated  their learning capabilities and efficiency in mining text classification  knowledge.
we describe the design and implementation of our system, stressing how to exploit standard, efficient relational operations like sorts and joins.
micheline beaulieu and ricardo baeza-yates  and sung hyon myaeng and kalervo j{\"{a}}rvelin}
maintaining catalogues  manually is becoming increasingly difficult due to the sheer amount of material  on the web, and therefore it will be soon necessary to resort to techniques for  automatic classification of documents.
its main feature is to bind the searching space so that optimal parameters can be selected quickly.
in proceedings of icml-96, 13th international conference on machine learning], which is one of greedy feature selection methods, and conventional information gain which is commonly used in feature selection for text categorization.
petra perner and azriel rosenfeld}, year = {2003}, address = {leipzig, de}, publisher
diederich, joachim and kindermann, j{\"{o}}rg and leopold, edda and paass, gerhard}, title = {authorship attribution with support vector machines}, journal = {applied intelligence}, year = {2003}, volume = {19}, number = {1/2}, pages = {109--123}, url = {http://ipsapp007.kluweronline.com/content/getfile/4504/36/6/abstract.htm}, abstract = {
we also present results suggesting that traditional term clustering methods are unlikely to provide significantly improved text representations.
"september", address =
bayesian online  classifiers for text classification and filtering}, booktitle = {proceedings of  sigir-02, 25th acm international conference on research and development in  information retrieval}, editor = {micheline beaulieu and ricardo baeza-yates  and sung hyon myaeng and kalervo j{\"{a}}rvelin}, publisher = {acm press,  new york, us}, address = {
in this paper we propose instead that learning from the training data should also affect phase (ii), i.e.\ that information on the membership of training documents to categories be used to determine term weights.
} @inproceedings{gabrilovich:2005:odp, author = "gabrilovich, evgeniy and markovitch, shaul", title =
k-nnfp is similar to k-nn except it finds the nearest neighbors according to  each feature separately.
text genre classification with genre-revealing and  subject-revealing features}, booktitle = {proceedings of sigir-02, 25th acm  international conference on research and development in information retrieval},  editor = {micheline beaulieu and ricardo baeza-yates and sung hyon myaeng and  kalervo j{\"{a}}rvelin}, publisher = {acm press, new york, us}, address =
published in the ``lecture notes in computer science'' series, number 3238}, url = {http://www-ai.upb.de/aisearch/ki04-frame.pdf}, abstract = {genre classification means to discriminate between documents by means of their form, their style, or their targeted audience.
we show that the results outperform competing methods.
this task is an intermediate  process in many natural language processing tasks like machine translation or  multilingual information retrieval.
our results show that svm, knn and llsf significantly outperform  nnet and nb when the number of positive training instances per category are  small (less than ten), and that all the methods perform comparably when the  categories are sufficiently common (over 300 instances).}, } @article{yang99a,  author = {yiming yang}, title = {an evaluation of statistical approaches to  text categorization}, journal = {information retrieval}, year = {1999}, pages =  {69--90}, volume = {1}, number = {1/2}, url =  {http://www.cs.cmu.edu/~yiming/papers.yy/irj99.ps}, abstract = {
character shape coding is  a computationally efficient, extraordinarily robust means of providing access  to the character content of document images.
they applied it to two  real-world problems.}, } @inproceedings{blosseville92, author = {m.j.  blosseville and georges hebrail and m.g. montell and n. penot}, title =  {automatic document classification: natural langage processing and expert  system techniques used together}, booktitle = {proceedings of sigir-92, 15th  acm international conference on research and development in information  retrieval}, editor = {nicholas j. belkin and peter ingwersen and annelise mark  pejtersen}, publisher = {acm press, new york, us}, address = {
but automatic classification study also has been  thriving; some of the reasons for this are discussed.
extensive experiments demonstrate the efficiency  and effectiveness of the proposed approach.}, } @inproceedings{li91, author =
this analysis also revealed, for example,  that information gain and chi-squared have correlated failures, and so they  work poorly together.
{1997}, address = {madrid, es}, pages = {39--47}, url =  {http://xxx.lanl.gov/ps/cmp-lg/9705005}, abstract = {
initial experimental results demonstrate that this approach can  produce accurate recommendations.}, } @inproceedings{moschitti03, author =  {alessandro moschitti}, title = {a study on optimal parameter tuning for  rocchio text classifier}, booktitle = {proceedings of ecir-03, 25th
using ig thresholding with a k-nearest neighbor classifier on the reuters corpus, removal of up to 98\% removal of unique terms actually yielded an improved classification accuracy (measured by average precision).
"2005" }  @inproceedings{moyotlhernandez:2005:edf, author =
we first show that adaboost significantly outperforms another highly effective text filtering algorithm.
in empirical tests, it  consistently showed more than 10 points f-measure improvement for each of four  reuters categories tested." } @inproceedings{sindhwani:2006:lss, author =  "sindhwani, vikas and keerthi, s. sathiya", title =
"we study the  effects of feature selection and human feedback on features in active learning  settings.
in this research, our experiment dealt  with the classification of news wire articles using category profiles.
classifier of pagetypesearch classifies web pages into the document types by comparing their pages with typical structural characteristics of the types.
our method combines techniques from language engineering and image  analysis within a machine-learning framework.
finally, we measure the accuracy achieved with all words and all hm pairs combined, which turns out to be only marginally above the baseline.
the experiments were conducted on the standard reuters data set.
this paper describes a straightforward active learning heuristic, representative sampling, which explores the clustering structure of 'uncertain' documents and identifies the representative samples to query the user opinions, for the purpose of speeding up the convergence of support vector machine (svm) classifiers.
{madison, us}, url = {http://l2r.cs.uiuc.edu/~danr/papers/aaai98.ps.gz},  abstract =
rakesh agrawal and paul e. stolorz and gregory piatetsky-shapiro}, publisher = {aaai press, menlo park, us}, year = {1998}, address = {new york, us}, pages = {169--173}, url = {http://www.research.whizbang.com/~wcohen/postscript/kdd-98.ps}, abstract = {whirl is an extension of relational databases that can perform ``soft joins'' based on the similarity of textual identifiers; these soft joins extend the traditional operation of joining tables based on the equivalence of atomic values.
these performance measures often assume independence between categories and do not consider documents misclassified into categories that are similar or not far from the correct categories in the category tree.
moreover, the procedure of defining analysis-level markers can be followed in order to extract useful stylistic information using existing text processing tools.}, } @inproceedings{stamatatos00a, author = {efstathios stamatatos and nikos fakotakis and george kokkinakis}, title = {
the classifiers provide  heuristics to the crawler thus biasing it towards certain portions of the web  graph.
"a comparative study of citations and links in  document classification", booktitle =
}  @inproceedings{radovanovic:2006:ccm, author = {milo\v{s} radovanovi\'c and  mirjana ivanovi\'c}, title = {cat{s}: a classification-powered meta-search  engine}, booktitle = {
which proved effective.
using as testing ground a part of the wall street journal corpus, we show that the most frequent words of the british national corpus, representing the most frequent words of the written english language, are more reliable discriminators of text genre in comparison to the most frequent words of the training corpus.
in  this paper, we propose a new method of text categorization based on feature  space restructuring for svms.
given corpora of documents and a training set of examples of classified  documents, the technique locates a minimal set of co-ordinate keywords to  distinguish between classes of documents, reducing the dimensionality of the  keyword vectors.
{633--646}, url = {http://www3.interscience.wiley.com/cgi-bin/fulltext?id=72502965&placebo=ie.pdf}, abstract = {
we propose to use the following techniques to improve the classification performance of the baseline method: (1) use routing (classification) accuracy and the similarity values to refine the training set; (2) update the underlying term structures periodically during testing; and (3) apply k-means clustering to partition the newsgroup articles and represent each newsgroup by k vectors.
we propose an approach that utilizes the hierarchical topic structure to decompose the classification task into a set of simpler problems, one at each node in the classification tree.
} @inproceedings{tan:2005:nra, author =  "songbo tan and xueqi cheng and moustafa m. ghanem and bin wang and hongbo  xu", title =
gianni amati and fabio crestani}, title = {probabilistic learning for selective dissemination of information}, journal = {information processing and management}, pages = {633--654}, year = {1999}, number = {5}, volume = {35}, url = {http://www.cs.strath.ac.uk/~fabioc/papers/99-ipem.pdf}, abstract = {new methods and new systems are needed to filter or to selectively distribute the increasing volume of electronic information being produced nowadays.
recurrent  neural network learning for text routing}, booktitle = {proceedings of  icann-99, 9th international conference on artificial neural networks},  publisher = {institution of electrical engineers, london, uk}, editor = {},  year = {1999}, pages = {898--903}, address = {edinburgh, uk}, url =  {http://www.his.sunderland.ac.uk/ps/icann99.pdf}, abstract = {this paper  describes new recurrent plausibility networks with internal recurrent  hysteresis connections.
besides, we generate several knowledge base instead of one knowledge base for  the classification of new object, hoping that the combination of answers of the  multiple knowledge bases result in better performance.
text categorization is the process of assigning categories or labels to documents based entirely on their contents.
{ieee computer society press, los alamitos, us}, editor = {}, year = {1990}, address = {santa barbara, us}, pages = {320--326}, url = {}, abstract = {
we show that our method is especially useful for classification tasks  involving a large number of categories where co-training doesn't perform very  well by itself and when combined with ecoc, outperforms several other  algorithms that combine labeled and unlabeled data for text classification in  terms of accuracy, precision-recall tradeoff, and efficiency.}, }  @inproceedings{ghani02, author = {rayid ghani}, title = {
our results do not show a dominant algorithm nor method for making algorithms cost-sensitive, but are the best reported on the test collection used, and approach real-world hand-crafted classifiers accuracy.}, } @inproceedings{goodman90, author = {marc goodman}, title = {{\sc prism}: a case-based telex classifier}, booktitle = {proceedings of iaai-90, 2nd conference on innovative applications of artificial intelligence}, publisher = {aaai press, menlo park, us}, editor = {alain rappaport and reid smith}, year = {1990}, address = {}, pages = {25--37}, url = {}, abstract = {}, } @article{gray71, author = {w. a. gray and a. j. harley}, title = {computer-assisted indexing}, journal = {information storage and retrieval}, year = {1971}, volume = {7}, number = {4}, pages = {167--174}, url = {}, abstract = {}, } @inproceedings{guo04, author = {gongde guo and hui wang and david a. bell and yaxin bi and kieran greer}, title = {an knn model-based approach and its application in text categorization}, booktitle = {
we present such an approach - a sparse network of linear  separators, utilizing the winnow learning algorithm - and show how to use it in  a variety of ambiguity resolution problems.
we focus upon the routing of case law summaries to various secondary law volumes in which they should be cited.
"effects of  web document evolution on genre classification", pages =  "632--639", booktitle =
european conference on artificial intelligence}, publisher = {john wiley and sons, chichester, uk}, editor = {
to index successfully in the defense  documentation center's environment, an automated system must chose single words  or phrases (dependent upon context) rapidly and economically.
we evaluate empirically a scheme for combining classifiers, known as stacked generalization, in the context of anti-spam filtering, a novel cost-sensitive application of text categorization.
automatic text classification is necessary to store documents like that.
inproceedings % = an article in a conference proceedings % required: author, title, booktitle, year % optional: editor, volume or number, series, pages, address, month, % organization, publisher, note % % manual % =
we use a novel stop word identification method to  automatically generate domain-specific stoplists which are much larger than a  conventional domain-independent stoplist.
{apt\'{e}, chidanand and damerau, fred j. and weiss, sholom m.}, title = {automated learning of decision rules for text categorization}, journal =
in this  paper, we first perform the document categorization by using k-nn and then  employ the relationships to reduce the ambiguity.
text categorization is the process of assigning documents to a set of  previously fixed categories.
in this paper we are interested in a more general formulation where documents are organized as page sequences, as naturally occurring in digital libraries of scanned books and magazines.
{kluwer academic publishers}, address = {dordrecht, nl}, url = {}, abstract = {}, } @inproceedings{hadjarian01, author = {ali hadjarian and jerzy bala and peter pachowicz}, title = {
"2004" } @inproceedings{rosso:2004:irt, author = "paolo rosso and antonio molina and ferran pla and daniel jimenez and vicent vidal", title =
based on the lpt-model, we focus on learning patterns within a  relatively simple pattern language.
{basel, ch}, url = {http://airone.fub.it:8080/projects/pakm96.ps}, abstract =  {with the development and diffusion of the internet worldwide connection, a  large amount of information can be delivered to the users.
a belief networks-based generative model for  structured documents.
we  study the use of support vector machines (svms) in classifying email as spam or  nonspam by comparing it to three other classification algorithms: ripper,  rocchio, and boosting decision trees.
text  categorization by boosting automatically extracted concepts}, booktitle =
"2006", pages =  "179--190" } @inproceedings{freschi:2006:foe, author =
{ios press}, address = {amsterdam, nl}, pages = {124--143}, year =
we  experimented with pre-classified samples from {{\sc yahoo!}}\ and the us patent  database.
springer verlag, heidelberg, de}, address = {hong kong, cn}, note = {
"recomputation of class relevance scores for improving  text classification", booktitle =
we then show that a quantum leap in performance is achieved when we further modify the algorithms to better address some of the specific characteristics of the domain.
"ieee intelligent systems", year =
"the performance of search engines crucially depends on their ability to capture the meaning of a query most likely intended by the user.
here, the challenges are: a) obtain a high  accuracy classification model; b) consume low computational time for both model  training and operation; and c) occupy low storage space.
we give here a new, information theoretical interpretation of term strength, review some of its uses in focusing the processing of documents for information retrieval and describe new results obtained in document categorization.}, } @inproceedings{yang97, author = {yiming yang and jan o. pedersen}, title = {a comparative study on feature selection in text categorization}, booktitle = {proceedings of icml-97, 14th international conference on machine learning}, editor = {douglas h. fisher}, year = {1997}, address = {nashville, us}, pages = {412--420}, publisher = {morgan kaufmann publishers, san francisco, us}, url = {http://www.cs.cmu.edu/~yiming/papers.yy/ml97.ps}, abstract = {
we propose a model for basing classification of multimedia on broad, non-topical features, and show how information on targeted nearby pieces of text can be used to effectively classify photographs on a first such feature, distinguishing between indoor and outdoor images.
experimental results show that this variant provides an order of magnitude further improvement in training efficiency.
this inquiry examines a technique for automatically classifying  (indexing) documents according to their subject content.
romain vinot and fran{\c{c}}ois yvon},
(relevance sampling is the  application of relevance feedback to producing a training sample.)
{alexander gelbukh}, publisher = {springer verlag, heidelberg, de}, address =
"mexico city, mexico" }  @inproceedings{karakos:2007:tjr, author =
this  dissertation introduces a new theoretical model for text classification  systems, including systems for document retrieval, automated indexing,  electronic mail filtering, and similar tasks.
{djamel a. zighed and jan komorowski and jan zytkow}, publisher = {springer  verlag, heidelberg, de}, note = {
} @inproceedings{rousu:2005:lhm, author  =
the effectiveness of a classifier that uses supervised learning was analyzed in terms of its accuracy and ultimately its influence on filtering.
we will discuss the statistical and numerical properties of these algorithms, with a focus on text categorization.
these results are compared to an existing operational process using boolean queries manually constructed by domain experts.
(c) we present a new algorithm for semi-supervised learning based on a deterministic annealing (da) approach.
the paper in hand presents results from a user study on web genre usefulness as well as results from the construction of a genre classifier using discriminant analysis, neural network learning, and support vector machines.
booktitle = {proceedings of wcnn-93, world congress on  neural networks}, publisher = {}, editor = {}, year = {1993}, address =
tc has become very important in the information retrieval area, where information needs have tremendously increased with the rapid growth of textual information sources such as the internet.
we  take a radically new stand, and formulate the problem of automated survey  coding as a \emph{text categorization} problem, i.e.\ as the problem of  learning, by means of supervised machine learning techniques, a model of the  association between answers and codes from a training set of pre-coded answers,  and applying the resulting model to the classification of new answers.
categorization by context}, journal =
a central problem in information retrieval is the automated  classification of text documents.
published in the ``lecture notes in  computer science'' series, number 1846}, year = {2000}, address = {shanghai,  cn}, pages = {215--226}, url =  {http://link.springer.de/link/service/series/0558/papers/1846/18460215.pdf},  abstract = {
genre or style, on the other hand, is a different and important property of text, and automatic text genre classification is becoming important for classification and retrieval purposes as well as for some natural language processing research.
an implementation of  the algorithm is described which, starting with a small set of hand-labeled  instances, improves its results automatically via unsupervised training.
our results with the english newswire collection show a very large gain in performance as compared to published benchmarks, while our initial results with the german newswires appear very promising.
this can aid if/ir systems that rely on the acquisition of large numbers of term weights or other measures of relevance.
combining multiple learning strategies for effective  cross-validation}, booktitle = {proceedings of icml-00, 17th international  conference on machine learning}, editor =
we propose to exploit the natural hierarchy of topics, or taxonomy,  that many corpora, such as internet directories, digital libraries, and patent  databases enjoy.
"acm press", url =  "http://doi.acm.org/10.1145/1099554.1099714",
ji he and ah-hwee tan and chew-lim tan}, title = {on machine learning methods for chinese document categorization}, journal = {applied intelligence}, year = {2003}, volume =
the former hints a generative model of news articles, and the latter provides data enriched environments to perform red.
a test of different perspectives based on statistical  distribution divergence", booktitle =
{hull, david a.}, title = {
stanford, us}, pages = {431--438}, publisher = {morgan kaufmann publishers, san francisco, us}, url = {http://www-ai.cs.uni-dortmund.de/dokumente/joachims_00a.pdf}, abstract = {
using the reuters  collection, we show that adaptive resampling techniques can improve  decision-tree performance and that relatively small, pooled local dictionaries  are effective.
{marti a. hearst and fredric gey and richard tong}, publisher = {acm press, new york, us}, address = {berkeley, us}, year = {1999}, pages =
its probabilistic interpretation allows its predictions to be combined in a principled way with information from other sources.
dunja mladeni{\'{c}} and janez brank and marko grobelnik and natasa mili{\'{c}}-frayling}, title = {feature selection using linear classifier weights: interaction with classification models}, booktitle = {proceedings of sigir-04, 27th acm international conference on research and development in information retrieval}, editor =
if the occurrence of a single word determines whether an article belongs  to a category or not (and it often does) any compression scheme will likely  fail to classify the article correctly.
using a representation of the content of web documents that captures these two  characteristics and (2) using more effective classifiers.
{information processing and management}, year = {2006}, volume = {42}, number =  {1}, pages = {155--165}, abstract = {most previous works of feature selection  emphasized only the reduction of high dimensionality of the feature space.
when the data streams are produced in a changing environment the filtering has to adapt too in order to remain effective.
latent semantic indexing (lsi)  has been successfully used for ir purposes as a technique for capturing  semantic relations between terms and inserting them into the similarity measure  between two documents.
{}, url = {}, abstract = {}, } @article{lodhi02, author = {huma lodhi and craig  saunders and john shawe-taylor and nello cristianini and chris watkins}, title  = {
{1980}, number = {6}, pages = {396--402}, volume = {33}, url = {}, abstract =  {}, } @inproceedings{han01, author = {eui-hong han and george karypis and vipin  kumar}, title = {
{morgan kaufmann  publishers, san francisco, us}, pages = {200--209}, url =  {http://www-ai.cs.uni-dortmund.de/dokumente/joachims_99c.ps.gz}, abstract =  {this paper introduces transductive support vector machines (tsvms) for text  classification.
requirements of any such system include speed and minimal end-user effort.
21st acm international  conference on research and development in information retrieval}, editor = {w.  bruce croft and alistair moffat and van rijsbergen, cornelis j. and ross  wilkinson and justin zobel}, publisher = {acm press, new york, us}, year =
the authors' initial work  with this algorithm has demonstrated that probabilistic structures can be  automatically acquired from a training set of documents with respect to a  single target concept, or a set of related concepts.
these knowledge-based categorization methods are more powerful and  accurate than statistical techniques.
the categorization, which consists in assigning an international code of disease (icd) to the medical document under examination, is based on well-known information retrieval techniques.
expnet is used for relevance  ranking of candidate categories of an arbitrary text in the case of text  categorization, and for relevance ranking of documents via categories in the  case of text retrieval.
denotes the task of automatically finding relevant categories for a (new)  document which is to be inserted into a web catalogue like yahoo!.
the user then selects/labels some words from the ranked list for each class.
we tested the two filtering methods on svms as well  as a decision tree algorithm, c4.5.
the kind of application that the text categorization shell, tcs, can produce is characterized.
{morgan kaufmann publishers, san francisco, us}, url =  {http://www-ai.cs.uni-dortmund.de/dokumente/joachims_97a.ps.gz}, abstract =  {the rocchio relevance feedback algorithm is one of the most popular and widely  applied learning methods from information retrieval.
the first set of results applies rankboost to a text representation produced using modern term weighting methods.
we report on experiences with the reuters newswire  benchmark, the us patent database, and web document samples from {{\sc  yahoo!}}\.}, } @inproceedings{wei01, author = {chih-ping wei and yuan-xin  dong}, title = {a mining-based category evolution approach to managing online  document categories},
we adapted several supervised text  categorization methods, specifically several new variants of the k-nearest  neighbor (knn) algorithm and a rocchio approach, to track events.
"proceedings of the  twenty-second aaai conference on artificial intelligence", year =  "2007", month =
{pat langley}, year = {2000}, address = {
ray liere and prasad tadepalli}, title = {active learning with committees for text categorization}, booktitle = {proceedings of aaai-97, 14th
"proceedings of coling 2004", year = 2004, month =
they may not perform effectively in adaptive text filtering which is  a more realistic problem.
the strategy is generic and takes advantage of training errors to successively refine the classification model of a base classifier.
{proceedings of ijcai-97, 15th international joint conference on artificial  intelligence}, editor = {martha e. pollack}, publisher =
finally, the generative model may be used to derive a  fisher kernel expressing similarity between documents.}, } @article{gentili01,  author = {g.l. gentili and mauro marinilli and alessandro micarelli and filippo  sciarrone}, title = {
"acm press", url = "http://doi.acm.org/10.1145/1099554.1099715", abstract =
such a group can be associated with an external ontology.
in this paper a system for analysis and automatic indexing of imaged documents for high-volume applications is described.
the system accurately classifies porn sites from 8 european languages.
we then systematically study the key factors in the can model that can influence the classification performance, and analyze the strengths and weaknesses of the model.} } @article{makkonen:2004:sst, author = {juha makkonen and helena ahonen-myka and marko salmenkivi}, title = {simple semantics in topic detection and tracking}, journal = {information retrieval},
our experiments in a transductive classification setting indicate that accuracy can be significantly improved by modeling relational dependencies.
the  system employs several knowledge sources including a letter database, word  frequency statistics for german, lists of message type specific words,  morphological knowledge as well as the underlying document structure.
advances in web intelligence and data mining}, year =
"jun yan  and ning liu and benyu zhang and shuicheng yan and zheng chen and qiansheng
existing techniques for such "distributional  clustering" of words are agglomerative in nature and result in (i)  sub-optimal word clusters and (ii) high computational cost.
existing techniques for such "distributional clustering" of words are agglomerative in nature and result in (i) sub-optimal word clusters and (ii) high computational cost.
this structure can be  expressed as a sequence of subtopic text blocks, or passages.
while document categorization is quite a  mature, the issue of utilizing hypertext structure and hyperlinks has been  relatively unexplored.
this paper  presents a novel approach for adapting the complexity of a text categorization  system to the difficulty of the task.
by emphasizing the category discrimination capability of features, the paper firstly puts forward a new weighting scheme tf*idf*ig.
survey coding has several applications, especially in the social sciences, ranging from the simple classification of respondents to the extraction of statistics on political opinions, health and lifestyle habits, customer satisfaction, brand fidelity, and patient satisfaction.
we use image features such as percentages of text and non-text (graphics, images, tables, and rulings) content regions, column structures, relative point sizes of fonts, density of content area, and statistics of features of connected components which can be derived without class knowledge.
{2001}, pages = {121--132}, note = {
our results show simple  windows are best for all test collections tested in these experiments.
published in the  ``lecture notes in computer science'' series, number 1923}, year = {2000},  address = {lisbon, pt}, pages = {59--68}, url =  {http://www.math.unipd.it/~fabseb60/publications/ecdl00.pdf}, abstract = {
how  to appropriately represent that information and automatically learn statistical  patterns for solving hypertext classification problems is an open question.
an extended version appears  as~\cite{nigam00}}, url =  {http://www.cs.cmu.edu/~knigam/papers/emcat-aaai98.ps}, abstract = {
"acm press", url =  "http://doi.acm.org/10.1145/1099554.1099734", abstract =
= {fabrizio sebastiani},  title = {a tutorial on automated text categorisation}, booktitle = {proceedings  of asai-99, 1st
"multi-labelled classification using  maximum entropy method", pages =
"comparison of manual and automatic constructions of  category hierarchy for classifying large corpora", booktitle =  "proceedings of conll-2004", year =
learning from both, labeled and unlabeled documents, in a semi-supervised framework is a promising approach to reduce the need for labeled training documents.
mh} and \textsc{adaboost.
systematic experiments and their results are  reported and analyzed.}, } @inproceedings{chuang00, author = {wesley t. chuang  and asok tiyyagura and jihoon yang and giovanni giuffrida}, title = {a fast  algorithm for hierarchical text classification}, booktitle = {proceedings of  dawak-00, 2nd international conference on data warehousing and knowledge  discovery}, editor = {yahiko kambayashi and mukesh mohania and a.min tjoa},  year = {2000}, publisher = {springer verlag, heidelberg, de}, note = {
in the framework of multi-domain text-to-speech synthesis it is essential to (i) design a hierarchically structured database for allowing several domains in the same speech corpus and (ii) include a text classification module that, at run time, assigns the input sentences to a domain or set of domains from the database.
k-nn is one of the most popular document categorization methods because it shows relatively good performance in spite of its simplicity.
{published in the ``lecture notes in computer science'' series, number 2945},  pages = {584--595}, url = {}, abstract = {}, } @inproceedings{yamazaki97,  author = {takefumi yamazaki and ido dagan}, title = {mistake-driven learning  with thesaurus for text categorization}, booktitle = {proceedings of nlprs-97,  the natural language processing pacific rim symposium}, editor = {}, publisher  = {}, address = {phuket, th}, pages = {369--374}, year = {1997}, url =  {ftp://www.links.nectec.or.th/pub/nlprs/paper/dana4r.ps.gz}, abstract = {
these algorithms  both construct classifiers that allow the ``context'' of a word w to affect how  (or even whether) the presence or absence of w will contribute to a  classification.
there is strong empirical and theoretic evidence that combination of retrieval methods can improve performance.
in two different domains, relevancy  signatures produced better results than the simple indexing terms.
"when search results against digital libraries and web resources have limited metadata, augmenting them with meaningful and stable category information can enable better overviews and support user exploration.
it not only approaches and sometimes exceeds svm accuracy, but also beats svm running time by orders of magnitude.
we describe an automatic system that starts with a small sample of the  corpus in which topics have been assigned by hand, and then updates the  database with new documents as the corpus grows, assigning topics to these new  documents with high speed and accuracy.
inbook % = a part of a book, usually  untitled; it may be a chapter % (or other sectional unit) and/or range of pages  % required: author or editor, title, chapter and/or pages, publisher, year %  optional: volume or number, series, type, address, edition, month, note % %
we propose a new on-line learning algorithm, known as  the atf (adaptive text filtering) algorithm, to tackle the adaptive filtering  problem.
the input space was gradually increased by using mutual information (mi) filtering and part-of-speech (pos) filtering, which determine the portion of words that are appropriate for learning from the information-theoretic and the linguistic perspectives, respectively.
the article describes a pilot version of  a commercial application of natural language processing techniques to the  problem of categorizing new stories into broad topic categories.
our contribution is to propose robust statistical models and a relaxation labeling technique for better classification by exploiting link information in a small neighborhood around documents.
"computational linguistics and intelligent text processing", year = "2005" } @inproceedings{anagnostopoulos:2006:eec, author =
the fixed number of  regular words from each class will be used as a feature vectors together with  the reduced principal components from the pca.
hang li and kenji  yamanishi}, title = {
{309--324}, url = {}, abstract = {
this paper proposes and analyzes an efficient and effective approach for estimating the generalization performance of a support vector machine (svm) for text classification.
previous reports indicate that human-engineered rule-based systems, requiring many man-years of developmental efforts, have been successfully built to ``read'' documents and assign topics to them.
this chapter will outline the fundamental traits of the technologies involved,  of the applications that can feasibly be tackled through text classification,  and of the tools and resources that are available to the researcher and  developer wishing to take up these technologies for deploying real-world  applications.}, } @incollection{sebastiani05a, author = {fabrizio sebastiani},  title = {
"salvador, brazil", abstract = "text categorization is an important research area in many information retrieval (ir) applications.
we also investigated  the effect of discarding terms, using either a dynamic stoplist or the winnow  heuristic.}, } @inproceedings{rahal04, author = {imad rahal and william  perrizo}, title = {
published in the ``lecture notes in computer science'' series, number 2997}, pages = {181--196}, url = {http://springerlink.metapress.com/openurl.asp?genre=article&issn=0302-9743&volume=2997&spage=181}, abstract =
this paper explores the use of association rule mining in building a text categorization system and proposes a new fast algorithm for building a text classifier.
document feature characteristics, derived from the training document set, capture some inherent properties of a particular category.
the derived models, inequality me models, in effect have regularized estimation with l1 norm penalties of bounded parameters.
an implementation of the algorithm is described which, starting with a small set of hand-labeled instances, improves its results automatically via unsupervised training.
selforganizing classification on the reuters news corpus}, booktitle = {proceedings of coling-02, the 19th international conference on computational linguistics}, year = {2002}, editor = {}, pages = {}, address = {taipei, tw}, url = {http://www.his.sunderland.ac.uk/ps/coling-232.pdf}, abstract = {
{cambridge university  press}, editor =
text categorization presents unique challenges due to the large number of attributes present in the data set, large number of training samples, attribute dependency, and multi-modality of categories.
an experiment has been carried out to  measure the performance of our proposed hierarchical classification method.
we then introduce an algorithm for  learning from labeled and unlabeled text based on the combination of  expectation-maximization with a naive bayes classifier.
because the analyses of data are generally so  expensive, most parts in databases remains as raw, unanalyzed primary data.
in this paper, an optimized k-nearest neighbors (knn)  classifier that uses intervalization and the p-tree1 technology to achieve a  high degree of accuracy, space utilization and time efficiency is proposed: as  new samples arrive, the classifier finds the k nearest neighbors to the new  sample from the training space without a single database scan.}, }  @inproceedings{raskutti01, author = {bhavani raskutti and herman ferr{\'{a}}  and adam kowalczyk}, title = {
it investigates how these methods combine with various learning models.
abstract =  {}, } @inproceedings{karypis00, author = {
we present very preliminary results of the  application of this model to a standard test collection, evaluating it in  supervised mode in order to facilitate comparison with other methods, and  showing initial results of its use in unsupervised mode.}, }  @proceedings{sahami98a, editor = {mehran sahami}, title = {proceedings of the  1998 workshop on learning for text categorization}, institution = {americal  association for artificial intelligence}, note = {available as technical report  ws-98-05}, address = {
incollection % = a part of a book with its own title % required: author, title,  booktitle, publisher, year % optional: editor, volume or number, series, type,  chapter, pages, % address, edition, month, note
in this paper, we present pva, an adaptive personal view  information agent system for tracking, learning and managing user interests in  internet documents.
using  hcl, a hierarchical classification method can be materialized easily with the  help of a method generator system.}, } @inproceedings{sun03b, author = {
evaluating text categorization in the presence of ocr errors}, booktitle = {proceedings of the 8th spie conference on document recognition and retrieval}, editor = {paul b. kantor and daniel p. lopresti and jiangying zhou}, year = {2000}, address = {san jose, us}, pages = {68--74}, publisher =
in two experiments, the research method of information integration theory was employed in order to test two hypotheses relating to the radical conventionalist and traditional positions on the role of specific formal textual features in the categorization of poetic texts.
"proceedings of the twelfth acm sigkdd international conference on knowledge discovery and data mining", year =
{mclean, us}, editor = {arvin agah and jamie callan and elke rundensteiner},  year = {2000}, pages = {86--93}, url =  {http://www.cs.cmu.edu/~knigam/papers/cotrain-cikm00.pdf}, abstract = {
"proceedings of the 28th european conference on ir research (ecir)", year =
this system worked very well for language classification, achieving in one test a 99.8\% correct classification rate on usenet newsgroup articles written in different languages.
we review some of the variations of naive bayes models used for text retrieval and classification, focusing on the distributional assumptions made about word occurrences in documents.}, } @inproceedings{lewis99, author = {lewis, david d. and daniel l. stern and amit singhal}, title = {{\sc attics}: a software platform for on-line text classification}, booktitle = {proceedings of sigir-99, 22nd
previously, documents have been classified according to their contents manually.
it is shown to be  more robust with respect to the training set size and to improve the  performance both for ranking and classification, specially for classes with few  training examples.}, } @inproceedings{denoyer03, author = {ludovic denoyer and  patrick gallinari}, title = {
our results show that naive bayes is a weak choice for guiding a topical crawler when compared with support vector machine or neural network.
{843--873}, year = {2001}, volume = {15}, number = {9}, url = {}, abstract =  {the volume of electronically stored information increases exponentially as the  state of the art progresses.
the structure of the web is increasingly being used to improve organization, search, and analysis of information on the web.
the algorithms we present are simple to implement and are time and memory  efficient.
we call this idea \emph{supervised term weighting} (stw).
our feature selection method strives to reduce redundancy between features while maintaining information gain in selecting appropriate features for text categorization.
using {wordnet} to complement training information in text categorization}, booktitle = {proceedings of ranlp-97, 2nd international conference on recent advances in natural language processing}, publisher = {}, editor = {ruslan milkov and nicolas nicolov and nilokai nikolov}, address = {tzigov chark, bl}, pages = {}, year = {1997}, url = {http://xxx.unizar.es/ps/cmp-lg/9709007}, abstract = {
"cohen, william  w. and carvalho, vitor r. and mitchell, tom m.", title =
in this paper we present a hybrid text categorization  method based on rough sets theory.
{storer, james a. and cohn, martin}, publisher =
{320--334}, url = {http://www.math.unipd.it/~fabseb60/publications/ecir03.pdf}, abstract = {we focus on two recently proposed algorithms in the family of ``boosting''-based learners for automated text classification, \textsc{adaboost.
the scheme for automatic text classification proposed in the paper, is based on document indexing, where a document is represented as a list of keywords.
in this paper, we do some explorations on both  directions based on the following two characteristics of news articles.
"we describe the experience and lessons learned from developing a range of electronic services for a specialist engineering company.
we present experimental results  showing that employing our active learning method can significantly reduce the  need for labeled training instances in both the standard inductive and  transductive settings.}, } @inproceedings{tong92, author = {richard tong and  adam winkler and pamela gage}, title = {classification trees for document  routing: a report on the trec experiment}, booktitle = {proceedings of trec-1,  1st
this means that two (or several) different techniques are used to optimize the performances even if a single algorithm may have more chances to operate the right choices.
the experimental results suggest that the bigrams can substantially raise the quality of feature sets, showing increases in the break-even points and f1 measures.
in this study, we proposed a mining-based category  evolution (mice) technique to adjust document categories based on existing  categories and their associated documents.
to evaluate classifiers in our multipath framework, we define a new hierarchical loss function, the h-loss, capturing the intuition that whenever a classification mistake is made on a node of the taxonomy, then no loss should be charged for any additional mistake occurring in the subtree of that node.
in these  applications, several classes of documents are involved.
{acm press, new york, us}, booktitle = {proceedings of the 1st international  conference on autonomous agents}, address = {marina del rey, us}, year =
{jamie callan and gordon cormack and charles clarke and david hawking and alan  smeaton}, publisher = {acm press, new york, us}, address = {toronto, ca}, year  = {2003}, pages = {190--197}, url = {http://doi.acm.org/10.1145/860435.860471},  abstract = {real-world applications often require the classification of  documents under situations of small number of features, mis-labeled documents  and rare positive examples.
the svm approach as  represented by schoelkopf was superior to all the methods except the neural  network one, where it was, although occasionally worse, essentially comparable.
we show that machine-generated  decision rules appear comparable to human performance, while using the  identical rule-based representation.
extensive experimentation on  representative classifiers, rocchio and svm, as well as a careful analysis of  the literature have been carried out to study how some nlp techniques used for  indexing impact tc.
here we show how the classification accuracy of  foil on this task can be improved by discovering additional regularities on the  test set pages that must be classified.
a major application of air/x is the air/phys system developed for a large physics database.
these feature vectors are then  used as the input to the neural networks for classification.
its main originality is its ability to simultaneously take into account the structural and the content information present in a structured document, and also to cope with different types of content (text, image, etc).
since the single layers of  self-organizing maps represent different aspects of the document collection at  different levels of detail, the neural network shows the document collection in  a form comparable to an atlas where the user may easily select the most  appropriate degree of granularity depending on the actual focus of interest  during the exploration of the document collection.}, } @inproceedings{meyer04,  author = {
the category discrimination method (cdm) is a new machine  learning algorithm designed specifically for text categorization.
using the reuters-21578 corpus, we obtain an  improvement in running time of over a factor of three and a 5\% improvement in  f-measure.}, } @inproceedings{dalessio98, author = {stephen d'alessio and  keitha murray and robert schiaffino and aaron kershenbaum}, title = {category  levels in hierarchical text categorization}, booktitle = {proceedings of  emnlp-98, 3rd conference on empirical methods in natural language processing},  year = {1998}, publisher = {association for computational linguistics,  morristown, us}, editor = {}, pages = {}, address = {granada, es}, url =  {http://www.iona.edu/cs/facultypublications/emnlpf.pdf}, abstract = {
traditionally, the categories are arranged in hierarchical manner to achieve  effective searching and indexing, as well as easy comprehension for humans.
mathematically, the feature quantity is defined as a product of probabillty and information, and maintains a good correspondence with the tfidf-like measures popularly used in today's ir systems.
grenoble, fr}, pages = {333--342}, year = {1988}, note = {
in this  article, an approach based on unknown words is proposed for meaningful term  extraction and discriminative term selection in text categorization.
international joint conference on artificial intelligence}, editor = {chris e. mellish}, publisher = {morgan kaufmann publishers, san francisco, us}, year = {1995}, address = {montreal, ca}, pages = {1322--1327}, url = {}, abstract = {
feature engineering for a symbolic approach to text  classification}, school = {computer science department, university of ottawa},  address = {
however, centralized  classification approaches often are limited due to constraints on knowledge and  computing resources.
such systems typically have to cope with sets of rectors of many tens of  thousands of dimensions.
support vector machines (svms) excel in binary classification, but  the elegant theory behind large-margin hyperplane cannot be easily extended to  multi-class text classification.
{hershey, us}, pages = {78--102}, url = {http://www.math.unipd.it/~fabseb60/publications/td01a.pdf}, abstract = {
"d. tikk and {gy.} biro and j. d. yang", title =  "experiments with a hierarchical text categorization method on {wipo}  patent collections", booktitle =
candidate feature subsets are evaluated by using three-layer feedforward neural networks.
the system is composed of two  main modules: the central indexer (extraction and weighting of indexing terms)  and the classifier (classification of business letters into given types).
both  algorithms have been among the best performers in text categorization  experiments so far.
using kullback-leibler distance for text categorization}, booktitle = {proceedings of ecir-03, 25th european conference on information retrieval}, publisher = {springer verlag}, editor = {fabrizio sebastiani}, address = {
using  this method, we achieved high performance in text categorization both with  small number and large numbers of labeled data.}, } @inproceedings{tan01,  author = {
the text categorization system teklis at  {trec-6}}, booktitle = {proceedings of trec-6, 6th text retrieval conference},  publisher = {national institute of standards and technology, gaithersburg, us},  editor = {
{carl l. sable and vasileios  hatzivassiloglou}, title = {text-based approaches for the categorization of  images}, booktitle = {proceedings of ecdl-99, 3rd european conference on  research and advanced technology for digital libraries}, editor
address = {basel, ch}, url = {http://airone.fub.it:8080/projects/pakm96.ps}, abstract = {with the development and diffusion of the internet worldwide connection, a large amount of information can be delivered to the users.
we show that for problems plagued with numerous  redundant features the performance of c4.5 is significantly superior to that of  svm, while aggressive feature selection allows svm to beat c4.5 by a narrow  margin.}, } @inproceedings{galavotti00, author = {luigi galavotti and fabrizio  sebastiani and maria simi}, title = {experiments on the use of feature  selection and negative evidence in automated text categorization}, booktitle =  {proceedings of ecdl-00, 4th european conference on research and advanced  technology for digital libraries}, editor = {jos{\'e} l. borbinha and thomas  baker}, publisher = {springer verlag, heidelberg, de}, note = {
kalervo j{\"{a}}rvelin and james allan and peter bruza and mark sanderson}, publisher
text  categorization for a comprehensive time-dependent benchmark}, journal =
previous reports indicate that human-engineered rule-based systems, requiring  many man-years of developmental efforts, have been successfully built to  ``read'' documents and assign topics to them.
k-nnfp is similar to k-nn except it finds the nearest neighbors according to each feature separately.
the best variant, which we call lazyboosting, is  tested on the largest sense-tagged corpus available containing 192,800 examples  of the 191 most frequent and ambiguous english words.
we introduce three basic types (namely a type for text, one for words  and one for positions in texts) and three simple predicate definitions over  these types which enable us to write tc and ie rules as logic programs.
"support vector machines (svms) have been very  successful in text classification.
{tampere, fi}, year = {2002}, pages = {151--158}, url =  {http://doi.acm.org/10.1145/564376.564404}, abstract = {
our experiments are conducted in the text categorization domain, which is characterized by a large number of features, many of which are irrelevant.
in particular, enhanced versions of the rocchio text classifier, characterized by high performance, have been proposed.
one important issue in a  large-scale meta-search engine is to select text databases that are likely to  contain useful documents for a given query.
the paper describes the novel  technique of categorization by context, which instead extracts useful  information for classifying a document from the context where a url referring  to it appears.
by using generalized singular value decomposition (gsvd), a coordinate transformation that reflects the inherent class structure indicated by the generalized singular values is identified.
this model is naturally well-suited to clustering documents in preset or automatically generated hierarchies, as well as categorising new documents in an existing hierarchy.
for illustration we give a brief description of the  content-based personal intelligent agent named personal webwatcher that uses  text-learning for user customized web browsing.}, } @inproceedings{mladenic99a,  author = {dunja mladeni{\'{c}} and marko grobelnik}, title = {feature selection  for unbalanced class distribution and naive bayes}, booktitle = {proceedings of  icml-99, 16th international conference on machine learning}, editor = {ivan  bratko and saso dzeroski}, year = {1999}, address = {bled, sl}, pages =
several studies on improving accuracy of fast but  less accurate classifiers have been recently carried out.
with the former, we provide an enhanced document representation  that incorporates the structural and heterogeneous nature of web documents.
{139--145}, url = {http://www.acm.org/pubs/articles/proceedings/dl/313238/p139-lim/p139-lim.pdf}, abstract = {automatic categorization of multimedia documents is an important function for a digital library system.
"2005" }  @inproceedings{xia:2005:fae, author =
then, we show that feature selection speeds up  the time required to automatically build the categorization system.}, }  @inproceedings{myers00, author = {kary myers and michael kearns and satinder  singh and marilyn a. walker}, title = {
boosting trees for  anti-spam email filtering}, year = {2001}, editor = {}, booktitle =
the concept learning model emphasizes the role manual and automated feature selection and classifier formation in text classification.
{association for computational linguistics, morristown, us}, editor = {claire  cardie and ralph weischedel}, year = {1997}, address = {providence, us}, pages  = {55--63}, url = {http://l2r.cs.uiuc.edu/~danr/papers/categ.ps.gz}, abstract =  {learning problems in the text processing domain often map the text to a space  whose dimensions are the measured features of the text, e.g., its words.
it is possible to  create additional hierarchy among the categories.
previously, research has focused predominantly on developing or adopting  statistical classification or inductive learning methods for automatically  discovering text categorization patterns for a pre-defined set of categories.
the experiments of  the categorization of news articles show that the proposed schemes of text  categorization outperform the schemes with crisp sets.}, } @incollection{jo99a,  author = {taeho c. jo}, title = {news article classification based on  categorical points from keywords in backdata}, booktitle = {computational  intelligence for modelling, control and automation}, editor = {
our experiments show that our system has low overhead and achieves high classification ac-curacy across a variety of databases.}, } @inproceedings{ittner95, author = {david j. ittner and lewis, david d. and david d. ahn}, title = {
our theoretical results are complemented by a number of experiments on texual corpora.
text classification by  a neural network}, booktitle = {proceedings of the 23rd annual summer computer  simulation conference}, editor = {}, publisher = {}, address = {baltimore, us},  pages = {313--318}, year = {1991}, url = {}, abstract = {when banks process  their free-form telex traffic, the first task is the classification of the  telexes.
the euratom automatic indexing project}, booktitle = {proceedings of the ifip congress (booklet j)}, publisher = {}, editor = {}, year = {1968}, address = {edinburgh, uk}, pages = {66--70}, url = {}, abstract = {}, } @inproceedings{fangmeyer70, author
the combination of text classifiers using  reliability indicators}, journal = {information retrieval}, number = {1},  volume = {8}, pages = {67--100}, year = {2005}, url =  {http://www.kluweronline.com/issn/1386-4564}, abstract = {the intuition that  different text classifiers behave in qualitatively different ways has long  motivated attempts to build a better metaclassifier via some combination of  classifiers.
to remedy the drawback, we employ concept-based thesauri in the categorization.
in a batch mode, the programs to accomplish this indexing would require no more than fifteen minutes of cpu time per week.}, } @article{klingbiel73a, author = {paul h. klingbiel}, title = {a technique for machine-aided indexing}, journal = {information storage and retrieval}, year = {1973}, volume = {9}, number = {9}, pages = {477--494}, url = {}, abstract = {subject indexing of text can, in principle, be accomplished in many ways.
we  describe a method for improving the classification of short text strings using  a combination of labeled training data plus a secondary corpus of unlabeled but  related longer documents.
taken as a whole, the set of web pages lacks a unifying structure and shows far more authoring style and content variation than that seen in traditional text-document collections.
this tight packaging of word pairs could bring in some semantic  value.
in this paper we study the use of a semi-supervised agglomerative  hierarchical clustering (ssahc) algorithm to text categorization, which  consists of assigning text documents to predefined categories.
= {an extended version appears as~\cite{craven00}}, url = {http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/www/wwkb/overview-aaai98.ps.gz}, abstract = {the world wide web is a vast source of information accessible to computers, but understandable only to humans.
{ieee computer society press, los alamitos, us}, } @inproceedings{theeramunkong02, author = {thanaruk theeramunkong and verayuth lertnattee}, title = {multi-dimensional text classification}, booktitle = {proceedings of coling-02, the 19th international conference on computational linguistics}, year = {2002}, editor = {}, pages = {}, address = {taipei, tw}, url = {http://acl.ldc.upenn.edu/coling2002/proceedings/data/area-28/co-399.pdf}, abstract =
"2006", pages = "157--166", } @inproceedings{joachims:2006:tls, author =
in this paper, we propose a simple generalization of svm:  weighted margin svm (wmsvms) that permits the incorporation of prior knowledge.
in this paper, we show how  an operator-based view of rule induction enables the easy integration of a  thesaurus as background knowledge.
{283--290}, year = {2000}, address = {hong kong, cn}, publisher = {ieee computer society press, los alamitos, us}, volume = {1}, url = {http://panda.cs.binghamton.edu/~meng/pub.d/wise00.doc}, abstract = {document categorization, as a technique to improve the retrieval of useful documents, has been extensively investigated.
we show that our  algorithm minimizes the "within-cluster jensen-shannon divergence"  while simultaneously maximizing the "between-cluster jensen-shannon  divergence".
we also observe that despite  similar performances, different topical crawlers cover subspaces on the web  with low overlap.} } @article{peng:2004:anb, author = {fuchun peng and dale  schuurmans and shaojun wang}, title = {
we propose a new method of classifying documents into categories.
first, we train the linear svm on a subset of training data and retain only those features that correspond to highly weighted components (in absolute value sense) of the normal to the resulting hyperplane that separates positive and negative examples.
norbert g{\"{o}}vert and mounia lalmas and norbert fuhr}, title = {a probabilistic description-oriented approach for categorising web documents}, booktitle = {proceedings of cikm-99, 8th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, editor = {}, year = {1999}, address
a major difficulty of this problem stems from the  high dimensionality of its feature space.
little words can make a big difference for text classification},
"hierarchical topic segmentation of websites", booktitle =
how is this related to the statistical properties of text?
{78--85}, url =  {http://www.math.unipd.it/~fabseb60/publications/cikm00.pdf}, abstract = {
the mit press}, address
the method avoids two main problems with existing work in inductive transfer: scalability and the risk of negative transfer.
those pages are implicitly linked.
{174--181}, url = {http://doi.acm.org/10.1145/860435.860469}, abstract = {a novel maximal figure-of-merit (mfom) learning approach to text categorization is proposed.
furthermore, based on this approach, we build an interactive red system, hiscovery, which provides additional functions to present events, photo story and chronicle."
in addition, the training time and scaling are  also important concerns.
in this paper we present a learning system for information  filtering and selective information dissemination.
improving text categorization using the importance of  sentences}, journal = {information processing and management}, year = {2004},  volume = {40}, number = {1}, pages = {65--79}, url = {}, abstract = {}, }  @article{ko04a, author
"we present a principled methodology for filtering news stories by formal measures of information novelty, and show how the techniques can be used to custom-tailor newsfeeds based on information that a user has already reviewed.
results of other issues related to building an effective personal e-mail classifier are presented and discussed.
context, as it applies to document similarity,  can be accommodated by a well-defined procedure.
the authors' approach has been to use statistics in the  knowledge acquisition component of a linguistic pattern-based categorization  system, using statistical methods, for example, to associate words with  industries and identify phrases that information about businesses or products.
{1980}, number = {6}, pages = {396--402}, volume = {33}, url = {}, abstract = {}, } @inproceedings{han01, author = {eui-hong han and george karypis and vipin kumar}, title =
the key issue of the approach is how to obtain a set of  representative words for each class.
we present a new method for sentiment  classification based on extracting and analyzing appraisal groups such as  ``very good'' or ``not terribly funny''.
analysis and empirical evidence suggest that the evaluation results on some versions of reuters were significantly affected by the inclusion of a large portion of unlabelled documents, making those results difficult to interpret and leading to considerable confusions in the literature.
"el{\'i}as f. combarro and elena  monta{\~n}{\'e}s and irene d{\'i}az and jos{\'e} ranilla and ricardo  mones", journal = "ieee trans. knowl.
term selection according to this criterium  is performed by the elimination of noisy terms on a class-by-class basis,  rather than by selecting the most significant ones.
zelikovitz, sarah and cohen, william w.  and hirsh, haym}, title = {
the approach is compared to other attempts at homograph disambiguation using both machine readable dictionaries and unrestricted text and the use of training instances is determined to be a crucial difference.}, } @proceedings{hearst96a, editor =
after training, the incoming news articles are classified based on their similarity to the existing newsgroup categories.
when a natural split does not  exist, co-training algorithms that manufacture a feature split may outperform  algorithms not using a split.
jochen d{\"o}rre and  peter gerstl and roland seiffert}, title = {
in the flat non-hierarchical case, a  model distinguishes a second-level category from all other second-level  categories.
{51--59}, url =  {http://www.usenix.org/publications/library/proceedings/sec02/liao.html},  abstract = {
for second one, the standard reuters benchmark, svm classifier using augmentation with pairs outperforms all previously reported results.}, } @inproceedings{rau91, author = {
the automatic indexing system {air/phys}.
learning question classifiers}, booktitle = {proceedings of coling-02, 19th international conference on computational linguistics}, editor = {}, publisher = {}, address = {taipei, tw}, url = {http://l2r.cs.uiuc.edu/~danr/papers/qc-coling02.pdf}, year = {2002}, abstract = {
{4}, pages = {69--83}, url = {http://link.springer-ny.com/link/service/journals/10032/papers/1004002/10040069.pdf},
for indirect comparisons, knn, llsf and word  were used as baselines, since they were evaluated on all versions of reuters  that exclude the unlabelled documents.
phase ii consists of two stages.
experimental data is presented showing widrow-hoff and eg to be more effective  than the widely used rocchio algorithm on several categorization and routing  tasks.}, } @misc{lewis97a, author = {lewis, david d.}, title = {reuters-21578  text categorization test collection.
we have empirically demonstrated that  rule-based methods like ours result in high classification accuracy when the  categories to which texts are to be assigned are relatively specific ones and  when the texts tend to be short.
text categorization based on {k}-nearest neighbor approach for web  site classification}, journal = {information processing and management}, year =
in  this paper we propose instead that learning from training data should also  affect phase (ii), i.e.\ that information on the membership of training  documents to categories be used to determine term weights.
although such learning models succeed in exploiting relational knowledge, they are highly demanding in terms of labeled examples, because the number of categories is related to the dimension of the corresponding hierarchy.
"collective multi-label classification",  booktitle =
{halifax, ca}, year = {2003}, pages = {505--509}, url = {}, abstract = {}, }  @inproceedings{spitz00, author = {larry spitz and arman maghbouleh}, title =
ron bekkerman and ran el-yaniv and naftali tishby and yoad winter}, title = {
the algorithm checks the context  surrounding the target noun against that of previously observed instances and  chooses the sense for which the most evidence is found, where evidence consists  of a set of orthographic, syntactic, and lexical features.
using k-nn, naive bayes and centroid-based classifiers, the experimental results show that the multi-dimensional-based and hierarchical-based classification performs better than the flat-based classifications.}, } @inproceedings{thompson01, author = {paul thompson}, title = {automatic categorization of case law}, booktitle = {proceedings of icail-01, 8th international conference on artificial intelligence and law}, editor = {}, year = {2001}, address = {st.\ louis, us}, pages = {70--77}, publisher = {acm press, new york, us}, url = {http://doi.acm.org/10.1145/383535.383543}, abstract =
in the hierarchical  case, a model is learned to distinguish a second-level category from other  categories within the same top level.
{12}, pages = {1269--1277}, url =  {http://www.math.unipd.it/~fabseb60/publications/jasist03.pdf}, abstract =  {\emph{survey coding} is the task of assigning a symbolic code from a  predefined set of such codes to the answer given in response to an open-ended  question in a questionnaire (aka \emph{survey}).
proceedings of cicling-04, 5th international conference on computational linguistics and intelligent text processing},
these are based on ranking text sequences by their cross-entropy calculated using a fixed order character-based markov model adapted from the ppm text compression algorithm.
the architecture, modules, and  practical results are described.}, } @article{manevitz01, author = {larry m.  manevitz and malik yousef}, title = {one-class {svms} for document  classification}, journal = {journal of machine learning research}, volume =
stanford, us}, pages = {895--902}, publisher = {morgan kaufmann publishers,  san francisco, us}, url = {http://www.cs.cmu.edu/~sean/papers/icml2000.ps},  abstract = {machine learning typically involves discovering regularities in a  training set, then applying these learned regularities to classify objects in a  test set.
the biological literature presents a  difficult challenge to information processing in its complexity, diversity, and  in its sheer volume.
we propose that two machine learning algorithms, the widrow-hoff and eg algorithms, be used in training linear text classifiers.
we evaluate different measures of similarity -- five  derived from the citation information of the collection, and three derived from  the structural content -- and determine how they can be fused to improve  classification effectiveness.
we introduce a novel hybrid system specifically designed for multi-page text documents.
we use a training set of texts with expert assigned categories to construct a network which approximately reflects the conditional probabilities of categories given a text.
a stochastic  decision list is an ordered sequence of if-then-else rules, and our method can  be viewed as a rule-based method for text classification having advantages of  readability and refinability of acquired knowledge.
the svm approach as represented by schoelkopf was superior to all the methods except the neural network one, where it was, although occasionally worse, essentially comparable.
{420--435}, url = {http://link.springer.de/link/service/series/0558/papers/2633/26330420.pdf},
"august", address = "bonn, germany", url = "http://www.machinelearning.org/proceedings/icml2005/papers/053_generalizedlars_keerthi.pdf", abstract =
"proceedings of the 29th european conference on  information retrieval", year =
ron bekkerman and ran el-yaniv and  naftali tishby and yoad winter}, title = {
part ii: additional experiments}, journal =
we present a system for searching and classifying u.s. patent documents, based on inquery.
we improve a high-accuracy maximum entropy classifier by combining an ensemble of classifiers with neural network voting.
using the same representation of documents, charade offers better performance than earlier reported experiments with decision trees on the same corpus.
pisa, it}, year = {2003}, pages = {420--435},  url =  {http://link.springer.de/link/service/series/0558/papers/2633/26330420.pdf},  abstract = {current trend in operational text categorization is the designing  of fast classification tools.
a hierarchical classification is  obtained by evaluating the trained node classifiers in a top-down fashion.
the categorization algorithm used is a supervised learning procedure  that uses a linear classifier based on the category levels.
however, in carefully controlled experiments using  syntactic phrases produced by church's stochastic bracketer, in conjunction  with reciprocal nearest neighbor clustering, term clustering was found to  produce essentially no improvement in the properties of the phrasal  representation.
{montreal, ca}, pages = {137--144}, url =  {http://dlib.computer.org/conferen/ideas/0265/pdf/02650137.pdf}, abstract = {
the efficiency, effectiveness, and noise tolerance of this search strategy were  confirmed to be better than those of a full search, a category based search,  and a cluster based search with nonprobabilistic clustering.}, }  @inproceedings{iwayama95a, author = {makoto iwayama and takenobu tokunaga},  title = {hierarchical bayesian clustering for automatic text classification},  booktitle = {proceedings of ijcai-95, 14th international joint conference on  artificial intelligence}, editor =
in some applications, there might be human knowledge available that, in principle, could compensate for the lack of data.
in this paper, we propose to select relevant features by means of a family of linear filtering measures which are simpler than the usual measures applied for this purpose.
"text categorization, which consists of automatically assigning documents to a set of categories, usually involves the management of a huge number of features.
this disser- tation  demonstrates that supervised learning algorithms that use a small number of  labeled examples and many inexpensive unlabeled examples can create  high-accuracy text classifiers.
"2006", pages =  "505--509" } @inproceedings{bouma:2006:sht, author =
{tampere, fi}, year = {2002}, pages = {145--150}, url =  {http://doi.acm.org/10.1145/564376.564403}, abstract = {subject or  prepositional content has been the focus of most classification research.
improving text categorization  methods for event tracking}, booktitle = {proceedings of sigir-00, 23rd acm  international conference on research and development in information retrieval},  editor = {nicholas j. belkin and peter ingwersen and mun-kew leong}, publisher  = {acm press, new york, us}, address = {athens, gr}, year = {2000}, pages =
booktitle = {proceedings of ecir-03, 25th european conference on information retrieval}, publisher = {springer verlag, heidelberg, de}, editor = {fabrizio sebastiani}, address = {pisa, it}, year = {2003}, pages = {161--176}, url = {http://www.cs.kun.nl/~kees/peking/ecir03.pdf}, abstract = {
{journal of universal computer science}, year = {1998}, number = {9}, volume =
categorizing documents with the aid of knowledge-based features leverages  information that cannot be deduced from the documents alone.
we have manually selected the most regular words that exist in each  class and weighted them using an entropy weighting scheme.
empirical results indicate that these classifiers are comparable with the best text classification systems.
only relatively recently did detailed studies on the impact of various document representations step into the spotlight, showing that there may be statistically significant differences in classifier performance even among variations of the classical bag-of-words model.
class labels can also be assigned based on known document types, or can be defined by the user.
the proposed thresholding strategy is parameter free, relying on a process of retrofitting and cross validation to set algorithm parameters empirically, whereas our previous approach required the specification of two parameters (beta and gamma).
mathematically, the feature quantity is  defined as a product of probabillty and information, and maintains a good  correspondence with the tfidf-like measures popularly used in today's ir  systems.
in our method, independent components of document  vectors are extracted using ica and concatenated with the original vectors.
"text  categorization using bibliographic records: beyond document content",  booktitle =
{proceedings of ecir-02, 24th european colloquium on information retrieval research}, editor =
coupled with the ability  to generalize concepts using the ontology, this approach addresses the two main  problems of natural language processing—synonymy and polysemy.
in many settings, we also have the option of using  pool-based active learning.
kyoto, jp}, year = {1994}, pages =
} @article{kyber03, author =
{sofus a. macskassy and haym hirsh},  title = {
the corresponding classifier parameters are learned by optimizing an overall objective function of interest.
experiments are conducted to  compare our multi-agent approach with a centralized approach.
the proposed learning model is the core of a prototype information filtering system called profile.}, } @inproceedings{androutsopoulos00, author = {ion androutsopoulos and john koutsias and konstandinos v. chandrinos and constantine d. spyropoulos}, title = {
= {{\sc construe/tis}: a system for  content-based indexing of a database of news stories}, booktitle = {proceedings  of iaai-90, 2nd conference on innovative applications of artificial  intelligence}, publisher = {aaai press, menlo park, us}, editor = {alain  rappaport and reid smith}, year = {1990}, pages = {49--66}, url = {}, abstract  = {}, address = {boston, us}, } @inproceedings{hayes90a, author = {
{fuhr, norbert and hartmann,  stephan and knorz, gerhard and lustig, gerhard and schwantner, michael and  tzeras, konstadinos}, title = {{air/x} -- a rule-based multistage indexing  system for large subject fields}, booktitle = {proceedings of riao-91, 3rd  international conference ``recherche d'information assistee par ordinateur''},  publisher = {elsevier science publishers, amsterdam, nl}, editor = {andr{\'e}  lichnerowicz}, address = {barcelona, es}, year = {1991}, pages = {606--623},  url = {http://www.darmstadt.gmd.de/~tzeras/fullpapers/gz/fuhr-etal-91.ps.gz},  abstract = {air/x is a rule-based system for indexing with terms (descriptors)
the results that can be obtained  in this space are satisfactory with respect to the best state-of-the-art  performances.}, } @inproceedings{dorre99, author = {
we found that lr performs strongly and robustly in optimizing  t11su (a trec utility function) while rocchio is better for optimizing ctrk  (the tdt tracking cost), a high-recall oriented objective function.
generally, filtering systems calculate  the similarity between the profile and each incoming document, and retrieve  documents with similarity higher than a threshold.
{donna k. harman}, year = {1992},  address = {gaithersburg, us}, pages =
the experimental evaluation demonstrates that the wpcm method provides acceptable classification accuracy with the sports news datasets.}, } @inproceedings{sevillano04, author = {sevillano dominguez, xavier and alias pujol, francesc and socoro carrie, joan c.}, title = {ica-based hierarchical text classification for multi-domain text-to-speech synthesis}, booktitle = {proceedings of icassp-04, proceedings of the 29th ieee international conference on acoustics, speech, and signal processing}, editor = {}, publisher = {ieee computer society press, los alamitos, us}, address = {
we report the results obtained using the mean reciprocal  rank as a measure of overall performance, a commonly used evaluation measure  for question answering tasks.
further, we propose to use a special kernel function called the tree kernel to enable the svm to take advantage of the syntactic structures of questions.
the  effectiveness of a classifier that uses supervised learning was analyzed in  terms of its accuracy and ultimately its influence on filtering.
it also suggests improvements which lead to a probabilistic variant of the rocchio classifier.
we address the problem of evaluating the effectiveness of summarization techniques for the task of document categorization.
we define parameters of categories that make it possible to  acquire numerous datasets with desired properties, which in turn allow better  control over categorization experiments.
we designed a special evolutionary algorithm with a two-pool strategy  for this changing environment.}, } @inproceedings{tauritz99, author = {
one important aspect of text mining is on automatic  text categorization, which assigns a text document to some predefined category  if the document falls into the theme of the category.
as the amount of data stored in storage media is increased  exponentially, it becomes necessary to store documents according to their  category, to access them easily.
"a comparison and semi-quantitative analysis of  words and character-bigrams as features in chinese text categorization",  booktitle =
"comparison of manual and automatic constructions of category hierarchy for classifying large corpora", booktitle = "proceedings of conll-2004", year =
in this chapter, we therefore propose a specification  language known as hcl (hierarchical classification language).
because  of efficiency, the latter is more suitable for text data such as web documents.
"determining term subjectivity and term orientation for opinion mining", booktitle =
{acm press, new york, us}, address = {grenoble, fr}, pages = {333--342}, year =
}  @inproceedings{raghavan:2005:ifs, author =
published in the ``lecture notes in computer science''  series, number 2997}, pages = {112--126}, url =  {http://springerlink.metapress.com/openurl.asp?genre=article&issn=0302-9743&volume=2997&spage=112},  abstract = {
{hierarchical text classification and evaluation}, booktitle = {proceedings of  icdm-01, ieee international conference on data mining}, publisher = {ieee  computer society press, los alamitos, us}, editor = {
the stepwise feature selection in  the decision tree algorithm is particularly effective in dealing with the large  feature sets common in text categorization.
preliminary experimental results are provided  to compare the proposed database categorization algorithms.}, }  @inproceedings{wang01, author = {ke wang and senquiang zhou and yu
however, as documents accumulate, such categories may not capture a document's  characteristics correctly.
in  order to explicitly capture the optimality of word clusters in an information  theoretic framework, we first derive a global criterion for feature clustering.
this method can be applied in a  post-processing step and therefore be combined with other known  (non-hierarchical) categorization approaches.}, } @inproceedings{fuhr84, author  = {fuhr, norbert and knorz, gerhard}, title = {retrieval test evaluation of a  rule-based automated indexing {(air/phys)}}, booktitle = {proceedings of  sigir-84, 7th acm international conference on research and development in  information retrieval}, year = {1984}, publisher =
we investigate four different methods for document classification: the naive bayes classifier, the nearest neighbour classifier, decision trees and a subspace method.
the question addressed in this paper is to find a bidimensional  representation of textual documents for the problem of text categorisation.
by providing a  formal analysis of the computational complexity of each classification method,  followed by an investigation on the usage of different classifiers in a  hierarchical setting of categorization, we show how the scalability of a method  depends on the topology of the hierarchy and the category distributions.
"we demonstrate  the value of using context in a new-information detection system that achieved  the highest precision scores at the text retrieval conference's novelty track  in 2004.
this  paper extends the mistake-driven learner winnow to better utilize thesauri for  text categorization.
text  categorization is the problem of automatically assigning predefined categories  to natural language texts.
morgan kaufmann publishers, san francisco, us}, url =  {http://www.cs.cornell.edu/people/tj/publications/joachims_etal_01a.pdf},  abstract = {
benchmark experiments showed that their predictive performance were  roughly comparable, especially on clean and well organized data sets.
for example, we have previously shown how foil, a relational learner, can learn to classify web pages by discovering training set regularities in the words occurring on target pages, and on other pages related by hyperlinks.
text categorization is the natural consequence of such automatic category generation process.}, } @inproceedings{yang00c, author = {hsin-chang yang and chung-hong lee}, title = {automatic category structure generation and categorization of chinese text documents}, booktitle = {proceedings of pkdd-00, 4th european conference on principles of data mining and knowledge discovery}, editor =
in general, the disambiguation rules differ  for different words.
= {sarah zelikovitz and haym hirsh}, title = {
on the web, category-based portals such as yahoo!
ron bekkerman and ran el-yaniv and naftali tishby and yoad winter}, title = {distributional word clusters vs.\ words for text categorization}, journal = {journal of machine learning research}, volume = {3}, pages = {1183--1208}, year = {2003}, url = {http://www.jmlr.org/papers/volume3/bekkerman03a/bekkerman03a.pdf}, abstract = {we study an approach to text categorization that combines distributional clustering of words and a support vector machine (svm) classifier.
the goal of the work described here is to automatically categorize web documents in order to enable effective retrieval of web information.
it is demonstrated that the probabilistic  corpus model which emerges from the automatic or unsupervised hierarchical  organisation of a document collection can be further exploited to create a  kernel which boosts the performance of state-of-the-art support vector machine  document classifiers.
these  distributions are useful to increase classification accuracy by exploiting  information of (1) term distribution among classes, (2) term distribution  within a class and (3) term distribution in the whole collection of training  data.
indeed we found strong correlations between the df, ig and chi values of a term.
aleksander kolcz and vidya prabakarmurthi and jugal k. kalita}, title = {string match and text extraction: summarization as feature selection for text categorization}, booktitle = {proceedings of cikm-01, 10th acm international conference on information and knowledge management}, publisher =
a test of different perspectives based on statistical distribution divergence", booktitle =
"alm, cecilia ovesdotter and roth,  dan and sproat, richard", title =
evgeniy gabrilovich % % department of computer science % % technion - israel institute of technology % % technion city, haifa 32000, israel % %
booktitle = {proceedings of icml-03, 20th international conference on machine learning}, editor = {}, year = {2003}, address = {washington, dc}, pages = {}, publisher = {morgan kaufmann publishers, san francisco, us}, url = {}, abstract = {}, } @inproceedings{kumaran04, author = {giridhar kumaran and james allan}, title = {text classification and named entities for new event detection}, booktitle = {proceedings of sigir-04, 27th acm international conference on research and development in information retrieval}, editor = {
} @inproceedings{wu:2004:ipk, author =
{classification of document pages using structure-based features}, journal =
considerable improvement in the  classification accuracies of two popular classification algorithms on standard  labeled data-sets with and without artificially introduced noise, as well as in  the presence and absence of unlabeled data, indicates that this may be a  promising method to reduce the burden of manual labeling."
a boosting machine learning approach is applied to classifying  web chinese documents that share a topic hierarchy.
the algorithm iteratively estimates the usefulness of features and selects them accordingly, using either forward selection or backward elimination.
our experiments demonstrate that this new  approach is able to learn more accurate classifiers than either of its  constituent methods alone.}, } @inproceedings{craven98, author = {mark craven  and dan dipasquo and dayne freitag and andrew k. mccallum and tom m. mitchell  and kamal nigam and se{\'{a}}n slattery}, title = {learning to extract symbolic  knowledge from the world wide web}, booktitle = {proceedings of aaai-98, 15th  conference of the american association for artificial intelligence}, publisher  = {aaai press, menlo park, us}, year = {1998}, pages = {509--516}, address =
in this paper, we present a method for automatic genre classification that is based on statistically selected features obtained from both subject-classified and genre classified training data.
various non-binary weighting schemes are widely used for this purpose.
a newer paradigm based on machine learning has superseded the previous approach.
we describe how the tree kernel can be computed efficiently by  dynamic programming.
by observing  that people who search the web with the same queries often click on different,  but related documents together, we draw implicit links between web pages that  are clicked after the same queries.
experiments on the newsgroups and the reuters-21578 dataset indicate improved performance of the proposed classifier in comparison to other state-of-the-art methods on datasets with a small number of positive examples.}, } @article{tsay04, author = {jyh-jong tsay and jing-doo wang}, title = {
however these assumptions are often violated in  practice, and poor performance can result.
{mexico city, me}, note = {
it uses the internet as source of knowledge and extends it to categorize very short (less than 5 words) documents with reasonable accuracy.
this method also has the benefit to make feature  selection implicit, since useless features for the categorization problem  considered get a very small weight.
published in the ``lecture notes for computer science'' series, number 2004}, pages = {423--436}, url = {http://link.springer.de/link/service/series/0558/papers/2004/20040437.pdf}, abstract = {
"583--600" } @article{ajiips04a, author = "d. tikk and {gy.} biro and j. d. yang", title =
in general,  only references specific to atc are considered % % pertinent to this  bibliography; in particular, references that % % *are* considered pertinent  are:
computer programs scan text in a document and apply a model that assigns the document to one or more prespecified topics.
{ios press}, address = {amsterdam, nl}, pages = {211--214}, year = {1999}, url = {}, abstract = {
this  could be due to the fact that when a word along with its adjoining word - a  phrase - is considered towards building a category profile, it could be a good  discriminator.
we also explore the use of different kinds of codes, namely error-correcting codes, random codes, and domain and data-specific codes and give experimental results for each of them.
our substantial experimental results show that with  several dimension reduction methods that are designed particularly for  clustered data, higher efficiency for both training and testing can be achieved  without sacrificing prediction accuracy of text classification even when the  dimension of the input space is significantly reduced.} }  @article{yang:2005:act, author = {hsin-chang yang and chung-hong lee}, title =  {automatic category theme identification and hierarchy generation for chinese  text categorization}, journal = {journal of intelligent information systems},  year = {2005}, volume = {25}, number = {1}, pages = {47--67}, abstract =  {recently research on text mining has attracted lots of attention from both  industrial and academic fields.
"40--47", url = "http://doi.ieeecomputersociety.org/10.1109/mis.2005.49", abstract = "machine learning has become one of the main approaches to tackling text categorization.
this paper presents an alter-native approach for the use of text classification methods for super-vised learning problems with numerical-valued features in which the numerical features are converted into bag-of-words features, thereby making them directly usable by text classification methods.
an efficient approach to generating word sequences is proposed.
we first show that  adaboost significantly outperforms another highly effective text filtering  algorithm.
furthermore, linear and logistic regression are compared.}, } @article{furnkranz02, author = {johannes f{\"{u}}rnkranz}, title = {hyperlink ensembles: a case study in hypertext classification}, journal = {information fusion}, year = {2002}, number = {4}, volume = {3}, pages = {299--312}, url = {}, abstract = {
using text categorization techniques for intrusion detection},  booktitle = {proceedings of the 11th usenix security symposium}, publisher =
"granada, spain", url = "http://www.jrc.cec.eu.int/langtech/documents/0509_sepln-05_montejo-et-al.pdf", abstract =
by applying tdfa to the document set that  belongs to a given class and a set of documents that is misclassified as  belonging to that class by an existent classifier, we can obtain features that  take large values in the given class but small ones in other classes, as well  as features that take large values in other classes but small ones in the given  class.
using this method, we achieved high performance in text categorization both with small number and large numbers of labeled data.}, } @inproceedings{tan01, author = {
{american society for information science,  washington, us}, year = {1999}, address = {washington, us}, pages = {}, url =  {http://www.cs.uiowa.edu/~mruiz/papers/sigcr_10}, abstract = {
cc and or are one-sided metrics while ig and chi are two-sided.
these generative models do not capture all the intricacies of text; however on  some domains this technique substan- tially improves classification accuracy,  especially when labeled data are sparse.
we also observed that extracting meta data from related web sites was extremely useful for improving classification accuracy in some of those domains.
the use of titles for automatic document classification}, journal = {journal of the american society for information science}, year =
in phase i, two kinds of classifiers are developed as the base classifiers.
a hyperlink ensemble is formed by  obtaining one prediction for each hyperlink that points to a page.
the learning process constructs a relationship between an index term and the words relevant and irrelevant to it, based on the positive training set and negative training set, which are sample documents indexed by the index term, and those not indexed by it, respectively.
naively using terms in neighboring documents increased the error to 38\%; our hypertext classifier reduced it to 21\%.
our approach is similar to the query by committee framework, where disagreement among the committee members on the predicted label for the input part of the example is used to signal the need for knowing the actual value of the label.
particular attention is turned to a classifier's underlying feature set: aside from the standard feature types we introduce new features that are based on word frequency classes and that can be computed with minimum computational effort.
sample were more  dramatic: the text classifier showed a 68\% error, whereas our hypertext  classifier reduced this to just 21\%.}, } @article{chakrabarti98c, author =
while text categorization has received  much attentions by ir researchers, classification of visual data is at its  infancy stage.
meta-clustering algorithm, a new extension of the recent double clustering (dc)  method of slonim and tishby that exhibited impressive performance on text  categorization tasks.
"wenyuan dai and gui-rong xue and  qiang yang and yong yu", title =
in this paper we dissect concept drift into three main subtypes.
we present detailed experimental results using naive bayes and support vector machines on the 20 newsgroups data set and a 3-level hierarchy of html documents collected from dmoz open directory.}, } @article{dhillon03, author = {inderjit dhillon and subramanyam mallela and rahul kumar}, title = {
hbc can reconstruct the  original clusters more accurately than other non-probabilistic algorithms.
{2000}, pages = {176--183}, url =  {http://www.acm.org/pubs/articles/proceedings/ir/345508/p176-hoashi/p176-hoashi.pdf},  abstract = {document filtering is a task to retrieve documents relevant to a  user's profile from a flow of documents.
"665--670", booktitle =
we call $n$-gram a set $g_k$ of $n$ word  stems, and we say that $g_k$ occurs in a document $d_j$ when a sequence of  words appears in $d_j$ that, after stop word removal and stemming, consists  exactly of the $n$ stems in $g_k$, in some order.
in contrast, programs that filter text  streams, software that categorizes documents, agents which alert users, and  many other ir systems must make decisions without human input or supervision.
formally, it can be viewed as a mapping from the document space into a set of predefined class labels (aka subjects or categories); f: d <- {c1, c2...cn} where f is the mapping function, d is the document space and {c1, c2...cn} is the set of class labels.
in two different domains, relevancy signatures produced better results than the simple indexing terms.
documents are represented as feature-vectors that include n-grams instead of including only single words (unigrams) as commonly used when learning on text data.
we propose a novel probabilistic method, based on latent variable models, for unsupervised topographic visualisation of dynamically evolving, coherent textual information.
an experimental comparison of naive bayesian and keyword-based anti-spam  filtering with personal e-mail messages}, booktitle = {proceedings of sigir-00,  23rd acm international conference on research and development in information  retrieval}, editor = {nicholas j. belkin and peter ingwersen and mun-kew  leong}, publisher = {acm press, new york, us}, address = {athens, gr}, year =
text categorization must work reliably on all input, and thus must tolerate some level of these kinds of problems.
in an empirical study we compared representative sampling both with  random sampling and with svm active learning.
when linear svm classifiers are used.}, } @inproceedings{bruckner97, author = {t. bruckner}, title = {
{proceedings of ecir-02, 24th european colloquium on information retrieval research}, editor = {
using these, we build a  multi-level classifier.
several studies on improving accuracy of fast but less accurate classifiers have been recently carried out.
text classification from labeled and unlabeled documents using em}, journal = {machine learning}, year = {2000}, number = {2/3}, volume = {39}, pages = {103--134}, url = {http://www.cs.cmu.edu/~knigam/papers/emcat-mlj99.ps}, abstract = {this paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents.
"326-333", month = "august", address =
the new approach  is both theoretically well-founded as well as effective and efficient in  practice.
we also  found that passages have different degrees of contribution to the main  topic(s), depending on their location in the test document.}, }  @inproceedings{kindermann01, author = {j{\"{o}}rg kindermann and gerhard  paa{{\ss}} and edda leopold}, title = {error correcting codes with optimized  kullback-leibler distances for text categorization},
we provide experimental results demonstrating that the approach can significantly improve performance, and that it does not impair it.} } @incollection{cristianini01a, author = {huma lodhi and john shawe-taylor and nello cristianini and christopher j. watkins}, title = {discrete kernels for text categorisation}, booktitle = {
in concrete, we have evaluated a range of machine learning methods for the task (c4.5, naive bayes, part, support vector machines and rocchio), made cost sensitive through several methods (threshold optimization, instance weighting, and meta-cost).
each node's vocabulary is filtered and its  words assigned weights with respect to the specific category.
as an example, the proposed scheme is applied to the classification of news articles into 3 categories: politics, sports, and business.
labeling is usually done manually by human experts (or the users), which is a labor intensive and time consuming process.
the category discrimination method (cdm) is  a new learning algorithm designed for text categorization.
given a user's information need, some patterns in sentences such as combinations of query words, named entities and phrases, may contain more important and relevant information than single words.
"aaai'06", booktitle = "proceedings of the 21st national conference on artificial intelligence", year =
there are about 350 different codes to be assigned.
{proceedings of ranlp-01, 4th international conference on recent advances in  natural language processing}, address = {tzigov chark, bg}, pages = {}, url =  {http://www.lsi.upc.es/~carreras/pub/boospam.ps}, } @inproceedings{cavnar94,  author = {
in this paper, we propose an  approach to speedup the process of text classification based on pruning the  training corpus.
{2003}, pages = {393--407}, url =  {http://link.springer.de/link/service/series/0558/papers/2633/26330393.pdf},  abstract = {
in particular, we evaluate  the vector and latent semantic analysis (lsa) methods, a classifier based on  support vector machines (svm) and the k-nearest neighbor variations of the  vector and lsa models.
we evaluate four different measures of subject similarity,  derived from the web link structure, and determine how accurate they are in  predicting document categories.
instead of using a randomly selected training set,  the learner has access to a pool of unlabeled instances and can request the  labels for some number of them.
{24--27}, url = {http://werner.ira.uka.de/papers/speech/1993/wcnn_93_petra_geutner.ps.gz}, abstract = {
our example problem is automatic document categorization using machine learning, where we want to identify documents relevant for the selected category.
{proceedings of the 11th usenix security symposium}, publisher = {}, editor = {dan boneh}, year = {2002}, address = {san francisco, us}, pages = {51--59}, url = {http://www.usenix.org/publications/library/proceedings/sec02/liao.html}, abstract = {a new approach, based on the k-nearest neighbor (knn) classifier, is used to classify program behavior as normal or intrusive.
{hans-peter frei and donna harman and peter sch{\"{a}}uble and ross wilkinson}, publisher = {acm press, new york, us}, year = {1996}, address = {z{\"{u}}rich, ch}, pages = {298--306}, url = {http://www.research.att.com/~lewis/papers/lewis96d.ps}, abstract = {
this paper proposes a new approach for text categorization, based on a feature projection technique.
"proceedings of  the 29th european conference on information retrieval", year =  "2007", month = "april", address =
the results show  that our approach outperformed the bayesian independence classifier as measured  by a metric that combines precision and recall measures.}, }  @inproceedings{lam98, author = {wai lam and chao y. ho}, title = {using a  generalized instance set for automatic text categorization}, booktitle =  {proceedings of sigir-98, 21st acm international conference on research and  development in information retrieval}, editor = {w. bruce croft and alistair  moffat and van rijsbergen, cornelis j. and ross wilkinson and justin zobel},  publisher = {acm press, new york, us}, year = {1998}, address = {melbourne,  au}, pages = {81--89}, url =  {http://www.acm.org/pubs/articles/proceedings/ir/290941/p81-lam/p81-lam.pdf},  abstract = {
text classification with self-organizing maps: some lessons  learned}, journal = {neurocomputing}, year = {1998}, volume = {21}, number =
in this paper text classification (tc) has been taken as the ir task and the effect of linguistic capabilities of the underlying system have been studied.
this is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available.
{djamel a. zighed and jan komorowski and jan zytkow}, publisher = {springer verlag, heidelberg, de}, note = {
it also can be used for creating training documents.}, } @inproceedings{ko02, author = {youngjoong ko and jinwoo park and jungyun seo}, title =
"betts,  tom and milosavljevic, maria and oberlander, jon", title =
= a ph.d. thesis % required: author, title, school, year % optional: type, address, month, note % % proceedings % =
{acm press, new york, us}, editor = {henrique paques and ling liu and david  grossman}, year = {2001}, address = {atlanta, us}, pages = {365--370}, url =  {http://doi.acm.org/10.1145/502585.502647}, abstract = {
{2000}, address = {san antonio, us}, pages = {195--204}, url =  {ftp://ftp.cs.utexas.edu/pub/mooney/papers/libra-dl-00.ps.gz}, abstract =  {recommender systems improve access to relevant products and information by  making personalized suggestions based on previous examples of a user's likes  and dislikes.
we view this result as a confirmation of the usefulness of classifiers that represent contextual information.}, } @inproceedings{cohen98, author = {william w. cohen and haym hirsh}, title = {joins that generalize: text classification using {{\sc whirl}}}, booktitle = {proceedings of kdd-98, 4th international conference on knowledge discovery and data mining}, editor = {
= {springer verlag,  heidelberg, de}, address = {freiburg, de}, year = {2001}, pages = {338--349},  note = {
using a commercial medline product based on the vector space model, these physicians searched just as effectively as more experienced searchers using boolean searching.
{proceedings of sigir-03, 26th acm international conference on research and  development in information retrieval}, editor = {jamie callan and gordon  cormack and charles clarke and david hawking and alan smeaton}, publisher =
this paper proposes six fast-feature techniques that use only features available in the search result list, such as title, snippet, and url, to categorize results into meaningful categories.
,  abstract = {categorization of web documents poses a new challenge for automatic  classification methods.
leopold, edda and kindermann, j{\"{o}}rg},  title =
we show that the advantage of  using supervised clustering is that it is possible to have some control over  the range of subjects that one would like the categorization system to address,  but with a precise mathematical definition of each category.
until the late '80s, the dominant approach to the problem involved knowledge-engineering automatic categorisers, i.e. manually building a set of rules encoding expert knowledge on how to classify documents.
published in the ``lecture  notes in computer science'' series, number 2035}, pages = {53--65}, url =  {http://link.springer.de/link/service/series/0558/papers/2035/20350053.pdf},  abstract =
using ig  thresholding with a k-nearest neighbor classifier on the reuters corpus,  removal of up to 98\% removal of unique terms actually yielded an improved  classification accuracy (measured by average precision).
machine learning}, publisher = {springer science}, year = {2005}, volume = {60}, number = {1-3}, pages =
overall, we show that by using a few terms, categorisation accuracy can be improved substantially: unstructured leaf level categorisation can be improved by up to 8.6\%, while top-down hierarchical categorisation accuracy can be improved by up to 12\%.
{morgan kaufmann publishers, san francisco, us}, url = {}, abstract = {}, } @inproceedings{li03b, author = {tao li and shenghuo zhu and mitsunori ogihara}, title = {efficient multi-way text categorization via generalized discriminant analysis}, booktitle = {proceedings of cikm-03, 12th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, editor = {}, year = {2003}, address = {new orleans, us}, pages =
{1995}, pages = {258--265}, url = {}, note = {an extended version appears  as~\cite{goldberg96}}, abstract = {
"sentiment classification on customer feedback data: noisy data, large feature vectors, and the role of linguistic analysis", booktitle =
experimental comparison of the described measures is given on real-world data collected from the web.
booktitle = {proceedings of emnlp-01, 6th conference on empirical methods in natural language processing}, year = {2001}, publisher = {association for computational linguistics, morristown, us}, editor = {lillian lee and donna harman}, pages = {44--50}, address = {pittsburgh, us}, url = {http://arxiv.org/pdf/cs/0106040}, abstract = {
this paper presents an examination of the effect of  thresholding strategies on the performance of a classifier under various  conditions.
"specificity helps text  classification", booktitle =
we take advantage of this fact by using a hierarchically organized neural network, built up from a number of independent self-organizing maps in order to enable the true establishment of a document taxonomy.
published in the ``lecture notes in computer science''  series, number 3202}, url = {}, abstract = {}, } @inproceedings{goevert99,  author = {
both svm and knn are tested and compared on the 20-newsgroups database.
the intuition that different text classifiers behave in qualitatively different ways has long motivated attempts to build a better metaclassifier via some combination of classifiers.
title = {document representation for one-class svm}, booktitle = {proceedings of ecml-04, 15th european conference on machine learning}, editor = {jean-fran{\c{c}}ois boulicaut and floriana esposito and fosca giannotti and dino pedreschi}, address = {
several different algorithms have been  applied, and support vector machines (svm) have shown very good results.
we suggest a way for locating duplicates and plagiarisms in a text collection using an r-measure, which is the normalized sum of the lengths of all suffixes of the text repeated in other documents of the collection.
we show that foil's performance can be improved by relation selection, a first order analog of feature selection.
the system includes a unique phrase help facility, which helps users find and add phrases and terms related to those in their query.},
{475--482}, url =  {http://ls6-www.informatik.uni-dortmund.de/ir/publications/1999/goevert_etal:99.html},  abstract = {
in addition, the training time and scaling are also important concerns.
we study to what extent ocr errors affect stylistic text classification from scanned documents.
results show that the use of metadata is almost  as good as the full-text version of papers.
the system of descriptors is prescribed.
in this case the assumptions can be made more representative in two ways: by modeling sub-topic class structure, and by modeling super-topic hierarchical class relationships.
we also observe that despite similar performances, different topical crawlers cover subspaces on the web with low overlap.} } @article{peng:2004:anb, author = {fuchun peng and dale schuurmans and shaojun wang}, title = {
abstract = {the world wide web is a vast source of information accessible to computers, but understandable only to humans.
it is demonstrated that the performance of such a classifier is further enhanced when employing the kernel derived from an appropriate hierarchic mixture model used for partitioning a document corpus rather than the kernel associated with a at non-hierarchic mixture model.
fukuoka, jp}, pages = {305--314}, url = {}, abstract = {}, } @inproceedings{iyer00, author =
in previous research, a text document is commonly represented by the term frequency and the inverted document frequency of each feature.
we compare different search heuristics and  pruning methods known from various symbolic rule learners on a set of  representative text categorization problems.
categorization of text images into content-oriented classes would  be a useful capability in a variety of document handling systems.
"xiaoyun wu and rohini srihari",  title =
{spam filtering using statistical data compression models}, journal = {journal  of machine learning research}, volume = {7}, pages = {2673--2698}, year =
in this paper, we propose a notion of visual keywords for similarity matching between visual contents.
{92--115}, url =  {http://ls6-www.informatik.uni-dortmund.de/bib/fulltext/ir/fuhr_pfeifer:94.ps.gz},  abstract = {
feedback algorithm or genetic algorithms.
"andrea esuli and fabrizio sebastiani", title =  "determining the semantic orientation of terms through gloss  classification", pages = "617--624", booktitle =  "proceedings of the 14th {acm} international conference on information and  knowledge management", year =
the personal view constructor mines user interests and maps them to a class  hierarchy (i.e., personal view).
the average precision of the  document type-based search is 88.9\%, while the average precision of the  keyword-based search is 31.2\%.
this paper proposes a new approach for classifying text documents into two disjoint classes.
moreover, by ranking words and phrases in the citing documents according to expected entropy loss, we are able to accurately name clusters of web pages, even with very few positive examples.
there exist  many approaches for performing this difficult task.
svms achieve substantial improvements  over the currently best performing methods and behave robustly over a variety  of different learning tasks.
"scoring and selecting terms for text categorization", author =
much of previous work focused on binary document classification  problems.
published in the ``lecture notes in computer science'' series, number 3201}, url = {}, abstract = {}, } @inproceedings{biebricher88, author = {peter biebricher and norbert fuhr and gerhard knorz and gerhard lustig and michael schwantner},
the best performance was  achieved by the feature selection methods based on the feature scoring measure  called odds ratio that is known from information retrieval.}, }  @phdthesis{mladenic98c, author = {
automatic classification is traditionally  performed by extracting the information for representing a document  (``indexing'') from the document itself.
experienced users can make effective use of such engines for tasks that can be solved by searching for tightly constrained keywords and phrases.
"intelligent {gp} fusion from multiple sources for text classification", booktitle = "proceedings of the 14th {acm} international conference on information and knowledge management", year =
in this paper, we describe an active learning method based on query by  committee (qbc) that reduces the number of labeled training examples (text  documents) required for learning by 1-2 orders of magnitude.}, }  @inproceedings{lim99, author = {
hyperlinks, html tags, category  labels distributed over linked documents, and meta data extracted from related  web sites all provide rich information for classifying hypertext documents.
most of them are irrelevant and others introduce noise  which could mislead the classifiers.
published in the ``lecture notes in computer science'' series, number 1609}, url = {}, abstract = {}, } @inproceedings{wang99a, author = {ke wang and senquiang zhou and shiang chen liew}, title = {
the proposed performance measures consist of category  similarity measures and distance based measures that consider the contributions  of misclassified documents.
in many real-world domains, supervised learning requires a large number of training examples.
we obtained some  encouraging results on two-category situations, and the results on the general  problem seem reasonably impressive---in one case outstanding.
several measures of categorization success are described and  evaluated.
however, such a  search is difficult with current search services, since these services only  provide keyword-based search methods that are equivalent to narrowing down the  target references according to domains.
"we present a kernel-based algorithm for hierarchical text  classification where the documents are allowed to belong to more than one  category at a time.
our results on a collection of scanned journals from the making of america project
= {springer verlag, heidelberg, de},  address = {seoul, ko}, note = {
then, a machine learning method may be used in this simple bidimensional space to classify the documents.
the research described in this paper details the progress made in a prototype adaptive information filtering system based on weighted trigram analysis and evolutionary computation.
three  characteristic properties of this domain are (a) very high dimensionality, (b)  both the learned concepts and the instances reside very sparsely in the feature  space, and (c) a high variation in the number of active features in an  instance.
using a collection factor, based on 87 per cent human consistency from other courses, the computer appears then to index with 90 per cent accuracy in this case.
= {chien chin chen and chang chen, meng and yeali sun}, title = {
booktitle = {proceedings of ecir-01, 23rd
we present an analysis which characterizes the expected generalization error of the hypothesis with least training error in terms of the distribution of error rates of the hypotheses in the model.
{2003}, pages = {57--72}, url =  {http://link.springer.de/link/service/series/0558/papers/2633/26330057.pdf},  abstract = {
the proposed model represents an effective feature selection methodology.
"1265--1270", abstract =  "evaluating text fragments for positive and negative subjective  expressions and their strength can be important in applications such as single-  or multi- document summarization, document ranking, data mining, etc.
it has often been observed that compression  seems to provide a very promising approach to categorization.
"2006", pages =  "821--826", } @inproceedings{forman:2006:qta, author =
experiments show the ngd kernel on the multinomial manifold to be effective for text classification, significantly outperforming standard kernels on the ambient euclidean space."
this paper shows that the accuracy of text classifiers trained with a  small number of labeled documents can be improved by augmenting this small  training set with a large pool of unlabeled documents.
"553--560" } @inproceedings{lin:2006:atd, author =
= {iterative double clustering for unsupervised and  semi-supervised learning}, booktitle = {proceedings of ecml-01, 12th european  conference on machine learning}, editor =
it supports incremental training and online application of classifiers and predictive models to streams of textual, numeric, symbolic, and hybrid data records.
compared with other active learning algorithms, the proposed representative  sampling explicitly addresses the problem of selecting more than one unlabeled  documents.
this analysis also revealed, for example, that information gain and chi-squared have correlated failures, and so they work poorly together.
this paper describes a new method for the classification of a html document into a hierarchy of categories.
} @inproceedings{montejo-raez:2005:tcu, author =
{jian zhang and rong jin and yiming yang and alex hauptmann}, title = {modified  logistic regression: an approximation to svm and its applications in  large-scale text categorization}, booktitle = {proceedings of icml-03, 20th  international conference on machine learning}, editor = {}, year = {2003},  address = {washington, dc}, pages = {}, publisher = {morgan kaufmann  publishers, san francisco, us}, url = {}, abstract = {}, }  @inproceedings{zhang03b, author = {jian zhang and yiming yang}, title =
the proposed architecture matches the hierarchical structure of the topic space, as opposed to a flat model that ignores the structure.
"enhancement of dtp feature selection method for text categorization", booktitle = "computational linguistics and intelligent text processing", year =
"raising high-degree overlapped character bigrams into trigrams for dimensionality reduction in chinese text categorization", booktitle = "computational linguistics and intelligent text processing (lecture notes in computer science, vol. 2945)", year =
journal of the american society for information science and technology}, year = {2003}, volume = {54}, number = {12}, pages = {1269--1277}, url = {http://www.math.unipd.it/~fabseb60/publications/jasist03.pdf}, abstract = {\emph{survey coding} is the task of assigning a symbolic code from a predefined set of such codes to the answer given in response to an open-ended question in a questionnaire (aka \emph{survey}).
we present a new method for sentiment classification based on extracting and analyzing appraisal groups such as ``very good'' or ``not terribly funny''.
{a new effective approach for categorizing web documents}, booktitle =  {proceedings of bcsirsg-00, the 22nd annual colloquium of the british computer  society information retrieval specialist group}, editor = {}, address =
we also propose an  iterative web unit mining (iwum) method that first finds subgraphs of web pages  using some knowledge about web site structure.
}  @inproceedings{zhu:2005:mlc, author =
in natural language tasks like text categorization, we usually have an enormous amount of unlabeled data in addition to a small amount of labeled data.
the results suggest  that the spatial and the temporal similarity measures need to be improved.
{174--181}, url = {http://doi.acm.org/10.1145/860435.860469},  abstract = {a novel maximal figure-of-merit (mfom) learning approach to text  categorization is proposed.
we  describe a method for classifying pages of sequential ocr text documents into  one of several assigned categories and suggest that taking into account  contextual information provided by the whole page sequence can significantly  improve classification accuracy.
we use a training set of texts with  expert assigned categories to construct a network which approximately reflects  the conditional probabilities of categories given a text.
the measurement is called the strength of a term and is a measure of how strongly the term`s occurrences correlate with the subjects of documents in the database.
our method scales well to large data sets, with  numerous categories in large hierarchies.
in this paper, we describe a comparative study on techniques of feature transformation and classification to improve the accuracy of automatic text classification.
this neural model is based on significance vectors and benefits from the presentation of document clusters.
we describe  an approach to text classification that represents a compromise between  traditional word-based techniques and in-depth natural language processing.
morgan kaufmann publishers, san francisco, us}, address = {san diego, us}, pages = {245--255}, year = {1991}, url = {http://www.research.att.com/~lewis/papers/lewis91c.ps}, abstract = {
we define for each document category a finite mixture model, which is a linear combination of the probability distributions of the clusters.
we found that a larger reference library is not necessarily better.
we show that although whirl is designed for more general similarity-based reasoning tasks, it is competitive with mature inductive classification systems on these classification tasks.
classification is traditionally performed by extracting information for indexing a document from the document itself.
improving short text classification using unlabeled background knowledge},  booktitle = {proceedings of icml-00, 17th international conference on machine  learning}, editor = {
estimating the generalization performance of a svm efficiently}, booktitle = {
on compression-based text classification}, booktitle = {proceedings of ecir-05, 27th european conference on information retrieval}, publisher = {springer verlag}, editor = {david e. losada and juan m. fern{'{a}}ndez-luna}, address = {santiago de compostela, es}, year = {2005}, pages = {300--314}, url = {}, abstract = {compression-based text classification methods are easy to apply, requiring virtually no preprocessing of the data.
we can choose to make use of none, part or all of the hierarchical structure to  improve the categorization effectiveness and efficiency.
we investigate three  approaches to automatically classifying documents by genre: traditional bag of  words techniques, part-of-speech statistics, and hand-crafted shallow  linguistic features.
"it is well known  that links are an important source of information when dealing with web  collections.
for example, in hypertext classification, the labels of linked pages are highly correlated.
compared with traditional document-level categorization, two additional steps, passage splitting and category merging, are required in this model.
after 8 cycles the  computer is found to have formed 9 groups consisting of about 50 per cent of  documents that were also lumped together by professional indexers on the basis  of subject content.
the originality of stretch lies principally in the possibility for unskilled users to define the indexes relevant to the document domains of their interest by simply presenting visual examples and applying reliable automatic information extraction methods (document classification, flexible reading strategies) to index the documents automatically, thus creating archives as desired.
the expert literary readers were found to assign significantly higher ratings to all versions of the manipulated poems than the novice readers.}, } @inproceedings{hayes88, author = {philip j. hayes and laura e. knecht and monica j. cellio}, title = {a news story categorization system}, booktitle = {proceedings of anlp-88, 2nd conference on applied natural language processing}, publisher =
"we introduce a bayesian model, bayesanil, that is capable of estimating uncertainties associated with the labeling process.
while developing simpl, we also make a detailed experimental  analysis of the cache performance of svms.}, } @inproceedings{chakrabarti97,  author = {soumen chakrabarti and byron e. dom and rakesh agrawal and prabhakar  raghavan}, title = {using taxonomy, discriminants, and signatures for  navigating in text databases}, booktitle = {proceedings of vldb-97, 23rd  international conference on very large data bases}, publisher = {morgan  kaufmann publishers, san francisco, us}, editor = {matthias jarke and michael  j. carey and klaus r. dittrich and frederick h. lochovsky and pericles  loucopoulos and manfred a. jeusfeld}, year = {1997}, address = {athens, gr},  pages = {446--455}, url = {http://www.vldb.org/conf/1997/p446.pdf}, note = {an  extended version appears as~\cite{chakrabarti98c}}, abstract = {
booktitle = {proceedings of cikm-04, 13th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, address =
our example problem is automatic document  categorization using machine learning, where we want to identify documents  relevant for the selected category.
"schiffman, barry and mckeown, kathleen r.", title =
most importantly, this regularized estimation  enables the model parameters to become sparse.
this knowledge is represented using publicly available ontologies that contain  hundreds of thousands of concepts, such as the open directory; these ontologies  are further enriched by several orders of magnitude through controlled web  crawling.
a novel application of evolutionary computation is its use in adaptive information filtering for optimizing various parameters, notably the weights associated with trigrams.
furthermore, a learning feedback technique is introduced.
for our experiments we decided on using the text corpus  reuters-21578.
text categorization: the assignment of subject descriptors to magazine articles}, journal = {information processing and management}, pages = {841--861}, year = {2000}, number = {6}, volume = {36}, url = {}, abstract = {automatic text categorization is an important research area and has a potential for many text-based applications including text routing and filtering.
{acm press, new york, us}, editor = {}, year = {2003}, address = {new orleans,  us}, pages = {232--239}, url = {http://doi.acm.org/10.1145/956863.956909},  abstract = {most existing studies of text classification assume that the  training data are completely labeled.
this work addresses a logical approach to text categorization  inside a framework aimed at full automatic paper document processing.
an hybrid approach to optimize feature selection process in text classification}, booktitle = {proceedings of ai*ia-01, 7th congress of the italian association for artificial intelligence}, publisher = {springer verlag, heidelberg, de}, note = {
"we enhance machine learning algorithms for text categorization  with generated features based on domain-specific and common-sense knowledge.
the use of a vector space  classifier and training method robust to large feature sets, combined with  discarding of low frequency ocr output strings are the key to our approach.}, }  @inproceedings{iwayama94, author = {makoto iwayama and takenobu tokunaga},  title = {a probabilistic model for text categorization: based on a single  random variable with multiple values}, booktitle = {proceedings of anlp-94, 4th  conference on applied natural language processing}, publisher = {association  for computational linguistics, morristown, us}, editor = {}, year = {1994},  address = {
with an appropriate level of  relation selection, foil appears to be competitive with or superior to existing  propositional techniques.}, } @incollection{cohen95a, author = {
our first set of experiments applies the c4.5 decision tree  induction algorithm to this learning task.
acquaintance is the name of a novel vector-space n-gram for categorizing documents.
combining statistical and relational methods for  learning in hypertext domains}, booktitle = {proceedings of ilp-98, 8th  international conference on inductive logic programming}, publisher = {springer  verlag, heidelberg, de}, note = {
therefore, it is very costly to assign a category for them because humans investigate their contents.
in this paper we will evaluate the effectiveness  of several ilp methods for text categorization, and also compare them to their  propositional analogs.
{springer-verlag}, url =  {http://www.springerlink.com/content/4ytxvxmjea83ctqv/}, abstract = {feature  selection method for text classification based on information gain ranking,  improved by removing redundant terms using mutual information measure and  inclusion index, is proposed.
we also investigate codes with  optimized kl distance between the text categories which are merged in the  code-words.
our investigation leads to conclude that association rule mining is a good and promising strategy for efficient automatic text categorization.}, } @inproceedings{zelikovitz00, author = {sarah zelikovitz and haym hirsh}, title = {
"geneva, switzerland", publisher =
consistent with previous findings, we  find that feature selection based on the labeled training set has little  effect.
the example application, categorization  of e-mail messages, is described.
we describe in detail an  implementation, called boostexter, of the new boosting algorithms for text  categorization tasks.
misc % = use this type when nothing else fits % required: none % optional: author, title, howpublished, month, year, note % % phdthesis %
published in the ``lecture notes in computer science'' series, number 1910}, url = {http://link.springer.de/link/service/series/0558/papers/1910/19100490.pdf}, abstract = {supervised learning algorithms usually require large amounts of training data to learn reasonably accurate classifiers.
the text categorization module described in the paper provides a front-end filtering function for the larger dr-link text retrieval system (liddy and myaeng 1993).
mastersthesis % = a master's thesis % required: author, title, school, year % optional: type, address, month, note % %
pat  langley}, year = {2000}, address = {
"we present a kernel-based algorithm for hierarchical text classification where the documents are allowed to belong to more than one category at a time.
the technical details of the solutions from the three award winning teams are available in their papers separately in this issue of sigkdd explorations.
all three algorithms achieved high precision on both test sets, with the augmented relevancy signatures algorithm and the case-based algorithm reaching 100\% precision with over 60\% recall on one set.
if a category's recall exceeds  its precision, the category is too strong and its level is reduced.
the system  selects the category whose profile has the smallest distance to the document's  profile.
we address  these problems with a system for topical information space navigation that  combines the query-based and taxonomic systems.
we present an efficient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables.
at the end, we  also share the results of a survey conducted with this year’s cup  participants.
we study the problem of mapping a search engine query to those nodes of a given subject taxonomy that characterize its most likely meanings.
for example, in a scientific paper  domain, papers are related to each other via citation, and are also related to  their authors.
"feng he and xiaoqing ding",  title =
"learning hierarchical multi-category text classification models", booktitle = "proceedings of the twenty-second international conference on machine learning", year =
this paper presents our research work on automatic question classification through machine learning approaches.
and i % %  will substitute the link.
text classification is becoming more and more important with the  rapid growth of on-line information available.
in this paper, it is  proposed that a document be represented into a fuzzy set of informative  keywords, instead of a crisp set of informative keywords.
we have implemented our gis algorithm, the  expnet algorithm, and some linear classifiers.
we  present an efficient optimization algorithm based on incremental conditional  gradient ascent in single-example subspaces spanned by the marginal dual  variables.
then we describe a larger more difficult newswire classification task from information retrieval.
context, as it applies to document similarity, can be accommodated by a well-defined procedure.
the indexing strategy first automatically classifies the document, thus avoiding pre-sorting, then locates and reads the information pertaining to the specific document class.
this paper presents a simple and efficient solution to multi-class text categorization.
on the use of bernoulli mixture models for text  classification}, journal = {pattern recognition}, year = {2002}, volume = {35},  number = {12}, pages = {2705--2710}, url = {}, abstract = {mixture modelling of  class-conditional densities is a standard pattern recognition technique.
"the performance of search engines crucially depends on their  ability to capture the meaning of a query most likely intended by the user.
it is shown that the performance of such a classifier is  further enhanced when employing the kernel derived from an appropriate  hierarchic mixture model used for partitioning a document corpus rather than  the kernel associated with a flat non-hierarchic mixture model.
given these inputs, the system learns to extract information from other pages and hyperlinks on the web.
experiments in text classification}, booktitle = {proceedings of ecir-03, 25th european conference on information retrieval}, publisher = {springer verlag}, editor = {fabrizio sebastiani}, address = {pisa, it}, year = {2003}, pages = {41--56}, url = {http://link.springer.de/link/service/series/0558/papers/2633/26330041.pdf}, abstract = {link analysis methods have become popular for information access tasks, especially information retrieval, where the link information in a document collection is used to complement the traditionally used content information.
these feature vectors are then used as the input to the neural networks for classification.
{264--271}, url = {http://www.acm.org/pubs/articles/proceedings/ir/345508/p264-oh/p264-oh.pdf}, abstract = {as www grows at an increasing speed, a classifier targeted at hypertext has become in high demand.
in this approach, there are two processes: (1) the learning  process and (2) the indexing process.
this paper presents this feature selection  method and its results, and how we have predetermined some of its parameters  through experimentation.}, } @inproceedings{soucy01a, author = {pascal soucy  and guy w. mineau}, title = {a simple knn algorithm for text categorization},  booktitle = {proceedings of icdm-01, ieee international conference on data  mining}, publisher =
{303--309}, url = {http://portal.acm.org/citation.cfm?doid=383952.384011},  abstract = {
{187--194}, url =  {http://dlib.computer.org/conferen/icdm/1754/pdf/17540187.pdf}, abstract =  {with the rapid growth of textual information available on the internet, having  a good model for classifying and managing documents automatically is undoubtly  important.
in this paper, we discuss cost-sensitive text categorization  methods for ube filtering.
{george forman and ira cohen}, title = {
"common approaches to multi-label classification learn independent classifiers for each category, and employ ranking or thresholding schemes for classification.
sharon mcdonald and john tait}, year = {2004}, address = {sunderland, uk}, publisher = {springer verlag, heidelberg, de}, note = {
within this paradigm, a general inductive process  automatically builds a classifier by ``learning'', from a set of previously  classified documents, the characteristics of one or more categories; the  advantages are a very good effectiveness, a considerable savings in terms of  expert manpower, and domain independence.
in the hierarchical case, a model is learned to distinguish a second-level category from other categories within the same top level.
a major difficulty of this problem stems from the high dimensionality of its feature space.
a classifier was constructed applying the above features to complement  the knn classifier.
relevance feedback on a small portion (0.05~0.2%) of the tdt5 test documents yielded significant performance improvements, measuring up to a 54% reduction in ctrk and a 20.9% increase in t11su (with b=0.1), compared to the results of the top-performing system in tdt2004 without relevance feedback information."
applying the multiple  cause mixture model to text categorization}, booktitle = {proceedings of  icml-96, 13th international conference on machine learning}, editor = {lorenza  saitta}, year = {1996}, address = {bari, it}, pages = {435--443}, publisher =
morgan kaufmann publishers, san francisco, us}, url = {}, abstract = {}, } @inproceedings{lanquillon00, author = {carsten lanquillon}, title = {learning from labeled and unlabeled documents: a comparative study on semi-supervised text classification}, booktitle = {proceedings of pkdd-00, 4th european conference on principles of data mining and knowledge discovery}, editor = {djamel a. zighed and henryk jan komorowski and jan m. zytkow}, address = {lyon, fr}, pages = {490--497}, year = {2000}, publisher = {springer verlag, heidelberg, de}, note = {
the construction of a text classifier usually involves (i) a phase of \emph{term selection}, in which the most relevant terms for the classification task are identified, (ii) a phase of \emph{term weighting}, in which document weights for the selected terms are computed, and (iii) a phase of \emph{classifier learning}, in which a classifier is generated from the weighted representations of the training documents.
we found small advantages in accuracy for hierarchical models over flat models.
our second set of results examines the behavior of rankboost when it has to learn not only a ranking function but also all aspects of term weighting from raw data.
we formulate the problem of  automated survey coding as a \emph{text categorization} problem, i.e.\ as the  problem of learning, by means of supervised machine learning techniques, a  model of the association between answers and codes from a training set of  pre-coded answers, and applying the resulting model to the classification of  new answers.
in previous work, such "distributional clustering" of  features has been found to achieve improvements over feature selection in terms  of classification accuracy, especially at lower number of features [2, 28].
profile allows the user to update on-line his profile and to check the discrepancy between his assessment and the prediction of relevance of the system.
we present our methodology, which seems to be insensitive to the language of the document collections, and discuss issues related to the differences in results that we have obtained for the two collections.}, } @article{attardi98, author = {attardi, giuseppe and di marco, sergio and salvi, davide}, title = {
{kian m. chai and hwee t. ng and hai l. chieu}, title = {
machine learning schemes fare better  because they automatically eliminate irrelevant features and concentrate on the  most discriminating ones.}, } @inproceedings{frasconi01, author = {paolo  frasconi and giovanni soda and alessandro vullo}, title = {
our  theoretical results are complemented by a number of experiments on texual  corpora.
we describe athena: a system for creating, exploiting, and  maintaining a hierarchical arrangement of textual documents through interactive  mining-based operations.
"proceedings of the 29th european conference  on information retrieval", year =
by asymmetric misclassification costs we mean that one of the class values is the target class value for which we want to get predictions and we prefer false positive over false negative.
these  individual predictions for each hyperlink are subsequently combined to a final  prediction for the class of the target page.
a comparative study of classification-based personal e-mail filtering}, booktitle = {proceedings of pakdd-00, 4th pacific-asia conference on knowledge discovery and data mining}, editor =
this paper compares three commonly applied text classifiers in the light of semi-supervised learning, namely a linear support vector machine, a similarity-based tfidf and a naive bayes classifier.
accessing information from world wide web  pages as an approach to problem solving has become commonplace.
rough set (rs) theory can be applied to reducing the dimensionality of data used in if/ir tasks, by providing a measure of the information content of datasets with respect to a given classification.
we compare their effectiveness in classifying ocr  texts and the corresponding correct ascii texts in two domains: business  letters and abstracts of technical reports.
first, we  explain how the extraction patterns can be generated automatically using only  preclassified texts as input.
{donna k.  harman and ellen m. voorhees}, year = {1995}, address = {gaithersburg, us},  pages = {165--180}, url =  {http://www.research.att.com/~lewis/papers/lewis96b.ps}, abstract = {
published in the ``lecture notes in computer science'' series, number  2291}, pages = {229--247}, url =  {http://link.springer.de/link/service/series/0558/papers/2291/22910229.pdf},  abstract =
boosting is based on the idea of relying on the collective judgment of a committee of classifiers that are trained sequentially.
this paper presents a simple and efficient solution to multi-class text  categorization.
mh$^kr$}, an improved boosting algorithm, and its  application to text categorization.
"forman,  george", title =
instead of labeling a set of documents, the proposed method labels a set of representative words for each class.
the results are compared to 1-of-n coding (i.e.\ one svm for each text category).
abstract = {there is an increasing interest in categorizing texts using  learning algorithms.
in this paper we address the problem of exploiting the potential of weighted representations in the context of \textsc{adaboost}-like algorithms by discretizing the continuous attributes through the application of entropy-based discretization methods.
{23--30}, year = {1994}, note = {
however, none of the existing classification approaches could achieve all of these requirements.
in order to reduce  dimensionality, feature selection has been introduced as a preprocessing step.
} @inproceedings{ghamrawi:2005:cmc, author =
current technology for automating this process consists of building a classifier that uses the categorization of documents in the master catalog to construct a model for predicting the category of unknown documents.
short sequences of system calls have been used by others to characterize a program's normal behavior before.
however, this perspective is not appropriate in the case of many digital libraries that offer as contents scanned and optically read books or magazines.
text categorization for multiple users based on semantic features from a machine-readable dictionary}, journal = {acm transactions on information systems}, year = {1994}, number = {3}, volume =
this paper describes a learning news agent hynet which uses hybrid neural network techniques for classifying news titles as they appear on an internet newswire.
the  construction of a text classifier usually involves (i) a phase of \emph{term  selection}, in which the most relevant terms for the classification task are  identified, (ii) a phase of \emph{term weighting}, in which document weights  for the selected terms are computed, and (iii) a phase of \emph{classifier  learning}, in which a classifier is generated from the weighted representations  of the training documents.
"sang-bum kim and hae-chang rim", title =
the technique for machine-aided indexing (mai)  developed at the defense documentation center (ddc) is illustrated on a  randomly chosen abstract.
to verify our new method, we conducted experiments on two language newsgroup data sets: one written by english and the other written by korean.
this paper explores the use of bayesian online classifiers to classify text documents.
it is simple to implement and has strong justifications from computational learning theory.
from a machine learning point of view, this study was motivated by the size of the inspected data in such applications.
we approached the task with careful consideration of the specialized terminology and paid special attention to dealing with various forms of gene synonyms, so as to exhaustively locate the occurrences of the target gene.
similar to indices for relational data, taxonomies make search and access more efficient.
we call the algorithm hierarchical bayesian clustering (hbc).
hirotoshi taira and masahiko haruno}, title
for meaningful term extraction, a phrase-like unit (plu)-based likelihood ratio is proposed to estimate the likelihood that a word sequence is an unknown word.
this paper presents a hierarchical mixture model which extends the  standard naive bayes classifier and previous hierarchical approaches.
the normalization to the relative word frequency, the principal component analysis (k-l transformation) and the power transformation were applied to the feature vectors, which were classified by the euclidean distance, the linear discriminant function, the projection distance, the modified projection distance and the svm.}, } @inproceedings{yan:2005:ocfs, author =
it is based on a hybrid case-based architecture, where two multilayer perceptrons are integrated into a case-based reasoner.
the average precision of the document type-based search is 88.9\%, while the average precision of the keyword-based search is 31.2\%.
we distinguish model-oriented and description-oriented approaches in probabilistic information retrieval.
{}, year = {2001}, address = {darmstadt, de}, publisher = {}, pages = {24--40},  url = {http://cis.paisley.ac.uk/vino-ci0/fisher_hierarchic.ps}, abstract =  {this paper demonstrates that the probabilistic corpus model which emerges from  the automatic or unsupervised hierarchical organisation of a document  collection can be further exploited to create a kernel which boosts the  performance of state-of-the-art support vector machine document classifiers.
{chai k. adam and hwee t. ng and hai l. chieu}, title = {bayesian online  classifiers for text classification and filtering}, booktitle = {proceedings of  sigir-02, 25th acm international conference on research and development in  information retrieval}, editor = {micheline beaulieu and ricardo baeza-yates  and sung hyon myaeng and kalervo j{\"{a}}rvelin}, publisher = {acm press,  new york, us}, address = {
published in the ``lecture notes in computer science'' series, number 2167}, url = {http://link.springer.de/link/service/series/0558/papers/2167/21670121.pdf}, abstract =
{se{\'{a}}n slattery and  mark craven}, title = {
editor = {marti a. hearst and fredric gey and richard tong}, publisher = {acm  press, new york, us}, address = {berkeley, us}, year = {1999}, pages =
we extracted the words around the gene  occurrences and used them to represent the gene for go domain code annotation.
we propose a completely new approach to the problem of  text classification and extracting keywords by using ml techniques.
conventional classifiers are mostly based on the  vector space model of document, which views a document simply as an  n-dimensional vector of terms.
{1982}, editor = {gerard salton and hans-jochen schneider}, pages = {174--193},  address = {berlin, de}, publisher = {springer verlag, heidelberg, de}, note =
the trec-3 conference provided the first public demonstration and evaluation of this new technique, and trec-4 provided an opportunity to test its usefulness on several types of text retrieval tasks.}, } @inproceedings{hull94, author = {hull, david a.}, title = {
this paper  explores the use of hierarchical structure for classifying a large,  heterogeneous collection of web content.
moreover, a guided strategy for the ocat-based approach is presented for deciding which document one needs to consider next while building the training example sets.}, } @article{nigam00, author = {kamal nigam and andrew k. mccallum and sebastian thrun and tom m. mitchell}, title = {
the final classifier is a hierarchical array of neural networks.
our methodology makes use of a well-known corpus of transcribed and topic-labeled speech (the switchboard corpus), and involves an interesting double use of the boostexter learning algorithm.
{enhanced word clustering for  hierarchical text classification}, booktitle = {proceedings of kdd-02, 8th acm  international conference on knowledge discovery and data mining}, publisher =
in this chapter, we describe two  advances that significantly improve the practicality of our approach.
{proceedings of coling-02, the 19th international conference on computational linguistics}, year = {2002}, editor = {}, pages = {}, address =
this calls for using a feature selection method, not only to  reduce the number of features but also to increase the sparsity of document  vectors.
we  suggest a way for locating duplicates and plagiarisms in a text collection  using an r-measure, which is the normalized sum of the lengths of all suffixes  of the text repeated in other documents of the collection.
adelaide, au}, year = {2003}, pages = {211--214},  url = {}, abstract = {}, } @article{carbonell00, author = {jaime carbonell and  william w. cohen and yiming yang}, title = {guest editors' introduction to the  special issue on machine learning and information retrieval}, journal =
published in the  ``lecture notes in computer science'' series, number 3201}, url = {}, abstract  = {}, } @inproceedings{xu03, author =
most of traditional text categorization approaches utilize term frequency (tf) and inverse document frequency (idf) for representing importance of words and/or terms in classifying a text document.
autoslog is a dictionary construction system that has been shown to  substantially reduce the time required for knowledge engineering by learning  extraction patterns automatically.
we present an extensive experimental evaluation of both algorithms  on three english collections and one chinese corpus.
"karakos, damianos and eisner, jason and khudanpur, sanjeev and priebe, carey e.", title =
we show how to train these models effectively, and how  to use approximate probabilistic inference over the learned model for  collective classification of multiple related entities.
{1994}, address = {dublin, ie}, pages = {3--12}, note = {
eric j. glover and kostas tsioutsiouliklis and steve lawrence and david m. pennock and gary w. flake}, title = {
based on the darmstadt indexing approach, the indexing task is divided into a description step and a decision step.
it is shown that the performance of such a classifier is further enhanced when employing the kernel derived from an appropriate hierarchic mixture model used for partitioning a document corpus rather than the kernel associated with a flat non-hierarchic mixture model.
even though the learning ability and computational complexity of training in  support vector machines may be independent of the dimension of the feature  space, reducing computational complexity is an essential issue to efficiently  handle a large number of terms in practical applications of text  classification.
improving text categorization methods for event tracking}, booktitle = {proceedings of sigir-00, 23rd acm international conference on research and development in information retrieval}, editor = {nicholas j. belkin and peter ingwersen and mun-kew leong}, publisher = {acm press, new york, us}, address = {athens, gr}, year = {2000}, pages = {65--72}, url = {http://www.cs.cmu.edu/~yiming/papers.yy/sigir00.ps}, abstract = {automated tracking of events from chronologically ordered document streams is a new challenge for statistical text classification.
feature generation is accomplished through contextual analysis of document text, implicitly performing word sense disambiguation.
the texts  represent short web-page descriptions from the dmoz open directory web-page  ontology.
{tzeras, konstadinos and hartmann, stephan}, title = {automatic indexing based  on bayesian inference networks}, booktitle = {proceedings of sigir-93, 16th acm  international conference on research and development in information retrieval},  editor = {robert korfhage and edie rasmussen and peter willett}, publisher =
published in the ``lecture notes in  computer science'' series, number 2416}, } @inproceedings{zelikovitz03, author  = {sarah zelikovitz and haym hirsh}, title = {
experiments show that feature selection  using weights from linear svms yields better classification performance than  other feature weighting methods when combined with the three explored learning  algorithms.
the essential formula is cue validity borrowed from cognitive psychology, and used to select from all possible single word based features, the best predictors of a given category.
we provide a theoretical motivation for the algorithm.
experiments on  two real-world spidering tasks show a three-fold improvement in spidering  efficiency over traditional breadth-first search, and up to a two-fold  improvement over reinforcement learning with immediate reward only.}, }  @article{ribeironeto01, author = {berthier ribeiro-neto and alberto h.f.  laender and luciano r. {de lima}}, title = {
while most work on classification either  concentrates on structured data or free text, the work in this paper deals with  both of them.
our approach to text classification uses case-based reasoning to represent natural language contexts that can be used to classify texts with extremely high precision.
"665--670",  booktitle =
conference % = same as 'inproceedings', included for compatibility with older versions % %
df thresholding performed similarly.
oh-woog kwon and jong-hyeok lee},  title = {
(b) we propose a variant of tsvm that involves multiple switching of labels.
in comparison to the  previously proposed agglomerative strategies our divisive algorithm is much  faster and achieves comparable or higher classification accuracies.
show positive results on modestly sized datasets.
in this paper we propose an integration of a selforganizing map and semantic networks from wordnet for a text classification task using the new reuters news corpus.
in this paper we use this as a way to learn on problems  that involve a combination of text and numbers.
"techniques for improving the performance of naive bayes for text classification", booktitle =
the hierarchical structure is initially used to train different second-level classifiers.
a new classifier-centric measure called blocking measure is also defined to examine the performance of subtree classifiers in a top-down level-based hierarchical classification method.}, } @incollection{sun03a, author = {aixin sun and ee-peng lim and wee-keong ng}, title = {hierarchical text classification methods and their specification}, booktitle = {cooperative internet computing}, editor = {alvin t. chan and stephen c. chan and h. v. leong and vincent t. y. ng}, year = {2003}, pages = {236--256}, publisher =
our  approach has the advantage of a very fast training phase, and the rules of the  classifier generated are easy to understand and manually tuneable.
"interactive feature selection", pages =
we demonstrate that, although this categorization problem is  quite different from 'topical' text classification, certain categories of  messages can nonetheless be detected with high precision (above 80%) and  reasonable recall (above 50%) using existing text-classification learning  methods.
"the effect of ocr  errors on stylistic text classification", booktitle =
experimental results show that the improvements to active learning require less than two-thirds as many labeled training examples as previous qbc approaches, and that the combination of em and active learning requires only slightly more than half as many labeled training examples to achieve the same accuracy as either the improved active learning or em alone.}, } @inproceedings{mccallum98b, author = {andrew k. mccallum and ronald rosenfeld and tom m. mitchell and andrew y. ng}, title = {improving text classification by shrinkage in a hierarchy of classes}, booktitle = {proceedings of icml-98, 15th international conference on machine learning}, editor = {jude w. shavlik}, year = {1998}, address = {
this information is combined properly and summarized in two  coordinates.
in earlier work we described an approach for  converting numerical features into bags of tokens so that text classification  methods can be applied to numerical classification problems, and showed that  the resulting learning methods are competitive with traditional numerical  classification methods.
fast-feature techniques have been implemented in a prototype search  engine.
we introduce a new algorithm for performing active learning with support vector machines, i.e., an algorithm for choosing which instances to request next.
with this extended model, we also have improved the well-known probabilistic classification method based on the bernoulli document generation model.
second, they are inaccurate, because  of mistakes made by these indexers as well as the difficulties users have in  choosing keywords for their queries, and the ambiguity a keyword may have.
using four corpora from the topic detection and tracking (tdt) forum and the text retrieval conferences (trec) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings.
{http://www.rdrop.com/~lierer/conald98.ps}, abstract = {
the same techniques can be used to determine if a document is fiction or non-fiction with approximately 98 per cent accuracy.}, } @inproceedings{kosmynin96, author = {arkadi kosmynin and ian davidson}, title = {using background contextual knowledge for documents representation}, booktitle = {proceedings of podp-96, 3rd international workshop on principles of document processing}, editor = {charles k. nicholas and derick wood}, year = {1996}, address = {palo alto, ca}, pages = {123--133}, publisher = {springer verlag, heidelberg, de}, note =
ran el-yaniv and oren souroujon}, title
our approach combines standard information retrieval methods with a text categorization meta-learning scheme that determines when to even venture a guess." } @inproceedings{dayanik:2006:cip, author =
mining the  web's link structure}, journal = {ieee computer}, year = {1999}, number = {8},  volume = {32}, pages = {60--67}, url =  {http://dlib.computer.org/co/books/co1999/pdf/r8060.pdf}, abstract = {
{information retrieval}, publisher = {kluwer academic publishers}, issn =  {1386-4564}, number = {1}, volume = {6}, pages = {49--73}, year = {2003}, url =  {http://www.kluweronline.com/issn/1386-4564}, abstract = {
the other is that there are no training data for this classification problem.
the  relevancy signatures algorithm uses linguistic phrases, the augmented relevancy  signatures algorithm uses phrases and local context, and the case-based text  classification algorithm uses larger pieces of context.
the  recently introduced information bottleneck method provides an information  theoretic framework, for extracting features of one variable, that are relevant  for the values of another variable.
sarah  zelikovitz and haym hirsh}, title = {
text categorization (also known as text classification, or topic spotting) is the task of automatically sorting a set of documents into categories from a predefined set.
this approach builds on previous thresholding work based upon the  beta-gamma algorithm.
this kind of representational problem is also studied in this paper  where traditional methods for statistical text categorization are augmented via  a systematic use of linguistic information.
sven {meyer zu eissen} and benno stein}, title = {genre classification of web pages}, booktitle = {proceedings of ki-04, 27th german conference on artificial intelligence}, publisher = {}, editor = {biundo, susanne and fr{\"{u}}hwirth, thom and palm, g{\"{u}}nther}, address = {ulm, de}, year = {2004}, pages = {}, note = {
{247--254}, url = {http://doi.acm.org/10.1145/956863.956911}, abstract = {support vector machine (svm) learning algorithms focus on finding the hyperplane that maximizes the margin (the distance from the separating hyperplane to the nearest examples)
"2005", month =  "november", address =
by emphasizing  the category discrimination capability of features, the paper firstly puts  forward a new weighting scheme tf*idf*ig.
alexios chouchoulas and qiang shen}, title = {rough set-aided keyword reduction for text categorization}, journal = {applied artificial intelligence}, pages = {843--873}, year = {2001}, volume = {15}, number = {9}, url = {}, abstract = {the volume of electronically stored information increases exponentially as the state of the art progresses.
the use of syntactic parsing to produce indexing phrases has  been widely investigated as a possible route to better text representations.
"2006", pages =  "539--542" } @inproceedings{fu:2005:atc, author =
distribution 1.0}, year = {1997}, note =
several different algorithms have been applied, and support vector machines (svm) have shown very good results.
with the use of  suitable dimensionality reduction techniques and efficient algorithms, both  llsf and expnet successfully scaled to this very large problem with a result  significantly outperforming word-matching and other automatic learning methods  applied to the same corpus.}, } @article{yang96b, author =
"bouma, lucas and de rijke, maarten", title =
our implementation uses an inverted file to store the trained term structures of each newsgroup, and uses a list similar to the inverted file to buffer the newly arrival articles, for efficient routing and updating purposes.
we also show that whirl can  be efficiently used to select from a large pool of unlabeled items those that  can be classified correctly with high confidence.}, } @article{cohen99, author  = {william w. cohen and yoram singer}, title = {context-sensitive learning  methods for text categorization}, journal = {acm transactions on information  systems}, year = {1999}, volume = {17}, number = {2}, pages = {141--173}, url =  {http://www.acm.org/pubs/articles/journals/tois/1999-17-2/p141-cohen/p141-cohen.pdf},  abstract =
moreover, bwm-nbs exhibits the strong stability in categorization performance.}, } @inproceedings{xue04, author =
"we demonstrate that it is possible to perform automatic sentiment  classification in the very noisy domain of customer feedback data.
the combination with content-based  methods can further improve the results, but too much noise may be introduced,  since the text of web pages is a much less reliable source of information.
on uncertainty modeling and analysis (isuma'03)", page = "104--109", address =
the method harnesses reliability indicators---variables that provide a valuable signal about the performance of classifiers in different situations.
this new synthesis of neural networks, learning and information  retrieval techniques allows us to scale up to a real-world task and  demonstrates a lot of potential for hybrid plausibility networks for semantic  text routing agents on the internet.}, } @inproceedings{wermter99a, author =
text categorization is the task of classifying natural language documents into a set of predefined categories.
we describe a classifier of email queries, which executes text  categorization by topic.
we also show that less aggressive clustering sometimes results in  improved classification accuracy over classification without clustering.}, }  @inproceedings{bao01, author = {yongguang bao and satoshi aoyama and xiaoyong  du and kazutaka yamada and naohiro ishii}, title = {a rough set-based hybrid  method to text categorization}, booktitle = {proceedings of wise-01, 2nd  international conference on web information systems engineering}, editor = {m.  tamer {\"o}zsu and hans-j{\"{o}}rg schek and katsumi tanaka and  yanchun zhang and yahiko kambayashi}, publisher =
for these reasons, tcfp can be a useful classifier in the areas, which need a  fast and high-performance text categorization task.}, } @inproceedings{ko02a,  author =
a system that performs text categorization aims to assign  appropriate categories from a predefined classification scheme to incoming  documents.
{2003}, pages = {305--319}, url =  {http://link.springer.de/link/service/series/0558/papers/2633/26330305.pdf},  abstract = {
the paper shows that the accuracy of a naive bayes text classifier can be significantly improved by taking advantage of a hierarchy of classes.
as output, the system evaluates a set of weighted hypotheses about the type of the actual letter.
"beyond {tfidf} weighting  for text categorization in the vector space model", pages =  "1130--1135", booktitle =
"659--660", abstract =
when using a state-of-the-art classifier, knn, the average  accuracy is 96.40\%, outperforming all the other systems evaluated on the same  collection, including the traditional term-word by knn (88.52\%);  sleeping-experts (82.22\%); sparse phrase by four-word sleeping-experts  (86.34\%); and boolean combinations of words by ripper (87.54\%).
modeled as semi-structured documents, e-mail messages consist of a set of fields with predefined semantics and a number of variable length free-text fields.
then, a filtering system with the neural network integrated into it was used to filter the medical documents and this performance was compared with the filtering results achieved using the baseline system.
we also show that  addressing named entities preferentially is useful only in certain situations.
this collection is tailored for automating the attribution of international patent classification codes to patent applications and is made publicly available for future research work.
the model is based on the concept of ``uncertainty sampling'', a  technique that allows for relevance feedback both on relevant and non relevant  documents.
the categorization approach is derived from a combination of a  learning paradigm known as instance-based learning and an advanced document  retrieval technique known as retrieval feedback.
our key insight is that many of  the data sources have their own categorization, and classification accuracy can  be improved by factoring in the implicit information in these source  categorizations.
using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on tdt5, trec10 and trec11.
searching for documents by their type or genre is a natural way to  enhance the effectiveness of document retrieval.
in particular, we develop metrics that  estimate the difficulty of a dataset by examining the host directory structure.
we describe an experience integrating different  innovative ai technologies such as hierarchical pattern matching and  information extraction to provide flexible multilingual classification  adaptable to user needs.
we show that  by using large feature vectors in combination with feature reduction, we can  train linear support vector machines that achieve high classification accuracy  on data that present classification challenges even for a human annotator.
the first approach is the linear combination  approach.
we then discuss an alternative that masks the misclassification based on a well known software fault tolerance technique.
"udo hahn and joachim wermter", title =
it is  natural to assume that the documents are on the multinomial manifold, which is  the simplex of multinomial models furnished with the riemannian structure  induced by the fisher information metric.
we introduce a new algorithm for performing  active learning with support vector machines, i.e., an algorithm for choosing  which instances to request next.
few research works have been carried out on finding better  usages of time information.
"hierarchical text categorization using fuzzy relational  thesaurus", journal =
in addition, we investigate alternative classification and evaluation methods, and the effects that secondary features have on indoor/outdoor classification.
in brief, it tries to avoid considering features whose discrimination capability is sufficiently covered by already selected features, reducing in size the set of the features used to characterize the document set.
for the application of the air/phys system a large-scale dictionary containing more than 600000 word-descriptor relations resp.
{vasant honavar and giora slutzki}, year = {1998}, pages = {244--256}, publisher = {springer verlag, heidelberg, de}, note = {
a new selection technique, scar, is proposed for k-dnf  (disjunctive normal form) learners and evaluated on the reuters financial data  set.
% % % % * publications that discuss novel atc methods, novel % %  experimentation of previously known methods, or resources for % % atc  experimentation; % % % % * publications that discuss applications of atc (e.g.  % % automated indexing for boolean ir systems, filtering, etc.).
our findings suggest that the best results occur when using the very brief descriptions of the {{\sc yahoo!}}\ categorized entries; these brief descriptions are provided either by the entries' submitters or by the {{\sc yahoo!}}\ human indexers and accompany most {{\sc yahoo!}}\-indexed entries.}, } @inproceedings{lai01, author = {kwok-yin lai and wai lam}, title = {meta-learning models for automatic textual document categorization}, booktitle = {proceedings of pakdd-01, 5th pacific-asia conferenece on knowledge discovery and data mining}, editor = {david cheung and qing li and graham williams}, year = {2001}, publisher = {springer verlag, heidelberg, de}, address = {hong kong, cn}, note = {
we proposed and compared four dimensionality reduction techniques to reduce the feature space into an input space of much lower dimension for the neural network classifier.
these assignments might be used for varied purposes such as  filtering, or retrieval.
a stochastic decision list is an ordered sequence of if-then  rules, and our method can be viewed as a rule-based method for text  clsssification having advantages of readability and refinability of acquired  knowledge.
in addition, we investigate alternative classification and  evaluation methods, and the effects that secondary features have on  indoor/outdoor classification.
a number of methods for feature reduction and feature selection in text classification and information retrieval systems are compared.
this knowledge-engineering bottleneck must be addressed before knowledge-based systems will be practical for real-world applications.
one important aspect of text mining is on automatic text categorization, which assigns a text document to some predefined category if the document falls into the theme of the category.
the other unusual features of our research are the longevity of our agents and the fact that they undergo a continual training process; feedback from the user enables the agent to adapt to the user's long-term information requirements.}, } @inproceedings{cohen95, author = {william w. cohen}, title = {
this approach is well suited to learning in hypertext domains because its  statistical component allows it to characterize text in terms of word  frequencies, whereas its relational component is able to describe how  neighboring documents are related to each other by hyperlinks that connect  them.
{ieee computer society press, los alamitos, us}, address = {la jolla, us}, year = {1998}, pages = {2827--2830}, url = {http://www-a2k.is.tokushima-u.ac.jp/member/sasaki/frame_home/papers/smc.ps}, abstract = {document categorization, which is defined as the classification of text documents into one of several fixed classes or categories, has become important with the explosive growth of the world wide web.
however these assumptions are often violated in practice, and poor performance can result.
a patent search and classification system}, booktitle =  {proceedings of dl-99, 4th acm conference on digital libraries}, editor =
however, no significant gain was obtained in the digital library.
"hulth, anette and megyesi, beata  b.", title =
automated information filtering (if) and information retrieval (ir) systems are therefore acquiring rapidly increasing prominence.
{khalid al-kofahi and alex tyrrell and arun  vachher and tim travers and peter jackson}, title = {
cases are categorized into 40 broad,  high-level categories.
the approach benefits from two existing, and in our view complimentary, sets of categorization techniques: those based on rocchio's algorithm and those belonging to the rule learning class of machine learning algorithms.
an effective information filtering system is one that provides the  exact information that fulfills a user's interest with the minimum effort by  the user to describe it.
experimental results indicate that, at the same  level of vector sparsity, feature selection based on svm normals yields better  classification performance than odds ratio- or information gainbased feature  selection when linear svm classifiers are used.}, } @inproceedings{bruckner97,  author = {t. bruckner}, title = {
the use of stw allows the terms that are distributed most differently in the positive and negative examples of the categories of interest to be weighted highest.
{morgan kaufmann  publishers, san francisco, us}, url =  {http://www.cs.technion.ac.il/~gabr/papers/fs-svm.pdf}, abstract = {
for example, google uses the  text in citing documents (documents that link to the target document) for  search.
}  @inproceedings{olsson:2006:act, author =
this dissertation addresses the knowledge-engineering bottleneck for a natural language processing task called ``information extraction''.
using cases to  represent context for text classification}, booktitle =
seattle, us}, year = {2001}, pages = {1286--1291}, url = {}, abstract = {although several attempts have been made to introduce natural language processing (nlp) techniques in information retrieval, most ones failed to prove their effectiveness in increasing performances.
"edgar moyotl-hernandez and hector jimenez-salaz", title =
these experiments have shown that  {\sc adaboost.
[no  abstract]}, } @inproceedings{lewis92, author = {lewis, david d.}, title = {an  evaluation of phrasal and clustered representations on a text categorization  task}, booktitle = {proceedings of sigir-92, 15th acm international conference  on research and development in information retrieval},
we propose a method to solve  the problem.
multiple noise reduction strategies are proposed and evaluated, including: an aggressive removal of ``noninformative words'' from texts before training; the use of a truncated singular value decomposition to cut off noisy ``latent semantic structures'' during training; the elimination of noninfluential components in the llsf solution (a word concept association matrix) after training.
argentinian symposium on artificial intelligence}, editor =
proceedings of ranlp-97, 2nd international conference on recent advances in natural language processing}, publisher
this suggests that df thresholding, the simplest method  with the lowest cost in computation, can be reliably used instead of ig or chi  when the computation of these measures are too expensive.
text categorization experiments supported a  number of predictions of the concept learning model about properties of phrasal  representations, including dimensionality properties not previously measured  for text representations.
it combines clustering and feature selection.
a method for disambiguating word senses in a large corpus}, journal = {computers and the humanities}, year = {1993}, number = {5}, volume = {26}, pages = {415--439}, url = {http://www.research.att.com/~kwc/published_1993_sense.ps}, abstract = {}, } @inproceedings{gao03, author = {sheng gao and wen wu and chin-hui lee and tat-seng chua}, title = {
for both data sets,  boosting trees and svms had acceptable test performance in terms of accuracy  and speed.
published in the ``lecture  notes in computer science'' series, number 2035}, pages = {66--77}, url =  {http://link.springer.de/link/service/series/0558/papers/2035/20350066.pdf},  abstract =
morgan kaufmann publishers, san francisco, us}, url = {http://www.robotics.stanford.edu/~stong/papers/tong_koller_ml00.ps.gz}, abstract = {support vector machines have met with significant success in numerous real-world learning tasks.
daniel p.  lopresti and jiangying zhou}, year =
the profiles involved are quite small, typically 10k bytes for a category training set, and less than 4k bytes for an individual document.
"2006", pages = "257--266", } @inproceedings{zhang:2006:hts, author =
"prague, czech republic" } @inproceedings{li:2007:eci, author =
tampere, fi}, year = {2002}, pages = {137--144}, url  = {http://doi.acm.org/10.1145/564376.564402}, abstract = {to improve  performance in text categorization, it is important to extract distinctive  features for each class.
{morgan kaufmann publishers, san francisco, us}, url = {}, abstract = {}, }  @inproceedings{li03a, author = {fan li and yiming yang}, title = {a loss  function analysis for classification methods in text categorization}, booktitle  =
we conclude by describing the application of our system to automatic  call-type identification from unconstrained spoken customer responses.}, }  @inproceedings{schapire98, author =
in spite of these differences, both ripper and sleeping experts perform extremely well across a wide variety of categorization problems, generally outperforming previously applied learning methods.
the crucial question of  the quality of automatic classification is treated at considerable length, and  empirical data are introduced to support the hypothesis that classification  quality improves as more information about each document is used for input to  the classification program.
{tokyo, jp}, year = {2001}, pages = {307--314}, url =  {http://www.afnlp.org/nlprs2001/pdf/0079-01.pdf}, abstract = {this paper  presents a method for incorporating natural language processing into existing  text categorization procedures.
abstract = "knn and svm are two machine learning approaches to text categorization (tc) based on the vector space model.
text categorization through multistrategy  learning and visualization}, booktitle = {proceedings of cicling-01, 2nd  international conference on computational linguistics and intelligent text  processing}, year = {2001}, editor = {alexander gelbukh}, publisher = {springer  verlag, heidelberg, de}, address = {mexico city, me}, note = {
{lillian lee and donna harman}, pages = {51--57}, address = {pittsburgh, us}, url = {http://www.cs.cornell.edu/home/llee/emnlp/papers/takamura.pdf}, abstract = {
dunja mladeni{\'{c}}}, title
we also compare our empirical results to semi-theoretical results and find that the two closely agree.}, } @inproceedings{ghani01, author = {rayid ghani and se{\'{a}}n slattery and yiming yang}, title = {hypertext categorization using hyperlink patterns and meta data}, booktitle = {proceedings of icml-01, 18th international conference on machine learning}, editor = {carla brodley and andrea danyluk}, address = {williams college, us}, year = {2001}, pages = {178--185}, publisher =
in this paper we explore the use of text-mining methods for the  identification of the author of a text.
we show that although whirl is  designed for more general similarity-based reasoning tasks, it is competitive  with mature inductive classification systems on these classification tasks.
our experiments demonstrate that the part-of-speech approach  is better than traditional bag of words techniques, particularly in the domain  transfer conditions.}, } @inproceedings{fisher03, author = {michelle fisher and  richard everson}, title = {when are links useful?
gori and simone marinai and giovanni soda}, title = {automatic document classification and indexing in high-volume applications}, journal = {international journal on document analysis and recognition}, year = {2001}, number = {2}, volume =
in our experiments we demonstrate significantly superior performance both over a single classifier as well as over the use of the traditional weighted-sum voting approach.
conversely, a category's level is increased to strengthen it if its precision exceeds its recall.
the system could potentially scale up to an operational size of 10 million words of text per year - the equivalent of a dozen bibles or a third of the encyclopedia britannica.
"kolcz,  aleksander and chowdhury, abdur", title = "avoidance of model  re-induction in svm-based feature selection for text categorization",  booktitle =
in this paper, we introduce a method for automating this classi-fication process by using a small number of query probes.
the proposed method divides the documents into sentences, and categorizes each sentence using keyword lists of each category and sentence similarity measure.
information today,  medford, usa}, address = {new york, us}, year = {2000}, pages =
text classification algorithms were used to automatically classify arbitrary search results into an existing category structure on-the-fly.
{46}, number = {1/3}, pages = {423--444}, url =  {http://www.wkap.nl/article.pdf?380516}, abstract = {
the automatic categorisation of web documents is becoming crucial for organising the huge amount of information available in the internet.
they allow us to construct compact feature sets with few elements, with which a satisfactory genre diversi- fication is achieved.
improving rocchio with weakly supervised clustering}, booktitle = {proceedings of ecml-03, 14th european conference on machine learning}, publisher = {springer verlag, heidelberg, de}, editor = {}, year = {2003}, address = {dubrovnik, hk}, pages = {456--467}, url = {}, abstract = {
in this paper we are interested in a  more general formulation where documents are organized as page sequences, as  naturally occurring in digital libraries of scanned books and magazines.
this  paper proposes and analyzes an efficient and effective approach for estimating  the generalization performance of a support vector machine (svm) for text  classification.
we describe a class of text categorization problems that are characterized with many redundant features.
as a first  step toward automatic go annotation, we aim to assign go domain codes given a  specific gene and an article in which the gene appears, which is one of the  task challenges at the trec 2004 genomics track.
this survey discusses the main approaches to  text categorization that fall within the machine learning paradigm.
{95--100}, year = {1998}, url =  {http://www-ai.ijs.si/dunjamladenic/papers/pww/pwwecml98.ps.gz}, abstract =  {this paper describes several known and some new methods for feature subset  selection on large text data.
{international journal of uncertainty, fuzziness and knowledge-based systems},  volume = {9}, number = {6}, pages = {731--741}, year = {2001}, }  @inproceedings{liu02, author = {
in three  tests the percent of results categorized for five representative queries was  high enough to suggest practical benefits: general web search (76-90\%),  government web search (39-100\%), and the bureau of labor statistics website  (48-94\%).
again, as in [14], the addition of  nlp capabilities also suggested a different application of existing methods in  revised forms.
for larger training sizes, accuracy becomes increasingly stable with respect to k and the risk decreases."
ann arbor, michigan},  publisher =
% % % updated and maintained by % % % %
class labels can also be assigned based on known document types, or can  be defined by the user.
on the other hand, a discriminative measure is proposed for term selection and is combined with the plu-based likelihood ratio to determine the text category.
extensive experiments using two benchmarks and a large  real-life collection are conducted.
"the  detection of new information in a document stream is an important component of  many potential applications.
in this paper, we present a weight adjusted k-nearest neighbor (waknn) classification that learns feature weights based on a greedy hill climbing technique.
although we usually estimate the model so that it completely satisfies the equality constraints on feature expectations with the me method, complete satisfaction leads to undesirable overfitting, especially for sparse features, since the constraints derived from a limited amount of training data are always uncertain.
thus, our method seems to be well suited for  heterogeneous document collections.}, } @article{klingbiel73, author = {paul h.  klingbiel}, title = {machine-aided indexing of technical literature}, journal =
an automatic document classification system using pattern recognition techniques}, booktitle = {proceedings of asis-78, 41st
we demonstrate the  effectiveness of our categorization approach using two real-world document  collections from the medline database.
with the increasing availability of lexical resources  in electronic form (including lexical databases (ldbs), machine readable  dictionaries, etc.), there is an interesting opportunity for the integration of  them in learning-based atc.
we conclude that our techniques permit automatic categorisation using very large train-ing collections, vocabularies, and numbers of categories.}, } @article{shin01, author =
{published in the ``lecture notes in computer science'' series, number 1398},  editor = {
in this paper, based on the rule learning algorithm ripper (repeated incremental pruning to produce error reduction), we propose an efficient method for hierarchical document categorization.}, } @inproceedings{sasaki98a, author = {minoru sasaki and kenji kita}, title = {rule-based text categorization using hierarchical categories}, booktitle = {proceedings of smc-98, ieee international conference on systems, man, and cybernetics}, editor = {}, publisher =
"{ocfs}: optimal orthogonal centroid feature selection for text categorization", pages = "122--129", booktitle =
our approach integrates wordnet information with two training approaches through the vector space model.
we propose that two machine learning algorithms, the  widrow-hoff and eg algorithms, be used in training linear text classifiers.
the results we obtain allow us to determine the  relative difficulty of these subsets, thus establishing an indirect means for  comparing tc systems that have, or will be, tested on these different  subsets.}, note = {
in contrast, the text categorization task allows much cleaner determination of text representation properties.
however, the intrinsic geometric structure  of text data has been ignored by standard kernels commonly used in svms.
walid g. aref},  publisher = {acm press, new york, us}, year = {2001}, address
our contributions in this paper are as follows: (a) we provide an  implementation of transductive svm (tsvm) that is significantly more efficient  and scalable than currently used dual techniques, for linear classification  problems involving large, sparse datasets.
our results show that overall,  svms and k-nn lsa perform better than the other methods, in a statistically  significant way.}, } @incollection{caropreso01, author = {maria fernanda  caropreso and stan matwin and fabrizio sebastiani}, title = {a  learner-independent evaluation of the usefulness of statistical phrases for  automated text categorization}, year = {2001}, booktitle = {text databases and  document management: theory and practice}, editor = {amita g. chin}, publisher  = {idea group publishing}, address =
an evaluation of phrasal and clustered representations on a text categorization task}, booktitle = {proceedings of sigir-92, 15th acm international conference on research and development in information retrieval}, editor = {nicholas j. belkin and peter ingwersen and annelise mark pejtersen}, publisher = {acm press, new york, us}, address = {kobenhavn, dk}, pages = {37--50}, year =
this understanding led us to develop a new learning model that transfers induced knowledge through time to benefit future classifier learning tasks.
text categorization}, editor = {laura c. rivero and jorge h. doorn and viviana e. ferraggine}, year = {2005}, booktitle = {
world scientific, singapore, sn}, address = {iizuka, jp}, year = {1998}, pages = {935--938}, url = {http://www-a2k.is.tokushima-u.ac.jp/member/sasaki/frame_home/papers/iizuka98.ps}, abstract = {document categorization, which is defined as the classification of text documents into one of several fixed classes or categories, has become important with the explosive growth of the world wide web.
genre  classification means to discriminate between documents by means of their form,  their style, or their targeted audience.
scoring rules can further take advantage of the hierarchy by  considering only second-level categories that exceed a threshold at the top  level.
experimental results  indicate that our method outperforms not only the method using distributions  based on hard clustering, but also the method using word-based distributions  and the method based on cosine-similarity.}, } @article{li98a, author = {li,  yong h. and jain, anil k.}, title = {classification of text documents}, journal  = {the computer journal}, year = {1998}, volume = {41}, number = {8}, pages =
the discrimination between informative keywords and functional keywords is not crisp.
published  in the ``lecture notes in computer science'' series, number 1609}, url = {},  abstract = {}, } @inproceedings{wang99a, author = {
however, many systems set a relatively high threshold to reduce retrieval of non-relevant documents, which results in the ignorance of many relevant documents.
for example, google uses the text in citing documents (documents that link to the target document) for search.
a relatively high degree of accuracy was achieved by the supervised method, however, classification accuracy varied across classes.
a probabilistic model of dictionary-based automatic  indexing}, booktitle = {proceedings of riao-85, 1st international conference  ``recherche d'information assistee par ordinateur''}, publisher = {}, editor =
we investigate this problem by looking at the task of designing kernels for hypertext classification, where both words and links information can be exploited.
first, unlabeled data can hurt performance in domains where the  generative modeling assumptions are too strongly violated.
this  work provides an important insight on which measures derived from links are  more appropriate to compare web documents and how these measures can be  combined with content-based algorithms to improve the effectiveness of web  classification.}, } @inproceedings{caldon03, author = {patrick caldon}, title =
the learning approach presented is  attribute-efficient and, therefore, appropriate for domains having very large  number of attributes.
for example, in email classification, it is possible to use instance representations that consider not only the text of each message, but also numerical-valued features such as the length of the message or the time of day at which it was sent.
the kind of application that the text categorization  shell, tcs, can produce is characterized.
the method is  quantitative analysis of the glosses of such definitions that these terms are  given in on- dictionaries, and on the use of the resulting term representations  semi-supervised term classification.
the text categorization (tc) is the automated assignment of text documents to predefined categories based on document contents.
address = {honolulu, us}, year = {2002}, pages = {562--569}, publisher = {acm press, new york, us}, url = {http://www.cs.princeton.edu/~kt/www02.ps}, abstract = {
in addition, we investigate alternative classification and  evaluation methods, and the effect that a secondary feature can have on  indoor/outdoor classification.
{georg gottlob and toby walsh}, publisher
in our learning experiments naive bayesian classifier was used on text data.
here, a probabilistic analysis of this algorithm is presented in a text categorization framework.
we provide sufficient conditions that indicate when an  improvement can be expected, highlighting and formalising the notion of  ``independent kernels''.
the projection of documents is performed following subsequent steps.
{association for computational linguistics, morristown, us}, address = {austin, us}, editor = {}, year =
the experimental results show that our new approaches give better results for both micro-averaged f1 and macro-averaged f1 scores.}, } @article{lehnert94, author =
adaptive information filtering (aif) is concerned  with filtering in changing environments.
a new evaluation  methodology is offered that focuses on the needs of the data mining  practitioner who seeks to choose one or two metrics to try that are mostly  likely to have the best performance for the single dataset at hand.
in this paper, we propose a neuro-genetic approach to feature selection in text categorization.
the recently introduced information bottleneck method provides an information theoretic framework, for extracting features of one variable, that are relevant for the values of another variable.
examples are agents for locating information on  world wide web and usenet news filtering agents.
{proceeding of ijcai-01, 17th international joint conference on  artificial intelligence}, editor = {bernhard nebel}, address = {
this paper studies the iterative double clustering (idc)
{yiming yang}, title = {
{sang-bum kim and hae-chang rim}, title = {recomputation of class relevance scores for improving text classification}, booktitle = {proceedings of cicling-04, 5th international conference on computational linguistics and intelligent text processing}, year = {2004}, editor =
however, there is still much room for improving the  effectiveness of these classifiers, and new models need to be examined.
by observing that people who search the web with the same queries often click on different, but related documents together, we draw implicit links between web pages that are clicked after the same queries.
we are also given a training corpus of documents already placed in one or more categories.
"automatic classification of data items, based on training samples, can be boosted by considering the neighborhood of data items in a graph structure (e.g., neighboring documents in a hyperlink environment or co-authors and their publications for bibliographic data entries).
such engines can build  giant indices that let you quickly retrieve the set of all web pages containing  a given word or string.
in general, the disambiguation rules differ for different words.
wendy lehnert and stephen soderland and david aronow and fangfang feng and avinoam shmueli}, title = {inductive text classification for medical applications}, journal = {journal of experimental and theoretical artificial intelligence}, year = {1994}, number = {1}, volume = {7}, pages = {49--80}, url = {}, abstract = {}, } @article{leopold02, author = {leopold, edda and kindermann, j{\"{o}}rg}, title = {
using latent semantic indexing}, booktitle = {proceedings of the 17th acm  international conference on research and development in information retrieval},  editor = {w. bruce croft and van rijsbergen, cornelis j.}, publisher =
"proceedings of the twenty-second international conference on machine learning", year =
= {an application of {least squares fit} mapping to text information retrieval}, booktitle = {proceedings of sigir-93, 16th acm international conference on research and development in information retrieval}, editor = {
we do not believe our results are specific to  ppm.
most existing recommender systems use collaborative filtering  methods that base recommendations on other users' preferences.
we demonstrate that the idc algorithm is especially  advantageous when the data exhibits high attribute noise.
the experimental result on the fbis document corpus shows that the atf algorithm outperforms the pure eg (exponentiated-gradient) algorithm.}, } @inproceedings{yu99, author = {
"it is well known that web-page classification can be enhanced by using hyperlinks that provide linkages between web pages.
several known and some new feature scoring measures appropriate for  feature subset selection on large text data are described and related to each  other.
in order to respond correctly to a free form factual question given a large collection of texts, one needs to understand the question to a level that allows determining some of the constraints the question imposes on a possible answer.
we learn a hierarchical classifier that is guided by a layered semantic hierarchy of answer types, and eventually classifies questions into finegrained classes.
{178--185}, publisher = {
in this paper, a new novelty detection approach  based on the identification of sentence level patterns is proposed.
an effective information filtering system is one that provides the exact information that fulfills user's interests with the minimum effort by the user to describe it.
the results show that filtering significantly improves the recall of the method, and that also has the effect of significantly improving the overall performance.} } @article{combarromdrm05, title =
pisa, it}, pages = {161--172}, year =
"it is  well known that web-page classification can be enhanced by using hyperlinks  that provide linkages between web pages.
proceedings of cicling-02, 3rd international conference on computational linguistics and intelligent text processing}, publisher = {springer verlag, heidelberg, de}, editor = {alexander f. gelbukh}, note = {
"andrea esuli and fabrizio  sebastiani", title =
an existing approach to coping with this problem requires  terms also to be arranged hierarchically.
"111--116", volume = 7, number = 2, year = 2005, url = "http://www.acm.org/sigs/sigkdd/explorations/issues/7-2-2005-12/kddcup2005report_ferrety.pdf", abstract =
the second is a set of training data consisting of labeled regions of hypertext that represent instances of these classes and relations.
{luc de  raedt and arno siebes}, publisher = {springer verlag, heidelberg, de}, address  = {freiburg, de}, year = {2001}, pages =
however, most of existing boosting algorithms are based on classifiers that use binary-valued features.
this paper investigates  the use of supervised clustering in order to create sets of categories for  classification of documents.
finally, a  user study was performed to further investigate the causes for these results.
our experimental results  on three real world data sets show that we achieve substantial improvements  over standard naive bayes classification, while also achieving state of the art  performance that competes with the best known methods in these cases.}, }  @inproceedings{peng03a, author = {fuchun peng and dale schuurmans and shaojun  wang}, title = {language and task independent text categorization with simple  language models}, booktitle = {proceedings of hlt-03, 3rd human language  technology conference}, publisher = {},
our categorisation  approach is based on a probabilistic description-oriented representation of web  documents, and a probabilistic interpretation of the k-nearest neighbour  classifier.
"91--99", volume = 7, number = 2, year = 2005, url = "http://www.acm.org/sigs/sigkdd/explorations/issues/7-2-2005-12/kddcup2005report_organizers.pdf", abstract =
"gabrilovich, evgeniy and markovitch, shaul", title =
the experimental results  show that the proposed method outperforms a direct application of a statistical  learner often used for subject classification.
the levels are adjusted in order to obtain a balance  between recall and precision for each category.
"yiming yang and  shinjae yoo and jian zhang and bryan kisiel", title =
this model allows us to simultaneously take  into account structure and content information.
the algorithms are compared on learning speed and error rate.
this hybrid approach of neural selforganization and symbolic hypernym relationships is successful to achieve good classification rates on 100,000 full-text news articles.
"2004" } @inproceedings{xue:2004:rhd, author =  "dejun xue and maosong sun", title =
taken as a whole, the set of web pages lacks a unifying  structure and shows far more authoring style and content variation than that  seen in traditional text-document collections.
{information processing and management}, year = {2006}, volume = {42}, number = {1}, pages = {155--165}, abstract = {most previous works of feature selection emphasized only the reduction of high dimensionality of the feature space.
in our  classifier, web documents are represented by n-grams (n$\leq 4$) that are easy  to be extracted.
a  tc system for chinese texts using words as features is implemented.
we describe how it is applied to generate two new classification  algorithms; a refined centroid classifier and a refined naïve bayes  classifier.
experiments on 20 newsgroups (20ng), reuters corpus volume 1 (rcv1) and open directory project (odp) data show that ocfs is consistently better than ig and chi with smaller computation time especially when the reduced dimension is extremely small."
{journal of intelligent information systems}, year = {2002}, note = {special  issue on automated text categorization}, volume = {18}, number = {2/3}, pages =
{combining classifiers in text categorization}, booktitle = {proceedings of  sigir-96, 19th acm international conference on research and development in  information retrieval}, editor = {hans-peter frei and donna harman and peter  sch{\"{a}}uble and ross wilkinson}, publisher = {acm press, new york, us},  year = {1996}, address = {z{\"{u}}rich, ch}, pages =
building hierarchical classifiers using class proximity}, booktitle = {proceedings of vldb-99, 25th international conference on very large data bases}, publisher = {morgan kaufmann publishers, san francisco, us}, editor = {
this paper  describes a straightforward active learning heuristic, representative sampling,  which explores the clustering structure of 'uncertain' documents and identifies  the representative samples to query the user opinions, for the purpose of  speeding up the convergence of support vector machine (svm) classifiers.
= {william hersh and  christopher buckley and t.j. leone and david hickman}, title = {{{\sc  ohsumed}}: an interactive retrieval evaluation and new large text collection  for research}, booktitle = {proceedings of sigir-94, 17th acm international  conference on research and development in information retrieval}, editor = {w.  bruce croft and van rijsbergen, cornelis j.}, publisher = {springer verlag,  heidelberg, de}, address = {dublin, ie}, pages = {192--201}, year = {1994}, url  =  {http://www.acm.org/pubs/articles/proceedings/ir/188490/p192-hersh/p192-hersh.pdf},  abstract = {a series of information retrieval experiments was carried out with  a computer installed in a medical practice setting for relatively inexperienced  physician end-users.
we show accurate results on a  large collection of free-form questions used in trec 10.}, }  @inproceedings{li03, author = {cong li and ji-rong wen and hang li}, title =
performances of all three classifiers degrade from the reuters collection to the ohsumed collection, but decision forest remains to be superior.}, } @inproceedings{chen01, author
published in the ``lecture notes in  computer science'' series, number 2997}, pages = {197--208}, url =  {http://springerlink.metapress.com/openurl.asp?genre=article&issn=0302-9743&volume=2997&spage=197},  abstract = {high dimensionality of feature space is a main obstacle for text  categorization (tc).
in many  important text classification problems, acquiring class labels for training  documents is costly, while gathering large quantities of unlabeled data is  cheap.
both autoslog and the text classification algorithms are  evaluated in three domains: terrorism, joint ventures, and microelectronics.
this resulted in slightly reduced performance.
"pascal soucy and guy mineau", title =
we  report on computational experience using this procedure.
although both the contents and time information of news articles are helpful to red, most researches focus on the utilization of the contents of news articles.
data eng", year =  "2005", number =
however, the phrasal pre-processing and pattern matching methods that seem to work for categorization have the disadvantage of requiring a fair amount of knowledge-encoding by human beings.
we then compare adaboost and rocchio over three large text filtering  tasks.
"xiaoyan li and croft, w. bruce", title =
{69--113}, url =  {http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/www/wwkb/overview-aij99.ps.gz},  abstract = {
on feature distributional clustering  for text categorization}, booktitle = {proceedings of sigir-01, 24th acm  international conference on research and development in information retrieval},  editor = {croft, w. bruce and harper, david j. and kraft, donald h. and zobel,  justin}, publisher = {acm press, new york, us}, address = {new orleans, us},  year = {2001}, pages = {146--153}, url =  {http://www.cs.huji.ac.il/labs/learning/papers/sigir.ps.gz}, abstract = {
we present experimental results showing that employing our active  learning method can significantly reduce the need for labeled training  instances in both the standard inductive and transductive settings.}, note =
in  contrast, the text categorization task allows much cleaner determination of  text representation properties.
these  experiments suggest that stopword lists and stemming algorithms may remove or  conflate many words that could be used to create more effective indexing  terms.}, } @inproceedings{riloff96, author = {ellen riloff}, title = {using  learned extraction patterns for text classification}, booktitle =  {connectionist, statistical, and symbolic approaches to learning for natural  language processing}, editor = {stefan wermter and ellen riloff and gabriele  scheler}, pages = {275--289}, year = {1996}, publisher = {springer verlag,  heidelberg, de}, note = {
in general, based on the current recall and precision performance,  as well as the detailed analysis, we show that recurrent plausibility networks  hold a lot of potential for developing learning and robust newswire agents for  the internet.}, } @inproceedings{wibowo02, author = {wahyu wibowo and hugh e.  williams}, title = {simple and accurate feature selection for hierarchical  categorisation},
published in the ``lecture notes in computer science'' series, number 2175}, editor = {floriana esposito}, year = {2001}, pages = {320--325}, address = {bari, it}, url = {http://link.springer.de/link/service/series/0558/papers/2175/21750320.pdf}, abstract = {feature selection and weighting are the primary activity of every learning algorithm for text classification.
documents are typically represented by sparse vectors under the vector space model, where each word in the vocabulary is mapped to one coordinate axis and its occurrence in the document gives rise to one nonzero component in the vector representing that document.
our algorithm has the following  features: it does not need any natural language processing technique and it is  robust for noisy data.
our approach uses a natural language processing task called information extraction as a basis for high-precision text classification.
{recomputation of class relevance scores for improving text classification},  booktitle = {proceedings of cicling-04, 5th international conference on  computational linguistics and intelligent text processing}, year = {2004},  editor = {
this corpus has been developed by eads company in the context of a large web site filtering application.}, } @article{denoyer04, author = {ludovic denoyer and patrick gallinari}, title = {bayesian network model for semi-structured document classification}, journal = {information processing and management}, year = {2004}, volume = {40}, number = {5}, pages = {807--827}, url = {}, abstract = {}, } @article{devel01, author =
{schapire, robert e. and singer, yoram and  singhal, amit}, title =
when user interests change, in  pva, not only the contents, but also the structure of the user profile are  modified to adapt to the changes.
il % % www: http://www.cs.technion.ac.il/~gabr %
neural networks perform equally  well with either set of features and can take advantage of the additional  information available when both feature sets are used as input.}, }  @article{schutze98, author = {hinrich sch{\"{u}}tze}, title = {automatic  word sense discrimination}, journal = {computational linguistics}, year =
text categorization by boosting automatically extracted concepts}, booktitle = {proceedings of sigir-03, 26th acm international conference on research and development in information retrieval}, editor = {jamie callan and gordon cormack and charles clarke and david hawking and alan smeaton}, publisher =
with term-descriptor rules from the dictionary, descriptor indications are formed.
pos tagging and recognition of proper nouns received a specific experimental attention and provided significant effects on measured accuracy.}, } @inproceedings{basili01, author = {roberto basili and alessandro moschitti and maria t. pazienza}, title = {nlp-driven ir: evaluating performances over a text classification task}, booktitle = {proceeding of ijcai-01, 17th international joint conference on artificial intelligence}, editor = {bernhard nebel}, address = {
finally, we propose a natural extension of idc for (semi-supervised) transductive learning where we are given both labeled and unlabeled examples, and present preliminary empirical results showing the plausibility of the extended method in a semi-supervised setting.}, } @inproceedings{escudero00, author = {gerard escudero and llu{\'{\i}}s m{\`{a}}rquez and german rigau}, title = {
text classification using {{\sc whirl}}}, booktitle =
we  proposed and compared four dimensionality reduction techniques to reduce the  feature space into an input space of much lower dimension for the neural  network classifier.
"283--302" }  @inproceedings{isuma03, author =
the voting for a classification is processed on the basis of individual feature projections.
the system selects the best collections for the query.
the category discrimination method (cdm) is a new machine learning algorithm designed specifically for text categorization.
since in both tasks text as a sequence of words is of crucial  importance, propositional learners have strong limitations, although viewing  learning for tc and ie as inductive logic programming (ilp) problems is  obvious, most approaches rather use proprietary formalisms.
{311--322}, url = {http://www.kluweronline.com/issn/0924-669x}, abstract =  {this paper reports our comparative evaluation of three machine learning  methods, namely k nearest neighbor (knn), supportvector machines (svm), and  adaptive resonance associative map (aram) for chinese document categorization.
{william w. cohen}, title = {
the approach makes use of the classification and regression trees (cart) algorithm that has seen application in various areas of machine learning.
= {youngjoong ko and jinwoo park and jungyun seo}, title = {using the feature projection technique based on a normalized voting method for text classification}, journal = {information processing and management}, year = {2004}, volume = {40}, number = {2}, pages = {191--208}, url = {}, abstract = {}, } @inproceedings{koehn02, author = {philipp koehn}, title = {
we tackle two different problems of {\em text categorization} (tc), namely feature selection and classifier induction.
} @inproceedings{seki:2005:atc, author =
brodley}, year = {2004}, address = {banff, ca}, pages = {}, publisher =
then active learning is combined with  expectation-maximization in order to ``fill in'' the class labels of those  documents that remain unlabeled.
"acm press", url = "http://doi.acm.org/10.1145/1099554.1099714",
"274--281", booktitle =  "proceedings of the 28th annual international {acm} {sigir} conference on  research and development in information retrieval", year =  "2005", month =
susan t. dumais and hao chen}, title = {hierarchical classification of web content}, booktitle = {proceedings of sigir-00, 23rd acm international conference on research and development in information retrieval}, editor = {nicholas j. belkin and peter ingwersen and mun-kew leong}, publisher = {acm press, new york, us}, address = {athens, gr}, year = {2000}, pages = {256--263}, url = {http://research.microsoft.com/~sdumais/sigir00.pdf}, abstract = {
for a slim fraction of all  documents (0.77\% for category coding and 1.4\% for subcategory coding), the  algorithm makes assignments that are clearly incorrect.
{2002}, address = {glasgow, uk}, publisher = {springer verlag, heidelberg, de},
we also explore some variants of our gis approach.
we make available detailed, per-category experimental results, as well as corrected versions of the category assignments and taxonomy structures, via online appendices.}, } @inproceedings{lewis91, author = {lewis, david d.}, title = {data extraction as text categorization: an experiment with the {muc-3} corpus.}, booktitle = {proceedings of muc-3, 3rd message understanding conference}, editor = {}, publisher =
experiments with the muc corpus suggest that  case-based text classification can achieve very high levels of precision and  outperforms our previous algorithms based on relevancy signatures.}, }  @article{riloff94, author = {ellen riloff and wendy lehnert}, title =
from a machine learning point of view, this study was  motivated by the size of the inspected data in such applications.
"2004" } @inproceedings{rosso:2004:irt, author =  "paolo rosso and antonio molina and ferran pla and daniel jimenez and  vicent vidal", title =
this application is described in more detail along with experimental  results.}, } @inproceedings{fuhr91b, author = {norbert fuhr and ulrich  pfeifer}, title = {
however, such a search is difficult with current search services, since these services only provide keyword-based search methods that are equivalent to narrowing down the target references according to domains.
comparative analysis shows that the performances achieved are  relatively close to the best tc models (e.g. support vector machines).}, }  @inproceedings{moschitti04, author = {alessandro moschitti and roberto basili},  title = {complex linguistic features for text classification: a comprehensive  study}, booktitle = {proceedings of ecir-04, 26th european conference on  information retrieval research}, editor = {sharon mcdonald and john tait}, year  = {2004}, address = {sunderland, uk}, publisher = {springer verlag, heidelberg,  de}, note = {
the performance of the system using the neural network classifier was generally satisfactory and, as expected, the filtering performance varied with regard to the accuracy rates of classes.}, } @inproceedings{moulinier96, author = {isabelle moulinier and gailius ra{\u{s}}kinis and jean-gabriel ganascia}, title = {
{ieee computer society press, los alamitos, us}, year = {1999}, address = {washington, dc}, pages = {2924--2927}, url = {}, abstract = {
{\em feature selection} (fs) refers to the  activity of selecting, from the set of $r$ distinct features (i.e.\ words)
informative keywords are the ones which reflect the contents of a document.
editor = {}, year = {2003}, address = {grenoble, fr}, pages = {118--120}, url = {http://doi.acm.org/10.1145/958220.958242}, abstract = {
{2003}, booktitle = {proceedings of sac-03, 18th acm symposium on applied  computing}, address = {melbourne, us}, publisher = {acm press, new york, us},  pages = {784--788}, url =  {http://www.math.unipd.it/~fabseb60/publications/sac03b.pdf}, note = {
walid g. aref}, publisher = {acm press, new york, us}, year = {2001}, address
"2006", pages = "210--219", abstract =
we modify the query-by-committee (qbc) method of active learning to  use the unlabeled pool for explicitly estimating document density when  selecting examples for labeling.
but in cases where many features are highly redundant with each other, we must utilize other means, for example, more complex dependence models such as bayesian network classifiers.
from this perspective, bns was the top  single choice for all goals except precision, for which information gain  yielded the best result most often.
the feature selection method presented in this paper is rather simple and computationally efficient.
we explore four different ways of  combining the individual predictions and four different techniques for  identifying relevant text portions.
we believe that database  categorization can be a potentially effective technique for good database  selection, especially in the internet environment, where short queries are  usually submitted.
{153--172}, url = {http://www.wkap.nl/article.pdf?391244}, abstract = {
with the continuing exponential growth of the internet and the more recent growth of business intranets, the commercial world is becoming increasingly aware of the problem of electronic information overload.
published  in the ``lecture notes in computer science'' series, number 2035}, pages =
in the course of investigating this idea, we address the problem of automatic categorization of webpages in the {{\sc yahoo!}}\ directory.
the levels are adjusted in order to obtain a balance between recall and precision for each category.
previous work of this kind has been confined to the range of one through eight words per document.
bayesian online classifiers for text classification and filtering}, booktitle = {proceedings of sigir-02, 25th acm international conference on research and development in information retrieval}, editor = {micheline beaulieu and ricardo baeza-yates and sung hyon myaeng and kalervo j{\"{a}}rvelin}, publisher = {acm press, new york, us}, address = {
statistical methods for categorization, on the other hand, are easy to implement and require little or no human customization.
we show that optimal effectiveness occurs when using only a small proportion of the indexing terms available, and that effectiveness peaks at a higher feature set size and lower effectiveness level for a syntactic phrase indexing than for word-based indexing.
the relevancy signatures algorithm uses linguistic phrases, the augmented relevancy signatures algorithm uses phrases and local context, and the case-based text classification algorithm uses larger pieces of context.
each group in turn is used as the basis for indexing the other.
these scores were combined with several other summary text measures  using linear regression.
{acm press, new york, us}, editor = {}, year = {2002}, address = {edmonton,  ca}, pages = {191--200}, url = {}, abstract = {in this paper we propose a new  information-theoretic divisive algorithm for word clustering applied to text  classification.
we conclude by comparing our approach of representing texts and rules as logic programs to others.}, } @inproceedings{junker01, author = {markus junker and andreas dengel}, title = {preventing overfitting in learning text patterns for document categorization}, booktitle =
this process requires less effort than providing words with no help or manual labeling of documents.
we present experimental results obtained on the  standard \textsf{reuters-21578} benchmark with three classifier learning  methods (rocchio, $k$-nn, and support vector machines), three term selection  functions (information gain, chi-square, and gain ratio), and both local and  global term selection and weighting.}, } @inproceedings{debole04c, author =
specifically,  we define five {\em hypertext regularities} which may (or may not) hold in a  particular application domain, and whose presence (or absence) may  significantly influence the optimal design of a classifier.
the prevailing approach to atc is making use of a collection of prelabeled texts for the induction of a document classifier through learning methods.
lexical databases accumulate information on the lexical items of one or several languages.
researchers have used benchmark data, such as the  reuters-21578 test collection, to measure advances in automated text  categorization.
extensive experiments reported in the paper shows that this new weighting method improves significantly the classification accuracy as measured on many categorization tasks."
a combination of all the above results in a multi-stage ned system that performs much better than baseline single-stage ned systems.}, } @inproceedings{kwok98, author = {james t. kwok}, title = {automated text categorization using support vector machine}, booktitle = {proceedings of iconip'98, 5th international conference on neural information processing}, editor = {}, year = {1998}, address = {kitakyushu, jp}, pages = {347--351}, url = {http://www.comp.hkbu.edu.hk/7ejamesk/papers/iconip98.ps.gz}, abstract = {
six nonjudgmental criteria are used in testing the hypothesis for 100 keyword lists (each list representing a document) for a series of computer runs in which the number of words per document is increased progressively from 12 to 36.
"252--259", abstract = "machine  learning is the mainstay for text classification.
an improved boosting algorithm and  its application to automated text categorization}, booktitle = {proceedings of  cikm-00, 9th acm international conference on information and knowledge  management}, address = {mclean, us}, editor = {arvin agah and jamie callan and  elke rundensteiner}, publisher = {acm press, new york, us}, year = {2000},  pages =
results  show that the text in citing documents, when available, often has greater  discriminative and descriptive power than the text in the target document  itself.
instead, the probability estimates must be renormalized using logistic regression on the known relevance judgements.
the input space  was gradually increased by using mutual information (mi) filtering and  part-of-speech (pos) filtering, which determine the portion of words that are  appropriate for learning from the information-theoretic and the linguistic  perspectives, respectively.
when few examples are  available, we observe that accuracy is sensitive to k and that best k tends to  increase with training size.
this allows the individual models for each topic on the second level to focus on finer discriminations within the group.
"linear prediction models with graph regularization for web-page categorization", booktitle = "proceedings of the twelfth acm sigkdd international conference on knowledge discovery and data mining", year =
ke wang and senquiang zhou
to retain the information in the structure, we have developed a structured vector model, which represents a document with a structured vector, whose elements can be either terms or other structured vectors.
experiments show the ngd kernel on  the multinomial manifold to be effective for text classification, significantly  outperforming standard kernels on the ambient euclidean space."
expnet is used for relevance ranking of candidate categories of an arbitrary text in the case of text categorization, and for relevance ranking of documents via categories in the case of text retrieval.
"shenghuo zhu and xiang ji and wei xu and yihong gong ", title =
the utility of our approach is demonstrated  on a set of web-pages that relate to computer science departments.}, }  @inproceedings{furnkranz99, author = {johannes f{\"{u}}rnkranz}, title =
{edward a. fox and neil rowe}, publisher = {acm press, new york, us}, year = {1999}, address = {berkeley, us}, pages =
{596--600}, url = {}, abstract = {}, } @inproceedings{roth98, author = {dan roth}, title = {learning to resolve natural language ambiguities: a unified approach}, booktitle = {proceedings of aaai-98, 15th conference of the american association for artificial intelligence}, publisher = {aaai press, menlo park, us}, editor = {}, year = {1998}, pages = {806--813}, address = {madison, us}, url = {http://l2r.cs.uiuc.edu/~danr/papers/aaai98.ps.gz},
"proceedings  of the eighth international conference on intelligent text processing and  computational linguistics", year =
{ieee computer society press, los alamitos, us},  editor = {amari, shun-ichi and giles, c. lee and gori, marco and piuri,  vincenzo}, year = {2000}, address = {como, it}, volume = {3}, pages =
{221--226}, url = {}, abstract = {
we describe a text categorization approach that is based on a combination of feature distributional clusters with a support vector machine (svm) classifier.
they  can be used not only to estimate the error rate, but also to estimate recall,  precision, and f1.
about 70\% of the web-documents are assigned to their true genre;  note in this connection that no genre classification benchmark for web pages  has been published so far.}, } @article{mladenic03, author = {
{proceedings of sigir-03, 26th acm international conference on research and development in information retrieval}, editor =
while knn and aram yield better performances than svm on small and clean data sets, svm and aram significantly outperformed knn on noisy data.
however, in the web space, hyperlinks are usually sparse, noisy and thus in many situations can only provide limited help in classification.
bayesian independence classifiers and  k-nearest-neighbor classifiers were trained to assign scores to manually-graded  essays.
with the  development and diffusion of the internet worldwide connection, a large amount  of information is available to the users.
"rome, italy" } @inproceedings{he:2007:inb, author =
{integrating lexical knowledge in learning-based text categorization},  booktitle = {proceedings of jadt-02, 6th international conference on the  statistical analysis of textual data}, publisher = {}, editor = {}, address =
"collective multi-label classification", booktitle =
"pumping documents through a domain and genre classification pipeline", booktitle = "lrec'04", year = "2004" } @inproceedings{cohen:2004:lce, author =
luc de raedt and arno siebes}, publisher = {springer verlag, heidelberg, de}, address = {freiburg, de}, year = {2001}, pages =
in the '90s, with the booming production and availability of on-line documents, automated text categorisation has witnessed an increased and renewed interest.
in addition, the evaluation results given by the kddcup 2005 organizer confirm the effectiveness of our proposed approaches.
occurring in the collection, the subset of $r'\ll r$ features that are most  useful for compactly representing the meaning of the documents.
these maps were then analyzed to obtain the category themes and their structure.
improving linear classifier for  chinese text categorization}, journal = {information processing and  management}, year = {2004}, volume = {40}, number = {2}, pages = {223--237},  url = {}, abstract = {}, } @article{turney00, author = {peter d. turney}, title  = {
the high number of features is reduced by feature subset selection and additionally by using `stop-list', pruning low-frequency features and using a short description of each document given in the hierarchy instead of using the document itself.
experiments over real-world text corpus are carried out, which validates the effectiveness and efficiency of the proposed approach.
this paper we present a new  method for determining orientation of subjective terms.
by using  generalized singular value decomposition (gsvd), a coordinate transformation  that reflects the inherent class structure indicated by the generalized  singular values is identified.
we experiment with various types of descriptions for the {{\sc yahoo!}}\ categories and the webpages to be categorized.
some experiments applying different base classifiers for a multi-label  classifier in the domain of high energy physics on several of these possible  sources have been carried out.
the chain augmented  naive bayes classifiers we propose have two advantages over standard naive  bayes classifiers.
{ieee computer society  press, los alamitos, us}, address = {maebashi city, jp}, year = {2002}, pages =
"isbn 0-387-23535-3", publisher = "springer", year = 2005, pages =
(2) can these genres be reliably identified?
this approach clusters words into groups based on the  distribution of class labels associated with each word.
in contrast to ranking systems, binary text classification systems may need to produce result sets of any size, requiring that sampling be used to estimate their effectiveness.
our method scales well to large data sets, with numerous categories in large hierarchies.
the back data  stores the information about keywords: the frequency for each category, the  number of documents for each category.
one problem is  that it is difficult to create the labeled training documents.
text learning and related intelligent agents: a survey}, journal = {ieee intelligent systems}, year = {1999}, number = {4}, volume = {14}, pages = {44--54}, url = {http://www-ai.ijs.si/dunjamladenic/papers/pww/agentoverieee.ps.gz}, abstract = {analysis of text data using intelligent information retrieval, machine learning, natural language processing or other related methods is becoming an important issue for the development of intelligent agents.
the problem of automatically determining the gender of a document's author would appear to be a more subtle problem than those of categorization by topic or authorship attribution.
it also outperforms the popular svm method in micro-averaging  f1.
gianni  amati and fabio crestani}, title =
{247--254}, url =  {http://doi.acm.org/10.1145/956863.956911}, abstract = {support vector machine  (svm) learning algorithms focus on finding the hyperplane that maximizes the  margin (the distance from the separating hyperplane to the nearest examples)
"large scale semi-supervised linear svms", booktitle =
{boosting and {rocchio} applied to text filtering}, booktitle = {proceedings of sigir-98, 21st acm international conference on research and development in information retrieval},
in addition, aram predictive accuracy and learning  efficiency can be improved by incorporating a set of rules derived from the  reuters category description.
"classifying search  engine queries using the web as background knowledge", journal =  "{sigkdd} explorations", pages =
we  provide an approach for automatically building the implicit links between web  pages using web query logs, together with a thorough comparison between the  uses of implicit and explicit links in web page classification.
the em algorithm is then applied to build the  classifier.
this novel  combination of svm with word-cluster representation is compared with svm-based  categorization using the simpler bag-of-words (bow) representation.
"gongde guo and hui  wang and david bell and yaxin bi and kieran greer", title =
takahiko  kawatani}, title = {topic difference factor extraction between two document  sets and its application to text categorization},
in some applications,  there might be human knowledge available that, in principle, could compensate  for the lack of data.
in contrast, software for text categorization, message filtering, textual data mining, and related tasks is less common.
a set of new scoring measures for feature selection taken from the machine learning domain were evaluated over two well-known collections of documents.
our evaluation using the 1996 reuters corpus which consists of 806,791 documents shows that automatically constructing hierarchy improves classification accuracy."
most of traditional  text categorization approaches utilize term frequency (tf) and inverse document  frequency (idf) for representing importance of words and/or terms in  classifying a text document.
the best of all connectionist architectures presented here achieves near human performance (79.1\%).
we introduce a model for learning patterns for text categorization (the lpt-model) that does not rely on an attribute-value representation of documents but represents documents essentially "as they are".
we  describe a text categorization approach that is based on a combination of  feature distributional clusters with a support vector machine (svm) classifier.
published in the ``lecture notes in computer science'' series, number 2945}, pages = {580--583}, url = {}, abstract = {}, } @article{kim04a, author = {kim, j. and kim, m.}, title = {an evaluation of passage-based text categorization}, journal = {journal of intelligent information systems}, year = {2004}, volume = {23}, number = {1}, pages = {47--65}, url = {http://dx.doi.org/10.1023/b:jiis.0000029670.53363.d0}, abstract = {researches in text categorization have been confined to whole-document-level classification, probably due to lack of full-text test collections.
"recent advances in natural language  processing", year =
this dissertation also describes three algorithms that use information extraction to support high-precision text classification.
the second set of experiments applies the genex algorithm to the task.
the results show multi-agent classification can achieve promising classification results
"on the impact of lexical and linguistic features in  genre and domain-based text categorization", booktitle =
a dynamic probabilistic model to visualise topic evolution in text streams}, journal = {journal of intelligent information systems}, year = {2002}, note = {special issue on automated text categorization}, volume = {18}, number = {2/3}, pages = {107--125}, url = {http://www.wkap.nl/article.pdf?391242}, abstract = {
seattle, us},  year = {2001}, pages = {885--890}, url =  {http://www.cs.rutgers.edu/~sofmac/paper/ijcai2001/macskassy-ijcai2001.pdf},  abstract = {consider a supervised learning problem in which examples contain  both numerical- and text-valued features.
in this paper, we present an alternative  framework that builds on (conditional) markov networks and addresses two  limitations of the previous approach.
physica-verlag,  heidelberg, de}, series = {number 138 in the ``studies in fuzziness and soft  computing'' series}, pages = {81--98}, url =  {http://www.math.unipd.it/~fabseb60/publications/nemis04.pdf}, abstract = {
when an existing document is  used as an exemplar, the completeness and accuracy with which topically related  documents are retrieved is comparable to that of the best existing systems.
"udo hahn and  joachim wermter", title =
{proceedings of coling-02, the 19th international conference on  computational linguistics}, year = {2002}, editor = {}, pages = {}, address =
"proceedings of the 14th {acm}  international conference on information and knowledge management", year =  "2005", month =
"rada mihalcea and samer hassan", title =
to facilitate web object searching and organizing, in this paper, we propose a novel approach to web object indexing, by discovering its inherent structure information with existed domain knowledge.
in this paper, advanced document representations have been investigated.
hong  kong, cn}, publisher =
automated information filtering (if) and  information retrieval (ir) systems are therefore acquiring rapidly increasing  prominence.
adding numbers to text classification}, booktitle = {proceedings of cikm-03, 12th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, editor = {}, year = {2003}, address = {new orleans, us}, pages = {240--246}, url = {http://doi.acm.org/10.1145/956863.956910}, abstract = {many real-world problems involve a combination of both text- and numerical-valued features.
text classification with self-organizing maps: some lessons learned}, journal = {neurocomputing}, year = {1998}, volume = {21}, number = {1/3}, pages = {61--77}, url = {}, abstract = {
in contrast, for the reuters collection, we only  achieve mediocre results.
we present results from text classification experiments that compare relevancy signatures, which use local linguistic context, with corresponding indexing terms that do not.
results obtained from evaluation show that the integration of  wordnet clearly outperforms training approaches, and that an integrated  technique can effectively address the classification of low frequency  categories.}, } @inproceedings{delima98, author = {de lima, luciano r. and  laender, alberto h. and ribeiro-neto, berthier a.}, title = {
the graph neighborhood is taken  into consideration to exploit locality patterns while at the same time avoiding  overfitting.
our experiments on the reuters-21578 benchmark show that boosting is not effective in improving the performance of the base classifiers on common categories.
george  forman}, title =
the module evaluates a large incoming stream of documents to determine which documents are sufficiently similar to a profile at the broad subject level to warrant more refined representation and matching.
{1289--1305}, year = {2003}, url =  {http://www.jmlr.org/papers/v3/forman03a.html}, abstract = {machine learning  for text classification is the cornerstone of document categorization, news  filtering, document routing, and personalization.
"omar zaidan and jason eisner and  christine piatko", title =
the method is evaluated using the umls metathesaurus as the underlying hierarchical structure, and the ohsumed test set of medline records.
specifically, the classifier provides an efficient information extraction and takes the meaning of words into consideration.
this architecture acquires its language model and dictionary adaptively and hence avoids handcoding of either.
physica-verlag, heidelberg, de}, series = {number 138 in the ``studies in fuzziness and soft computing'' series}, pages = {81--98}, url = {http://www.math.unipd.it/~fabseb60/publications/nemis04.pdf}, abstract = {
the choice of the kernel function is crucial to most applications of support vector machines.
an on-demand web crawler and iii.)
after identifying the weakness and strength of each  technique, we propose a new technique known as the generalized instance set  (gis) algorithm by unifying the strengths of lnn and linear classifiers and  adapting to characteristics of text categorization problems.
for the svm implementation we used  both a version of schoelkopf et al.
in assigning 124 documents to 9  categories, there were 97 cases of agreement with professional indexers.
three different cost scenarios are identified, and suitable cost-sensitive evaluation functions are employed.
this is achieved by the exploitation of the a priori  domain knowledge available, that there are relatively homogeneous temporal  segments in the data stream.
we view this result as a confirmation of the usefulness of classifiers that represent contextual information.}, } @inproceedings{crammer02, author = {koby crammer and yoram singer}, title = {a new family of online algorithms for category ranking}, booktitle = {proceedings of sigir-02, 25th acm international conference on research and development in information retrieval}, editor =
{348--355}, url = {http://doi.acm.org/10.1145/584792.584850}, abstract = {
experiments  on a large-scale chinese document collection with 71,674 texts show that the f1  metric of categorization performance of bwm-nbs gets to 94.9\% in the best  case, which is 26.4\% higher than that of tf*idf, 19.1\% higher than that of  tf*idf*ig, and 5.8\% higher than that of bwm under the same condition.
in addition, unlike other feature selection models --- which typically require different feature selection parameters for categories at different hierarchical levels --- our technique works equally well for all categories in a hierarchical structure.
we have performed experiments which results show that the ideas we describe are promising and deserve further investigation.}, } @inproceedings{gomez02a, author = {g{\'o}mez-hidalgo, jos{\'e} m.}, title = {
the design goals for tcs are discussed, and other approaches  to text categorization in the light of these goals are examined.
because of efficiency, the latter is more suitable for text data such as web documents.
{lewis, david d. and fan li and tony rose and yiming  yang}, title = {{reuters corpus volume 1} as a text categorization test  collection}, journal = {journal of machine learning research}, volume = {5},  month = {april}, pages = {361--397}, year = {2004}, url =  {http://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf}, abstract = {reuters  corpus volume i (rcv1) is an archive of over 800,000 manually categorized  newswire stories recently made available by reuters, ltd. for research  purposes.
{warsaw, pl}, publisher = {springer verlag, heidelberg, de}, note = {
even better results can be obtained, when  replacing the likelihood-ratio-statics by a new measure for pruning; this we  call l-measure.
we consider a family of models that take into account the fact that relevant documents may contain irrelevant passages; the originality of the model is that it does not explicitly segment documents but rather considers all possible segmentations in its final score.
{ieee computer society press,  los alamitos, us}, address = {la jolla, us}, year = {1998}, pages =
"rochester, ny" }  @inproceedings{zaidan:2007:uar, author =
the document collection is  trained by a self-organizing map to form two feature maps.
we then consider bayesian extensions to nb that achieve higher accuracy by relaxing its strong independence assumptions.
"this  paper shows how citation-based information and structural content (e.g., title,  abstract) can be combined to improve classification of text documents into  predefined categories.
our approach is based on iterative relaxation labeling and can be combined with either bayesian or svm classifiers on the feature spaces of the given data items.
for this reason we propose a method for the bootstrapping process that makes a first hypothesis of categorization for a set of unlabeled documents, with respect to a given empty hierarchy of concepts.
a number of linear classification methods such as the linear least squares fit (llsf), logistic regression, and support vector machines (svm's) have been applied to text categorization problems.
furthermore we demonstrate that the hsom is able to  map large text collections in a semantically meaningful way and therefore  allows a ``semantic browsing'' of text databases.}, } @article{paijmans98,  author = {paijmans, hans}, title = {
"tailby, ross and dean, richard and milner, ben and smith,  dan", title =
this algorithm is the ``semi-supervised fuzzy c-means'' (ssfcm).
a hierarchical approach to the automatic categorization of medical documents}, booktitle = {proceedings of cikm-98, 7th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, editor
therefore, this method can be used in areas where low-cost text categorization  is needed.
"olsson, j. scott", title =  "an analysis of the coupling between training set and neighborhood sizes  for the knn classifier", booktitle =
furthermore, detailed analysis of the retrieval  performance on each individual test query is provided.}, }  @inproceedings{lang95, author = {ken lang}, title = {{\sc newsweeder}: learning  to filter netnews}, booktitle = {proceedings of icml-95, 12th international  conference on machine learning}, editor = {armand prieditis and stuart j.  russell}, address = {lake tahoe, us}, pages = {331--339}, year = {1995},  publisher =
we also incorporate a batch updating scheme to periodically do maintenance on the term structures of the news database after training.
{information fusion}, year = {2002}, number = {4}, volume = {3}, pages =  {299--312}, url = {}, abstract = {
a new reference collection of patent documents for training and  testing automated categorization systems is established and described in  detail.
the main problem is to identify what words are best suited to classify the documents in such a way as to discriminate between them.
this paper proposes a semi-automatic process (interleaved with human suggestions) whose aim is to minimize (simplify) the work required to the administrators when creating, modifying, and maintaining directories.
this weight is then assigned to all of the words in the bin.
when combined with the classification power of the  svm, this method yields high performance in text categorization.
the two main factors that characterize a  text are its content and its style, and both can be used as a means of  categorization.
"we demonstrate that it is possible to perform automatic sentiment classification in the very noisy domain of customer feedback data.
combining machine  learning and hierarchical indexing structures for text categorization},  booktitle = {proceedings of the 10th asis/sigcr workshop on classification  research}, editor = {}, publisher =
the authors developed an approach to automatically generate categories and reveal the hierarchical structure among them.
the paper in hand presents results from a user  study on web genre usefulness as well as results from the construction of a  genre classifier using discriminant analysis, neural network learning, and  support vector machines.
we discuss the potential reasons for this dependency.}, } @article{bekkerman03, author
{37--53}, url = {http://www.jmlr.org/papers/volume6/kim05a/kim05a.pdf}, abstract = {
"john blitzer and mark dredze and fernando pereira", title =
} @inproceedings{boese:2005:ewd, author =  "boese, elizabeth sugar and howe, adele e.", title =
this paper addresses personal e-mail filtering by casting it in the  framework of text classification.
with the latter, we provide a theoretical sound justification for the various parameters of the k-nearest neighbour classifier.
two ways to respond to this challenge are (1)
experiments on  the newsgroups and the reuters-21578 dataset indicate improved performance of  the proposed classifier in comparison to other state-of-the-art methods on  datasets with a small number of positive examples.}, } @article{tsay04, author  = {jyh-jong tsay and jing-doo wang}, title = {
this hypothesis is  systematically tested with three different measures of word relevance, on two  different corpus (one of them considered in three different splits), and with  both local and global vocabularies.
using the reuters-21578 corpus, we obtain an improvement in running time of over a factor of three and a 5\% improvement in f-measure.}, } @inproceedings{dalessio98, author = {stephen d'alessio and keitha murray and robert schiaffino and aaron kershenbaum}, title = {category levels in hierarchical text categorization}, booktitle = {proceedings of emnlp-98, 3rd conference on empirical methods in natural language processing}, year = {1998}, publisher = {association for computational linguistics, morristown, us}, editor = {}, pages = {}, address = {granada, es}, url = {http://www.iona.edu/cs/facultypublications/emnlpf.pdf}, abstract = {
documents are represented as feature-vectors that include n-grams  instead of including only single words (unigrams) as commonly used when  learning on text data.
the new approach is based on extracting patterns, in the form of two  logical expressions, which are defined on various features (indexing terms) of  the documents.
empirical  results indicate that these classifiers are comparable with the best text  classification systems.
"2005", month =  "august", address =
simpl uses efficient sequential scans and sorts, and  is comparable in speed and memory scalability to widely-used naive bayes (nb)  classifiers, but it beats nb accuracy decisively.
this is still true even for the  maximum entropy (me) method, whose flexible modeling capability has alleviated  data sparseness more successfully than the other probabilistic models in many  nlp tasks.
we show on three text  categorization data sets that this approach can rescue what would otherwise be  disastrously bad training situations, producing much more effective  classifiers."
these are based on ranking text sequences by their cross-entropy calculated  using a fixed order character-based markov model adapted from the ppm text  compression algorithm.
at  their core, our algorithms employ recently developed modified finite newton  techniques.
{proceedings of  cikm-93, 2nd international conference on information and knowledge management},  publisher = {acm press, new york, us}, editor = {bharat bhargava and timothy  finin and yelena yesha}, year = {1993}, address = {new york, us}, pages =
"intelligent  {gp} fusion from multiple sources for text classification", booktitle =  "proceedings of the 14th {acm} international conference on information and  knowledge management", year =
{tzigov chark, bl}, pages = {}, year = {1997}, url =  {http://xxx.unizar.es/ps/cmp-lg/9709007}, abstract = {automatic text  categorization (tc) is a complex and useful task for many natural language  applications, and is usually performed through the use of a set of manually  classified documents, a training collection.
published in the ``lecture notes in computer science'' series, number  2734}, abstract = {
df thresholding  performed similarly.
this  distribution can be estimated very efficiently from the data which immediately  leads to an efficient model selection algorithm.
we show  that our specialization of the naive bayes classifier is considerably more  accurate (7 to 29\% absolute increase in accuracy) than a standard  implementation.
{luc devroye and claude chrisment}, address =
the idea is to identify sub-topics of the original classes which can help improve the categorization process.
we identify the inductive biases of each  classifier and explore how boosting, as an error-driven resampling mechanism,  reacts to those biases.
} @inproceedings{sahlgren:2004:ubc, author =
experimental results on these data sets confirm  that waknn consistently outperforms other existing classification algorithms.},  } @article{hanauer96, author = {david hanauer}, title = {
the automation of  ddc's indexing has been machine-aided from its inception.
we propose a theory of genres as bundles of facets, which correlate with various surface cues, and argue that genre detection based on surface cues is as successful as detection based on deeper structural properties.}, } @inproceedings{khmelev03, author = {dmitry v. khmelev and william j. teahan}, title = {a repetition based measure for verification of text collections and for text categorization}, booktitle = {proceedings of sigir-03, 26th acm international conference on research and development in information retrieval}, editor = {jamie callan and gordon cormack and charles clarke and david hawking and alan smeaton}, publisher = {acm press, new york, us}, address = {toronto, ca}, year = {2003}, pages = {104--110}, url = {http://doi.acm.org/10.1145/860435.860456}, abstract = {
the second concerns  abis, an intelligent agent for supporting users in filtering data from  distributed and heterogeneous archives and repositories.
the algorithm checks the context surrounding the target noun against that of previously observed instances and chooses the sense for which the most evidence is found, where evidence consists of a set of orthographic, syntactic, and lexical features.
published in the ``lecture notes in computer science'' series, number 2004}, pages = {423--436}, url = {http://link.springer.de/link/service/series/0558/papers/2004/20040423.pdf}, abstract = {a new way of representing texts written in natural language is introduced, as a conditional probability distribution at the letter level learned with a variable length markov model called adaptive context tree model.
the high number of features is reduced by taking into account the hierarchical structure and using feature subset selection based on the method known from information retrieval.
"edgar moyotl-hernandez  and hector jimenez-salaz", title =
to this end, we propose a set of style markers  including analysis-level measures that represent the way in which the input  text has been analyzed and capture useful stylistic information without  additional cost.
we demonstrate that precision and recall can be significantly improved by solving the categorization problem taking hierarchy into account.
by assuming that documents are created by a parametric generative model, expectation-maximization (em) finds local maximum a posteriori models and classifiers from all the data|labeled and unlabeled.
the goal of the work  described here is to automatically categorize web documents in order to enable  effective retrieval of web information.
booktitle = {proceedings of pkdd-00, 4th european  conference on principles of data mining and knowledge discovery}, editor =
} } @inproceedings{kibriya:2004:mnb, author = {ashraf m. kibriya and eibe frank and bernhard pfahringer and geoffrey holmes}, title = {multinomial naive bayes for text categorization revisited}, booktitle = {proceedings of ai-04, 17th australian joint conference on artificial intelligence}, year = {2004}, pages = {488--499}, address = {cairns, australia}, series = {lecture notes in artificial intelligence}, volume = {3339}, publisher = {springer-verlag}, url = {http://www.cs.waikato.ac.nz/~eibe/pubs/kibriya_et_al_cr.ps.gz}, abstract =
the paper firstly  defines a criterion to identify the high-degree biased chinese bigrams.
the experimental results show that the system using a simple classifier achieved 95.31\% accuracy.
the innovative  aspects of this work are the feature selection process, the automated threshold  determination for classification scores, and an experimental study on real-word  web documents that can be associated to any node in the hierarchy.
this paper presents  an alter-native approach for the use of text classification methods for  super-vised learning problems with numerical-valued features in which the  numerical features are converted into bag-of-words features, thereby making  them directly usable by text classification methods.
this paper introduces a new effective model for text  categorization with great corpus (more or less 1 million documents).
benchmark experiments showed that their predictive performance were roughly comparable, especially on clean and well organized data sets.
this algorithm alleviates the  problem of local minimum in the tsvm optimization procedure while also being  computationally attractive.
tdfa obtains the axes that  maximize the ratio between the document sets as to the sum of squared  projections by solving a generalized eigenvalue problem.
international  symposium on computer and information sciences}, editor = {u. gudukbay and t.  dayar and a. gursoy and erol gelenbe}, publisher = {ios press, amsterdam, nl},  year = {1998}, address = {ankara, tr}, pages = {135--142}, url =  {ftp://ftp.cs.bilkent.edu.tr/pub/tech-reports/1998/bu-ceis-9809.ps.z}, abstract  = {
fast-feature techniques have been implemented in a prototype search engine.
integrating linguistic resources in tc through wsd}, journal = {computers and the humanities}, year = {2001}, number = {2}, volume = {35}, pages =
experiments show the bin-based method is highly competitive with other current methods.
relevance feedback  on a small portion (0.05~0.2%) of the tdt5 test documents yielded significant  performance improvements, measuring up to a 54% reduction in ctrk and a 20.9%  increase in t11su (with b=0.1), compared to the results of the top-performing  system in tdt2004 without relevance feedback information."
in comparison against a recently proposed technique that appears to be the only one of the kind, we obtained up to 18.5\% of improvement in effectiveness while reducing the processing time dramatically.
{proceedings of kdd-98, 4th international conference on knowledge discovery and  data mining}, editor = {
the feature quantity: an information-theoretic perspective of  tfidf-like measures}, booktitle = {proceedings of sigir-00, 23rd acm  international conference on research and development in information retrieval},  editor = {nicholas j. belkin and peter ingwersen and mun-kew leong}, publisher  = {acm press, new york, us}, address = {athens, gr}, year = {2000}, pages =  {104--111}, url = {http://doi.acm.org/10.1145/345508.345556}, abstract = {
{1999}, address = {tokushima, jp}, pages = {153--158}, url = {}, abstract = {},  } @inproceedings{labrou99, author = {yannis labrou and tim finin}, title =  {{{\sc yahoo!}} as an ontology: using {{\sc yahoo!}}\ categories to describe  documents}, booktitle = {proceedings of cikm-99, 8th acm international  conference on information and knowledge management}, publisher = {acm press,  new york, us}, editor = {}, year = {1999}, address = {kansas city, us}, pages =
in the literature, many feature types are proposed for document classification.
we introduce appropriate cost-sensitive measures, investigating at the same time the effect of attribute-set size, training-corpus size, lemmatization, and stop lists, issues that have not been explored in previous experiments.
-means is to cluster the given categories in a hierarchy.
most of  all, tcfp is about one hundred times faster than k-nn.
our experiments on a variety of text categorization tasks indicate  that there is significant potential in improving classifier performance by  feature reweighting, beyond that achieved via selective sampling alone  (standard active learning) if we have access to an oracle that can point to the  important (most predictive) features.
different transformations of input data: stemming, normalization, logtf and idf, together with dimensionality reduction, are found to have a statistically significant improving or degrading effect on classification performance measured by classical metrics -- accuracy, precision, recall, f$_1$ and f$_2$. the emphasis of the study is not on determining the best document representation which corresponds to each classifier, but rather on describing the effects of every individual transformation on classification, together with their mutual relationships.} } @inproceedings{radovanovic:2006:ibd, author = {milo\v{s} radovanovi\'c and mirjana ivanovi\'c}, title = {interactions between document representation and feature selection in text categorization}, booktitle = {proceedings of dexa-06, 17th international conference on database and expert systems applications}, year = {2006}, pages = {489--498}, series = {lecture notes in computer science}, volume = {4080}, address = {krakow, poland}, publisher =
we call the algorithm  hierarchical bayesian clustering (hbc).
given a  user's information need, some patterns in sentences such as combinations of  query words, named entities and phrases, may contain more important and  relevant information than single words.
however, it significantly degrades precision when ambiguity arises, i.e., when there exist more than one candidate category to which a document can be assigned.
the results support the conjecture that it is the sophistication of  the feature weighting method rather than its apparent compatibility with the  learning algorithm that improves classification performance.}, }  @inproceedings{mladenic98a, author = {
the proposed learning model is the core of a prototype information filtering system called profile.}, } @article{amati97b, author = {gianni amati and daniela d'aloisi and vittorio giannini and flavio ubaldini}, title = {
in particular, it addresses the following questions: why can support vector machines handle the large feature spaces in text classification effectively?
to solve this highly nonlinear optimization problem, we use a generalized probabilistic descent algorithm.
our categorisation approach is based on a probabilistic description-oriented representation of web documents, and a probabilistic interpretation of the k-nearest neighbour classifier.
montreal, ca}, year = {2000}, pages =
douglas h. fisher}, year = {1997}, address = {
these results help explain why co-training algorithms are both discriminative in nature and robust to the assumptions of their embedded classifiers.}, } @phdthesis{nigam01, author = {kamal nigam}, title = {using unlabeled data to improve text classification}, school = {computer science department, carnegie mellon university}, address = {pittsburgh, us}, year = {2001}, url = {http://www-2.cs.cmu.edu/~knigam/papers/thesis-nigam.pdf}, abstract = {
"feature selection based on the shapley value", pages =
it is used in our research to guide and improve the  ga-based evolution of the feature subsets.
improving text categorization using the importance of sentences}, journal = {information processing and management}, year = {2004}, volume = {40}, number = {1}, pages = {65--79}, url = {}, abstract = {}, } @article{ko04a, author
documents are typically represented by  sparse vectors under the vector space model, where each word in the vocabulary  is mapped to one coordinate axis and its occurrence in the document gives rise  to one nonzero component in the vector representing that document.
{st-malo, fr}, pages = {}, year = {2002}, url = {http://www.cavi.univ-paris3.fr/lexicometrica/jadt/jadt2002/pdf-2002/gomez_debuenaga_urena_martin_garcia.pdf}, abstract = {automatic text categorization (atc) is an important task in the field of information access.
"hema raghavan and omid madani and rosie jones", title =
text categorization is then cast as the problem of finding coordinate transformations that reflects the inherent similarity from the data.
three characteristic properties of this domain are (a) very high dimensionality, (b) both the learned concepts and the instances reside very sparsely in the feature space, and (c) a high variation in the number of active features in an instance.
therefore, the proposed novelty detection approach focuses on the identification of previously unseen query-related patterns in sentences.
we then categorize  documents using this a priori knowledge of the definition of each category.
} @inproceedings{zhang:2005:igf, author =
this information is combined properly and summarized in two coordinates.
"mark sandler", title = "on the use of linear programming for unsupervised text classification", booktitle =
can models have two advantages  over standard naive bayes classifiers.
we argue that these algorithms which categorize documents by learning a linear separator in the feature space have a few properties that make them ideal for this domain.
our experimental results demonstrate that the technique of refining the training set reduces from one-third to two-thirds of the storage.
second, they permit straightforward application of  sophisticated smoothing techniques from statistical language modeling, which  allows one to obtain better parameter estimates than the standard laplace  smoothing used in naive bayes classification.
for second one, the standard  reuters benchmark, svm classifier using augmentation with pairs outperforms all  previously reported results.}, } @inproceedings{rau91, author = {lisa f. rau  and paul s. jacobs}, title = {
{aaai press, menlo park, us}, editor = {alain rappaport and reid smith}, year =
in the framework of multi-domain  text-to-speech synthesis it is essential to (i) design a hierarchically  structured database for allowing several domains in the same speech corpus and  (ii) include a text classification module that, at run time, assigns the input  sentences to a domain or set of domains from the database.
published in the ``lecture notes in computer science'' series, number 3201}, url = {}, abstract = {}, } @inproceedings{xu03, author = {
we refer to the original data as rcv1-v1, and the corrected data as rcv1-v2.
"proceedings of the 6th acm/ieee-cs joint  conference on digital libraries", year =
yet, for many text classification tasks, providing labeled training documents is expensive, while unlabeled documents are readily available in large quantities.
booktitle = {proceedings of ecdl-00, 4th european conference on research  and advanced technology for digital libraries}, editor = {jos{\'e} l. borbinha  and thomas baker}, publisher = {springer verlag, heidelberg, de}, note =
in our literature survey, we have found that the existing  hierarchical classification experiments used a variety of measures to evaluate  performance.
for the first of them, a  collection of articles from pc week magazine, the addition of word-pairs  increases micro-averaged breakeven accuracy by more than 6\% point from a  baseline accuracy (without pairs) of around 40\%.
however, the overall  conclusion of our paper is that support vector machines are still the method of  choice if the aim is to maximize accuracy.} }  @inproceedings{makrehchi:2005:tcu, author = {masoud makrehchi and mohamed s.  kamel}, title = {
{acm transactions on information systems}, year = {1994}, number = {3}, volume = {12}, pages = {233--251}, url = {http://www.acm.org/pubs/articles/journals/tois/1994-12-3/p233-apte/p233-apte.pdf}, abstract =
we look at some of their statistical properties, and determine for each representation the optimal choice of classification parameters and the effect of term selection.
evaluation against the reuters-21578 collection  shows both techniques have levels of performance that approach benchmark  methods, and the ability of one of the classifiers to produce realistic  measures of confidence in its decisions is shown to be useful for prioritizing  relevant documents.}, } @inproceedings{lee02c, author = {kang hyuk lee and judy  kay and byeong ho kang and uwe rosebrock}, title = {
the goal of the research  described here is to automatically create a computer understandable world wide  knowledge base whose content mirrors that of the world wide web.
{journal of the association for computing machinery}, year = {1964}, volume =
we have developed a text classifier that misclassifies only 13\% of the documents in the reuters benchmark; this is comparable to the best results ever obtained.
we describe a methodology  and system (named accio) for automatically acquiring labeled datasets for text  categorization from the world wide web, by capitalizing on the body of  knowledge encoded in the structure of existing hierarchical directories such as  the open directory.
we demonstrate the potential of these networks using an 82,339 word corpus from the reuters newswire, reaching recall and precision rates above 92\%.
"we  describe the experience and lessons learned from developing a range of  electronic services for a specialist engineering company.
these include feature sets that are constructed by latent semantic indexing, `local dictionaries' in the form of the words that score highest in frequency in positive class examples and feature sets that are constructed by relevance feedback strategies such as j.j. rocchio's (1971) feedback algorithm or genetic algorithms.
support  vector machines (svm) is a machine-learning algorithm which has been shown to  be highly effective for automatic text categorisation.
"text categorization, which consists of automatically assigning  documents to a set of categories, usually involves the management of a huge  number of features.
moreover, a new measure for the evaluation of system performances has been introduced in order to compare three different techniques (flat, hierarchical with proper training sets, hierarchical with hierarchical training sets).
we describe the results of extensive experiments using optimized  rule-based induction methods on large document collections.
comparisons with traditional rocchio's algorithm  adapted for text categorization, as well as flat neural network classifiers are  provided.
the measurement is called the strength of a term and is a  measure of how strongly the term`s occurrences correlate with the subjects of  documents in the database.
in this chapter, we describe two advances that significantly improve the practicality of our approach.
in our learning experiments, for each of the  subproblems, naive bayesian classifier was used on text data.
the algorithm was evaluated on a large database of email messages that fall into five subjective categories.
the hierarchical structure of  categories may be generated by recursively applying the same method.
booktitle = {proceedings of sigir-95, 18th acm international conference on research and development in information retrieval}, editor =
our approach to classification  is based on "visual similarity" of layout structure and is  implemented by building a supervised classifier, given examples of each class.
this ability depends on both text representation and feature filtering.
the dimension of the feature vectors is then reduced by linear transformation, keeping the essential information.
skarmeta and amine bensaid and nadia tazi}, title = {data  mining for text categorization with semi-supervised agglomerative hierarchical  clustering}, journal = {international journal of intelligent systems}, year =
{acm press, new york, us}, editor = {henrique paques and ling liu and david grossman}, year = {2001}, address = {atlanta, us}, pages = {105--113}, url = {http://www.stanford.edu/~krist/papers/cikm2001.pdf}, abstract = {documents are commonly categorized into hierarchies of topics, such as the ones maintained by yahoo! and the open directory project, in order to facilitate browsing and other interactive forms of information retrieval.
{lewis, david d.}, title = {representation and learning in information  retrieval}, school = {department of computer science, university of  massachusetts}, address = {
{hanley and belfus}, year = {1996}, address = {washington, us}, pages = {358--362}, url = {http://www.cs.cmu.edu/afs/cs/user/yiming/www/courses/bibliography/papers/scamc96.ps}, abstract = {whether or not high accuracy classification methods can be scaled to large applications is crucial for the ultimate usefulness of such methods in text categorization.
the effectiveness of the llsf mapping and the significant improvement over alternative approaches was evident in the tests.}, } @article{yang94, author = {yiming yang and christopher g. chute}, title = {
"recent advances in natural language processing", year =
we develop a framework to incorporate unlabeled data in the error-correcting output coding (ecoc) setup by decomposing multiclass problems into multiple binary problems and then use co-training to learn the individual binary classification problems.
the rationale behind our proposal is that taking into account contextual information provided by the whole page sequence can help disambiguation and improves single page classification accuracy.
training algorithms for linear text classifiers}, booktitle = {proceedings of sigir-96, 19th acm international conference on research and development in information retrieval}, editor =
we treat a document as a set of phrases, which  the learning algorithm must learn to classify as positive or negative examples  of keyphrases.
{ieee computer society press, los alamitos, us}, address = {maebashi city, jp}, year = {2002}, pages = {187--194}, url = {http://dlib.computer.org/conferen/icdm/1754/pdf/17540187.pdf}, abstract = {with the rapid growth of textual information available on the internet, having a good model for classifying and managing documents automatically is undoubtly important.
in this article, an approach based on unknown words is proposed for meaningful term extraction and discriminative term selection in text categorization.
experimental results show  that (1) using an enhanced representation of web documents is crucial for an  effective categorisation of web documents, and (2) a theoretical interpretation  of the k-nearest neighbour classifier gives us improvement over the standard  k-nearest neighbour classifier.}, } @inproceedings{goldberg95, author =
springer verlag, heidelberg, de}, address = {seoul, ko}, note =
defined as the activity of automatically % % building, by means of machine  learning techniques, automatic text % % classifiers, i.e. systems capable of  assigning to a text % % document one or more thematic categories from a  predefined set.
to specify a user's problem solving task, we introduce  the concept of document types that directly relate to the problem solving  tasks; with this approach, users can easily designate problem solving tasks.
malcolm p. atkinson and maria e. orlowska and patrick  valduriez and stanley b. zdonik and michael l. brodie}, year = {1999}, address  = {edinburgh, uk}, pages = {363--374}, url =  {http://www.comp.nus.edu.sg/~wangk/pub/vldb99.ps}, abstract = {
"ling yin and richard power", title =
in this paper a system for analysis and automatic indexing of  imaged documents for high-volume applications is described.
text retrieval systems typically produce a ranking of documents and let a user decide how far down that ranking to go.
the results are analyzed from multiple goal  perspectives-accuracy, f-measure, precision, and recall-since each is  appropriate in different situations.
for this purpose, we describe a new  probabilistic model #rst, which is then combined with logistic regression, thus  yielding a generalization of the original model.
combining labeled and unlabeled data for multiclass text categorization}, booktitle = {proceedings of icml-02, 19th international conference on machine learning}, editor = {}, year = {2002}, address = {sydney, au}, pages = {}, publisher = {morgan kaufmann publishers, san francisco, us}, url = {http://www.accenture.com/xdoc/en/services/technology/publications/ghani-icml02.pdf}, abstract =
our approach is based on a new and improved family of boosting algorithms.
in preparation for the use of text  categorization to study text representation, a more effective and theoretically  well-founded probablistic text categorization algorithm was developed, building  on work by maron, fuhr, and others.
the architecture relies on hidden markov models whose emissions are bag-of-words resulting from a multinomial word event model, as in the generative portion of the naive bayes classifier.
evaluating cost-sensitive  unsolicited bulk email categorization}, booktitle = {proceedings of sac-02,  17th acm symposium on applied computing}, editor = {}, address = {madrid, es},  pages = {615--620}, year = {2002}, url =  {http://doi.acm.org/10.1145/508791.508911}, abstract = {
"stein, sterling stuart and argamon, shlomo and frieder, opher", title =
in this paper, we introduce a new weighting method based on  statistical estimation of the importance of a word for a specific  categorization problem.
text categorization using feature projections}, booktitle = {proceedings of coling-02, the 19th international conference on computational linguistics}, year = {2002}, editor = {}, pages = {}, address = {taipei, tw}, url = {http://acl.ldc.upenn.edu/coling2002/proceedings/data/area-28/co-269.pdf}, abstract = {automatic text categorization is a problem of automatically assigning text documents to predefined categories.
even when multi-labels are sparse, the models improve subset classification error by as much as 40%."
{marti a. hearst and haym hirsh}, title = {machine learning in  information access.
in three tests the percent of results categorized for five representative queries was high enough to suggest practical benefits: general web search (76-90\%), government web search (39-100\%), and the bureau of labor statistics website (48-94\%).
the set of all indications from a document leading to the same descriptor is called a relevance description.
tackling the poor assumptions  of naive bayes text classifiers}, booktitle = {proceedings of icml-03, 20th  international conference on machine learning}, editor = {}, year = {2003},  address = {washington, dc}, pages = {}, publisher =
four of the six criteria indicate the hypothesis holds, and two point to no effect.
accessing information from world wide web pages as an approach to problem solving has become commonplace.
journal of machine learning research}, volume = {3}, month = {march}, pages = {1289--1305}, year = {2003}, url = {http://www.jmlr.org/papers/v3/forman03a.html}, abstract = {
we describe a method for improving the classification of short text strings using a combination of labeled training data plus a secondary corpus of unlabeled but related longer documents.
kernels are problem-specific functions that act as an interface  between the learning system and the data.
for each category, all corresponding document texts from the  training sample are concatenated to a megadocument, which is indexed using  standard methods.
on the web,  category-based portals such as yahoo!
text classification and segmentation  using minimum cross-entropy}, booktitle =
machine learning schemes fare better because they automatically eliminate irrelevant features and concentrate on the most discriminating ones.}, } @inproceedings{frasconi01, author = {paolo frasconi and giovanni soda and alessandro vullo}, title = {
the rule base defines what categories the application can assign to texts and contains rules that make the categorization decisions for particular texts.
{3}, pages =  {268-278}, url = {}, abstract = {}, } @inproceedings{hearst91, author = {marti  a. hearst}, title = {
toward optimal  feature selection.
combining statistical and relational methods for learning in hypertext domains}, booktitle = {proceedings of ilp-98, 8th international conference on inductive logic programming}, publisher = {springer verlag, heidelberg, de}, note = {
using three  hypertext datasets and three well-known learning algorithms (naive bayes,  nearest neighbor, and first order inductive learner), we examine these  regularities in different domains, and compare alternative ways to exploit  them.
{analia amandi and ricardo zunino}, year = {1999}, address = {buenos aires, ar}, pages = {7--35}, url = {http://www.math.unipd.it/~fabseb60/publications/asai99.pdf}, note = {
this  problem is pervasive in web marketplaces and portals.
takahiko kawatani}, title = {topic difference factor extraction between two document sets and its application to text categorization}, booktitle = {proceedings of sigir-02, 25th acm international conference on research and development in information retrieval}, editor = {micheline beaulieu and ricardo baeza-yates and sung hyon myaeng and kalervo j{\"{a}}rvelin}, publisher = {acm press, new york, us}, address = {
therefore, the indexing system needs some fault-tolerating features.
the essential formula is cue validity borrowed from cognitive psychology, and used to select from all possible single word-based features the `best` predictors of a given category.
in an effort to deal more  effectively with this large vocabulary and improve information processing, a  method of focus has been developed which allows one to classify terms based on  a measure of their importance in describing the content of the documents in  which they occur.
{ieee computer society, los alamitos, us}, url = {http://www.cs.cmu.edu/~rayid/mypapers/icdm01.ps}, abstract = {
we investigate two meta-model approaches for the task of automatic textual document categorization.
{1999}, pages = {167--196}, publisher =
first, terms (single words or phrases) are identified  in the document text.
oh-woog kwon and jong-hyeok lee}, title = {
experimental results indicate that our method outperforms not only the method using distributions based on hard clustering, but also the method using word-based distributions and the method based on cosine-similarity.}, } @article{li98a, author = {li, yong h. and jain, anil k.}, title = {classification of text documents}, journal = {the computer journal}, year = {1998}, volume = {41}, number = {8}, pages = {537--546}, url = {}, abstract = {
we analyze the relative utility of document text, and the text in citing documents near the citation, for classification and description.
users can search for pants or classify patent text.
"proceedings of the  twelfth acm sigkdd international conference on knowledge discovery and data  mining", year =
feature selection using one-sided metrics selects the features most indicative of membership only, while feature selection using two-sided metrics implicitly combines the features most indicative of membership (e.g. positive features) and non-membership (e.g. negative features) by ignoring the signs of features.
we consider three classification techniques which have decision rules that are derived via explicit error minimization: linear discriminant analysis, logistic regression, and neural networks.
with term-descriptor rules from the dictionary,  descriptor indications are formed.
the feature selection method presented in this paper is  rather simple and computationally efficient.
they are  accurate, robust, and quick to apply to test instances.
"incorporating prior knowledge with weighted margin support vector machines", booktitle = "kdd'04", year = "2004", pages = "326-333", month = "august", address =
this paper proposes a new method to recognize and handle concept changes with support vector machines.
this paper describes an approach to feature subset selection that takes into account problem specifics and learning algorithm characteristics.
we find that our simple corrections result in a fast algorithm that is competitive with state-of-the-art text classification algorithms such as the support vector machine.}, } @inproceedings{rennie99, author = {jason rennie and andrew k. mccallum}, title = {using reinforcement learning to spider the web efficiently}, booktitle = {proceedings of icml-99, 16th international conference on machine learning}, editor = {ivan bratko and saso dzeroski}, year = {1999}, address = {bled, sl}, publisher = {morgan kaufmann publishers, san francisco, us}, pages = {335--343}, url = {http://www.watson.org/~jrennie/papers/icml99.ps.gz}, abstract = {consider the task of exploring the web in order to find pages of a particular kind or on a particular topic.
stanford, us},  year = {1996}, note = {
martha e. pollack}, publisher  = {morgan kaufmann publishers, san francisco, us}, year = {1997}, address =
"aaai press", abstract =
this combination is based on the notion of classifier reliability and presented gains of up to 14\% in micro-averaged f1 in the web collection.
"baoping zhang and yuxin chen and weiguo fan and edward a. fox and marcos  goncalves and marco cristo and pavel calado", title =
especially the vagueness of spatial and temporal terms needs to be addressed.
{probabilistic learning for information filtering}, booktitle = {proceedings of riao-97, 1st international conference ``recherche d'information assistee par ordinateur''}, editor =
for this specific medical categorization  problem, new query formulation and weighting methods used in the  k-nearest-neighbor classifier improved performance.}, }  @inproceedings{larkey98, author =
both autoslog and the text classification algorithms are evaluated in three domains: terrorism, joint ventures, and microelectronics.
a study on thresholding strategies for text  categorization}, booktitle = {proceedings of sigir-01, 24th acm international  conference on research and development in information retrieval}, editor =
learning}, in which information on the membership of training documents in  categories is used.
{1995}, address = {seattle, us}, pages = {246--254}, url =  {http://www.research.att.com/~lewis/papers/lewis95b.ps}, abstract = {
despite a limited number of training examples, combining an effective feature selection with the chi(2) learning algorithm for training the text classifier results in an adequate categorization of new magazine articles.}, } @inproceedings{mooney00, author = {raymond j. mooney and loriene roy}, title = {content-based book recommending using learning for text categorization}, booktitle = {proceedings of dl-00, 5th acm conference on digital libraries}, editor = {}, publisher = {acm press, new york, us}, year = {2000}, address = {san antonio, us}, pages = {195--204}, url = {ftp://ftp.cs.utexas.edu/pub/mooney/papers/libra-dl-00.ps.gz}, abstract = {recommender systems improve access to relevant products and information by making personalized suggestions based on previous examples of a user's likes and dislikes.
computers in biology and medicine}, year = {1996}, volume = {26}, number = {3}, pages = {209--222}, url = {}, abstract = {
we look at some of their statistical  properties, and determine for each representation the optimal choice of  classification parameters and the effect of term selection.
we propose  to use adaboost to optimally combine weak hypotheses based on both types of  features.
the hypothesis that cdm's performance will exceed two non domain specific algorithms, bayesian classification and decision tree learners, is empirically tested.}, } @inproceedings{gomez02, author = {g{\'o}mez-hidalgo, jos{\'e} m. and de buenaga rodr{\'{\i}}guez, jos{\'e} m. and ureña l{\'o}pez, luis a. and mart{\'{\i}}n valdivia, maria t. and garc{\'{\i}}a vega, manuel}, title = {integrating lexical knowledge in learning-based text categorization}, booktitle = {proceedings of jadt-02, 6th international conference on the statistical analysis of textual data}, publisher = {}, editor = {}, address =
in an unsupervised setting, our models produced  coherent clusters with a very natural interpretation, even for instance types  that do not have any attributes.}, } @inproceedings{taskar02, author = {ben  taskar and pieter abbeel and daphne koller}, title = {discriminative  probabilistic models of relational data}, booktitle = {proceedings of uai-02,  18th conference on uncertainty in artificial intelligence}, year = {2002},  address = {edmonton, ca}, pages = {485--492}, publisher = {morgan kaufmann  publishers, san francisco, us}, editor = {}, url = {}, abstract = {
in addition, a comparison of the two filtering methods clarified that pos filtering on svms consistently outperformed mi filtering, which indicates that svms cannot find irrelevant parts of speech.
published in the  ``lecture notes in computer science'' series, number 3201}, url = {}, abstract  = {}, } @inproceedings{biebricher88, author = {peter biebricher and norbert  fuhr and gerhard knorz and gerhard lustig and michael schwantner}, title = {
the  determination of categories and their hierarchical structures were most done by  human experts.
"the world wide web is a massive corpus that constantly evolves.
the results that can be obtained in this space are satisfactory with respect to the best state-of-the-art performances.}, } @inproceedings{dorre99, author = {
the use of classification algorithms to guide topical crawlers has been sporadically suggested in the literature.
amherst, us}, year = {1994}, url =
we present the results of experimenting with theseus, a  classifier that exploits this technique.}, } @inproceedings{avancini03, author  = {henri avancini and alberto lavelli and bernardo magnini and fabrizio  sebastiani and roberto zanoli}, title = {expanding domain-specific lexicons by  term categorization}, year = {2003}, booktitle = {proceedings of sac-03, 18th  acm symposium on applied computing}, address = {melbourne, us}, publisher =
kalervo j{\"{a}}rvelin  and james allan and peter bruza and mark sanderson}, publisher
william w.  cohen}, title = {
usually, these bigrams are likely to survive for their strength of discriminating documents after the process of feature selection.
experimental comparison given on real-world data collected from web users shows that characteristics of the problem domain and machine learning algorithm should be considered when feature scoring measure is selected.
serge  abiteboul and anne-marie vercoustre}, publisher = {springer verlag, heidelberg,  de}, note = {
"many classification problems require classifiers  to assign each single document into more than one category, which is called  multi-labelled classification.
= {hermann fangmeyer and gerhard lustig}, title = {experiments with the cetis automated indexing system}, booktitle = {proceedings of the symposium on the handling of nuclear information}, publisher = {international atomic energy agency}, editor = {}, year = {1970}, address = {}, pages = {557--567}, url = {}, abstract = {}, } @inproceedings{ferilli01, author = {stefano ferilli and nicola fanizzi and gianni semeraro}, title = {learning logic models for automated text categorization}, booktitle = {proceedings of ai*ia-01, 7th congress of the italian association for artificial intelligence}, publisher = {springer verlag, heidelberg, de}, note = {
this paper presents a probabilistic mixture modeling framework for the hierarchic organisation of document collections.
the final classifier is  a hierarchical array of neural networks.
one way is to ask the user to provide them, which is difficult because the user usually can only give a few words (which are insufficient for accurate learning).
stanford, us},  pages = {1183--1190}, publisher = {morgan kaufmann publishers, san francisco,  us}, url = {ftp://ftp.cs.rutgers.edu/pub/zelikovi/bg1.ps}, abstract = {
for  meaningful term extraction, a phrase-like unit (plu)-based likelihood ratio is  proposed to estimate the likelihood that a word sequence is an unknown word.
in order to reduce human efforts, there has been increasing  interest in applying active learning for training text classifiers.
in the flat non-hierarchical case, a model distinguishes a second-level category from all other second-level categories.
last, m. and szczepaniak, p. s. and volkovich, z. and kandel, a.}, pages = {191--200}, series = {studies in computational intelligence}, volume = {23}, publisher = {springer-verlag}, url = {http://perun.im.ns.ac.yu/radovanovic/publications/2006-awic-cats.pdf}, abstract = {
"on the impact of lexical and linguistic features in genre and domain-based text categorization", booktitle =
% % % % this bibliography resides at % %  http://www.cs.technion.ac.il/~gabr/resources/atc/atcbibliography.bib % %
many approaches have been devised for mining various kinds of knowledge from texts.
with the advent of centralized data warehouses, where data might be  stored as electronic documents or as text fields in databases, text mining has  increased in importance and economic value.
sentiment classification using machine  learning techniques}, booktitle = {proceedings of emnlp-02, 7th conference on  empirical methods in natural language processing}, year = {2002}, publisher =
svms performed best when using binary features.
w. bruce croft and alistair moffat and van rijsbergen, cornelis j. and ross wilkinson and justin zobel}, publisher = {acm press, new york, us}, year = {1998}, address
in this paper, we assess to what extent feature selection can be used without  causing a loss in effectiveness.
these assignments might be used for varied purposes such as filtering, or retrieval.
eric j. glover and kostas tsioutsiouliklis  and steve lawrence and david m. pennock and gary w. flake}, title = {
robert korfhage and edie rasmussen and peter willett}, publisher =
this problem can be tackled since a couple of  recent learners (ripper and scar) do not require a preprocessing step.
since tcfp algorithm is very simple, its implementation and training process can be done very easily.
{seattle, us}, pages = {229--237}, url = {ftp://parcftp.xerox.com/pub/qca/papers/sigir95.ps.gz}, abstract = {
finally, we measure the  accuracy achieved with all words and all hm pairs combined, which turns out to  be only marginally above the baseline.
{aaai press, menlo park, us}, year = {1997}, pages = {591--596}, address =
we propose a  new method of text classification using stochastic decision lists.
the training approaches we test are the rocchio (relevance feedback) and the widrow-hoff (machine learning) algorithms.
in a second experiment, we ignored nouns, verbs and adjectives and replaced them by grammatical tags and bigrams.
our experiments using the webkb dataset showed that iwum improves the overall classification performance and works very well on the more structured parts of a web site.}, } @inproceedings{taghva00, author = {taghva, kazem and nartker, thomas a. and julie borsack and steven lumos and allen condit and ron young}, title = {
kobenhavn, dk},  pages =
we show that the benefit of using a first-order representation in this domain is relatively modest; in particular, the performance difference between flipper and foil and their propositional counterparts is quite small, compared to the differences between foil and flipper.
given an unlabeled  document d, we need to find its class label, ci, using the mapping function f  where f(d) =
{http://ls6-www.informatik.uni-dortmund.de/bib/fulltext/ir/klas_fuhr:00.ps.gz}
all the presented experiments are based on unrestricted text downloaded from the world wide web without any manual text preprocessing or text sampling.
without a doubt, updating the classification model frequently rather than using the old model for a very long period is absolutely essential.
thomas dean}, publisher = {morgan kaufmann publishers, san francisco, us}, year = {1999}, pages = {674--679}, address = {stockholm, se}, url = {http://www.iit.demokritos.gr/~paliourg/papers/ijcai99.ps.gz}, abstract = {word sense disambiguation (wsd) is the process of distinguishing between different senses of a word.
in this work we reproduce these results and go further to show that when the training sample is small word clusters can yield significant improvement in classification accuracy (up to 18\%) over the performance using the words directly.}, } @inproceedings{soucy01, author = {pascal soucy and guy w. mineau}, title = {a simple feature selection method for text classification}, booktitle = {proceeding of ijcai-01, 17th international joint conference on artificial intelligence}, editor = {bernhard nebel}, address = {
we conclude that memory-based  anti-spam filtering for mailing lists is practically feasible, especially when  combined with additional safety nets.
we present a new approach to learning hypertext classifiers that combines a statistical text-learning method with a relational rule learner.
while most of the existing investigations of an automated genre classification are based on news articles corpora, the idea here is applied to arbitrary web pages.
machine learning methods for automatically producing categorization rules have similarly seen increased attention, as a way to reduce the cost of fielding categorization systems.
a lot of research is going on with the goal of automating this time-consuming task.
experiments also show that the proportion of  meaningful terms extracted from training data is relative to the classification  accuracy in outside testing.}, } @inproceedings{lam01, author = {wai lam and  kwok-yin lai}, title = {a meta-learning approach for text categorization},  booktitle = {proceedings of sigir-01, 24th acm international conference on  research and development in information retrieval}, editor =
the concept learning  model suggests that the poor statistical characteristics of a syntactic  indexing phrase representation negate its dsirable semantic characteristics.
conventional methods such as decision trees have had competitive, but not optimal, predictive performance.
we describe the new representations and try to justify our hypothesis that they could improve the performance of a rule based learner.
"2006", pages = "179--190" } @inproceedings{freschi:2006:foe, author =
the hypothesis  that cdm's performance will exceed two non domain specific algorithms, bayesian  classification and decision tree learners, is empirically tested.}, }  @inproceedings{gomez02, author = {g{\'o}mez-hidalgo, jos{\'e} m. and de buenaga  rodr{\'{\i}}guez, jos{\'e} m. and ureña l{\'o}pez, luis a. and  mart{\'{\i}}n valdivia, maria t. and garc{\'{\i}}a vega, manuel}, title =
page variation is more prodigious than the data's raw scale:
using k-nearest neighbor (knn) as the classifier and five evaluation benchmark collections as the testbets, three common thresholding methods were investigated, including rank-based thresholding (rcut), proportion-based assignments (pcut) and score-based local optimization (scut); in addition, new variants of these methods are proposed to overcome significant problems in the existing approaches.
the importance of  different features is reported.
patents are distributed through hundreds of collections, divided up by general area.
we will  discuss in detail issues pertaining to three different problems, namely  document representation, classifier construction, and classifier evaluation.},  } @incollection{sebastiani05, author = {fabrizio sebastiani}, title = {
"gabrilovich, evgeniy  and markovitch, shaul", title =
eibe frank and chang chui and ian h. witten}, title = {
{http://www.research.att.com/~lewis/papers/lewis94c.ps}, abstract = {
"2005", month = "july", address =
hence, different algorithms may be employed for different categories.
this paper describes a new method for the classification of a html  document into a hierarchy of categories.
lexical databases accumulate information on  the lexical items of one or several languages.
} @article{lee:2006:igd, author = {lee, changki and lee, gary geunbae}, title = {information gain and divergence-based feature selection for machine learning-based text categorization}, journal =
% % % % concerning urls from which to download on-line copies of the % % papers, where possible i have included urls with unrestricted % % access (e.g. home pages of authors).
we investigate this  problem by looking at the task of designing kernels for hypertext  classification, where both words and links information can be exploited.
it has often been observed that compression seems to provide a very promising approach to categorization.
the automation of ddc's indexing has been machine-aided from its inception.
in this paper, we describe an automated learning approach to text categorization based on perceptron learning and a new feature selection metric, called correlation coefficient.
we use a novel stop word identification method to automatically generate domain-specific stoplists which are much larger than a conventional domain-independent stoplist.
"chen wenliang and zhu jingbo  and wu honglin and yao tianshun", title =
the hypernym relation in wordnet supplements  the neural model in classification.
{proceeding of riao-00, 6th  international conference ``recherche d'information assistee par ordinateur''},  editor = {}, address = {paris, fr}, year = {2000}, pages = {}, url = {},  abstract = {several methods for classifying and segmenting text are described.
ottawa, ca}, year = {1998}, url = {http://ai.iit.nrc.ca/ii_public/classification/thesis.pdf}, abstract = {}, } @inproceedings{scott99, author
finally, the effectiveness of term distributions to improve classification accuracy is explored with regard to the training set size and the number of classes.}, } @article{leung97, author = {chi-hong leung and wing-kay kan}, title = {a statistical learning approach to automatic indexing of controlled index terms}, journal = {journal of the american society for information science}, year = {1997}, number = {1}, pages = {55--67}, volume = {48}, url = {http://www3.interscience.wiley.com/cgi-bin/fulltext?id=39602&placebo=ie.pdf}, abstract = {
the accuracy of the supervised classifier was established by comparing  its performance with a baseline system that uses human classification  information.
atlanta, us}, pages = {97--104}, url = {http://doi.acm.org/10.1145/502585.502603}, abstract = {
in empirical tests, it consistently showed more than 10 points f-measure improvement for each of four reuters categories tested." } @inproceedings{sindhwani:2006:lss, author = "sindhwani, vikas and keerthi, s. sathiya", title =
an extended version appears as~\cite{debole04a}}, abstract = {
text categorization is typically formulated as a concept learning problem where each instance is a single isolated document.
{proceedings of icgi-98, 4th international colloquium on grammatical  inference}, address = {ames, us}, editor =
in this work we investigate the usefulness of {\em $n$-grams} for document indexing in text categorization (tc).
we developed a user interface that organizes web search results into hierarchical categories.
"jun yan and ning liu and benyu zhang and shuicheng yan and zheng chen and qiansheng cheng and weiguo fan and wei-ying ma", title =
in this paper, we compare a number of known linear  classification methods as well as some variants in the framework of regularized  linear systems.
the categorization approach is derived from a combination of a learning paradigm known as instance-based learning and an advanced document retrieval technique known as retrieval feedback.
{bringing order to the web: automatically categorizing search results},  booktitle = {proceedings of chi-00, acm international conference on human  factors in computing systems}, publisher = {acm press, new york, us}, editor =
attics is an extensible text classification system we have implemented in c++.
we use the technique of genetic programming (gp), (koza and rice  1992), to evolve classifying agents.
a method for using the knowledge about the hierarchy to gain better categorization results is discussed.
automatic text categorization (tc) is a complex and useful task for many natural language applications, and is usually performed through the use of a set of manually classified documents, a training collection.
using the results evaluated on the other versions of reuters which  exclude the unlabelled documents, the performance of twelve methods are  compared directly or indirectly.
the representations are evaluated using the ripper learning algorithm on the reuters-21578 and digitrad test corpora.
by doing so, model probability and classification accuracy come into  correspondence, allowing unlabeled data to improve classification performance.
} @inproceedings{keerthi:2005:gli,  author =
this paper describes an accurate, relatively inexpensive method for the disambiguation of noun homographs using large text corpora.
http://doi.acm.org/10.1145/585058.585079}, abstract = {categorisation of  digital documents is useful for organisation and retrieval.
we show that such unlabeled background knowledge can  greatly decrease error rates, particularly if the number of examples or the  size of the strings in the training set is small.
joining statistics with nlp for text categorization}, booktitle = {proceedings of anlp-92, 3rd conference on applied natural language processing}, publisher = {association for computational linguistics, morristown, us}, editor = {marcia bates and oliviero stock}, year = {1992}, address = {
both algorithms have been among the best performers in text categorization experiments so far.
then, we show that feature selection speeds up the time required to automatically build the categorization system.}, } @inproceedings{myers00, author = {kary myers and michael kearns and satinder singh and marilyn a. walker}, title = {a boosting approach to topic spotting on subdialogues}, booktitle = {proceedings of icml-00, 17th international conference on machine learning}, editor = {pat langley}, year = {2000}, address = {stanford, us}, pages = {655--662}, publisher =
the texts to be indexed are abstracts written in english.
published in the ``lecture notes in computer science'' series, number 2389}, } @inproceedings{kolcz01, author = {
{springer-verlag}, url = {http://www.springerlink.com/content/4ytxvxmjea83ctqv/}, abstract = {feature selection method for text classification based on information gain ranking, improved by removing redundant terms using mutual information measure and inclusion index, is proposed.
serrano, j.i. and del castillo, m.d.}, title = {evolutionary learning of document categories}, journal = {information retrieval}, volume = {10}, number = {1}, pages = {69--83}, year = {2007}, month = {january} } @article{ceci:2007:cwd, author = {michelangelo ceci and donato malerba}, title = {
journal of machine learning research}, volume = {7}, pages = {2673--2698}, year = {2006} } @article{bratko:2006:esi, author =
{published in the ``lecture notes in computer science'' series, number 1293},  url = {}, abstract = {
"2005", month =  "july", address =
"improving naive bayes text classifier using smoothing  methods", booktitle =
combining naive bayes $n$-gram and language models for text classification}, booktitle = {
to use traditional feature-vector- based learning methods, one could treat the presence or ab-sence of a word as a boolean feature and use these binary-valued features together with the numerical features.
in this paper, we propose a  different approach.
as more information becomes  available on-line, intelligent information retrieval will be crucial in order  to navigate the information highway efficiently and effectively.
one important issue in a large-scale meta-search engine is to select text databases that are likely to contain useful documents for a given query.
however, the complexity of natural languages and the extremely high dimensionality of the feature space of documents have made this classification problem very difficult.
we report the results of systematic experimentation of these two methods performed on the standard {\sc reuters-21578} benchmark.}, } @article{gale93, author = {
we report on experiments with different kernels for both of  these implementations and with different representations of the data, including  binary vectors, tf-idf representation and a modification called  ``hadamard" representation.
{hanley and belfus}, year = {1996}, address = {washington, us}, pages =  {358--362}, url =  {http://www.cs.cmu.edu/afs/cs/user/yiming/www/courses/bibliography/papers/scamc96.ps},  abstract = {whether or not high accuracy classification methods can be scaled  to large applications is crucial for the ultimate usefulness of such methods in  text categorization.
in this article, we apply ml to text-database analyses and knowledge acquisitions from text databases.
indexing text for accurate retrieval is a difficult and important problem.
{information processing and management}, year = {2004}, volume = {40}, number =
this distribution can be estimated very efficiently from the data which immediately leads to an efficient model selection algorithm.
= {dubrovnik, hk}, pages =
the use of titles for automatic document classification},  journal = {journal of the american society for information science}, year =
{using text classification to predict the gene knockout behaviour of {s.\  cerevisiae}}, booktitle = {proceedings of apbc-03, 1st asia-pacific  bioinformatics conference}, editor = {yi-ping p. chen}, publisher =
the use of stw allows the terms that are distributed most  differently in the positive and negative examples of the categories of interest  to be weighted highest.
{18}, number = {3}, pages = {311--322}, url = {http://www.kluweronline.com/issn/0924-669x}, abstract = {this paper reports our comparative evaluation of three machine learning methods, namely k nearest neighbor (knn), supportvector machines (svm), and adaptive resonance associative map (aram) for chinese document categorization.
successfully managing information means being able to find relevant new information and to correctly integrate it with pre-existing knowledge.
focusing on anti-spam filtering for mailing lists, a thorough  investigation of the effectiveness of a memory-based anti-spam filter is  performed using a publicly available corpus.
particular attention is turned to a classifier's  underlying feature set: aside from the standard feature types we introduce new  features that are based on word frequency classes and that can be computed with  minimum computational effort.
the multi-classifier approach helps us leverage all the relevant textual features and meta data, and appears to generalize to related classification tasks.}, } @inproceedings{amati96, author = {gianni amati and daniela d'aloisi and vittorio giannini and flavio ubaldini}, title = {an integrated system for filtering news and managing distributed data}, booktitle = {proceedings of pakm-96, 1st international conference on practical aspects of knowledge management}, editor = {}, publisher = {}, year = {1996}, pages = {}, note = {an extended version appears as~\cite{amati97b}},
this paper describes the proposed technique, discusses the integration of a  keyword acquisition algorithm, latent semantic indexing (lsi) with rough  set-based rule generate algorithm, and provides experimental results.
for this reason we propose a  method for the bootstrapping process that makes a first hypothesis of  categorization for a set of unlabeled documents, with respect to a given empty  hierarchy of concepts.
= {dublin, ie}, pages = {31--40}, url = {http://www.acm.org/pubs/articles/proceedings/ir/188490/p31-hoch/p31-hoch.pdf}, abstract = {this paper presents the infoclas system applying statistical methods of information retrieval for the classification of german business letters into corresponding message types such as order, offer, enclosure, etc.
inbook % = a part of a book, usually untitled; it may be a chapter % (or other sectional unit) and/or range of pages % required: author or editor, title, chapter and/or pages, publisher, year % optional: volume or number, series, type, address, edition, month, note % % incollection %
this paper examines the relationship between the idf  transform and several widely used feature selection methods, in the context of  naive bayes and support vector machines classifiers, on datasets extracted from  the dmoz ontology of web-page descriptions.
while this transform is lossy,  sufficient salient information is retained to support many applications.
in the work presented here, the decision tree learning algorithm c4.5 is applied on a corpus of financial news articles.
moreover, the procedure of defining  analysis-level markers can be followed in order to extract useful stylistic  information using existing text processing tools.}, }  @inproceedings{stamatatos00a, author = {efstathios stamatatos and nikos  fakotakis and george kokkinakis}, title = {
a system that performs text categorization aims to assign appropriate categories from a predefined classification scheme to incoming documents.
"proceedings of the 28th european  conference on ir research (ecir)", year =
in  particular, this method is most similar to naive bayes; it generally performs  at least as well as naive bayes, and sometimes better.}, }  @inproceedings{sable02, author = {carl sable and kathleen mckeown and kenneth  w. church}, title = {nlp found helpful (at least for one text categorization  task)}, booktitle = {proceedings of emnlp-02, conference on empirical methods  in natural language processing}, address = {philadelphia, us}, year = {2002},  publisher = {association for computational linguistics}, pages = {172--179}, }  @inproceedings{sable99, author =
the use  of bigrams to enhance text categorization}, journal = {information processing  and management}, year = {2002}, volume = {38}, number = {4}, pages =  {529--546}, url = {
for  solving these kinds of problems, neural networks have the advantage of  extracting the underlying relationships between the input data and the output  classes automatically.
{proceedings of ecai-98, 13th european conference on artificial intelligence},  publisher = {john wiley and sons, chichester, uk}, editor = {
similarity in word space is based on second-order co-occurrence: two tokens (or contexts) of the ambiguous word are assigned to the same sense cluster if the words they co-occur with in turn occur with similar words in a training corpus.
in this work we present a comparative study of digital library citations and web links, in the context of automatic text classification.
as an example, we propose a number of ``supervised variants'' of $tfidf$ weighting, obtained by replacing the $idf$ function with the function that has been used in phase (i) for term selection.
we show how these networks can  support text routing of noisy newswire titles according to different given  categories.
this makes it a good candidate  for a general "install-and-forget" term selection mechanism.
the author presents the core technology of  teklis, the results on the filtering and routing tasks and a discussion of the  insights gained through participation in the exercise.}, }  @inproceedings{cai03, author = {lijuan cai and thomas hofmann}, title = {
the proposed learning model is the core of a prototype information  filtering system called profile.}, } @article{amati97b, author =
based  on the proposed representation, we present an approach to the problem of  learning rules for tc and ie in terms of ilp.
a combination of different classifiers produced better results  than any single type of classifier.
published in the ``lecture notes in computer science''  series, number 2256}, url =  {http://link.springer.de/link/service/series/0558/papers/2256/22560309.pdf},
we propose research directions to improve categorization rates and make suggestions about how web site designers could re-organize their sites to support fast categorization of search results."
in this paper, we propose a simple generalization of svm: weighted margin svm (wmsvms) that permits the incorporation of prior knowledge.
most text classification efforts so far concentrated on developing centralized solutions.
for this task, an indexing dictionary with rules for mapping terms from the text onto descriptors is required, which can be derived automatically from a set of manually indexed documents.
text classification using small number of features}, booktitle = {proceedings of mldm-05, 4th international conference on machine learning and data mining in pattern recognition}, year = {2005}, pages = {580--589}, address = {leipzig, germany}, series = {lecture notes in artificial intelligence}, volume = {3587}, publisher =
since these key words are often phrases of two or more words, we prefer to call them keyphrases.
because text domains present much irrelevant information,  effective feature reduction is essential to improve classifiers' effectiveness  and efficiency.
"2004" } @inproceedings{schneider:2005:tip, author =
we developed the genex algorithm specifically for automatically extracting keyphrases from text.
by allowing this interactivity in the clustering process, c-evolve achieves considerably higher clustering accuracy (10 to 20\% absolute increase in our experiments) than the popular k-means and agglomerative clustering methods.}, } @inproceedings{agrawal01, author = {rakesh agrawal and ramakrishnan srikant}, title = {
our experiments clearly indicate that automatic categorization improves the retrieval performance compared with no categorization.
the use of a controlled vocabulary allows for a more  consistent description of corporate documents, and promotes easier access by  people across the company.
we adapted several supervised text categorization methods, specifically several new variants of the k-nearest neighbor (knn) algorithm and a rocchio approach, to track events.
when a probabilistic text categorization is extended to a cluster-based one, the use of hbc offers better performance than the use of non-probabilistic algorithms.}, } @inproceedings{iwazume96, author = {michiaki iwazume and hideaki takeda and toyoaki nishida}, title = {ontology-based information gathering and text categorization from the internet}, booktitle = {
furthermore, we focus on the fact that document collections lend themselves naturally to a hierarchical structure defined by the subject matter of the documents.
text  categorization (also known as text classification, or topic spotting) is the  task of automatically sorting a set of documents into categories from a  predefined set.
editor = {nicholas j. belkin and peter ingwersen and mun-kew leong}, publisher = {acm press, new york, us}, address = {athens, gr}, year =
we conclude that memory-based anti-spam filtering for mailing lists is practically feasible, especially when combined with additional safety nets.
"the ferrety algorithm for the {kdd} cup 2005 problem", journal = "{sigkdd} explorations", pages =
} @inproceedings{rousu:2005:lhm, author =
the impact of rule insertion is most significant for categories with a small number of relevant documents.}, } @article{tan02, author =
we tested this method on both  retrieval and indexing with a set of medline documents which has been used by  other information retrieval systems for evaluations.
we describe athena: a system for creating, exploiting, and maintaining a hierarchical arrangement of textual documents through interactive mining-based operations.
training algorithms are derived for both  cases, and illustrated on real data by clustering news stories and categorising  newsgroup messages.
} @inproceedings{liu:2004:tcl, author =
this paper presents  an extensive empirical evaluation of memory-based learning in the context of  anti-spam filtering, a novel cost-sensitive application of text categorization  that attempts to identify automatically unsolicited commercial messages that  flood mailboxes.
"proceedings of the eleventh acm sigkdd  international conference on knowledge discovery in data mining", year =  "2005", pages =
in this paper, we suggest, for text categorization, the integration of external wordnet lexical information to supplement training data for a semi-supervised clustering algorithm which can learn from both training and test documents to classify new unseen documents.
we find that error correcting codes perform better than 1-of-n  coding with increasing code length.
hynet is described for the first  time in this paper.
the prevailing approach to atc is making use of a  collection of prelabeled texts for the induction of a document classifier  through learning methods.
we show that sequential minimal optimization can be used in training wmsvm.
this model is naturally well-suited to clustering documents in  preset or automatically generated hierarchies, as well as categorising new  documents in an existing hierarchy.
however, we find  that ppm does not compete with the published state of the art in the use of  machine learning for text categorization.
this understanding led us to develop a new learning model that  transfers induced knowledge through time to benefit future classifier learning  tasks.
this paper presents the results of the application of an instance-based learning algorithm k-nearest neighbor method on feature projections (k-nnfp) to text categorization and compares it with k-nearest neighbor classifier (k-nn).
because they do not exploit dependencies between labels, such techniques are  only well-suited to problems in which categories are independent.
automated text categorization is generally cast as a multi-class classification  problem.
text  categorization is the natural consequence of such automatic category generation  process.}, } @inproceedings{yang00c, author = {hsin-chang yang and chung-hong  lee}, title = {automatic category structure generation and categorization of  chinese text documents},
{42--49}, url = {http://www.cs.cmu.edu/~yiming/papers.yy/sigir99.ps}, abstract  = {
in this paper, we present a formal description of the feature quantity, as well as some illustrative examples of applying such a quantity to different types of information retrieval tasks: representative term selection and text categorization.}, } @inproceedings{aizawa01, author = {akiko aizawa}, title = {linguistic techniques to improve the performance of automatic text categorization}, booktitle = {proceedings of nlprs-01, 6th natural language processing pacific rim symposium}, editor = {}, publisher = {}, address = {tokyo, jp}, year = {2001}, pages = {307--314}, url = {http://www.afnlp.org/nlprs2001/pdf/0079-01.pdf}, abstract = {this paper presents a method for incorporating natural language processing into existing text categorization procedures.
"retrospective news event detection (red) is defined as the discovery of previously unidentified events in historical news corpus.
we propose research directions to improve categorization rates and make  suggestions about how web site designers could re-organize their sites to  support fast categorization of search results."
we discovered that misclassifications by the citation-link based classifiers are in fact difficult cases, hard to classify even for humans."
we obtain a classification accuracy of 82\%, a  number that clearly outperforms baseline estimates and competing image-based  approaches and nears the accuracy of humans who perform the same task with  access to comparable information.}, } @inproceedings{sahami96, author = {
advances in neural information processing  systems}, editor = {todd k. leen and thomas g. dietterich and volker tresp},  volume = {13}, year = {2001}, pages = {563--569}, publisher = {mit press,  cambridge, ma}, url =  {http://www.support-vector.net/papers/lodhishawe-taylorcristianiniwatkins_ps.ps},  abstract = {}, } @article{cristianini02, author =
in brief, it tries to avoid considering  features whose discrimination capability is sufficiently covered by already  selected features, reducing in size the set of the features used to  characterize the document set.
we classify movie reviews using  features based upon these taxonomies combined with standard ``bag-of-words''  features, and report state-of-the-art accuracy of 90.2%.
the ddc method for subject indexing is very close to operational status for a data base which grows at the rate of two million words of text per year.}, } @inproceedings{klinkenberg00, author = {ralf klinkenberg and thorsten joachims}, title = {
{317--345}, abstract = {
"proceedings of the 19th international joint conference on artificial intelligence", year =
we do not believe our results are specific to ppm.
the same techniques  can be used to determine if a document is fiction or non-fiction with  approximately 98 per cent accuracy.}, } @inproceedings{kosmynin96, author =
reducing the dimensionality, or  selecting a good subset of features, without sacrificing accuracy, is of great  importance for neural networks to be successfully applied to the area.
when such urls were not % % available, sometimes a url with restricted access (e.g. the % % acm digital library or the ieee computing society digital % % library, which are accessible to subscribers only) is indicated.
as an example, the proposed scheme is applied to the  classification of news articles into 3 categories: politics, sports, and  business.
"bing liu and xiaoli li and wee sun lee and yu, philip s.", title =
{an integrated system for filtering news and managing distributed data},  booktitle = {proceedings of pakm-96, 1st international conference on practical  aspects of knowledge management}, editor = {}, publisher = {}, year = {1996},  pages = {}, note = {an extended version appears as~\cite{amati97b}}, address =
on their own, the new representations are not found to produce  significant performance improvements.
in this model, borrowed from information retrieval, documents are represented as a vector where each component is associated with a particular word from the vocabulary.
the categories in such problems usually are neither conditionally independent from each other nor mutually exclusive, therefore it is not trivial to directly employ state-of-the-art classification algorithms without losing information of relation among categories.
then, a machine learning method may be used in this simple  bidimensional space to classify the documents.
this paper proposes a multi-dimensional framework for classifying  text documents.
"2005", pages = "294--303", abstract =  "a web object is defined to represent any meaningful object embedded in  web pages (e.g. images, music) or pointed to by hyperlinks (e.g. downloadable  files).
on both corpora the algorithms we present  outperform adaptations to topic-ranking of rocchio's algorithm and the  perceptron algorithm.
we show that our specialization of the naive bayes classifier is considerably more accurate (7 to 29\% absolute increase in accuracy) than a standard implementation.
"proceedings of the twelfth acm sigkdd international conference on knowledge discovery and data mining", year = "2006", pages = "474--483", } @inproceedings{hulth:2006:sae, author =
this paper addresses personal e-mail filtering by casting it in the framework of text classification.
"introducing a family of linear measures for feature selection in text categorization", author =
published in the ``lecture notes in computer  science'' series, number 2945}, pages = {580--583}, url = {}, abstract = {}, }  @article{kim04a, author = {kim, j. and kim, m.}, title = {an evaluation of  passage-based text categorization}, journal = {journal of intelligent  information systems}, year = {2004}, volume = {23}, number = {1}, pages =  {47--65}, url = {http://dx.doi.org/10.1023/b:jiis.0000029670.53363.d0},  abstract = {researches in text categorization have been confined to  whole-document-level classification, probably due to lack of full-text test  collections.
but our experiments on human subjects indicate that human feedback on feature relevance can identify a sufficient proportion (65%) of the most relevant features.
the msdn corpus is  collected from an online news website maintained by the min-sheng daily news,  taiwan.
extensive tests of the model suggest its application as a viable and robust tool for large scale text classification and filtering, as well as a basic module for more complex scenarios.}, } @article{bayer98, author = {thomas bayer and ulrich kressel and heike mogg-schneider and ingrid renz}, title = {categorizing paper documents.
{many studies in automated text categorization focus on the performance of classifiers, with or without considering feature selection methods, but almost as a rule taking into account just one document representation.
morgan kaufmann publishers, san  francisco, us}, url = {}, abstract = {kernel methods like support vector  machines have successfully been used for text categorization.
combining model-oriented and description-oriented approaches  for probabilistic indexing}, booktitle = {proceedings of sigir-91, 14th acm  international conference on research and development in information retrieval},  editor = {abraham bookstein and yves chiaramella and gerard salton and vijay v.  raghavan}, publisher = {acm press, new york, us}, address = {chicago, us},  pages = {46--56}, year = {1991}, note = {an extended version appears  as~\cite{fuhr94}}, url =  {http://www.acm.org/pubs/articles/proceedings/ir/122860/p46-fuhr/p46-fuhr.pdf},  abstract =
"automated text  classification using a multi-agent framework", booktitle =  "proceedings of the 5th acm/ieee-cs joint conference on digital  libraries", year =
we present a method to detect automatically pornographic content on the web.
it employs a  combination of technologies that takes the results of queries to networked  information sources and, in real-time, automatically retrieve, parse and  organize these documents into coherent categories for presentation to the user.
making no assumptions on the mechanism generating the data instances, and  assuming a linear noise model for the labels, we bound the h-loss of our  on-line algorithm in terms of the h-loss of a reference classifier knowing the  true parameters of the label-generating process.
"bremen, germany", publisher = "acm press", url = "http://doi.acm.org/10.1145/1099554.1099688", abstract =
the graph neighborhood is taken into consideration to exploit locality patterns while at the same time avoiding overfitting.
we start from the observation that  support vector machines, one of the best text categorization methods cannot  scale up to handle the large document collections involved in many real word  problems.
{teytaud, olivier and jalam, radwan}, title = {kernel based text categorization}, booktitle =
we also used the generated structure to categorize text documents.
in this paper, we propose  the use of a non-relevant information profile to reduce the mistaken retrieval  of non-relevant documents.
we describe our approach to document representation that  captures contextual dependencies between terms in a corpus and makes use of  these dependencies to represent documents.
additionally, the  computation procedure can be improved to locate the sets of duplicate or  plagiarised documents.
we find that term selection and our modified lsi representations lead to similar topic spotting performance, and that this performance is equal to or better than other published results on the same corpus.}, } @mastersthesis{wiener95a, author = {erik d. wiener}, title = {
advances in neural information processing systems}, editor = {todd k. leen and thomas g. dietterich and volker tresp}, volume = {13}, year = {2001}, pages = {563--569}, publisher = {mit press, cambridge, ma}, url = {http://www.support-vector.net/papers/lodhishawe-taylorcristianiniwatkins_ps.ps}, abstract = {}, } @article{cristianini02, author = {nello cristianini and john shawe-taylor and huma lodhi}, title = {latent semantic kernels}, journal =
with an appropriate level of relation selection, foil appears to be competitive with or superior to existing propositional techniques.}, } @incollection{cohen95a, author = {
the method avoids two main problems with existing work in inductive  transfer: scalability and the risk of negative transfer.
in the first phase, a multilayer feed-forward  neural network was trained to classify medical documents in the area of cell  biology.
experimental evidence  indicates that k-nnfp is superior to k-nn in terms of classification accuracy  in the presence of irrelevant features in many real world domains.}, }  @inproceedings{yi00, author = {jeonghee yi and neel sundaresan}, title = {
in our literature survey, we have found that the existing hierarchical classification experiments used a variety of measures to evaluate performance.
we give here a  new, information theoretical interpretation of term strength, review some of  its uses in focusing the processing of documents for information retrieval and  describe new results obtained in document categorization.}, }  @inproceedings{yang97, author = {yiming yang and jan o. pedersen}, title = {
"using ``annotator rationales'' to improve machine learning for text categorization", booktitle = "proceedings of the annual conference of the north american chapter of the association for computational linguistics", year = "2007", month = "april", address =
knowledge base whose content mirrors that of the world wide web.
finally, it shows how the performance of multinomial naive bayes  can be improved using locally weighted learning.
{nagoya, jp}, pages = {745--750}, url = {}, abstract = {
"kolcz, aleksander and chowdhury, abdur", title = "avoidance of model re-induction in svm-based feature selection for text categorization", booktitle =
tokyo, jp}, year = {2002}, pages = {444--453}, note = {published in the ``lecture notes in computer science'' series, number 2417}, url = {http://link.springer.de/link/service/series/0558/papers/2417/24170444.pdf}, abstract = {two main research areas in statistical text categorization are similarity-based learning algorithms and associated thresholding strategies.
the best performance was achieved by the feature selection based on a  feature scoring measure known from information retrieval called odds ratio and  using relatively small number of features.}, } @inproceedings{mladenic04,  author = {
the result of learning is a set of independent  classifiers, each used to predict the probability that a new example is a  member of the corresponding category.
the kernel is an inner product in the feature  space generated by all subsequences of length k.
in contrast to this approach we use the frequencies of occurrence of the most frequent words of the entire written language.
available as technical report ss-96-05}, url = {}, abstract = {}, } @inproceedings{hersh94, author
we present results comparing the performance of boostexter and a number of other text-categorization algorithms on a variety of tasks.
such methods can provide automatic indexing and keyword assignment capabilities that are at least as accurate as human indexers in many applications.
{258--267}, publisher = {morgan kaufmann publishers, san francisco, us}, url =  {http://www-ai.ijs.si/dunjamladenic/papers/pww/pwwicml99final.ps.gz}, abstract  = {
more  concretely, a text classification task aimed at improving language modelling  for machine translation is considered.}, } @inproceedings{junker00, author =
boosting is a method for supervised  learning which has successfully been applied to many different domains, and  that has proven one of the best performers in text categorization exercises so  far.
we used precision and recall to measure the effectiveness of our classifier.
we review the results of a user study undertaken  to gauge the value of the approach over legacy time-based review of newsfeeds,  and also to compare the performance of alternate distance metrics that are used  to estimate the dissimilarity between candidate new articles and sets of  previously reviewed articles." } @inproceedings{gabrilovich04, author =
using a public corpus, we show that stacking can improve the efficiency of  automatically induced anti-spam filters, and that such filters can be used in  real-life applications.}, } @article{sakkis03, author = {georgios sakkis and  ion androutsopoulos and georgios paliouras and vangelis karkaletsis and  constantine d. spyropoulos and panagiotis stamatopoulos}, title = {
most  approaches to automatic tc are based on the utilization of a training  collection, which is a set of manually classified documents.
{\em classifier induction} refers instead to the problem of automatically building a text classifier by learning from a set of documents pre-classified under the categories of interest.
it also makes use of bayesian classification techniques to classify new documents within an existing categorization scheme.
empirical evaluation on 19  datasets shows substantial improvements.}, } @inproceedings{forman04a, author =
the author describes a series of experiments that show how the extraction patterns learned by autoslog can be used for text classification.
the accuracy of modern text classification systems  rivals that of trained human professionals, thanks to a combination of  information retrieval (ir) technology and machine learning (ml) technology.
one aspect of text mining is automatic text  categorization, which assigns a text document to some predefined category  according to the correlation between the document and the category.
we present the results of experimenting with theseus, a classifier that exploits this technique.}, } @inproceedings{avancini03, author = {henri avancini and alberto lavelli and bernardo magnini and fabrizio sebastiani and roberto zanoli}, title = {expanding domain-specific lexicons by term categorization}, year = {2003}, booktitle = {proceedings of sac-03, 18th acm symposium on applied computing}, address = {melbourne, us}, publisher =
a proposed  purification process can effectively reduce the dimensionality of the feature  space from 50,576 terms in the word-based approach to 19,865 terms in the  unknown word-based approach.
results with an algorithm extended by thesaurus knowledge are presented and interpreted.
these patterns are used to retrieve sentences, which are then determined  to be novel if it is likely that a new answer is present.
"zhang, dell and lee, wee  sun", title =
most text classification efforts so far  concentrated on developing centralized solutions.
chicago, us}, pages = {337--346}, year = {1991}, url = {http://www.acm.org/pubs/articles/proceedings/ir/122860/p337-rau/p337-rau.pdf}, abstract = {
{combining labeled and unlabeled data for text classification with a large  number of categories}, booktitle = {proceedings of the ieee international  conference on data mining}, editor = {nick cercone and tsau young lin and  xindong wu}, address = {san jose, us}, year = {2001}, pages = {597--598},  publisher = {ieee computer society, los alamitos, us}, url =  {http://www.cs.cmu.edu/~rayid/mypapers/icdm01.ps}, abstract = {
the analysis predicts learning curves with a very high precision and thus contributes to a better understanding of why and when over-fitting occurs.
this algorithm is the "semi-supervised agglomerative hierarchical  clustering algorithm" (ssahc).
because they do not exploit dependencies between labels, such techniques are only well-suited to problems in which categories are independent.
a very efficient categorizer system.
the rapid growth of data in large databases,  such as text databases and scientific databases, requires efficient computer  methods for automating analyses of the data with the goal of acquiring  knowledges or making discoveries.
experiments have been conducted on a  real-world document collection demonstrating the effectiveness of our approach.
} @inproceedings{angelova:2006:gbt, author =  "angelova, ralitsa andweikum, gerhard", title = "graph-based  text classification: learn from your neighbors", booktitle =  "proceedings of the 29th annual international acm sigir conference on  research and development in information retrieval", year =  "2006", pages =
above 90\%  correct recognition rates have been achieved for the major categories  concerned.
{2003}, volume = {39}, number = {1}, pages = {25--44}, url = {}, abstract = {},  } @inproceedings{kwon99, author = {oh-woog kwon and sung-hwa jung and  jong-hyeok lee and geunbae lee}, title = {evaluation of category features and  text structural information on a text categorization using memory based  reasoning}, booktitle = {proceedings of iccpol-99, 18th international  conference on computer processing of oriental languages}, editor = {}, year =
in this work we investigate the usefulness of $n$-grams in tc independently of any specific learning algorithm.
{78--85}, url = {http://www.math.unipd.it/~fabseb60/publications/cikm00.pdf}, abstract = {
empirical results support the theoretical findings.
the  algorithms are compared on learning speed and error rate.
in this model, borrowed from information retrieval,  documents are represented as a vector where each component is associated with a  particular word from the vocabulary.
this paper proposes topic difference factor analysis (tdfa) as a method to extract projection axes that reflect topic differences between two document sets.
next, we investigate the application of automatic categorization to text retrieval.
boosting support vector machines for text classification through parameter-free threshold relaxation}, booktitle = {proceedings of cikm-03, 12th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, editor = {}, year = {2003}, address = {new orleans, us}, pages =
we applied the decision forest classifier and compared its accuracies to those of c4.5 and knn classifiers, using both category dependent and category independent term selection schemes.
a hierarchical classification is obtained by evaluating the trained node classifiers in a top-down fashion.
feature reduction by this method remarkably outperforms information gain based feature selection.} } @inproceedings{kules:2006:cws, author =
"michael davy and saturnino luz", title =
we propose a model for basing classification of multimedia on broad,  non-topical features, and show how information on targeted nearby pieces of  text can be used to effectively classify photographs on a first such feature,  distinguishing between indoor and outdoor images.
{236--256}, publisher = {kluwer academic publishers}, address = {dordrecht,  nl}, url = {http://www.cais.ntu.edu.sg/~sunaixin/paper/sun_hcl.pdf}, abstract =  {hierarchical text classification refers to assigning text documents to the  categories in a given category tree based on their content.
specifically, the classifier provides an efficient information  extraction and takes the meaning of words into consideration.
we use techniques from statistical  pattern recognition to efficiently separate the feature words or discriminants  from the noise words at each node of the taxonomy.
additional text is provided in coded form so that the reader can more fully explore this technique and form his own opinion of the applicability and versatility of this particular procedure.
we believe this approach is  effective in reducing the development time to implement classification systems  involving large number of topics for the purpose of classification, message  routing etc.}, } @incollection{masand94, author = {briji masand}, title =  {optimising confidence of text classification by evolution of symbolic  expressions}, booktitle = {advances in genetic programming}, publisher = {
comparing efficiency,  knn was notably more costly in terms of time and memory than the other two  methods.
we use the  classification on keywords as the baseline, which we compare with the  contribution of the pure hm pairs to classification accuracy, and the  incremental contributions from heads and modifiers.
this (semi-supervised) learning paradigm falls somewhere between the  fully supervised and the fully unsupervised learning schemes, in the sense that  it exploits both class information contained in labeled data (training  documents) and structure information possessed by unlabeled data (test  documents) in order to produce better partitions for test documents.
in this paper, we explore  an efficient extension of the standard support vector machine (svm) approach,  called svmc (support vector mapping convergence)
"sang-bum kim and hae-chang  rim", title =
pisa, it}, pages = {87--98}, year = {2004}, publisher = {springer verlag, heidelberg, de}, note = {
tc has been an application for many learning approaches, which prove effective.
sharon  mcdonald and john tait}, year = {2004}, address = {sunderland, uk}, publisher =
such categories offer a standardized and universal way for referring to or describing the nature of real world objects, activities, documents and so on, and may be used (we suggest) to semantically characterize the content of documents.
"robustness of adaptive filtering methods in a cross-benchmark evaluation", pages = "98--105", booktitle =
experimental results show that the high-degree biased bigrams should be eliminated from the feature set, and the s-br1 scheme is quite effective for further dimensionality reduction in chinese text categorization, after a feature selection process with a chi-cig score function.}, } @inproceedings{xue04a, author = {xue, dejun and sun, maosong}, title = {
"91--99", volume = 7,  number = 2, year = 2005, url =  "http://www.acm.org/sigs/sigkdd/explorations/issues/7-2-2005-12/kddcup2005report_organizers.pdf",  abstract =
"2004", month = "july", address =
three aspects are considered in the  investigation: (i) a method for weighting terms based on the concept of a  probability weighted amount of information, (ii) estimation of term occurrence  probabilities using a probabilistic language model, and (iii) automatic  extraction of terms based on pos tags automatically generated by a  morphological analyzer.
in contrast to previous stylometric approaches, we attempt to take full advantage of existing natural language processing (nlp) tools.
this task is easy to understand, but the lack of straightforward training set, subjective user intents of queries, poor information in short queries, and high noise level make the task very challenge.
the recognition of proper nouns (pns) is considered an important  task in the area of information retrieval and extraction.
"1301--1306" } @inproceedings{lan:2006:pnt, author =
new filtering and disambiguation methods are used as pre-processing to solve the problems caused by the use of the thesaurus.
"overcoming the brittleness bottleneck using {w}ikipedia:
our results on two datasets of scanned journals from the making of america collection confirm the importance of using whole page sequences.
"proceedings of the 21st international conference on  computational linguistics and 44th annual meeting of the association for  computational linguistics", year =
"acm press", url =  "http://www.cs.technion.ac.il/~gabr/papers/newsjunkie.pdf", abstract  =
"2006", pages = "157--166", }  @inproceedings{joachims:2006:tls, author =
in our experiments we show that,  whenever an existing pn dictionary allows the identification of 50\% of the  proper nouns within a corpus, our technique allows, without additional manual  effort, the successful recognition of about 90\% of the remaining 50\%.}, }  @inproceedings{peters02, author = {c. peters and cornelis h. koster}, title =  {uncertainty-based noise reduction and term selection in text categorization},  booktitle = {proceedings of ecir-02, 24th european colloquium on information  retrieval research}, editor = {fabio crestani and mark girolami and van
the resulting system was adopted by the main italian financial news agency providing a pay-to-view service.}, } @inproceedings{clack97, author = {
our method is unique in that decision lists are automatically constructed on the basis of the principle of minimizing extended stochastic complexity (esc), and with it we are able to construct decision lists that have fewer errors in classification.
in this work, we present a family of semi-supervised  linear support vector classifiers that are designed to handle partially-labeled  sparse datasets with possibly very large number of examples and features.
alexander f. gelbukh}, publisher = {springer verlag, heidelberg, de}, address = {seoul, ko}, note = {published in the ``lecture notes in computer science'' series, number 2945}, pages =
a modular system is proposed which allows the integration of this technique with a large variety of different if/ir approaches.
we provide background, present procedures for building metaclassifiers that take into consideration both reliability indicators and classifier outputs, and review a set of comparative studies undertaken to evaluate the methodology.}, } @inproceedings{bennett03, author = {paul n. bennett}, title = {using asymmetric distributions to improve text classifier probability estimates}, booktitle = {proceedings of sigir-03, 26th acm international conference on research and development in information retrieval}, editor = {jamie callan and gordon cormack and charles clarke and david hawking and alan smeaton}, publisher = {acm press, new york, us}, address = {toronto, ca}, year = {2003}, pages = {111--118}, url = {http://doi.acm.org/10.1145/860435.860457},
the axes are called  topic difference factors (tdf's).
compared to a previously tested naive bayes filter, the memory-based filter performs on average better, particularly when the misclassification cost for non-spam messages is high.}, } @inproceedings{sasaki98, author = {minoru sasaki and kenji kita}, title = {automatic text categorization based on hierarchical rules}, booktitle = {proceedings of the 5th international conference on soft computing and information}, publisher =
we conjecture that this dual feature set approach can be  generalized to improve the performance of subject classification as well.}, }  @inproceedings{lee02a, author = {michael d. lee}, title = {fast text  classification using sequential sampling processes}, booktitle = {proceedings  of the 14th australian joint conference on artificial intelligence}, editor =
alexander f. gelbukh},  publisher = {springer verlag, heidelberg, de}, address = {seoul, ko}, note =
"a supervised clustering method for text  classification", booktitle =
at each node, this classifier can ignore the  large number of ``noise'' words in a document.
we also outline the formal analysis of the algorithm in the mistake bound model.
"proceedings of the 21st conference of the spanish society for  natural language processing (sepln'2005)", year =
this paper introduces a new type of self-organizing map (som) for  text categorization and semantic browsing.
no prior information about document content or language is required.
relation selection improves foil's performance as measured by any of recall, precision, f-measure, or error rate.
text categorization using transductive boosting}, booktitle = {proceedings of ecml-01, 12th european conference on machine learning}, editor = {
de buenaga rodr{\'{\i}}guez, manuel and g{\'o}mez-hidalgo, jos{\'e} mar{\'{\i}}a and d{\'{\i}}az-agudo, bel{\'e}n}, title = {
published in the ``lecture  notes in computer science'' series, number 2167}, url =  {http://link.springer.de/link/service/series/0558/papers/2167/21670454.pdf},
alex gammerman}, booktitle =
{acm press, new york, us}, address = {pittsburgh, us}, pages = {22--34}, year = {1993}, url = {http://www.darmstadt.gmd.de/~tzeras/fullpapers/gz/tzeras-hartmann-93.ps.gz}, abstract = {
"proceedings of the twelfth acm sigkdd  international conference on knowledge discovery and data mining", year =
then we study the problem of combining it with a standard bag of words kernel.
the ability, of the approach to generalize, given a minimum of training data is also addressed.
nevertheless, it is shown that automated text categorization techniques can exploit combinations of simple lexical and syntactic features to infer the gender of the author of an unseen formal written document with approximately 80 per cent accuracy.
this paper presents two approaches.
an empirical comparison of text categorization methods},
gauging similarity with n-grams: language-independent categorization of text}, journal = {
we  present the results of experiments with a preliminary implementation of the  technique.}, } @inproceedings{attardi99, author =
we test the use of one classifier (a highly efficient probabilistic one) to select examples for training another (the c4.5 rule induction program).
in this approach, there are two processes: (1) the learning process and (2) the indexing process.
in contrast to prior work along these lines, our approach employs  a number of novel techniques: dynamically inferring the link/class pattern in  the graph in the run of the iterative relaxation labeling, judicious pruning of  edges from the neighborhood graph based on node dissimilarities and node  degrees, weighting the influence of edges based on a distance metric between  the classification labels of interest and weighting edges by content similarity  measures.
confirm the importance of using whole page sequences.
it then trains a new  classifier using the labels for all the documents, and iterates to convergence.
the naive  bayes classifier, currently experiencing a renaissance in machine learning, has  long been a core technique in information retrieval.
editor = {}, address = {edmonton, ca},  year = {2003}, pages = {}, url = {}, abstract = {}, } @inproceedings{petasis00,  author = {georgios petasis and alessandro cucchiarelli and paola velardi and  georgios paliouras and vangelis karkaletsis and constantine d. spyropoulos},  title = {automatic adaptation of proper noun dictionaries through cooperation  of machine learning and probabilistic methods}, booktitle = {proceedings of  sigir-00, 23rd acm international conference on research and development in  information retrieval}, editor = {nicholas j. belkin and peter ingwersen and  mun-kew leong}, publisher = {acm press, new york, us}, address = {athens, gr},  year = {2000}, pages = {128--135}, url =  {http://www.acm.org/pubs/articles/proceedings/ir/345508/p128-petasis/p128-petasis.pdf},  abstract = {
it supports incremental  training and online application of classifiers and predictive models to streams  of textual, numeric, symbolic, and hybrid data records.
have developed a method using distributions based on hard clustering of words, i.e., in which a word is assigned to a single cluster and words in the same cluster are treated uniformly.
the result is a generalized naive bayes classifier which allows for  a local markov dependence among observations; a model we refer to as the c hain  a ugmented n aive bayes (can) bayes classifier.
experimental results show that this method improves the precision of k-nn up to 13.86% without compromising its recall.}
on the merits of building categorization systems by supervised clustering}, booktitle = {proceedings of edbt-00, 7th international conference on extending database technology}, publisher = {acm press, new york, us}, year = {1999}, address = {konstanz, de}, pages = {352--356}, url = {http://doi.acm.org/10.1145/312129.312279}, abstract = {
{proceeding of ijcai-01, 17th international joint conference on artificial  intelligence}, editor = {bernhard nebel}, address = {
using support vector machines.
"comparative experiments on sentiment classification for online product reviews", booktitle =
measuring differentiability:  unmasking pseudonymous authors}, journal = {journal of machine learning  research}, volume = {8}, pages = {1261--1276}, year = {2007}, month = {june} }
multiple knowledge bases  can be formulated precisely and in a unified way within the framework of rs.
for many learning tasks where data is collected over an extended  period of time, its underlying distribution is likely to change.
we observed that our new method made a significant improvement in all classifiers and both data sets.}, } @article{ko04, author = {youngjoong ko and jinwoo park and jungyun seo}, title = {
then, test documents are scanned and categories ranked based on the presence of vocabulary terms.
"little work to date in sentiment analysis (classifying texts by `positive' or `negative' orientation) has attempted to use fine-grained semantic distinctions in features used for classification.
machine learning methods  for automatically producing categorization rules have similarly seen increased  attention, as a way to reduce the cost of fielding categorization systems.
this result suggests that useful task-tracking tools could be constructed based on automatic classification into this taxonomy."
this paper is a  comparative study of feature selection methods in statistical learning of text  categorization.
using the lessons learned from our  previous crawler evaluation studies, we experiment with multiple versions of  different classification schemes.
these patterns are used to retrieve sentences, which are then determined to be novel if it is likely that a new answer is present.
this property causes k-nnfp to eliminate possible adverse effects of  irrelevant features on the classification accuracy.
"bouma,  lucas and de rijke, maarten", title =
instead of using the text on a page for deriving features that can be used for training a classifier, we suggest to use portions of texts from all pages that point to the target page.
word sequences as features in text-learning}, booktitle =
furthermore, the performance of the  refined centroid classifier implemented is comparable, if not better, to that  of state-of-the-art support vector machine (svm)-based classifier, but offers a  much lower computational cost."
in our reference collections, measures based on co-citation tend to perform better for pages in the web directory, with gains up to 37\% over text based classifiers, while measures based on bibliographic coupling perform better in a digital library.
the results is an original statistical  classifier fed with linguistic (i.e. more complex) features and characterized  by the novel feature selection and weighting model.
while this transform is lossy, sufficient salient information is retained to support many applications.
in addition, we carefully analyze the internal representation using cluster analysis and output representations using a new surface error technique.
it was found that categorizers consisting of the words with highest tf.idf values scored best.}, } @inproceedings{paliouras99, author = {georgios paliouras and vangelis karkaletsis and constantine d. spyropoulos}, title = {learning rules for large vocabulary word sense disambiguation}, booktitle = {proceedings of ijcai-99, 16th international joint conference on artificial intelligence}, editor = {
william w. cohen}, title =
in this paper we describe an automated method of classifying  research project descriptions: a human expert classifies a sample set of  projects into a set of disjoint and pre-defined classes, and then the computer  learns from this sample how to classify new projects into these classes.
"kazuhiro seki and javed mostafa", title =
the question addressed in this paper is to find a bidimensional representation of textual documents for the problem of text categorisation.
we use this to build an  argument for a data driven approach which merely searches for a good linear  separator in the feature space, without further assumptions on the domain or a  specific problem.
we adopt a machine learning approach and the model parameters are learned from a labeled training set of representative documents.
the goal of these experiments was to automatically discover classification patterns that can be used for assignment of topics to the individual newswires.
"recently, interest is growing in non-topical text classification tasks such as genre classification, sentiment analysis, and authorship profiling.
information fusion almost always gives  better results than the individual constituent feature sets, with certain  combinations doing better than the others.}, } @inproceedings{davidov04, author  = {dmitry davidov and evgeniy gabrilovich and shaul markovitch}, title =
{morgan kaufmann publishers, san francisco, us}, url = {http://www.cs.cmu.edu/~yiming/papers.yy/hypertext-icml01.ps.gz}, abstract = {hypertext poses new text classification research challenges as hyperlinks, content of linked documents, and meta data about related web sites all provide richer sources of information for hypertext classification that are not available in traditional text classification.
{published in the ``lecture notes in computer science'' series, number 2291},  pages = {353--362}, url =  {http://www.cs.ucd.ie/staff/nick/home/research/download/finn-ecir2002.ps.gz},  abstract = {the world wide web is a vast repository of information, but the  sheer volume makes it difficult to identify useful documents.
optimizing and estimating effectiveness is greatly aided if  classifiers that explicitly estimate the probability of class membership are  used.}, } @article{lewis95a, author = {lewis, david d.}, title = {
on investigating the root  cause, we find that a large class of feature scoring methods suffers a pitfall:  they can be blinded by a surplus of strongly predictive features for some  classes, while largely ignoring features needed to discriminate difficult  classes.
ruslan milkov and nicolas nicolov and nilokai nikolov}, address = {tzigov chark, bl}, pages = {202--207}, year = {1997}, url = {http://www.dfki.uni-kl.de/~junker/download/ranlp97.ps},
we use two well-known techniques, partitioning clustering, means  and a hierarchy.
moreover, it  is demonstrated that our proposed framework is successfully applied to another  task of the genomics track, showing comparable results to the best performing  system." } @inproceedings{yang:2005:raf, author =
in this paper we demonstrate this pitfall hurts performance even for a relatively uniform text classification task.
in this report, we try to prove that a previous filtering of the words used by svm in the classification can improve the overall performance.
we also propose an iterative web unit mining (iwum) method that first finds subgraphs of web pages using some knowledge about web site structure.
we also show that addressing named entities preferentially is useful only in certain situations.
} @article{lee:2006:igd, author = {lee, changki and  lee, gary geunbae}, title = {information gain and divergence-based feature  selection for machine learning-based text categorization}, journal =
se{\'{a}}n  slattery and mark craven}, title = {discovering test set regularities in  relational domains}, booktitle = {proceedings of icml-00, 17th international  conference on machine learning}, editor = {pat langley}, year = {2000}, address  = {
{evgeniy gabrilovich and shaul markovitch}, title = {
a machine learning approach to knowledge acquisitions from text databases}, year = {1996}, journal = {international journal of human computer interaction}, volume = {8}, number = {3}, pages = {309--324}, url = {}, abstract = {
published in the  ``lecture notes for computer science'' series, number 2004}, pages =  {423--436}, url =  {http://link.springer.de/link/service/series/0558/papers/2004/20040437.pdf},  abstract = {
an extended version appears as~\cite{craven00}}, url =  {http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/www/wwkb/overview-aaai98.ps.gz},  abstract = {
an additional test submitted 250 trec queries to a search engine and  successfully categorized 66\% of the top 100 using the odp and 61\% of the top  350.
motivated by applying text categorization to sorting web search results, this paper describes an extensive experimental study of the impact of bag-of-words document representations on the performance of five major classifiers -- naive bayes, svm, voted perceptron, knn and c4.5.
european colloquium on information retrieval research}, editor =
however, the quality of the probability estimates is crucial.
this collection is tailored for automating the attribution of  international patent classification codes to patent applications and is made  publicly available for future research work.
new cluster analysis approaches are proposed to remedy the  problems found in traditional term clustering methods.}, }  @inproceedings{lewis94, author = {lewis, david d. and marc ringuette}, title =
when more documents are archived, new terms, new concepts and concept-drift will frequently appear.
only papers that have % % been the object of formal publication (i.e. conferences and % % journals) are to be included in the bibliography, so as to avoid % % its explosion and the inclusion of material bound to obsolescence.
moreover, we also  compare them to two well-known methods: k-nn and naive bayes.
"borovets,  bulgaria" } @inproceedings{guo:2004:kmb, author =
hirotoshi taira and masahiko haruno}, title = {
moreover, a macro-averaged recall and precision was  calculated: the former reported a 0.72, the latter a 0.79.
a newer paradigm based on machine learning has superseded  the previous approach.
{chemnitz, de}, pages = {4--15}, year = {1998}, url =  {http://www.research.att.com/~lewis/papers/lewis98b.ps}, abstract = {
such systems typically have to cope with sets of rectors of many tens of thousands of dimensions.
twenty subjects from expert or novice literary reading experience backgrounds were, in two experiments, required to rate two parallel sets of graphically and phonetically manipulated poems.
then a document is represented as a vector of features with different weights according to the importance of each sentence.
"fu, yueyu and  ke, weimao and mostafa, javed", title =
ana cardoso-cachopo and arlindo l. oliveira}, title = {
learning  to construct knowledge bases from the world wide web}, journal = {artificial  intelligence}, volume = {118}, number = {1/2}, year = {2000}, pages =
dunja mladeni{\'{c}}}, title = {machine learning on non-homogeneous, distributed text data}, school = {j.\ stefan institute, university of ljubljana}, address = {ljubljana, sl}, year = {1998}, url = {http://www-ai.ijs.si/dunjamladenic/papers/phd/phdfinal.ps}, abstract = {}, } @inproceedings{mladenic98d, author = {
we extend a multi-class categorization scheme proposed by  dietterich and bakiri 1995 for binary classifiers, using error correcting  codes.
our fully implemented and recently deployed system shows  that a superior classification engine for this task can be constructed from a  combination of classifiers.
this task is an intermediate process in many natural language processing tasks like machine translation or multilingual information retrieval.
based on the idea of distilling the characteristics of how we  estimate the merits of each component algorithm, we propose three different  strategies for the linear combination approach.
cats is a meta-search engine that utilizes text classification techniques to improve the presentation of search results.
results obtained by using a set of 2,344  medline documents are presented and discussed.}, } @inproceedings{ruiz99,  author =
a document is represented with a list of keywords.
finally, the obtained structure attributes are used to re-organize and index the web objects.
today, text categorization is a necessity due to the very large amount of text documents that we have to deal with daily.
results from experiments show that this filter has successfully rejected a sufficient number of non-relevant documents, resulting in an improvement of filtering performance.}, } @inproceedings{hoch94, author = {rainer hoch}, title = {using ir techniques for text classification in document analysis}, booktitle = {proceedings of sigir-94, 17th acm international conference on research and development in information retrieval}, editor = {
"active learning with history-based query selection for text categorisation", booktitle =
in order to classify a new document, the most similar  megadocument determines the category to be assigned.
we focus  our discussion on the ability to discriminate between authors for the case of  both aggregated e-mail topics as well as across different email topics.
"proceedings of the 21st international  conference on computational linguistics and 44th annual meeting of the  association for computational linguistics", year =
the fragments it looks for are determined by a set of knowledge-based rules.
relevant phrases and  contexts are acquired automatically using a training corpus.
semi-automated methods were used to build a lexicon of  appraising adjectives and their modifiers.
to control overfitting in me estimation, we propose the  use of box-type inequality constraints, where equality can be violated up to  certain predefined levels that reflect this uncertainty.
text categorization using weight-adjusted $k$-nearest neighbor  classification}, booktitle = {proceedings of pakdd-01, 5th pacific-asia  conferenece on knowledge discovery and data mining}, editor = {david cheung and  qing li and graham williams}, year = {2001}, publisher = {springer verlag,  heidelberg, de}, address = {hong kong, cn}, note = {
we report the performance of this approach on data sets both with and without the inclusion of the background text, and compare our work to other efforts that can incorporate unlabeled data and other background text in the classification process.}, } @inproceedings{zelikovitz02, author
this objective raises two  questions: (1) what are useful genres when searching the www?
we present a  de-centralized approach and system implementation (named macci) for text  classification using a multi-agent framework.
= {acm press,  new york, us}, address = {sheffield, uk}, year = {2004}, pages = {297--304},  url = {http://doi.acm.org/10.1145/1008992.1009044}, abstract = {
{acm press, new york, us}, editor = {henrique paques and ling liu and david  grossman}, year = {2001}, address = {atlanta, us}, pages = {113--118}, url =  {ftp://ftp.cs.rutgers.edu/pub/zelikovi/lsi01.ps}, abstract = {
nello cristianini and john  shawe-taylor and huma lodhi}, title = {latent semantic kernels}, journal =
this study indicates that, while some document categorization algorithms could be adopted for database categorization, algorithms that take into consideration the special characteristics of databases may be more effective.
"in this paper,  we use a blog corpus to demonstrate that we can often identify the author of an  anonymous text even where there are many thousands of candidate authors.
empirical estimates of weights (likelihood ratios) become unstable when counts are small.
at each node, this classifier can ignore the large number of noise words in a document.
we report  the results of systematic experimentation of these two methods performed on the  standard {\sc reuters-21578} benchmark.}, } @article{gale93, author = {
description-oriented approaches are more flexible with respect to the underlying representations, but the definition of the feature vector is a heuristic step.
the results  suggest that information extraction techniques can support high-precision text  classification and, in general, using more extracted information improves  performance.
on  uncertainty modeling and analysis (isuma'03)", page =  "104--109", address =
experiments in the terrorism domain suggest that increasing the amount of linguistic context can improve performance.
we  discuss an approach to the automatic expansion of domain-specific lexicons by  means of \emph{term categorization}, a novel task employing techniques from  information retrieval (ir) and machine learning (ml).
however, a problem with  iterative training techniques such as svm is that during their learning or  training phase, they require the entire training collection to be held in  main-memory; this is infeasible for large training collections such as dmoz or  large news wire feeds.
we address this open challenge by using a combination of classifiers with different performance characteristics to effectively reduce the performance variance on average of the overall system across all classes, including those not seen before.
"proceedings of the 21st  international conference on computational linguistics and 44th annual meeting  of the association for computational linguistics", year =
on  compression-based text classification}, booktitle = {proceedings of ecir-05,  27th european conference on information retrieval}, publisher = {springer  verlag}, editor = {david e. losada and juan m. fern{'{a}}ndez-luna}, address =
finally, the effectiveness of term distributions to improve classification  accuracy is explored with regard to the training set size and the number of  classes.}, } @article{leung97, author = {chi-hong leung and wing-kay kan},  title = {a statistical learning approach to automatic indexing of controlled  index terms}, journal = {journal of the american society for information  science}, year = {1997}, number = {1}, pages = {55--67}, volume = {48}, url =  {http://www3.interscience.wiley.com/cgi-bin/fulltext?id=39602&placebo=ie.pdf},  abstract = {
text categorization with many redundant features: using aggressive feature selection to make {svm}s competitive with {c4.5}}, booktitle = {proceedings of icml-04, 21st international conference on machine learning}, editor = {carla e. brodley}, year = {2004}, address = {banff, ca}, pages = {}, publisher =
while many existing methods achieve good  levels of performance, they generally require levels of computation that  prevent them from making sufficiently fast decisions in some applied setting.
{hershey, us}, pages = {78--102}, url =  {http://www.math.unipd.it/~fabseb60/publications/td01a.pdf}, abstract = {
we found that a larger reference  library is not necessarily better.
the proposed thresholding strategy is parameter free,  relying on a process of retrofitting and cross validation to set algorithm
the learning algorithm is  described and the effectiveness of the system is evaluated in a true  information filtering style.}, } @inproceedings{amati97a, author =
this method also has the benefit to make feature selection implicit, since useless features for the categorization problem considered get a very small weight.
though our experiments with words yielded good results, we found instances where the phrase-based approach produced more effectiveness.
while the majority of approaches rely on learning linear classifiers, there is also some interest in describing document categories by text patterns.
we propose an  algorithm that interleaves labeling features and documents which significantly  accelerates active learning." } @inproceedings{cohen:2005:fsb, author =
for illustration we give a brief description of the content-based personal intelligent agent named personal webwatcher that uses text-learning for user customized web browsing.}, } @inproceedings{mladenic99a, author = {dunja mladeni{\'{c}} and marko grobelnik}, title = {feature selection for unbalanced class distribution and naive bayes}, booktitle = {proceedings of icml-99, 16th international conference on machine learning}, editor = {ivan bratko and saso dzeroski}, year = {1999}, address = {bled, sl}, pages = {258--267}, publisher = {morgan kaufmann publishers, san francisco, us}, url = {http://www-ai.ijs.si/dunjamladenic/papers/pww/pwwicml99final.ps.gz}, abstract = {
text mining and its applications to intelligence, crm and knowledge management}, pages = {109--129}, publisher = {wit press}, address = {
in this paper, we examine the effects of page evolution on genre  classification of web pages.
we discuss the role of importance-weights (e.g. document frequency and redundancy), which is not yet fully understood in the light of model complexity and calculation cost, and we show that time consuming lemmatization or stemming can be avoided even when classifying a highly inflectional language like german.}, } @article{lertnattee04, author = {verayuth lertnattee and thanaruk theeramunkong}, title = {effect of term distributions on centroid-based text categorization}, journal = {information sciences}, year = {2004}, number = {1}, volume = {158}, pages = {89--115}, url = {http://dx.doi.org/10.1016/j.ins.2003.07.007}, abstract = {
text categorization is then cast as the problem of  finding coordinate transformations that reflects the inherent similarity from  the data.
in this paper, we present a boosting-based learning method for text filtering that uses naive bayes classifiers as a weak learner.
morgan kaufmann publishers, san francisco, us}, url = {http://www.cs.cmu.edu/~mccallum/papers/emactive-icml98.ps.gz}, abstract = {
the growing number of  digital libraries imposes a review of the available data from those databases.
"in this paper, we describe our ensemble-search based approach, q2c@ust (http://webproject1.cs.ust.hk/q2c/), for the query classification task for the kddcup 2005.
improved estimates of the term distributions are made by differentiation of words in the hierarchy according to their level of generality/specificity.
finally, the naive bayesian filter is compared, in terms of  performance, to a filter that uses keyword patterns, and which is part of a  widely used e-mail reader.}, } @article{appiani01, author = {enrico appiani and  francesca cesarini and annamaria colla and massimiliano diligenti and marco  gori and simone marinai and giovanni soda}, title = {automatic document  classification and indexing in high-volume applications}, journal =
the result is a model for the automatic selection of parameters.
four well known learning algorithms: rocchio's  algorithm (w.w. cohen and y. singer, 1995), the simple bayesian classifier  (sec) (r.o. duda and p.e. hart, 1973), the sleeping experts (se) and winnow (i.  dagan et al., 1997) were implemented.
they were tested on a corpus of articles from the dutch newspaper nrc, and pre-classified into four categories.
in the first approach, the content (eg., text) plays an important role, while in the second approach, the existence of several knowledge sources (eg., several users) is required.
we present simpl, a nearly linear-time classification algorithm which mimics the strengths of svms while avoiding the training bottleneck.
here, special kinds of web catalogues, those whose category scheme is hierarchically ordered, are regarded.
furthermore, linear and logistic regression are compared.}, }  @article{furnkranz02, author = {johannes f{\"{u}}rnkranz}, title =
we achieve a precision recall break even point of 84\% which is  comparable to the best known published results.
in  this work we investigate the usefulness of {\em $n$-grams} for document  indexing in text categorization (tc).
our  approach is to develop a trainable information extraction system that takes two  inputs.
{}, editor = {dan boneh}, year = {2002}, address = {san francisco, us}, pages =
hyperlinks, html tags, category labels distributed over linked documents, and meta data extracted from related web sites all provide rich information for classifying hypertext documents.
in this  paper, however, we show that in the case of text classification, term-frequency  transformations have a larger impact on the performance of svm than the kernel  itself.
hcl is designed to describe a hierarchical classification method including the definition of a category tree and training of classifiers associated with the categories.
specifically, we apply the information bottleneck method to find word-clusters that preserve the information about document categories and use these clusters as features for classification.
the paper argues that the creation of efficient web spiders is best framed and solved by reinforcement learning, a branch of machine learning that concerns itself with optimal sequential decision making.
"text classification with kernels on the  multinomial manifold", pages = "266-273", booktitle =  "proceedings of the 28th annual international {acm} {sigir} conference on  research and development in information retrieval", year =  "2005", month =
while the  decision tree based classifier outperforms the bayesian classifier when  features and training size are selected optimally for both, a carefully  designed naive bayesian classifier is more robust.}, } @inproceedings{diaz98,  author = {d{\'{\i}}az esteban, alberto and de buenaga rodr{\'{\i}}guez, manuel  and ure{\~n}a l{\'o}pez, l. alfonso and garc{\'{\i}}a vega, manuel}, title =
we also present two performance optimizations of waknn that improve  the computational performance by a few orders of magnitude, but do not  compromise on the classification quality.
this means that the category of potentially relevant documents for most profiles would contain at least 80\% of all documents later determined to be relevant to the profile.
second, we present the word-augmented relevancy  signatures algorithm that uses lexical items to represent domain-specific role  relationships instead of semantic features.
the document collection is trained by a self-organizing map to form two feature maps.
(b) we propose a variant of tsvm  that involves multiple switching of labels.
here we only  highlight some key techniques used in submitted solutions.
the existence, public availability, and widespread acceptance of a standard benchmark for a given information retrieval (ir) task are beneficial to research on this task, since they allow different researchers to experimentally compare their own systems by comparing the results they have obtained on this benchmark.
our analysis and empirical evaluation show substantial improvement in the accuracy of catalog integration.}, } @inproceedings{aizawa00, author = {akiko aizawa}, title = {
however, the use of a text-classification system on this is a bit more problematic - in the most straight-forward approach each number would be considered a distinct token and treated as a word.
the best f1 value of our two solutions is 9.6% higher than the best of all  other participants’ solutions.
an important aspect of this dissertation is that autoslog and the text classification systems can be easily ported across domains.}, } @inproceedings{riloff95, author = {ellen riloff}, title = {
in web classification, most researchers assume that the objects to  classify are individual web pages from one or more web sites.
we propose a method that incorporates simple semantics into tdt by  splitting the term space into groups of terms that have the meaning of the same  type.
the accuracy  of classification achieved with our method appears better than or comparable to  those of existing rule-based methods.
"n. o. attok-okine and b. m. ayyub", series =
we tested our new prototype system on the  reuters-21578 text categorization test collection.}, } @inproceedings{teahan00,  author = {
we  report the performance of this approach on data sets both with and without the  inclusion of the background text, and compare our work to other efforts that  can incorporate unlabeled data and other background text in the classification  process.}, } @inproceedings{zelikovitz02, author = {sarah zelikovitz and haym  hirsh}, title = {
the  bin-based method is intended for tasks where there is insufficient training  data to estimate a separate weight for each word.
various relaxation approaches have been proposed to counter this problem including: asymmetric svm learning algorithms (soft svms with asymmetric misclassification costs); uneven margin based learning; and thresholding.
furthermore, based on  this approach, we build an interactive red system, hiscovery, which provides  additional functions to present events, photo story and chronicle."
in particular, we demonstrate (1) how variation in document length can be tolerated by either normalizing feature weights or by using negative weights, (2) the positive effect of applying a threshold range in training, (3) alternatives in considering feature frequency, and (4) the benefits of discarding features while training.
{edward a. fox and peter ingwersen and raya fidel}, publisher = {acm press, new york, us}, year = {1995}, address =
published in the ``lecture notes in computer science'' series, number  2175}, editor = {floriana esposito}, year = {2001}, pages = {320--325}, address  = {bari, it}, url =  {http://link.springer.de/link/service/series/0558/papers/2175/21750320.pdf},  abstract = {feature selection and weighting are the primary activity of every  learning algorithm for text classification.
they include two families of text categorization techniques, namely the k-nearest neighbor (k-nn) algorithm and linear classifiers.
{pisa, it}, pages = {489--500}, year = {2004},  publisher = {springer verlag, heidelberg, de}, note = {
"extracting key-substring-group features for text  classification", booktitle =
"256--264", } @inproceedings{zhang:2006:lpm, author =
"in this paper, we present a general solution for the kdd cup 2005 problem.
the combination of evidence from a document and citing documents can  improve on either information source alone.
the relevancy  signatures algorithm classifies texts using extraction patterns, and the  augmented relevancy signatures algorithm classifies texts using extraction  patterns and semantic features associated with role fillers (riloff and  lehnert, 1994).
unsolicited commercial e-mail, or "spam", floods mailboxes, causing  frustration, wasting bandwidth, and exposing minors to unsuitable content.
the results support the conjecture that it is the sophistication of the feature weighting method rather than its apparent compatibility with the learning algorithm that improves classification performance.}, } @inproceedings{mladenic98a, author = {
combining multiclass maximum entropy text classifiers with neural network voting}, booktitle = {proceedings of portal-02, 3rd international conference on advances in natural language processing}, year = {2002}, editor = {elisabete ranchod and nuno j. mamede}, pages =
{donna k. harman and ellen m. voorhees}, year =
usually, these bigrams are likely to survive for  their strength of discriminating documents after the process of feature  selection.
current technology for  automating this process consists of building a classifier that uses the  categorization of documents in the master catalog to construct a model for  predicting the category of unknown documents.
we introduce a new  frequency-based method for selecting the most useful citations from a document  collection for use in the model.}, } @inproceedings{forman02, author = {
the results indicate that  in each case, the refined classifiers achieve significant performance  improvement over the base classifiers used.
furthermore, the online approach offers the advantage  of continuous learning in the batch-adaptive text filtering task.}, }  @inproceedings{chakrabarti02, author = {soumen chakrabarti and shourya roy and  mahesh soundalgekar}, title = {fast and accurate text classification via  multiple linear discriminant projections}, booktitle = {proceedings of vldb-02,  28th international conference on very large data bases}, publisher = {}, editor  = {}, year = {2002}, address = {hong kong, cn}, pages = {658--669}, url =  {http://www.vldb.org/conf/2002/s19p01.pdf}, abstract = {support vector machines  (svms) have shown superb performance for text classification tasks.
the design considerations and implementation issues are discussed.
the experimental results support the claim that a custom-designed algorithm (genex), incorporating specialized procedural domain knowledge, can generate better keyphrases than a general-purpose algorithm (c4.5).
we present detailed experimental results using naive bayes and support vector machines on the 20newsgroups data set and a 3-level hierarchy of html documents collected from the open directory project (www.dmoz.org).}, } @inproceedings{diao00, author = {yanlei diao and hongjun lu and dekai wu}, title = {
{thorsten joachims and fabrizio sebastiani}, title = {guest editors' introduction to the special issue on automated text categorization}, journal =
we conclude by describing the application of our system to automatic call-type identification from unconstrained spoken customer responses.}, } @inproceedings{schapire98, author =
morgan kaufmann publishers, san francisco, us}, url = {http://www.research.att.com/~lewis/papers/lewis94e.ps}, abstract = {uncertainty sampling methods iteratively request class labels for training instances whose classes are uncertain despite the previous labeled instances.
on-line information services generally depend on keyword indices rather than other methods of retrieval, because of the practical features of keywords for storage, dissemination, and browsing as well as for retrieval.
published in the ``lecture notes in computer science'' series, number 1805}, url = {http://www.cs.berkeley.edu/~diaoyl/publications/pakdd00.ps}, abstract =
a review of these approaches is presented here.
this inquiry examines a technique for automatically classifying (indexing) documents according to their subject content.
proceeding of ijcai-01, 17th international joint conference on artificial intelligence}, editor = {bernhard nebel}, address = {
among our experiments are some  specifically designed to test whether the ability to capture non-word features  causes character-based text compression methods to achieve more accurate  classification.}, } @inproceedings{masand92, author = {briji masand and gordon  linoff and david waltz}, title = {
however, even this algorithm is aided by an initial prefiltering of features, confirming the results found by almuallim and dietterich on artificial data sets.
we will discuss in detail issues pertaining to three different problems, namely document representation, classifier construction, and classifier evaluation.}, } @incollection{sebastiani05, author = {fabrizio sebastiani}, title = {
we propose a  general class of models for classification and clustering in relational domains  that capture probabilistic dependencies between related instances.
{359--371}, url = {http://trec.nist.gov/pubs/trec4/papers/nsa.ps.gz}, abstract = {
a standard approach is to classify each entity independently, ignoring the correlations between them.
"744--751", booktitle =  "proceedings of the 14th {acm} international conference on information and  knowledge management", year =
we argue that this evaluation measure is also very well suited for text categorization tasks.
based on the hierarchical structure the problem is divided into subproblems, each representing one on the categories included in the yahoo hierarchy.
in this paper, we introduce can models and apply them to various text classification problems.
this simplifies the creation of knowledge-based if/ir systems,  speeds up their operation, and allows easy editing of the rule bases employed.
we developed the genex algorithm specifically for  automatically extracting keyphrases from text.
using four  corpora from the topic detection and tracking (tdt) forum and the text  retrieval conferences (trec) we evaluated these methods with non-stationary  topics at various granularity levels, and measured performance with different  utility settings.
as a global observation, knn, llsf and a  neural network method had the best performance; except for a naive bayes  approach, the other learning algorithms also performed relatively well.}, }  @inproceedings{yavuz98, author = {yavuz, tuba and g{\"u}venir, h. altay},  title = {application of k-nearest neighbor on feature projections classifier to  text categorization}, booktitle = {proceedings of iscis-98, 13th
this is particularly useful  when labeling text is a labor-intensive job and when there is a large amount of  information available about a particular problem on the world wide web.
tampere, fi}, year = {2002}, pages = {207--214}, url = {http://doi.acm.org/10.1145/564376.564413}, abstract = {
our problem domain consists of hyperlinks given in a form of small-documents represented with word vectors.
its main originality is its  ability to simultaneously take into account the structural and the content  information present in a structured document, and also to cope with different  types of content (text, image, etc).
they can be computed at essentially no extra cost immediately  after training a single svm.
in this paper we propose simple,  heuristic solutions to some of the problems with naive bayes classifiers,  addressing both systemic issues as well as problems that arise because text is  not actually generated according to a multinomial model.
in order to reflect the subtopic structure of a document, we propose a new passage-level or passage-based text categorization model, which segments a test document into several passages, assigns categories to each passage, and merges the passage categories to the document categories.
(c) we present a new algorithm for semi-supervised learning based  on a deterministic annealing (da) approach.
combining machine learning and hierarchical indexing structures for text categorization}, booktitle = {proceedings of the 10th asis/sigcr workshop on classification research}, editor = {}, publisher = {american society for information science, washington, us}, year = {1999}, address = {washington, us}, pages = {}, url = {http://www.cs.uiowa.edu/~mruiz/papers/sigcr_10}, abstract = {this paper presents a method that exploits the hierarchical structure of an indexing vocabulary to guide the development and training of machine learning methods for automatic text categorization.
to retain the information in the structure, we  have developed a structured vector model, which represents a document with a  structured vector, whose elements can be either terms or other structured  vectors.
stanford, us}, pages = {1183--1190}, publisher = {morgan kaufmann publishers, san francisco, us}, url = {ftp://ftp.cs.rutgers.edu/pub/zelikovi/bg1.ps}, abstract = {
parametric and  qualitative descriptors of user's interest must be generated.
the results confirm that our meta-model approach can exploit the advantage of its component algorithms, and demonstrate a better performance than existing algorithms.}, } @inproceedings{lam97, author = {wai lam and kon f. low and chao y. ho}, title = {using a bayesian network induction approach for text categorization}, booktitle = {proceedings of ijcai-97, 15th international joint conference on artificial intelligence}, editor = {martha e. pollack}, publisher = {morgan kaufmann publishers, san francisco, us}, year = {1997}, address = {nagoya, jp}, pages = {745--750}, url = {}, abstract = {
the use of naive bayes allows the boosting algorithm to utilize term  frequency information while maintaining probabilistically accurate confidence  ratio.
the goal of the research described here is to automatically create a computer understandable world wide
"2006" } @article{fumera:2006:sfb, author =
using as testing ground a part of the wall street journal corpus, we  show that the most frequent words of the british national corpus, representing  the most frequent words of the written english language, are more reliable  discriminators of text genre in comparison to the most frequent words of the  training corpus.
several previous works already suggested  applying this method for document clustering, gene expression data analysis,  spectral analysis and more.
we present empirical results on two real world data sets.
similarly, we use a training set of queries and their related documents to obtain empirical associations between query words and indexing terms of documents, and use these associations to predict the related documents of arbitrary queries.
proceeding of ijcnn-01, 12th international joint conference on neural networks}, editor = {}, address = {washington, us}, year = {2001}, pages = {}, url = {}, abstract = {}, publisher =
employing the thesaurus entails structuring categories into  hierarchies, since their structure needs to be conformed to that of the  thesaurus for capturing relationships between categories.
infoclas is a first step towards the understanding of documents proceeding to a  classification-driven extraction of information.
"introducing a family of linear measures for feature selection in  text categorization", author =
morgan kaufmann publishers,  san francisco, us}, url =  {http://www.robotics.stanford.edu/~stong/papers/tong_koller_ml00.ps.gz},  abstract = {support vector machines have met with significant success in  numerous real-world learning tasks.
"boston, ma", pages =
= {hermann fangmeyer and gerhard lustig},  title = {experiments with the cetis automated indexing system}, booktitle =  {proceedings of the symposium on the handling of nuclear information},  publisher = {international atomic energy agency}, editor = {}, year = {1970},  address = {}, pages = {557--567}, url = {}, abstract = {}, }  @inproceedings{ferilli01, author = {stefano ferilli and nicola fanizzi and  gianni semeraro}, title = {learning logic models for automated text  categorization}, booktitle = {proceedings of ai*ia-01, 7th congress of the  italian association for artificial intelligence}, publisher =
in addition, we compare the  algorithms on a larger collection of 1700 texts and describe an automated  method for empirically deriving appropriate threshold values.
"fumiyo fukumoto and yoshimi suzuki", title =
"tackling concept drift by temporal inductive transfer", booktitle =  "proceedings of the 29th annual international acm sigir conference on  research and development in information retrieval", year =
we propose a novel variant, based on the  exploitation of negative evidence, of the well-known $k$-nn method.
we develop an automatic text categorization approach and investigate its application to text retrieval.
the approach is also employed in deleted interpolation, a technique for smoothing n-grams in language modeling for speech recognition.
this study reports the results of a series of experiments in the techniques of automatic document classifications.
experimental results confirm improved performance, breaking through the plateau previously reached in the field." } @inproceedings{soucy:2005:btw, author =
this article uses the structure that is present in the semantic  space of topics in order to improve performance in text categorization:  according to their meaning, topics can be grouped together into  ``meta-topics'', e.g., gold, silver, and copper are all metals.
second, smoothing techniques from statistical language  modeling can be used to recover better estimates than the laplace smoothing  techniques usually used in naive bayes classification.
in addition, they work much better at certain tasks, such as identifying major  events in texts, than at others, such as determining what sort of business or  product is involved in a news event.
wen-lin hsu and sheau-dong lang}, title = {feature reduction and database maintenance in netnews classification}, booktitle = {proceedings of ideas-99, 1999 international database engineering and applications symposium}, publisher = {ieee computer society press, los alamitos, us}, editor = {}, year = {1999}, address = {montreal, ca}, pages = {137--144}, url = {http://dlib.computer.org/conferen/ideas/0265/pdf/02650137.pdf}, abstract = {we propose a statistical feature-reduction technique to filter out the most ambiguous articles in the training data for categorizing the netnews articles.
a probabilistic analysis of the {rocchio} algorithm with {tfidf} for text categorization}, booktitle = {proceedings of icml-97, 14th international conference on machine learning}, editor = {douglas h. fisher}, year = {1997}, address = {
on the other hand, we also  observe that linked pages can be more harmful than helpful when the linked  neighborhoods are highly ``noisy'' and that links have to be used in a careful  manner.
however, as the number of positive training data decreases, the boundary of svmc starts overfitting at some point and end up generating very poor results.
we  describe the new representations and try to justify our hypothesis that they  could improve the performance of a rule based learner.
furthermore, they are fully automatic, eliminating the need for manual parameter tuning.}, } @inproceedings{joachims99, author = {thorsten joachims}, title = {transductive inference for text classification
our experiments on human subjects indicate that  human feedback on feature relevance can identify a sufficient proportion of the  most relevant features (over 50% in our experiments).
the authors' approach has been to use statistics in the knowledge acquisition component of a linguistic pattern-based categorization system, using statistical methods, for example, to associate words with industries and identify phrases that information about businesses or products.
as a consequence, a novel approach named the binary weighting model with non-binary smoothing (bwm-nbs) is then proposed so as to overcome the drawback of bwm.
% % when this is the case, if you know of a url with unrestricted  access % % from which the paper is also available, please let me know
the first set of experiments tests a recurrent neural network for the task of library title classification.
one way is to ask the user to provide  them, which is difficult because the user usually can only give a few words  (which are insufficient for accurate learning).
we use  telltale as our classifier; telltale uses n-grams to compute the similarity  between documents.
searching for documents by their type or genre is a natural way to enhance the effectiveness of document retrieval.
noam slonim and naftali tishby}, title = {
booktitle = {proceedings of icail-01,  8th international conference on artificial intelligence and law}, editor = {},  year = {2001}, address = {st.\ louis, us}, pages =
research on machine  learning for text categorization, already advancing at a rapid pace, could be  further accelerated if better test collections were available.}, }  @article{lewis04, author =
we show that, even with an average word error rate of around 50\%, the  categorization performance loss with respect to the clean version of the same  documents is negligible.}, url =  {ftp://ftp.idiap.ch/pub/reports/2003/rr03-61.pdf}, }  @inproceedings{vinokourov01, author = {alexei vinokourov and mark girolami},  title = {document classification employing the fisher kernel derived from  probabilistic hierarchic corpus representations},
we implemented versions of the svm appropriate for one-class classification in the context of information retrieval.
in order to determine whether information within a sentence has been seen in material read previously, our system integrates information about the context of the sentence with novel words and named entities within the sentence, and uses a specialized learning algorithm to tune the system parameters."
in this paper we present an approach to text categorization in  terms of genre and author for modern greek.
a typical example is information filtering, i.e. the adaptive classification of documents with respect to a particular user interest.
one  of its main drawbacks, in ir, is its computational cost.
we describe {\sc adaboost.
the system is based on calculating and comparing profiles of n-gram frequencies.
{international journal on document analysis and recognition}, year = {2001},  number = {2}, volume = {4}, pages = {69--83}, url =  {http://link.springer-ny.com/link/service/journals/10032/papers/1004002/10040069.pdf},
results show that the text in citing documents, when available, often has greater discriminative and descriptive power than the text in the target document itself.
this paper describes our general approach, several machine learning algorithms for this task, and promising initial results with a prototype system.}, } @article{creecy92, author = {robert m. creecy and brij m. masand and stephen j. smith and david l. waltz}, title = {trading mips and memory for knowledge engineering: classifying census returns on the connection machine}, journal =
learning to classify english text with ilp methods}, booktitle  = {advances in inductive logic programming}, editor = {de raedt, luc},  publisher =
moreover, it is demonstrated that our proposed framework is successfully applied to another task of the genomics track, showing comparable results to the best performing system." } @inproceedings{yang:2005:raf, author =
teklis is a training based  statistical categorization system which incorporates shallow linguistic  processing and fuzzy set methods.
however, while tc deals  with documents represented as vectors in a space of terms, we formulate the  task of term categorization as one in which terms are (dually) represented as  vectors in a space of documents, and in which terms (instead of documents) are  labelled with domains.}, } @inproceedings{baker98, author = {douglas baker and  andrew k. mccallum}, title = {distributional clustering of words for text  classification}, booktitle = {proceedings of the 21st acm international  conference on research and development in information retrieval}, editor =
the task, in essence, is to have a computing machine read a document and on the basis of the occurrence of selected clue words decide to which of many subject categories the document in question belongs.
we propose a completely new approach to the problem of text classification and extracting keywords by using ml techniques.
in  addition, the way in which learning with redundancy influences categorization  performance is also studied.}, } @inproceedings{moulinier97, author = {isabelle  moulinier}, title = {
our experimental results indicate that the  naive bayes classifier and the subspace method outperform the other two  classifiers on our data sets.
we illustrate the approach using the recently available reuters corpus volume 1 (rcv1).
{traugott koch and torvik s{\o}lvberg, ingeborg}, publisher = {springer verlag,  heidelberg, de}, note = {
for this task, an indexing dictionary with rules  for mapping terms from the text onto descriptors is required, which can be  derived automatically from a set of manually indexed documents.
we provide sufficient conditions that indicate when an improvement can be expected, highlighting and formalising the notion of ``independent kernels''.
the concept learning model suggests that the poor statistical characteristics of a syntactic indexing phrase representation negate its dsirable semantic characteristics.
traditionally, supervised learning enters only phases (i) and (iii).
{cussens, james and saso dzeroski}, year = {2000}, address = {bled, sl}, pages  = {247--258}, publisher = {springer verlag, heidelberg, de}, note = {
furthermore, our analysis reveals the precise dependence of the rate of convergence on the eigenstructure of the data each node observes.
= {james g.  shanahan and norbert roma}, title = {
= {{\sc construe/tis}: a system for content-based indexing of a database of news stories}, booktitle = {
} @inproceedings{tailby:2006:eca, author =
we propose here the use of soft clustering of words, i.e., in which a word can be assigned to several different clusters and each cluster is characterized by a specific word probability distribution.
the  paper deals with a new and very efficient way of assessing this generalization  performance.
madison, us}, note = {an extended version appears as~\cite{nigam00}}, url = {http://www.cs.cmu.edu/~knigam/papers/emcat-aaai98.ps}, abstract = {
this task is usually carried out in order to group respondents according to a predefined scheme based on their answers.
if the occurrence of a single word determines whether an article belongs to a category or not (and it often does) any compression scheme will likely fail to classify the article correctly.
chicago, us}, pages = {46--56}, year = {1991}, note = {an extended version appears as~\cite{fuhr94}}, url = {http://www.acm.org/pubs/articles/proceedings/ir/122860/p46-fuhr/p46-fuhr.pdf}, abstract =
our simulation results also show the effectiveness of idc in text categorization problems.
using n-gram frequency profiles provides a simple and reliable way to categorize documents in a wide range of classification tasks.}, } @inproceedings{ceci03, author = {michelangelo ceci and donato malerba}, title = {hierarchical classification of html documents with {webclassii}}, booktitle = {
our experimental results using real netnews articles and newsgroups demonstrate (1) applying feature reduction to the training set improves the routing accuracy, efficiency, and database storage; (2) updating improves the routing accuracy; and (3) the batch technique improves the efficiency of the updating operation.}, } @inproceedings{huffman94, author = {stephen huffman and marc damashek}, title =
the paper describes  feature subset selection used in learning on text data (text learning) and  gives a brief overview of feature subset selection commonly used in machine  learning.
a number of  linear classification methods such as the linear least squares fit (llsf),  logistic regression, and support vector machines (svm's) have been applied to  text categorization problems.
the  trec-3 conference provided the first public demonstration and evaluation of  this new technique, and trec-4 provided an opportunity to test its usefulness  on several types of text retrieval tasks.}, } @inproceedings{hull94, author =
this process involves an activity of {\em supervised learning}, in  which information on the membership of training documents in categories is  used.
using these,  we build a multilevel classifier.
experimental results show that the high-degree biased bigrams should be  eliminated from the feature set, and the s-br1 scheme is quite effective for  further dimensionality reduction in chinese text categorization, after a  feature selection process with a chi-cig score function.}, }  @inproceedings{xue04a, author = {xue, dejun and sun, maosong}, title = {
sample were more dramatic: the text classifier showed a 68\% error, whereas our hypertext classifier reduced this to just 21\%.}, } @article{chakrabarti98c, author = {soumen chakrabarti and byron e. dom and rakesh agrawal and prabhakar raghavan}, title = {scalable feature selection, classification and signature generation for organizing large text databases into hierarchical topic taxonomies}, journal = {journal of very large data bases}, year = {1998}, number = {3}, volume = {7}, pages = {163--178}, url = {http://www.cs.berkeley.edu/~soumen/vldb54_3.pdf}, abstract = {we explore how to organize large text databases hierarchically by topic to aid better searching, browsing and filtering.
in this paper, we propose a more general formulation of text categorization, allowing documents to be organized as \textit{sequences} of pages.
we present the utilization of wsd as an aid  for tc.
using the vector space model, each document is represented by its original feature vector augmented with external feature vector generated using wordnet.
the growing problem of unsolicited bulk e-mail, also known as  ``spam'', has generated a need for reliable anti-spam e-mail filters.
a generic system for domain and language independent text categorization}, journal = {computer vision and image understanding}, year = {1998}, number = {3}, volume = {70}, pages = {299--306}, url = {http://www.idealibrary.com/links/doi/10.1006/cviu.1998.0687/pdf}, abstract = {text categorization assigns predefined categories to either electronically available texts or those resulting from document image analysis.
edmund s. yu and elizabeth d. liddy}, title = {feature selection in text categorization using the baldwin effect networks}, booktitle = {proceedings of ijcnn-99, 10th international joint conference on neural networks}, editor = {}, publisher =
in addition, a comparison of the two filtering methods  clarified that pos filtering on svms consistently outperformed mi filtering,  which indicates that svms cannot find irrelevant parts of speech.
{david j. ittner and lewis, david d. and david d. ahn}, title = {
traditionally the  categories are arranged in hierarchical manner to achieve effective searching  and indexing as well as easy comprehension for human beings.
experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 30\%.}, } @inproceedings{nigam00a, author = {kamal nigam and rayid ghani}, title = {analyzing the applicability and effectiveness of co-training}, booktitle = {proceedings of cikm-00, 9th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, address = {mclean, us}, editor = {arvin agah and jamie callan and elke rundensteiner}, year = {2000}, pages = {86--93}, url = {http://www.cs.cmu.edu/~knigam/papers/cotrain-cikm00.pdf}, abstract = {recently there has been significant interest in supervised learning algorithms that combine labeled and unlabeled data for text learning tasks.
fabio  crestani and mark girolami and van rijsbergen, cornelis j.}, year = {2002},  address = {glasgow, uk}, publisher =
the categories in such problems usually are  neither conditionally independent from each other nor mutually exclusive,  therefore it is not trivial to directly employ state-of-the-art classification  algorithms without losing information of relation among categories.
the comparison of the examined  models demonstrates that techniques from information retrieval integrated into  recurrent plausibility networks performed well even under noise and for  different corpora.}, } @inproceedings{wermter02, author = {stefan wermter and  chihli hung}, title = {
a robust model for intelligent text classification}, booktitle = {proceedings of ictai-01, 13th ieee international conference on tools with artificial intelligence}, publisher =
experimental results show  that this method improves the precision of k-nn up to 13.86% without  compromising its recall.}
we compare these methods in terms of their  loss functions and score distributions, and establish the connection between  their optimization problems and generalization error bounds.
the goal of the research described here is to automatically create a computer understandable knowledge base whose content mirrors that of the world wide web.
we also present results with  algorithms other than co-training in this framework and show that co-training  is uniquely suited to work well within ecoc.}, } @inproceedings{giorgetti03,  author = {daniela giorgetti and fabrizio sebastiani}, title = {multiclass text  categorization for automated survey coding}, year = {2003}, address =
in this paper we present a comprehensive comparison of the performance of a number of text categorization methods in two different data sets.
this paper describes the design, execution  and evaluation of a modest experimental study aimed at testing empirically one  statistical technique for automatic indexing.}, } @inproceedings{marton05,  author = {yuval marton and ning wu and lisa hellerstein}, title = {
{195--217}, url = {http://www.wkap.nl/article.pdf?391247}, abstract = {
} @inproceedings{gamon:2004:scc, author =
in this paper we describe an automated method of classifying research project descriptions: a human expert classifies a sample set of projects into a set of disjoint and pre-defined classes, and then the computer learns from this sample how to classify new projects into these classes.
} @inproceedings{stein:2006:eoe, author =
"the  world wide web is a massive corpus that constantly evolves.
we discuss the potential reasons for this  dependency.}, } @article{bekkerman03, author
we also demonstrate the impact of the time-varying nature of category definitions.}, } @inproceedings{lewis94a, author = {lewis, david d. and gale, william a.}, title = {a sequential algorithm for training text classifiers}, booktitle = {proceedings of sigir-94, 17th acm international conference on research and development in information retrieval}, editor = {w. bruce croft and van rijsbergen, cornelis j.}, publisher = {springer verlag, heidelberg, de}, year = {1994}, address = {dublin, ie}, pages = {3--12}, note = {
description-oriented approaches are more flexible with respect to the  underlying representations, but the definition of the feature vector is a  heuristic step.
chien chin chen and  chang chen, meng and yeali sun}, title = {pva: a self-adaptive personal view  agent}, journal = {journal of intelligent information systems}, year = {2002},  note = {special issue on automated text categorization}, volume = {18}, number  = {2/3}, pages = {173--194}, url = {http://www.wkap.nl/article.pdf?391245},  abstract = {
therefore, it is very costly to assign a  category to them because a human investigates their contents.
when applied to text classification, these learning algorithms lead to svms  with excellent precision but poor recall.
published in the ``lecture  notes in computer science'' series, number 2945}, pages = {571--579}, url = {},  abstract = {}, } @inproceedings{cheng01, author = {
first, terms (single words or phrases) are identified in the document text.
this kind of representational problem is also studied in this paper where traditional methods for statistical text categorization are augmented via a systematic use of linguistic information.
this approach is well suited to learning in hypertext domains because its statistical component allows it to characterize text in terms of word frequencies, whereas its relational component is able to describe how neighboring documents are related to each other by hyperlinks that connect them.
the word-pairs are selected automatically using a technique based on frequencies of n-grams (sequences of characters), which takes into account both the frequencies of word-pairs as well as the context in which they occur.
text  categorization is useful for indexing documents for information retrieval,  filtering parts for document understanding, and summarizing contents of  documents of special interests.
we have also empirically verified the  advantages of rule-based methods over non-rule-based ones.}, }  @inproceedings{li02a, author = {xin li and dan roth}, title = {
by using the hierarchically structured subject domain and classification rules, the classifier's engine assigns an email query to the most relevant category or categories.}, } @article{zheng04, author = {zhaohui zheng and xiaoyun wu and rohini srihari}, title = {
see  also~\cite{lewis95a}}, url =
the logic  representation of sentences required by the adopted learning algorithm is  obtained by detecting structure in raw text trough a parser.
"learning to  classify email into ``speech acts''", booktitle = "emnlp'04",  pages =
chien chin chen and chang chen, meng and yeali sun}, title = {pva: a self-adaptive personal view agent}, journal = {journal of intelligent information systems}, year = {2002}, note = {special issue on automated text categorization}, volume = {18}, number = {2/3}, pages = {173--194}, url = {http://www.wkap.nl/article.pdf?391245}, abstract = {
we have evaluated expnet in categorization and  retrieval on a document collection of the medline database, and observed a  performance in recall and precision comparable to the linear least squares fit  (llsf) mapping method, and significantly better than other methods tested.
this paper explores the techniques of utilizing n-gram information  to categorize chinese documents so that the classifier can shake off the burden  of large dictionaries and complex segmentation processing, and subsequently be  domain and time independent.
error minimization is difficult in high-dimensional  feature spaces because the convergence process is slow and the models are prone  to overfitting.
the efficiency of these three classifications is investigated on two data sets.
in experiments using  a standard text retrieval test collection, small effectiveness improvements  were obtained.
we present the results of some experiments done on real data: two different classifications of our research projects.}, } @article{borko63, author = {harold borko and myrna bernick}, title = {automatic document classification}, journal = {journal of the association for computing machinery}, year = {1963}, volume = {10}, number = {2}, pages = {151--161}, url = {http://www.acm.org/pubs/articles/journals/jacm/1963-10-2/p151-borko/p151-borko.pdf}, } @article{borko64, author = {harold borko and myrna bernick}, title = {automatic document classification.
wendy lehnert and  stephen soderland and david aronow and fangfang feng and avinoam shmueli},  title = {inductive text classification for medical applications}, journal =
this algorithm is the "semi-supervised agglomerative hierarchical clustering algorithm" (ssahc).
measuring the similarity of two documents is conducted by comparing a pair of their corresponding sub-vectors at a time.
{24}, number = {}, pages = {187--196}, url = {}, abstract = {}, }  @inproceedings{krishnapuram03, author = {raghu krishnapuram and krishna  chitrapura and sachindra joshi}, title = {classification of text documents  based on minimum system entropy}, booktitle = {proceedings of icml-03, 20th  international conference on machine learning}, editor = {}, year = {2003},  address = {washington, dc}, pages = {}, publisher = {morgan kaufmann  publishers, san francisco, us}, url = {}, abstract = {}, }  @inproceedings{kumaran04, author = {giridhar kumaran and james allan}, title =
the ecoc method scales well to large data sets with a large number of classes.
"hyderabad, india" } @inproceedings{dai:2007:tnb, author =
augmenting naive {b}ayes classifiers  with statistical language models}, journal = {information retrieval}, publisher  = {springer science}, year = {2004}, volume = {7}, number = {3-4}, pages =
{2006} } @article{bratko:2006:esi, author = {bratko, andrej and filipic,  bogdan}, title = {
in this  paper, we explore correlations among categories with maximum entropy method and  derive a classification algorithm for multi-labelled documents.
we evaluate the performance of nine  different configurations of c4.5.
however, most of them are useless for document categorization because of the weakness in representing document contents.
"fu, yueyu and ke, weimao and mostafa, javed", title = "automated text classification using a multi-agent framework", booktitle = "proceedings of the 5th acm/ieee-cs joint conference on digital libraries", year =
furthermore, a learning feedback technique is  introduced.
other extensions to design discriminative multiple-category mfom  classifiers for application scenarios with new performance metrics could be  envisioned too.}, } @inproceedings{gaussier02, author = {{\'e}ric gaussier and  cyril goutte and kris popat and francine chen}, title = {
this article reports on our experiments and results on the effectiveness of different feature sets and information fusion from some combinations of them in classifying free text documents into a given number of categories.
many techniques  and algorithms for automatic text categorization have been devised and proposed  in the literature.
but our experiments on human subjects indicate that human feedback on  feature relevance can identify a sufficient proportion (65%) of the most  relevant features.
"zhiwei li and bin wang and mingjing li and wei-ying ma", title =
"determining term subjectivity and term  orientation for opinion mining", booktitle =
it connects the statistical properties of text-classification tasks with the generalization performance of a svm in a quantitative way.
{honolulu, us}, year = {2002}, pages = {562--569}, publisher = {acm press, new  york, us}, url = {http://www.cs.princeton.edu/~kt/www02.ps}, abstract = {
given corpora of documents and a training set of examples of classified documents, the technique locates a minimal set of co-ordinate keywords to distinguish between classes of documents, reducing the dimensionality of the keyword vectors.
the automated  categorisation (or classification) of texts into topical categories has a long  history, dating back at least to 1960.
text mining: finding nuggets in  mountains of textual data}, booktitle = {proceedings of kdd-99, 5th acm  international conference on knowledge discovery and data mining}, publisher =
{an extended version appears as \cite{tong01}}, } @article{tong01, author =
the method outperforms all known methods  when tested recognized standard benchmarks for this task."
aaai press, menlo park, us}, editor = {}, year =  {1998}, pages = {329--334}, address = {san jose, us}, url = {}, abstract = {},  } @inproceedings{riloff93, author = {ellen riloff}, title = {
the set of all indications from a document  leading to the same descriptor is called a relevance description.
"dayanik, aynur and lewis, david d. and madigan, david and menkov, vladimir and genkin, alexander", title =
while most work on kdd has been concerned with structured databases,  there has been little work on handling the huge amount of information that is  available only in unstructured textual form.
this metric can be incorporated into the definition of radial basis kernels of support vector machines or directly used in a k-nearest neighbors algorithm.
we then  present a fast, divisive algorithm that monotonically decreases this objective  function value.
= {springer verlag, heidelberg, de}, year =
} @inproceedings{zhang:2005:tck, author =
"proceedings of the 21st international conference on computational linguistics and 44th annual meeting of the association for computational linguistics", year =
we can choose to make use of none, part or all of the hierarchical structure to improve the categorization effectiveness and efficiency.
in our reference collections, measures based on co-citation tend to  perform better for pages in the web directory, with gains up to 37\% over text  based classifiers, while measures based on bibliographic coupling perform  better in a digital library.
abstract = {we propose several algorithms using the vector space model to classify the news articles posted on the netnews according to the newsgroup categories.
on computational cybernetics (iccc03)", address = "siofok, hungary", year =
alternative methods are also tested on these data collections for comparison.
these constraints may include a semantic classification of the sought  after answer and may even suggest using different strategies when looking for  and verifying a candidate answer.
the modalities and techniques to fit this  objectives are still under discussion.
we use two different strategies, latent semantic indexing and  optimal term selection, to reduce the number of features.
such a knowledge base would enable much more effective retrieval of web information, and promote new uses of the web to support knowledge-based inference and problem solving.
in this paper we propose a technique for the automatic updating of a pn dictionary through the cooperation of an inductive and a probabilistic classifier.
in this paper, we summarize the competition task, the evaluation method, and the results of the competition.
on one of these datasets  (the 20 newsgroups) the method based on word clusters significantly outperforms  the word-based representation in terms of categorization accuracy or  representation efficiency.
error minimization is difficult in high-dimensional feature spaces because the convergence process is slow and the models are prone to overfitting.
the rationale behind our proposal is that taking into account  contextual information provided by the whole page sequence can help  disambiguation and improves single page classification accuracy.
have developed a method using distributions based on hard clustering of words,  i.e., in which a word is assigned to a single cluster and words in the same  cluster are treated uniformly.
however, such systems sacrifice efficiency to boost effectiveness.
booktitle = {proceedings of bcsirsg-97, the 19th annual colloquium of the british computer society information retrieval specialist group}, publisher = {springer verlag, heidelberg, de}, series = {electronic workshops in computing}, editor =
adaptive information filtering (aif) is concerned with filtering in changing environments.
the proposed approach does not require professional librarians or that the end users have extensive training.
ata kaban and mark  girolami}, title = {
for this reason, the automatic construction of  disambiguation rules is highly desirable.
an example-based mapping method for text  categorization and retrieval}, journal = {acm transactions on information  systems}, year = {1994}, number = {3}, volume = {12}, pages = {252--277}, url =
our experiments, make use of the reuters 21578 database of documents and consist of a binary classification for each of the ten most populous categories of the reuters database.
the r-measure can be effectively computed using the suffix array data structure.
we show that the advantage of using supervised clustering is that it is possible to have some control over the range of subjects that one would like the categorization system to address, but with a precise mathematical definition of each category.
we show that machine-generated decision rules appear comparable to human performance, while using the identical rule-based representation.
all the experiments result in a significant improvement with respect to other purely statistical methods (e.g. [yang, 1999]), thus stressing the relevance of the available linguistic information.
the results we have obtained significantly outperform the results achieved by previous automated survey coding approaches.}, } @inproceedings{glover02, author =
the neural network is simulated on a vax computer with a fast learning algorithm, and is combined with some non-statistical knowledge from the feature definition system.
{1997}, address = {madrid, es}, pages = {32--38}, url =  {ftp://parcftp.xerox.com/pub/qca/genre/paper.acl97.ps.z}, abstract = {as the  text databases available to users become larger and more heterogeneous, genre  becomes increasingly important for computational linguistics as a complement to  topical and structural principles of classification.
{87--103}, year = {2000}, url = {http://www.his.sunderland.ac.uk/ps/ir4.pdf}, abstract = {the research project agnet develops agents for neural text routing in the internet.
proceedings of  ecir-02, 24th european colloquium on information retrieval research}, editor =  {fabio crestani and mark girolami and van rijsbergen, cornelis j.}, year =
the paper is concerned with  categorization of electronic document images.
"context and learning in novelty detection", booktitle = "coling'04", pages = "487--493", year =
instead of labeling a set of documents, the proposed method  labels a set of representative words for each class.
we also used  the generated structure to categorize text documents.
additional text is provided in coded form so that the  reader can more fully explore this technique and form his own opinion of the  applicability and versatility of this particular procedure.
furthermore, the online approach offers the advantage of continuous learning in the batch-adaptive text filtering task.}, } @inproceedings{adami03, author = {giordano adami and paolo avesani and diego sona}, title = {
for n training instances held in memory, the best-known svm implementations take time proportional to n a , where a is typically between 1.8 and 2.1.
these results demonstrate that this approach can scale up to a large real-world task and show a lot of potential for text classification.}, } @inproceedings{wermter99, author = {stefan wermter and christo panchev and garen arevian}, title = {hybrid neural plausibility networks for news agents}, booktitle = {proceedings of aaai-99, 16th conference of the american association for artificial intelligence}, publisher = {aaai press, menlo park, us}, editor = {}, year = {1999}, pages = {93--98}, address = {orlando, us}, url = {http://www.his.sunderland.ac.uk/ps/aaai99.pdf}, abstract = {
among the three different combination approaches, our adaptive  classifier combination method introduced here performed the best.}, }  @inproceedings{li99, author = {
from these web subgraphs, web units are constructed and classified into semantic concepts (or categories) in an iterative manner.
ts compares favorably with the other methods with up to 50\% vocabulary reduction but is not competitive at higher vocabulary reduction levels.
"xiaoyun wu and rohini srihari", title =
to do this, we use techniques from statistical pattern recognition to efficiently separate the feature words, or discriminants, from thenoise words at each node of the taxonomy.
combinations of multiple classifiers did not always improve the classification accuracy compared to the best individual classifier.
results obtained from evaluation show that  the integration of wordnet can outperform approaches based only on training.},  } @article{diederich03, author = {
through our experiments  on the reuters-21578 news database, we showed that aram performed reasonably  well in mining categorization knowledge from sparse and high dimensional  document feature space.
we report here on experiments using a committee of winnow-based  learners and demonstrate that this approach can reduce the number of labeled  training examples required over that used by a single winnow learner by 1-2  orders of magnitude.}, } @inproceedings{liere98, author = {ray liere and prasad  tadepalli}, title = {active learning with committees: preliminary results in  comparing winnow and perceptron in text categorization}, booktitle =  {proceedings of conald-98, 1st
the benefit of the  approach has been assessed via extensive cross evaluation over three corpora in  two languages.
"proceedings of the eighteenth national  conference on artificial intelligence", year =
the author describes a series of experiments that show how the extraction  patterns learned by autoslog can be used for text classification.
the design of the new approach as well as its justification are presented.
the experimental results show that modulating the structure  of the user profile increases the accuracy of a personalization system.}, }  @article{chen03, author = {chen l. and tokuda n. and nagai a.}, title = {a new  differential lsi space-based probabilistic document classifier}, journal =
with the advent of centralized data warehouses, where data might be stored as electronic documents or as text fields in databases, text mining has increased in importance and economic value.
{2000}, number = {7}, volume = {15}, pages = {633--646}, url =  {http://www3.interscience.wiley.com/cgi-bin/fulltext?id=72502965&placebo=ie.pdf},  abstract = {
accordingly,' our approach is based on  problem solving tasks.
simpl uses efficient sequential scans and sorts, and is comparable in speed and memory scalability to widely-used naive bayes (nb) classifiers, but it beats nb accuracy decisively.
"edinburgh, scotand", url = "http://www.ijcai.org/papers/0304.pdf",
despite this, we show for one data set that fax quality images can be categorized with nearly the same accuracy as the original text.
text classification  tasks, like text categorization, help the users to access to the great amount  of text they find in the internet and their organizations.
text categorization: an experiment using phrases}, booktitle =
published in the ``lecture notes in computer science'' series, number 1642},  editor = {
we present a de-centralized approach and system implementation (named macci) for text classification using a multi-agent framework.
keywords alone cannot always distinguish the relevant from the irrelevant texts and some relevant texts do not contain any reliable keywords at all.
in a different manner from topographical  techniques previously utilized for static text collections, the topography is  an outcome of the coherence in time of the data stream in the proposed model.
for the first of them, a collection of articles from pc week magazine, the addition of word-pairs increases micro-averaged breakeven accuracy by more than 6\% point from a baseline accuracy (without pairs) of around 40\%.
[1], and shows that some of the modifications included in twcnb may not be necessary to achieve optimum performance on some datasets.
pva consists of three parts: a {\it proxy}, {\it personal view constructor}, and {\it personal view maintainer}.
furthermore, the effectiveness of word sense disambiguation for different parts of speech (nouns and verbs) is examined empirically.}, } @inproceedings{pang02, author = {
organizing search results allows users to focus on items in  categories of interest rather than having to browse through all the results  sequentially.}, } @inproceedings{chen00a, author = {hao chen and tin kam ho},  title = {evaluation of decision forests on text categorization}, booktitle =  {proceedings of the 7th spie conference on document recognition and retrieval},  publisher =
although both the contents and time information of news articles are  helpful to red, most researches focus on the utilization of the contents of  news articles.
hang li and kenji yamanishi}, title = {document classification using a finite mixture model}, booktitle = {proceedings of acl-97, 35th annual meeting of the association for computational linguistics}, publisher = {morgan kaufmann publishers, san francisco, us}, editor = {philip r. cohen and wolfgang wahlster}, year = {1997}, address = {madrid, es}, pages = {39--47}, url = {http://xxx.lanl.gov/ps/cmp-lg/9705005}, abstract = {
a  generic system for domain and language independent text categorization},  journal = {computer vision and image understanding}, year = {1998}, number =  {3}, volume = {70}, pages = {299--306}, url =  {http://www.idealibrary.com/links/doi/10.1006/cviu.1998.0687/pdf}, abstract =  {text categorization assigns predefined categories to either electronically  available texts or those resulting from document image analysis.
we test this approach on a large collection of personal e-mail messages, which we make publicly available in "encrypted" form contributing towards standard benchmarks.
adding numbers to text classification}, booktitle = {proceedings of  cikm-03, 12th acm international conference on information and knowledge  management}, publisher = {acm press, new york, us}, editor = {}, year = {2003},  address = {new orleans, us}, pages = {240--246}, url =
"proceedings of the international joint conference on  artificial intelligence", year =
current natural language text processing (nlp) methods help to overcome these  problems.
this corpus has been developed by eads company in the context of a large web  site filtering application.}, } @article{denoyer04, author =
we compare the  effectiveness of five different automatic learning algorithms for text  categorization in terms of learning speed, real-time classification speed, and  classification accuracy.
exploiting structural information for semi-structured document categorization}, journal = {information processing and management}, volume = {42}, number = {3}, pages =
however, we find that ppm does not compete with the published state of the art in the use of machine learning for text categorization.
how then, from this sea of pages, should a search engine select the correct ones-those of most value to the user?}, } @inproceedings{chandrinos00, author = {konstantinos v. chandrinos and ion androutsopoulos and georgios paliouras and constantine d. spyropoulos}, title = {automatic web rating:
"rome, italy"  } @inproceedings{he:2007:inb, author =
since the sequential approach is much more efficient, requiring only 14\%-16\% of the comparisons used in the other approaches, we find it to be a good choice for classifying text into large hierarchical structures.} } @inproceedings{dumais98, author = {
the results of these computational experiments on a sample of 2897 text documents from the tipster collection indicate that the first approach has many advantages over the vsm approach for solving this type of text document classification problem.
with the use of suitable dimensionality reduction techniques and efficient algorithms, both llsf and expnet successfully scaled to this very large problem with a result significantly outperforming word-matching and other automatic learning methods applied to the same corpus.}, } @article{yang96b, author =
first, we explain how the extraction patterns can be generated automatically using only preclassified texts as input.
prior to text categorization, a feature generator analyzes the  documents and maps them onto appropriate ontology concepts, which in turn  induce a set of generated features that augment the standard bag of words.
previous work of this kind has been confined  to the range of one through eight words per document.
"mexico city, mexico" }  @inproceedings{cleuziou:2007:oll, author =
this paper describes some key aspects of the system (including html parsing, classification and displaying of results), outlines the text categorization experiments performed in order to choose the right parameters for classification, and puts the system into the context of related work on (meta-)search engines.
we have also empirically verified the advantages of rule-based methods over non-rule-based ones.}, } @inproceedings{li02a, author = {xin li and dan roth}, title = {
{433--443}, url =  {http://link.springer-ny.com/link/service/series/0558/papers/2035/20350433.pdf},  abstract = {
then the system computes a profile for a particular document that is to be classified.
= {morgan  kaufmann publishers, san francisco, us}, year = {2003}, address = {acapulco,  mx}, pages = {581--586}, url = {}, abstract = {a new approach to the text  categorization problem is here presented.
we also demonstrate that the retrieval performance using automatic categorization achieves the same retrieval quality as the performance using manual categorization.
"robustness of  adaptive filtering methods in a cross-benchmark evaluation", pages =  "98--105", booktitle =
"proc. of the 4th int. symp.
djamel a. zighed and henryk jan komorowski and jan m. zytkow},  address = {lyon, fr}, pages = {490--497}, year = {2000}, publisher
until the late '80s, the dominant  approach to the problem involved knowledge-engineering automatic categorisers,  i.e. manually building a set of rules encoding expert knowledge on how to  classify documents.
comparison with manually  assigned classes shows that link information enhances classification in data  with sufficiently high link density, but is detrimental to performance at low  link densities or if the quality of the links is degraded.
we also used the  generated structure to categorize text documents.
in this paper, we adopt novel dimension reduction methods to  reduce the dimension of the document vectors dramatically.
a sequential  algorithm for training text classifiers: corrigendum and additional data},  journal = {sigir forum}, year = {1995}, pages = {13--19}, volume = {29}, number  = {2}, url = {http://www.research.att.com/~lewis/papers/lewis95g.ps}, abstract  = {
we suggest, for text categorization, the integration of external wordnet lexical information to supplement training data for a semi-supervised clustering algorithm which (i) uses a finite design set of labeled data to (ii) help agglomerative hierarchical clustering algorithms (ahc) partition a finite set of unlabeled data and then (iii) terminates without the capacity to classify other objects.
five methods were evaluated, including term selection based on document frequency (df), information gain (ig), mutual information (mi), a 2 -test (chi), and term strength (ts).
using various data sets, their performances are investigated and compared to a standard centroid-based classifier (tdidf) and a centroid-based classifier modified with information gain.
christopher manning and hinrich sch{\"{u}}tze}, title = {foundations of statistical natural language processing}, publisher =
the key issue of the approach is how to obtain a set of representative words for each class.
we see genre classification as a powerful instrument to bring web-based search services closer to a user's information need.
we experimentally evaluated waknn on  52 document data sets from a variety of domains and compared its performance  against several classification algorithms, such as c4.5, ripper,  naive-bayesian, pebls and vsm.
based on this consideration, the authors have built a neural network classification system, which has three subsystems: a user-maintainable feature definition subsystem, a feature extraction subsystem, and a neural network subsystem.
while the former is a realization of the well-known \textsc{adaboost} algorithm specifically aimed at multi-label text categorization, the latter is a generalization of the former based on the idea of learning a committee of classifier sub-committees.
the ability, of  the approach to generalize, given a minimum of training data is also addressed.
as the amount of  data stored in storage media is increased exponentially, it becomes necessary  to store documents according to their category, to access them easily.
"dell zhang and xi chen and lee,  wee sun", title =
"bremen,  germany", publisher =
however, even the most  successful techniques are defeated by many real-world applications that have a  strong time-varying component.
the tcs and how it meets its design goals are described, and examples of applications built with tcs are given.
based on the lpt-model, we focus on learning patterns within a relatively simple pattern language.
in this paper, we present the megadocument approach for categorization.
we describe an approach to text classification that represents a compromise between traditional word-based techniques and in-depth natural language processing.
among the four dimensionality reduction techniques proposed, principal component analysis was found to be the most effective in reducing the dimensionality of the feature space.}, } @article{lam99a, author = {lam, wai and ruiz, miguel e. and srinivasan, padmini}, title = {automatic text categorization and its applications to text retrieval}, journal = {ieee transactions on knowledge and data engineering}, year = {1999}, number = {6}, volume = {11}, pages = {865--879}, url = {http://www.cs.uiowa.edu/~mruiz/papers/ieee-tkde.ps}, abstract = {
{proceedings of ijcai-03, 18th international joint conference on artificial  intelligence}, editor =
"classifying search engine queries using the web as background knowledge", journal = "{sigkdd} explorations", pages =
the vast majority of them represent cases that can only be fully  categorized with the assistance of a human subject (because, for instance, they  require specific knowledge of a given pathology).
in order to respond correctly to a free form factual question given a large  collection of texts, one needs to understand the question to a level that  allows determining some of the constraints the question imposes on a possible  answer.
susan t. dumais and john platt and david heckerman and mehran sahami}, title = {inductive learning algorithms and representations for text categorization}, booktitle = {proceedings of cikm-98, 7th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, editor = {
in this paper, we propose a news web page  classification method (wpcm).
moreover, our feature selection method sometimes produces  more improvements of conventional machine learning algorithms over support  vector machines which are known to give the best classification accuracy.} }  @article{pant:2005:lcc, author = {gautam pant and padmini srinivasan}, title =
we propose an algorithm that interleaves labeling features and documents which significantly accelerates active learning." } @inproceedings{cohen:2005:fsb, author = "shay cohen and eytan ruppin and gideon dror", title =
yiming yang}, title = {noise reduction in a  statistical approach to text categorization}, booktitle = {proceedings of  sigir-95, 18th acm international conference on research and development in  information retrieval}, editor =
"applied research in uncertainty modelling and analysis", editor =
it is argued that for a large class of automatic  categorization algorithms, extraction-based document categorization can be  viewed as a particular form of feature selection performed on the full text of  the document and, in this context, its impact can be compared with  state-of-the-art feature selection techniques especially devised to provide  good categorization performance.
we start from the observation that support vector machines, one of the best text categorization methods cannot scale up to handle the large document collections involved in many real word problems.
we propose herein a new approach for automatic text categorization.
when the complete  feature set is available, the classifier learning algorithm can better relate  to the suitable representation level the different complex features like  linguistic ones (e.g. syntactic categories associated to words in the training  material or terminological expressions).
suppose all sentence vectors that compose each  document are projected onto projection axes.
in particular, evaluations on ocr documents are very rare.
morgan kaufmann publishers, san francisco, us}, url = {}, abstract  = {}, } @inproceedings{lanquillon00, author = {carsten lanquillon}, title =
masoud mohammadian}, publisher =
our enhancements include using lidstone's law of succession  instead of laplace's law, under-weighting long documents, and over-weighting  author and subject.
while regular support vector machines (svms) try to induce a general decision function for a learning task, tsvms take into account a particular test set and try to minimize misclassifications of just those particular examples.
it requires an indexing dictionary with rules mapping terms of the  respective subject field onto descriptors and inverted lists for terms  occurring in a set of documents of the subject field and descriptors manually  assigned to these documents.
since in both tasks text as a sequence of words is of crucial importance, propositional learners have strong limitations, although viewing learning for tc and ie as inductive logic programming (ilp) problems is obvious, most approaches rather use proprietary formalisms.
a filtering system should be able to  adapt to such concept changes.
these results  suggest a simple strategy for the svm text categorization: use a full number of  words found through a rough filtering technique like part-of-speech tagging.},  } @inproceedings{takamura01, author = {hiroya takamura and yuji matsumoto},  title = {feature space restructuring for svms with application to text  categorization}, booktitle = {proceedings of emnlp-01, 6th conference on  empirical methods in natural language processing}, year = {2001}, publisher =
the results are analyzed for various objectives.
we call this idea  \emph{supervised term weighting} (stw).
however, separate databases of short system call sequences have to be built for different programs, and learning program profiles involves time-consuming training and testing processes.
we  propose a statistical feature-reduction technique to filter out the most  ambiguous articles in the training data for categorizing the netnews articles.
traditional techniques for this purpose can  generally be classified into feature extraction and feature selection.
the expert literary readers were found to assign significantly  higher ratings to all versions of the manipulated poems than the novice  readers.}, } @inproceedings{hayes88, author = {philip j. hayes and laura e.  knecht and monica j. cellio}, title = {a news story categorization system},  booktitle = {proceedings of anlp-88, 2nd conference on applied natural language  processing}, publisher = {association for computational linguistics,  morristown, us}, address = {austin, us}, editor = {}, year = {1988}, pages =  {9--17}, url = {}, note = {
in our learning experiments  naive bayesian classifier was used on text data.
this method, which  we call uncertainty sampling, reduced by as much as 500-fold the amount of  training data that would have to be manually classified to achieve a given  level of effectiveness.}, } @article{lewis94b, author = {lewis, david d. and  philip j. hayes}, title = {guest editors' introduction to the special issue on  text categorization}, journal = {acm transactions on information systems},  volume = {12}, number = {3}, pages = {231}, year = {1994}, }  @inproceedings{lewis94c, author = {lewis, david d. and jason catlett}, title =
grammatical errors do not exceed five per cent of the output,  so human screening is satisfactorily low.
we find that even a  relatively high level of errors in the ocred documents does not substantially  affect stylistic classification accuracy."
this paper presents this feature selection method and its results, and how we have predetermined some of its parameters through experimentation.}, } @inproceedings{soucy01a, author = {pascal soucy and guy w. mineau}, title = {a simple knn algorithm for text categorization}, booktitle = {proceedings of icdm-01, ieee international conference on data mining}, publisher =
recently, probabilistic relational models, a relational version  of bayesian networks, were used to define a joint probabilistic model for a  collection of related entities.
automated text classification is attractive because it frees organizations from  the need of manually organizing document bases, which can be too expensive, or  simply infeasible given the time constraints of the application or the number  of documents involved.
the results reveal that subjects are sensitive to the manipulations of graphic and phonetic information and use the same additive information integration rule in making poetic text categorization judgements.
masoud  mohammadian}, publisher = {ios press}, address = {amsterdam, nl}, pages =
it is important to define what constitutes good effectiveness for these autonomous systems, tune the systems to achieve the highest possible effectiveness, and estimate how the effectiveness changes as new data is processed.
{proceedings of ecml-03, 14th european conference on machine learning},  publisher = {springer verlag, heidelberg, de}, editor = {}, year = {2003},  address = {dubrovnik, hk}, pages = {456--467}, url = {}, abstract = {
visualization is used for the presentation of the output  of learning}, } @inproceedings{hamill78, author = {
"it is well known that links are an important source of information when dealing with web collections.
we thereby treat the problem of classifying documents as that  of conducting statistical hypothesis testing over finite mixture models.
several known and some new feature scoring measures appropriate for feature subset selection on large text data are described and related to each other.
{seattle, us}, pages = {229--237}, url =  {ftp://parcftp.xerox.com/pub/qca/papers/sigir95.ps.gz}, abstract = {
} @inproceedings{shen:2006:cie, author =
the result is a model for the automatic selection of  parameters.
in this paper, advanced document  representations have been investigated.
{washington, us}, pages = {265}, url = {}, } @inproceedings{chai02, author = {kian m. chai and hwee t. ng and hai l. chieu}, title = {
when more documents are archived, new terms, new concepts and  concept-drift will frequently appear.
this model uses a divide and conquer principle to define smaller categorization problems based on a predefined hierarchical structure.
"138--145", booktitle =
"the ferrety algorithm  for the {kdd} cup 2005 problem", journal =
this paper proposes a new method to recognize  and handle concept changes with support vector machines.
this is a problem in which there are large amounts of data  available, but the rules for classification are not explicitly available.
these methods can greatly reduce the number of instances that an expert need label.
we review methods for analyzing novelty and then describe newsjunkie, a system that personalizes news for users by identifying the novelty of stories in the context of stories they have already reviewed.
thus, svm adapts efficiently in dynamic environments that require frequent additions to the document collection.
"computational linguistics and  intelligent text processing", year = "2005" }  @inproceedings{chowdhury:2005:utc, author =
however, when lsi is used is conduction with statistical classification, there is a dramatic improvement in performance.} } @inproceedings{hull96, author = {david a. hull and jan o. pedersen and hinrich sch{\"u}tze}, title = {method combination for document filtering}, booktitle = {proceedings of sigir-96, 19th acm international conference on research and development in information retrieval}, editor = {hans-peter frei and donna harman and peter sch{\"{a}}uble and ross wilkinson}, publisher = {acm press, new york, us}, year = {1996}, address = {z{\"{u}}rich, ch}, pages = {279--288}, url = {ftp://parcftp.xerox.com/pub/qca/papers/sigirfiltering96.ps}, abstract = {
} @inproceedings{cui:2006:ces, author =
we present experimental results obtained on the standard \textsf{reuters-21578} benchmark with three classifier learning methods (rocchio, $k$-nn, and support vector machines), three term selection functions (information gain, chi-square, and gain ratio), and both local and global term selection and weighting.}, } @inproceedings{debole04c, author = {franca debole and fabrizio sebastiani}, title = {an analysis of the relative hardness of reuters-21578 subsets}, year = {2004}, booktitle = {proceedings of lrec-04, 4th international conference on language resources and evaluation}, address = {lisbon, pt}, pages = {971--974}, url = {http://www.math.unipd.it/~fabseb60/publications/lrec04.pdf}, } @article{debole05, author = {franca debole and fabrizio sebastiani}, title = {an analysis of the relative hardness of reuters-21578 subsets}, journal = {journal of the american society for information science and technology}, year = {2004}, volume = {56}, number = {6}, pages = {584--596}, url = {http://www.math.unipd.it/~fabseb60/publications/jasist05.pdf}, abstract = {
{ellen riloff}, title = {information extraction as a basis for portable text  classification systems}, school = {department of computer science, university  of massachusetts}, address = {
{acm press, new york, us}, year = {1996}, address = {z{\"{u}}rich, ch},  pages = {307--315}, note = {
most approaches to automatic tc are based on the utilization of a training collection, which is a set of manually classified documents.
this paper studies the iterative double clustering (idc) meta-clustering algorithm, a new extension of the recent double clustering (dc) method of slonim and tishby that exhibited impressive performance on text categorization tasks.
% technion - israel institute of technology % % technion city, haifa 32000,  israel % %
classification  experiments usually grab a snapshot (temporally and spatially) of the web for a  corpus.
rather than performing lsi's singular value decomposition (svd) process solely on the training data, we instead use an expanded term-by-document matrix that includes both the labeled data as well as any available and relevant background text.
william j. teahan}, title = {
the experiments reported in this paper deal  with the relationship between specific formal textual features, i.e. graphic  and phonetic information, and the reader's literary educational background in  the categorization of poetic texts.
in a second  experiment, we ignored nouns, verbs and adjectives and replaced them by  grammatical tags and bigrams.
however, ripper and  sleeping-experts differ radically in many other respects: differences include  different notions as to what constitutes a context, different ways of combining  contexts to construct a classifier, different methods to search for a  combination of contexts, and different criteria as to what contexts should be  included in such a combination.
unlike other machine learning techniques, it allows easy incorporation of new documents into an existing trained system.
these theoretical findings are supported by our experiments, which show that hyperbolic soms can successfully be applied to text categorization and yield results comparable to other state-of-the-art methods.
{rayid ghani and se{\'{a}}n slattery and yiming yang}, title = {
in comparison to the previously proposed agglomerative strategies our divisive algorithm achieves higher classification accuracy especially at lower number of features.
further experiments using two less orthodox categorizers are also presented which suggest that combining text categorizers can be successful, provided the essential element of 'difference' is considered.}, } @article{urena01, author = {l. alfonso ure{\~{n}}a-l{\'{o}}pez and manuel buenaga and jos{\'{e}} m. g{\'{o}}mez}, title = {
in order to classify a new document, the most similar megadocument determines the category to be assigned.
seattle, us}, year = {2001}, pages = {885--890}, url = {http://www.cs.rutgers.edu/~sofmac/paper/ijcai2001/macskassy-ijcai2001.pdf}, abstract = {consider a supervised learning problem in which examples contain both numerical- and text-valued features.
as a consequence, these algorithms cannot  take full advantage of the ``weighted'' representations (consisting of vectors  of continuous attributes) that are customary in information retrieval tasks,  and that provide a much more significant rendition of the document's content  than binary representations.
a cluster based search with a probabilistic clustering algorithm is proposed and evaluated on two data sets.
the new approach is based on extracting patterns, in the form of two logical expressions, which are defined on various features (indexing terms) of the documents.
} @inproceedings{kolcz:2007:amr, author =
the encouraging results indicated that our  approach is hignhly feasible.}, } @article{chouchoulas01, author = {
published in the ``lecture notes in computer science'' series, number 1446}, editor = {david page}, year = {1998}, pages = {38--52}, address = {
"categorizing web search results into meaningful and stable categories using fast-feature techniques", booktitle =
experiments show the  bin-based method is highly competitive with other current methods.
the web  is a hypertext body of approximately 300 million pages that continues to grow  at roughly a million pages per day.
some attempts have been made at  automating this task, most of them based on detecting the similarity between  the answer and textual descriptions of the meanings of the candidate codes.
"text classification improved through multigram models", booktitle = "cikm", year =
stuttgart, de}, pages = {162--167}, url = {}, abstract = {}, } @inproceedings{iwayama95, author = {makoto iwayama and takenobu tokunaga}, title = {cluster-based text categorization: a comparison of category search strategies}, booktitle = {proceedings of sigir-95, 18th acm international conference on research and development in information retrieval}, editor = {edward a. fox and peter ingwersen and raya fidel}, publisher = {acm press, new york, us}, year = {1995}, address = {seattle, us}, pages = {273--281}, url = {http://www.acm.org/pubs/articles/proceedings/ir/215206/p273-iwayama/p273-iwayama.pdf}, abstract = {
improving text retrieval for the routing problem using latent semantic indexing}, booktitle = {proceedings of the 17th acm international conference on research and development in information retrieval}, editor
phrases, word senses and  syntactic relations derived by natural language processing (nlp) techniques  were observed ineffective to increase retrieval accuracy.
the relevancy signatures algorithm classifies texts using extraction patterns, and the augmented relevancy signatures algorithm classifies texts using extraction patterns and semantic features associated with role fillers (riloff and lehnert, 1994).
{697--700}, volume = {5}, url = {}, abstract = {
this technique used in the learning process modifies the  relationship between an index term and its relevant and irrelevant words to  improve the learning performance and, thus, the indexing performance.
the authors developed an approach to automatically generate  categories and reveal the hierarchical structure among them.
moreover, a  new measure for the evaluation of system performances has been introduced in  order to compare three different techniques (flat, hierarchical with proper  training sets, hierarchical with hierarchical training sets).
in this paper, we propose a practical method for  enhancing both the speed and the quality of hypertext categorization using  hyperlinks.
we present a new approach to learning hypertext classifiers that  combines a statistical text-learning method with a relational rule learner.
many methods can be used to categorize texts once their words are known, but ocr can garble a large proportion of words, particularly when low quality images are used.
"gamon,  michael", title =
{learning from message pairs for automatic email answering}, booktitle =  {proceedings of ecml-04, 15th european conference on machine learning}, editor  = {jean-fran{\c{c}}ois boulicaut and floriana esposito and fosca giannotti and  dino pedreschi}, address = {pisa, it}, pages = {87--98}, year = {2004},  publisher = {springer verlag, heidelberg, de}, note = {
{this paper has also been published in \emph{australian computer science  communications}, 24(2), 2002.}, abstract = {automatic text categorization has  always been an important application and research topic since the inception of  digital documents.
seoul, kr}, pages = {609--614}, year = {1999}, url = {}, abstract = {
the method we suggest is based on combining the lexical knowledge extracted from the ldb interpreted as a classifier with a learning-based classifier, through sg.
{efstathios stamatatos and nikos fakotakis and george kokkinakis}, title =  {automatic text categorization in terms of genre and author}, journal =
{storer, james a. and cohn, martin}, publisher = {ieee computer society press,  los alamitos, us}, year = {2000}, address = {snowbird, us}, pages = {200--209},  url = {http://dlib.computer.org/conferen/dcc/0592/pdf/05920555.pdf}, abstract =  {text categorization is the assignment of natural language texts to predefined  categories based on their content.
we have used the receiver  operating characteristic convex hull method for the evaluation, that best suits  classification problems in which target conditions are not known, as it is the  case.
{william w. cohen and haym hirsh}, year = {1994}, address
the average f1 value of our two submitted solutions is 94.4% higher than the average f1 value from all other submitted solutions."
{2001}, pages = {}, url = {}, abstract = {}, publisher = {ieee computer society  press, los alamitos, us}, } @inproceedings{theeramunkong02, author = {
{88}, number = {5}, doi = {http://dx.doi.org/10.1016/j.ipl.2003.09.002},  abstract = {
attics is an extensible  text classification system we have implemented in c++.
a combined use of the projections on and the distances to the dlsi spaces introduced from the differential document vectors improves the adaptability of the lsi (latent semantic indexing) method by capturing unique characteristics of documents.
{http://www.cs.utah.edu/~riloff/psfiles/ijcai-book-chapter.ps}, abstract = {
no systematic study, however,  has been done on their relative merits.
"2006", pages =  "217--226", } @inproceedings{kumar:2006:hts, author =
thus, feature reduction is often performed  in order to increase the efficiency and effectiveness of the classification.
{markus junker and michaell sintek and matthias rinck}, title = {
for this purpose, we describe a new probabilistic model #rst, which is then combined with logistic regression, thus yielding a generalization of the original model.
furthermore, the  supervised lower dimensional space greatly improves the retrieval performance  when compared to lsi.}, } @inproceedings{kawatani02, author = {
thus, the classifier has a small  model size and is very fast.
this article studies aggressive word removal in text categorization  to reduce the noise in free texts and to enhance the computational efficiency  of categorization.
text categorization is an important research area and has been receiving much attention due to the growth of the on-line information and of internet.
the crawling process is modeled as a parallel  best-first search over a graph defined by the web.
an extended version appears as~\cite{apte94}}, url =  {http://www.acm.org/pubs/articles/proceedings/ir/188490/p23-apte/p23-apte.pdf},  abstract = {
}  @inproceedings{sahlgren:2004:ubc, author =
in the second stage, the enriched queries  are classified through the base classifiers trained in phase i. based on the  classification results obtained by the base classifiers, two ensemble  classifiers based on two different strategies are proposed.
first, undirected models do not impose the acyclicity constraint that hinders representation of many important relational dependencies in directed models.
} @article{bianchi:2006:iah,  author =
moreover, our feature selection method sometimes produces more improvements of conventional machine learning algorithms over support vector machines which are known to give the best classification accuracy.} } @article{pant:2005:lcc, author = {gautam pant and padmini srinivasan}, title = {learning to crawl:
we discovered that the knowledge about relevance among queries and documents can be used to obtain empirical connections between query terms and the canonical concepts which are used for indexing the content of documents.
given a set of categories, with or without a preexisting hierarchy  among them, we consider the problem of assigning documents to one or more of  these categories from the point of view of a hierarchy with more or less depth.
experimental results show that the choice  of thresholding strategy can significantly influence the performance of knn,  and that the "optimal" strategy may vary by application.
text categorization using compression models}, booktitle = {proceedings of dcc-00, ieee data compression conference}, editor =
a maximal figure-of-merit learning approach to text  categorization}, booktitle = {proceedings of sigir-03, 26th acm international  conference on research and development in information retrieval}, editor =
experimental comparison given on real-world data  collected from web users shows that characteristics of the problem domain and  machine learning algorithm should be considered when feature scoring measure is  selected.
the document organization and classification  performance of our ica-based hierarchical classifier are evaluated in several  encouraging experiments conducted on a journalistic-style text corpus for  speech synthesis in catalan.}, } @inproceedings{shanahan03, author
taming wild phrases},
{103--105}, url = {http://www.wkap.nl/article.pdf?391241}, } @book{joachims02a,  author = {thorsten joachims}, title = {learning to classify text using support  vector machines}, publisher =
nick cercone and tsau y.  lin and xindong wu}, year = {2001}, address = {san jose, ca}, pages =  {521--528}, url =  {http://www.cais.ntu.edu.sg:8000/~sunaixin/paper/sun_icdm01.pdf}, abstract =  {hierarchical classification refers to assigning of one or more suitable  categories from a hierarchical category space to a document.
susan t. dumais and hao chen}, title  = {hierarchical classification of web content}, booktitle = {proceedings of  sigir-00, 23rd acm international conference on research and development in  information retrieval}, editor = {nicholas j. belkin and peter ingwersen and  mun-kew leong}, publisher = {acm press, new york, us}, address = {athens, gr},  year = {2000}, pages = {256--263}, url =  {http://research.microsoft.com/~sdumais/sigir00.pdf}, abstract = {
however, as documents accumulate, such categories may not capture a document's characteristics correctly.
"we consider the relationship between training set size and the  parameter k for the k-nearest neighbors (knn) classifier.
in proceedings of icml-96, 13th international conference on  machine learning], which is one of greedy feature selection methods, and  conventional information gain which is commonly used in feature selection for  text categorization.
"proceedings of the 6th  acm/ieee-cs joint conference on digital libraries", year =  "2006", pages =
we  discuss the issues of incorporating prior knowledge using this rather general  formulation.
giuseppe attardi and antonio  gull{\'{\i}} and fabrizio sebastiani}, title = {automatic web page  categorization by link and context analysis}, booktitle = {proceedings of  thai-99, 1st european symposium on telematics, hypermedia and artificial  intelligence}, editor = {chris hutchison and gaetano lanzarone}, year = {1999},  address = {varese, it}, pages = {105--119}, url =  {http://www.math.unipd.it/~fabseb60/publications/thai99.pdf}, abstract =  {assistance in retrieving documents on the world wide web is provided either by  search engines, through keyword-based queries, or by catalogues, which organize  documents into hierarchical collections.
our enhancements include using lidstone's law of succession instead of laplace's law, under-weighting long documents, and over-weighting author and subject.
profile allows  the user to update on-line the profile and to check the discrepancy between the  assessment and the prediction of relevance of the system.
we provide an intuitive modification to the em iterations by re-estimating the empirical distribution in order to reinforce feature values in unlabeled data and to reduce the influence of noisily labeled examples.
in addition, centralized approaches are more vulnerable to attacks or system failures and less robust in dealing with them.
[no abstract]}, } @inproceedings{lewis92, author = {lewis, david d.}, title = {
{tampere, fi}, year = {2002}, pages = {145--150}, url = {http://doi.acm.org/10.1145/564376.564403}, abstract = {subject or prepositional content has been the focus of most classification research.
our approach acts as a feature selection technique that is an alternative to applying the techniques from machine learning and numerical taxonomy.}, } @inproceedings{koster03, author = {cornelis h. koster and mark seutter}, title = {
we explore four different ways of combining the individual predictions and four different techniques for identifying relevant text portions.
"2006", pages =  "477--484", abstract =
"specificity helps text classification", booktitle =
only papers that  have % % been the object of formal publication (i.e. conferences and % %  journals) are to be included in the bibliography, so as to avoid % % its  explosion and the inclusion of material bound to obsolescence.
we used precision and recall to measure the effectiveness of our  classifier.
we propose a novel  probabilistic method, based on latent variable models, for unsupervised  topographic visualisation of dynamically evolving, coherent textual  information.
we discuss an approach to the automatic expansion of domain-specific lexicons by means of \emph{term categorization}, a novel task employing techniques from information retrieval (ir) and machine learning (ml).
because the sense  distinctions made are coarse, the disambiguation can be accomplished without  the expense of knowledge bases or inference mechanisms.
"2006", pages = "505--509" } @inproceedings{bouma:2006:sht, author =
this metric can be  incorporated into the definition of radial basis kernels of support vector  machines or directly used in a k-nearest neighbors algorithm.
while document categorization is quite a mature, the issue of utilizing hypertext structure and hyperlinks has been relatively unexplored.
{nuria bel and cornelis h. koster and marta villegas}, title = {cross-lingual  text categorization}, booktitle = {proceedings of ecdl-03, 7th european  conference on research and advanced technology for digital libraries}, editor =
this  characterization hence must consider the data at hand, both the kernels and  also the task, that is the information given by the labels.
moreover, the system can then save such document organizations in user profiles which can then be used to help classify future query results by the same user.
we report on experiences with the reuters newswire benchmark, the us patent database, and web document samples from {{\sc yahoo!}}\.}, } @inproceedings{wei01, author = {chih-ping wei and yuan-xin dong}, title = {a mining-based category evolution approach to managing online document categories}, booktitle = {proceedings of hicss-01, 34th annual hawaii international conference on system sciences}, publisher =
however, the phrasal pre-processing and  pattern matching methods that seem to work for categorization have the  disadvantage of requiring a fair amount of knowledge-encoding by human beings.
these connections do not depend on whether there are shared terms  among the queries and documents; therefore, they are especially effective for a  mapping from queries to the documents where the concepts are relevant but the  terms used by article authors happen to be different from the terms of database  users.
text  categorization algorithms usually represent documents as bags of words and  consequently have to deal with huge numbers of features.
the concept learning model  emphasizes the role manual and automated feature selection and classifier  formation in text classification.
when training classifiers on large collections of documents, both the time and memory requirements connected with processing of these vectors may be prohibitive.
we use two well-known techniques, partitioning clustering, means and a hierarchy.
{acm press, new york, us}, pages = {793--797}, url = {http://www.math.unipd.it/~fabseb60/publications/sac03c.pdf}, abstract = {
despite being chosen by this heterogeneous approach, the uncertainty samples  yielded classifiers with lower error rates than random samples ten times  larger.}, } @inproceedings{lewis95, author = {lewis, david d.}, title =
text classification using lattice machine}, booktitle = {proceedings of ismis-99, 11th international symposium on methodologies for intelligent systems}, editor = {
filters  of this type have so far been based mostly on manually constructed keyword  patterns.
{thorsten joachims and fabrizio sebastiani}, title = {guest editors'  introduction to the special issue on automated text categorization}, journal =
"li, jingyang and sun, maosong and zhang, xian", title =
the category that contains the largest categorical points is selected as the category of a document.
efficient text classification for huge document  collections}, booktitle = {proceedings of icdm-02, 2nd ieee international  conference on data mining}, editor = {}, publisher =
a maximal figure-of-merit learning approach to text categorization}, booktitle = {proceedings of sigir-03, 26th acm international conference on research and development in information retrieval}, editor =
however, this fraction  corresponds to only one-fourth of the mistakes made by the human specialists.},  } @inproceedings{riloff92, author = {ellen riloff and wendy lehnert}, title =
we then compare adaboost and rocchio over three large text filtering tasks.
the agreement between the automated grader and the final manual grade was as good as the agreement between human graders.}, } @inproceedings{larkey99, author =
the classification model is a variant of the maximum margin  markov network framework, where the classification hierarchy is represented as  a markov tree equipped with an exponential family defined on the edges.
we present here a transductive boosting method for text categorization in  order to make use of the large amount of unlabeled data efficiently.
performance is usually, though not always, less good here, but the term weighting functions implicit in the resulting ranking functions are intriguing, and the approach could easily be adapted to mixtures of textual and nontextual data.}, } @inproceedings{jacobs92, author = {paul s. jacobs}, title = {
this word-cluster representation is computed using the recently introduced information bottleneck method, which generates a compact and efficient representation of documents.
"exploiting category information and document information to improve term weighting for text categorization", booktitle = "proceedings of the eighth international conference on intelligent text processing and computational linguistics", year =
generalization  is an important ability specific to inductive learning that will predict unseen  data with high accuracy based on learned concepts from training examples.
alexander f. gelbukh}, publisher
adaptive information filtering is concerned with filtering  information streams in changing environments.
{junichi kazama and junichi tsujii}, title = {maximum entropy models with inequality constraints: a case study on text categorization}, journal =
the experimental results suggest that the bigrams can  substantially raise the quality of feature sets, showing increases in the  break-even points and f1 measures.
these improvements are reported for two different classifiers, support vector machines (svm) and k-nearest neighbours (knn), and two different text corpora.
traditionally, these categories as well as the correlations among  them are determined bp human experts.
"li, ying and zheng, zijian and dai, honghua (kathy)", title =
this hypothesis is systematically tested with three different measures of word relevance, on two different corpus (one of them considered in three different splits), and with both local and global vocabularies.
word sequences as features in text-learning}, booktitle = {proceedings of erk-98, the seventh electrotechnical and computer science conference}, year = {1998}, address = {ljubljana, sl}, pages = {145--148}, } @article{mladenic99, author = {
we present the results of experiments with a preliminary implementation of the technique.}, } @inproceedings{attardi99, author = {giuseppe attardi and antonio gull{\'{\i}} and fabrizio sebastiani}, title = {automatic web page categorization by link and context analysis},
neural networks were developed and examined for this topic since they support robustness and learning in noisy unrestricted real-world texts.
experimental results show that it achieves nearly perfect performance on a set of hard cases.}, } @inproceedings{chen00, author = {hao chen and susan t. dumais}, title = {bringing order to the web: automatically categorizing search results}, booktitle = {proceedings of chi-00, acm international conference on human factors in computing systems}, publisher = {acm press, new york, us}, editor = {}, year = {2000}, address = {den haag, nl}, pages = {145--152}, url = {http://www.acm.org/pubs/articles/proceedings/chi/332040/p145-chen/p145-chen.pdf}, abstract = {
using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines.
a boosting approach to topic spotting on  subdialogues}, booktitle = {proceedings of icml-00, 17th international  conference on machine learning}, editor
in the test of the expert network method on cacm documents, for  example, an 87\% removal of unique words reduced the vocabulary of documents  from 8,002 distinct words to 1,045 words, which resulted in a 63\% time saving  and a 74\% memory saving in the computation of category ranking, with a 10\%  precision improvement, on average, over not using word removal.
in many important text classification problems, acquiring class labels for training documents is costly, while gathering large quantities of unlabeled data is cheap.
"mark sandler", title =
svms achieve substantial improvements over the currently best performing methods and behave robustly over a variety of different learning tasks.
advances in web intelligence and data mining}, year = {2006}, editor = {
when user's interests  change, pva, in not only the contents, but also in the structure of user  profile, is modified to adapt to the changes.
the task, in essence,  is to have a computing machine read a document and on the basis of the  occurrence of selected clue words decide to which of many subject categories  the document in question belongs.
"111--116", volume = 7, number = 2, year  = 2005, url =  "http://www.acm.org/sigs/sigkdd/explorations/issues/7-2-2005-12/kddcup2005report_ferrety.pdf",  abstract =
the classification model is a variant of the maximum margin markov network framework, where the classification hierarchy is represented as a markov tree equipped with an exponential family defined on the edges.
{philip r. cohen and wolfgang wahlster}, year = {1997}, address
instead of using the text on a page for deriving features that can  be used for training a classifier, we suggest to use portions of texts from all  pages that point to the target page.
"text categorization is an important research area in many information  retrieval (ir) applications.
"text  classification improved through multigram models", booktitle =  "cikm", year =
pattern matching produces fairly accurate and fast categorisation over a large number of classes, while information extraction provides fine-grained classification for a reduced number of classes.
comparative analysis shows that the performances achieved are relatively close to the best tc models (e.g. support vector machines).}, } @inproceedings{moschitti04, author = {alessandro moschitti and roberto basili}, title = {complex linguistic features for text classification: a comprehensive study}, booktitle = {proceedings of ecir-04, 26th european conference on information retrieval research}, editor = {sharon mcdonald and john tait}, year = {2004}, address = {sunderland, uk}, publisher = {springer verlag, heidelberg, de}, note = {
empirical results are given on a number of dataset, showing that our feature selection method is more effective than koller and sahamis method [koller, d., & sahami, m. (1996).
automatic text categorization using the importance of sentences}, booktitle =
empirical evaluation  indicates that the error rate (as obtained by running a plain naive bayes  classifier on isolated page) can be roughly reduced by half if contextual  information is incorporated.}, } @article{frasconi02, author = {paolo frasconi  and giovanni soda and alessandro vullo}, title = {
{morgan kaufmann publishers, san francisco, us}, url = {}, abstract = {kernel methods like support vector machines have successfully been used for text categorization.
"it is often useful to classify email according to the intent of the sender (e.g., 'propose a meeting', 'deliver information').
{australian  computer society}, address = {
seattle, us}, pages = {246--254}, url = {http://www.research.att.com/~lewis/papers/lewis95b.ps}, abstract = {
however, there has been little firm evidence to confirm the  utility of link information.
however, in addition to relying on labeled training data, we improve  classification accuracy by also using unlabeled data and other forms of  available ``background" text in the classification process.
"proceedings of the joint conference on empirical methods in natural language processing and computational natural language learning", year =
this filtering process is a word sense disambiguation task.
in this paper we  present a comprehensive comparison of the performance of a number of text  categorization methods in two different data sets.
combining  link-based and content-based methods for web document classification},  booktitle = {proceedings of cikm-03, 12th acm international conference on  information and knowledge management}, publisher = {acm press, new york, us},  editor = {}, year = {2003}, address = {new orleans, us}, pages = {394--401},  url = {http://doi.acm.org/10.1145/956863.956938}, abstract = {
accordingly,' our approach is based on problem solving tasks.
we propose several algorithms using the vector space model to  classify the news articles posted on the netnews according to the newsgroup  categories.
its severe assumptions make such efficiency possible but also  adversely affect the quality of its results.
the svms' results common to both filtering are that 1) the optimal number of features differed completely across categories, and 2) the average performance for all categories was best when all of the words were used.
there is a wide variety of tasks for which keyphrases are useful, as we discuss in this paper.
hcl is designed  to describe a hierarchical classification method including the definition of a  category tree and training of classifiers associated with the categories.
we also describe and evaluate a hybrid term selection technique, first applying uc to eliminate noisy terms and then using another criterium to select the best terms.}, } @inproceedings{ragas98, author =
one strength of reinforcement learning is that it provides a formalism for  measuring the utility of actions that give benefit only in the future.
we've applied these techniques to online banking applications to  enhance automated e-mail routing.}, } @article{wermter00, author = {stefan  wermter}, title = {neural network agents for learning semantic text  classification}, journal = {information retrieval}, number = {2}, volume = {3},  pages = {87--103}, year = {2000}, url =  {http://www.his.sunderland.ac.uk/ps/ir4.pdf}, abstract = {
"proceedings of the 6th acm/ieee-cs joint conference on digital libraries", year = "2006", pages =
%% @inproceedings{adam02, author =
we identify several new issues to be addressed when the assumption is removed, and formulate the web unit mining problem.
the use of classification algorithms to guide topical crawlers has  been sporadically suggested in the literature.
to address this  limitation, we propose the second meta-model approach, called meta-learning  using document feature characteristics (mudof), which employs a meta-learning  phase using document feature characteristics.
experimental results show that our  neuro-genetic algorithm is able to perform as well as, if not better than, the  best results of neural networks to date, while using fewer input features.}, }  @inproceedings{zaiane02, author = {osmar r. za{\"{\i}}ane and maria-luiza  antonie}, title = {
hypertext poses new research challenges for text classification.
learning  question classifiers}, booktitle = {proceedings of coling-02, 19th  international conference on computational linguistics}, editor = {}, publisher  = {}, address = {taipei, tw}, url =  {http://l2r.cs.uiuc.edu/~danr/papers/qc-coling02.pdf}, year = {2002}, abstract  = {
{christian shin and david doermann and azriel rosenfeld}, title =
words, contexts and senses are represented in word space, a high-dimensional real-valued space in which closeness corresponds to semantic similarity.
simulation results on both toy-data settings and an actual application on  internet chat line discussion analysis is presented by way of demonstration.},  } @inproceedings{kao03, author = {anne kao and lesley quach and steve poteet  and steve woods}, title = {
however, in  many domains labels are highly interdependent.
{347--368}, abstract = {topic detection and tracking (tdt) is a research  initiative that aims at techniques to organize news documents in terms of news  events.
the results demonstrated that representative sampling offers excellent learning performance with fewer labeled documents and thus can reduce human efforts in text classification tasks.}, } @inproceedings{xue03, author = {dejun xue and maosong sun}, title = {chinese text categorization based on the binary weighting model with non-binary smoothing}, booktitle = {proceedings of ecir-03, 25th
in this paper we show how performance on new event  detection (ned) can be improved by the use of text classification techniques as  well as by using named entities in a new way.
the architecture, modules, and practical results are described.}, } @article{manevitz01, author = {larry m. manevitz and malik yousef}, title = {one-class {svms} for document classification}, journal = {journal of machine learning research}, volume = {2}, month = {december}, pages = {139--154}, year = {2001}, url = {http://www.ai.mit.edu/projects/jmlr/papers/volume2/manevitz01a/manevitz01a.pdf}, abstract = {
second order features for maximising text classification performance}, booktitle = {proceedings of ecml-01, 12th european conference on machine learning}, editor = {
this risk is  found to be most severe when little data is available.
in the context of text modeling, the topic probabilities provide an explicit representation of a document.
"proceedings of the  joint conference on empirical methods in natural language processing and  computational natural language learning", year =
our results show that the identification of hypertext regularities in the data and the selection of appropriate representations for hypertext in particular domains are crucial, but seldom obvious, in real-world problems.
{gianni  amati and fabio crestani and flavio ubaldini and stefano de nardis}, title =
we present the results of some experiments done on real data: two  different classifications of our research projects.}, } @article{borko63,  author = {harold borko and myrna bernick}, title = {automatic document  classification}, journal = {journal of the association for computing  machinery}, year = {1963}, volume = {10}, number = {2}, pages = {151--161}, url  =
we show how to do this for binary text classification systems, emphasizing that different goals for the system lead to different optimal behaviors.
similarly, we use a training set of queries  and their related documents to obtain empirical associations between query  words and indexing terms of documents, and use these associations to predict  the related documents of arbitrary queries.
our empirical results show that the proposed approach, text categorization using feature projections (tcfp), outperforms k-nn, rocchio, and naive bayes.
in this paper, we describe an active learning method that uses a committee of learners to reduce the number of training examples required for learning.
this paper is a comparative study of feature selection methods in statistical learning of text categorization.
{connectionist, statistical, and symbolic approaches to learning for natural  language processing}, editor = {stefan wermter and ellen riloff and gabriele  scheler}, pages = {343--354}, year = {1996}, publisher = {springer verlag,  heidelberg, de}, note = {
{193--216}, year = {1999}, url =
it uses the internet as source of knowledge and extends it to  categorize very short (less than 5 words) documents with reasonable accuracy.
using a public corpus, we show that stacking can improve the efficiency of automatically induced anti-spam filters, and that such filters can be used in real-life applications.}, } @article{sakkis03, author = {georgios sakkis and ion androutsopoulos and georgios paliouras and vangelis karkaletsis and constantine d. spyropoulos and panagiotis stamatopoulos}, title = {a memory-based approach to anti-spam filtering for mailing lists}, journal = {information retrieval}, publisher =
cases are categorized into 40 broad, high-level categories.
multiple knowledge bases can be formulated precisely and in a unified way within the framework of rs.
wsd is the identification of the sense of words in context.
we compare their effectiveness in classifying ocr texts and the corresponding correct ascii texts in two domains: business letters and abstracts of technical reports.
data eng", year = "2005", number =
in this paper, we present a boosting-based  learning method for text filtering that uses naive bayes classifiers as a weak  learner.
this article investigates the applicability of rs theory to the if/ir application domain and compares this applicability with respect to various existing tc techniques.
in this paper, we present a formal description of the feature  quantity, as well as some illustrative examples of applying such a quantity to  different types of information retrieval tasks: representative term selection  and text categorization.}, } @inproceedings{aizawa01, author = {akiko aizawa},  title = {linguistic techniques to improve the performance of automatic text  categorization}, booktitle = {proceedings of nlprs-01, 6th natural language  processing pacific rim symposium}, editor = {}, publisher = {}, address =
experiments in text  classification}, booktitle = {proceedings of ecir-03, 25th european conference  on information retrieval}, publisher = {springer verlag}, editor = {fabrizio  sebastiani}, address = {pisa, it}, year = {2003}, pages = {41--56}, url =  {http://link.springer.de/link/service/series/0558/papers/2633/26330041.pdf},  abstract = {link analysis methods have become popular for information access  tasks, especially information retrieval, where the link information in a  document collection is used to complement the traditionally used content  information.
{proceedings of icml-95, 12th international conference on machine  learning}, editor = {armand prieditis and stuart j. russell}, address = {lake  tahoe, us}, year = {1995}, pages = {124--132}, publisher =
we describe a method for classifying pages of sequential ocr text documents into one of several assigned categories and suggest that taking into account contextual information provided by the whole page sequence can significantly improve classification accuracy.
{114--121}, url =  {http://www.acm.org/pubs/articles/proceedings/cikm/319950/p114-hsu/p114-hsu.pdf},  abstract = {
the task of the kddcup 2005 competition was to classify 800,000 internet user search queries into 67 predefined categories.
the effects of these considerations are examined in the experiments using reuters-21578 and ntcir-j1 standard test collections.}, } @inproceedings{alias02, author = {francesc al{\'i}as and ignasi iriondo and pere barnola}, title = {multi-domain text classification for unit selection text-to-speech synthesis}, booktitle = {proceedings of icphs-03, 15th international congress on phonetic sciences}, address = {barcelona, es}, editor = {}, publisher = {}, year = {2003}, pages = {}, url = {
george forman}, title = {
published in the  ``lecture notes in computer science'' series, number 3202}, url = {}, abstract  = {}, } @incollection{forsyth99, author = {richard s. forsyth}, title =
in practice, the  assumption is too restrictive since a web page itself may not always correspond  to a concept instance of some semantic concept (or category) given to the  classification task.
these results reach most of the state-of-the-art techniques of machine learning applied to text categorization, demonstrating that this new weighting scheme does perform well on this particular task.}, } @inproceedings{dinunzio04, author = {giorgio m. {di nunzio}}, title = {
as the results, the micro averaged f1 measure for reuters-21578 improved from 83.69 to 87.27\%.}, } @article{kehagias03, title = {a comparison of word- and sense-based text categorization using several classification algorithms}, author = {athanasios kehagias and vassilios petridis and vassilis g. kaburlasos and pavlina fragkou}, journal = {journal of intelligent information systems}, year = {2003}, volume = {21}, number = {3}, pages =
comparing the accuracy of our method with other techniques, we observe significant dependency of the results on the data set.
keywords alone cannot always distinguish  the relevant from the irrelevant texts and some relevant texts do not contain  any reliable keywords at all.
we discuss  the role of structural information for classification and describe experiments  on a small collection of class labeled structured documents.
text categorization (tc)  is the automated assignment of text documents to predefined categories based on  document contents.
in this paper, we systematically compare combination strategies in the context of document filtering, using queries from the tipster reference corpus.
the power of word clusters for text classification}, booktitle = {proceedings of ecir-01, 23rd
{wen-lin hsu and sheau-dong lang}, title = {classification algorithms for  netnews articles}, booktitle =
consistent with previous findings, we find that feature selection based on the labeled training set has little effect.
"montejo-raez, arturo and  urena-lopez, l. alfonso and steinberger, ralf", title =
{180--187}, url =  {http://www.acm.org/pubs/articles/proceedings/cikm/319950/p180-labrou/p180-labrou.pdf},  abstract = {
this model gives new  representations of both news articles and news events.
we've applied these techniques to online banking applications to enhance automated e-mail routing.}, } @article{wermter00, author = {
with large number of categories organized as a tree, hierarchical text classification helps users to find information more quickly and accurately.
vs.\  words for text categorization}, journal = {journal of machine learning  research}, volume = {3}, pages = {1183--1208}, year = {2003}, url =  {http://www.jmlr.org/papers/volume3/bekkerman03a/bekkerman03a.pdf}, abstract =  {we study an approach to text categorization that combines distributional  clustering of words and a support vector machine (svm) classifier.
preliminary experiments with 1998 darpa bsm audit data show  that the knn classifier can effectively detect intrusive attacks and achieve a  low false positive rate.}, } @article{liddy94, author = {elizabeth d. liddy and  woojin paik and edmund s. yu}, title = {
the texts represent short web-page descriptions from the dmoz open directory web-page ontology.
to cope with this problem, we propose a hybrid technique using latent  semantic indexing (lsi) and rough sets theory (rs) to alleviate this situation.
in contrast to conventional measures for pruning, the l-measure  takes into account properties of the search space.}, } @inproceedings{junker97,  author = {markus junker and andreas abecker}, title = {exploiting thesaurus  knowledge in rule induction for text classification},
well in text categorization?}, booktitle = {proceedings of ijcai-03, 18th international joint conference on artificial intelligence}, editor =
{78--87}, url = {}, abstract = {}, } @inproceedings{calado03, author = {p{\'{a}}vel calado and marco cristo and edleno silva de moura and nivio ziviani and berthier a. ribeiro-neto and marcos andr{\'{e}} gon{\c{c}}alves}, title = {
this can be thought of as automatic feature selection, which is expected to improve generalization performance further.
we show that a good hierarchical  machine learning-based categoriser can be developed using small numbers of  features from pre-categorised training documents.
pos tagging and recognition of proper  nouns received a specific experimental attention and provided significant  effects on measured accuracy.}, } @inproceedings{basili01, author = {roberto  basili and alessandro moschitti and maria t. pazienza}, title = {nlp-driven ir:  evaluating performances over a text classification task}, booktitle =
the documents are retrieved by considering both the predicted relevance and its value as a training observation.
we also observe that the deviation formula and discrimination formula using document frequency ratios also work as expected.
}  @inproceedings{li:2005:pmr, author =
however, once a misclassification occurs at a high level class, it may result in a class that is far apart from the correct one.
to this end, we propose several clustering algorithms,  and report results of various evaluations on standard benchmark corpora such as  the newsgroups corpus.}, } @inproceedings{wang00, author = {
y. ho}, title = {using a bayesian network induction approach for text  categorization}, booktitle = {proceedings of ijcai-97, 15th international joint  conference on artificial intelligence}, editor =
lam, wai  and ruiz, miguel e. and srinivasan, padmini}, title = {automatic text  categorization and its applications to text retrieval}, journal = {ieee  transactions on knowledge and data engineering}, year = {1999}, number = {6},  volume = {11}, pages = {865--879}, url =  {http://www.cs.uiowa.edu/~mruiz/papers/ieee-tkde.ps}, abstract = {
a standard choice of kernel function has been the inner product between the vector-space representation of two documents, in analogy with classical information retrieval (ir) approaches.
} @article{kardcovacs:2005:ferrety, author =
"constructing informative prior distributions from domain knowledge in text classification", booktitle = "proceedings of the 29th annual international acm sigir conference on research and development in information retrieval", year = "2006", pages =
generally, filtering systems calculate the similarity between the profile and each incoming document, and retrieve documents with similarity higher than a threshold.
our approach maintains a pool of selective terms with potentially high predictive power.
using four subsets of the reuters text categorization test collection and a full-text test collection of which documents are varying from tens of kilobytes to hundreds, we evaluate the proposed model, especially the effectiveness of various passage types and the importance of passage location in category merging.
% inproceedings % = an  article in a conference proceedings % required: author, title, booktitle, year  % optional: editor, volume or number, series, pages, address, month, %  organization, publisher, note % % manual % =
the  method harnesses reliability indicators-variables that provide signals about  the performance of classifiers in different situations.
we show that our algorithm minimizes the "within-cluster jensen-shannon divergence" while simultaneously maximizing the "between-cluster jensen-shannon divergence".
we therefore propose the category-similarity measures and  distance-based measures to consider the degree of misclassification in  measuring the classification performance.
since there is no need to learn individual program profiles separately, the calculation involved is largely reduced.
document feature characteristics, derived from the training document set, capture some inherent category-specific properties of a particular category.
it is called gaussian weighting and it is a supervised learning algorithm that, during the training phase, estimates two very simple and easily computable statistics which are: the presence \emphp, how much a term \emph{t} is present in a category \emph{c}; the expressiveness \emphe, how much \emph{t} is present outside \emph{c} in the rest of the domain.
"2006" } @inproceedings{qi:2006:kwp, author =
"biographies, bollywood, boom-boxes and blenders: domain adaptation for sentiment classification", booktitle = "proceedings of the 45th annual meeting of the association for computational linguistics", year =
furthermore, our analysis reveals the precise dependence of the  rate of convergence on the eigenstructure of the data each node observes.
however, compression-based classification methods have drawbacks (such as slow running time), and not all such methods are equally effective.
parameter tuning through cross-validation becomes very difficult when the validation set contains no or only a few examples of the classes in the evaluation set.
"2006" } @inproceedings{gliozzo:2006:ecc, author =
a pitfall and solution in multi-class feature selection for text classification}, booktitle = {proceedings of icml-04, 21st international conference on machine learning}, editor =
in this work we present a novel implementation of this method for supervised text classification.
pisa, it}, year = {2003}, pages = {320--334},  url = {http://www.math.unipd.it/~fabseb60/publications/ecir03.pdf}, abstract =  {we focus on two recently proposed algorithms in the family of  ``boosting''-based learners for automated text classification,  \textsc{adaboost.
"proceedings of eacl-06, 11th conference of the european chapter of the association for computational linguistics", year =
learning from  both, labeled and unlabeled documents, in a semi-supervised framework is a  promising approach to reduce the need for labeled training documents.
"2006", pages = "257--266", }  @inproceedings{zhang:2006:hts, author =
we also demonstrate the  impact of the time-varying nature of category definitions.}, }  @inproceedings{lewis94a, author = {lewis, david d. and gale, william a.}, title  = {a sequential algorithm for training text classifiers}, booktitle =  {proceedings of sigir-94, 17th acm international conference on research and  development in information retrieval}, editor = {w. bruce croft and van
a cluster based search with a  probabilistic clustering algorithm is proposed and evaluated on two data sets.
in this paper, we study such a problem of performing text  classification without labeled negative data tc-won).
"2004", pages =  "65--72", abstract =
our solution obtained creativity and precision runner-up awards at the competition.
"an knn  model-based approach and its application in text categorization",  booktitle = "computational linguistics and intelligent text processing  (lecture notes in computer science, vol. 2945)", year =
the results of this experiment were  subsequently used to create a new large medical test collection, which was used  in experiments with the smart retrieval system to obtain baseline performance  data as well as compare smart with the other searchers.}, }  @inproceedings{hoashi00, author = {keiichiro hoashi and kazunori matsumoto and  naomi inoue and kazuo hashimoto}, title = {document filtering methods using  non-relevant information profile}, booktitle = {proceedings of sigir-00, 23rd  acm international conference on research and development in information  retrieval}, editor = {nicholas j. belkin and peter ingwersen and mun-kew  leong}, publisher = {acm press, new york, us}, address = {athens, gr}, year =
sonia uses a multi-tier approach to extracting relevant terms from documents as well as statistical clustering methods to determine potential topics within a document collection.
our approach is to develop a trainable information extraction  system that takes two inputs: an ontology defining the classes and relations of  interest, and a set of training data consisting of labeled regions of hypertext  representing instances of these classes and relations.
the document collection  was trained by a self-organizing map to form two feature maps.
the paper describes feature subset selection used in learning on text data (text learning) and gives a brief overview of feature subset selection commonly used in machine learning.
hyunsoo kim and peg howland and haesun park}, title = {dimension reduction in text classification with support vector machines}, journal = {journal of machine learning research}, year = {2005}, volume = {6}, pages =
the classifiers and regression equations were then applied to a new set of essays.
current natural language text processing (nlp) methods help to overcome these problems.
w. bruce croft and alistair moffat and van rijsbergen, cornelis j. and ross wilkinson and justin zobel}, publisher = {acm press, new york, us}, year = {1998}, address = {melbourne, au}, pages = {369--370}, url = {http://www.acm.org/pubs/articles/proceedings/ir/290941/p369-ragas/p369-ragas.pdf}, abstract =
the previous works in this area have used a large number of labeled training documents for supervised learning.
we also present a new interactive clustering algorithm, c-evolve, for topic discovery.
national institute of standards and  technology, gaithersburg, us}, editor
we provide experimental results demonstrating that the approach can  significantly improve performance, and that it does not impair it.} }  @incollection{cristianini01a, author = {huma lodhi and john shawe-taylor and  nello cristianini and christopher j. watkins}, title = {discrete kernels for  text categorisation}, booktitle = {
in our system, the user navigates through the query response  not as a flat unstructured list, but embedded in the familiar taxonomy, and  annotated with document signatures computed dynamically with respect to where  the user is located at any time.
ah-hwee tan}, title = {predictive self-organizing networks for text categorization}, booktitle = {proceedings of pakdd-01, 5th pacific-asia conferenece on knowledge discovery and data mining}, editor = {david cheung and qing li and graham williams}, year = {2001}, publisher =
{morgan kaufmann publishers, san francisco, us}, url = {http://doi.acm.org/10.1145/1015330.1015356}, abstract = {information gain is a well-known and empirically proven method for high-dimensional feature selection.
using a database of 20,569 documents, we verify that the algorithm attains levels of average precision in the 70-80\% range for category coding and in the 60-70\% range for subcategory coding.
the algorithm uses the information gain metric,  combined with various frequency thresholds.
in this paper the suitability of different document representations for automatic document classification is compared, investigating a whole range of representations between bag-of-words and bag-of-phrases.
athena satisfies these requirements through linear-time classification and clustering engines which are applied interactively to speed the development of accurate models.
in this paper, we  provide a solid basis for the application of ilp methods to these learning  problems.
we carry out experiments over two different corpora and find that the proposed measures perform better than the existing ones."
{a comparison of two learning algorithms for text categorization}, booktitle =  {proceedings of sdair-94, 3rd annual symposium on document analysis and  information retrieval}, publisher = {}, editor = {}, year = {1994}, address =
formally, it can be viewed as a mapping from  the document space into a set of predefined class labels (aka subjects or  categories); f: d <- {c1, c2...cn} where f is the mapping function, d is the  document space and {c1, c2...cn} is the set of class labels.
a  classifier for semi-structured documents}, booktitle = {proceedings of kdd-00,  6th acm international conference on knowledge discovery and data mining},  editor = {}, publisher = {acm press, new york, us}, address = {boston, us},  year = {2000}, pages = {340--344}, url =  {http://doi.acm.org/10.1145/347090.347164}, abstract = {
further, we propose to use a special kernel function called the  tree kernel to enable the svm to take advantage of the syntactic structures of  questions.
the derived models,  inequality me models, in effect have regularized estimation with l1 norm  penalties of bounded parameters.
boosting applied to word sense disambiguation}, booktitle = {proceedings of ecml-00, 11th european conference on machine learning}, editor =
suppose all sentence vectors that compose each document are projected onto projection axes.
the second is a set of training data  consisting of labeled regions of hypertext that represent instances of these  classes and relations.
we focus on domains with many features that also have a highly unbalanced class distribution and asymmetric misclassification costs given only implicitly in the problem.
editor = {}, year = {1998}, address = {kitakyushu, jp}, pages = {347--351}, url  = {http://www.comp.hkbu.edu.hk/7ejamesk/papers/iconip98.ps.gz}, abstract = {
we rely on the statistical properties of the  case base to determine whether similar cases are highly correlated with  relevance for the domain.
our working  hypothesis is that it is often easier to classify a hypertext page using  information provided on pages that point to it instead of using information  that is provided on the page itself.
the analysis predicts learning  curves with a very high precision and thus contributes to a better  understanding of why and when over-fitting occurs.
also, different derivations from the normal recall and precision performance indicators are discussed and compared.
rcut is most natural  for online response but is too coarse-grained for global or local optimization.
"2005", month = "august", address =
"mexico city, mexico" } @inproceedings{cleuziou:2007:oll, author =
"2004" } @inproceedings{wenliang:2004:alf, author =
using  a collection factor, based on 87 per cent human consistency from other courses,  the computer appears then to index with 90 per cent accuracy in this case.
"recently, interest is growing in  non-topical text classification tasks such as genre classification, sentiment  analysis, and authorship profiling.
hyperlinks pose new problems not addressed in the extensive text classification literature.
{yiming yang and john w. wilbur}, title = {
"koppel moshe  and schler, jonathan and argamon, shlomo and messeri, eran", title =  "authorship attribution with thousands of candidate authors",  booktitle =
nevertheless, as we show, it searches a space that is as rich as the space of all linear separators.
"biographies, bollywood, boom-boxes  and blenders: domain adaptation for sentiment classification", booktitle =  "proceedings of the 45th annual meeting of the association for  computational linguistics", year =
this article studies aggressive word removal in text categorization to reduce the noise in free texts and to enhance the computational efficiency of categorization.
typical text  classifiers learn from example texts that are manually categorized.
{saarbr{\"{u}}cken, de}, url = {http://nlp3.korea.ac.kr/proceeding/coling2000/coling/ps/066.ps}, abstract = {the goal of text categorization is to classify documents into a certain number of pre-defined categories.
while document categories can be a set of unstructured category labels, some document categories are hierarchically structured.
"barcelona, spain", publisher = "association for computational linguistics", abstract =
the system also worked reasonably well  for classifying articles from a number of different computer-oriented  newsgroups according to subject, achieving as high as an 80\% correct  classification rate.
while most of the previous approaches decompose a multi-class classification problem into multiple independent binary classification tasks, the proposed approach enables direct multi-class classification.
here the probabilistic  model forms a guideline for the definition of the feature vector.
to test the effectiveness of the proposed model,  experiments were conducted using a subset of the reuters-22173 test collection  for text categorization.
kluwer academic publishers}, address = {dordrecht, nl}, url = {http://www.cs.utah.edu/~riloff/psfiles/nlp-ir-chapter.ps}, abstract = {
thus, our method seems to be well suited for heterogeneous document collections.}, } @article{klingbiel73, author = {paul h. klingbiel}, title = {machine-aided indexing of technical literature}, journal = {information storage and retrieval}, year = {1973}, volume = {9}, number = {2}, pages = {79--84}, url = {}, abstract = {
the main  improvements of the algorithms employed by the system concern the computation  of the distance between weighted trigram vectors and further analysis of the  two-pool evolutionary algorithm.
these results suggest a simple strategy for the svm text categorization: use a full number of words found through a rough filtering technique like part-of-speech tagging.}, } @inproceedings{takamura01, author = {hiroya takamura and yuji matsumoto}, title = {feature space restructuring for svms with application to text categorization}, booktitle = {proceedings of emnlp-01, 6th conference on empirical methods in natural language processing}, year = {2001}, publisher = {association for computational linguistics, morristown, us}, editor =
in text domains, effective  feature selection is essential to make the learning task efficient and more  accurate.
it analyzes the particular properties of learning with text data and identifies why svms are appropriate for this task.
the automated categorisation (or classification) of texts into topical categories has a long history, dating back at least to 1960.
this can be  considered as the effective combination of documents with no topic or class  labels (unlabeled data), labeled documents, and prior domain knowledge (in the  form of the known hierarchic structure), in providing enhanced document  classification performance.}, } @article{vinokourov02, author = {
without any computation-intensive resampling, the new estimators are computationally much more efficient than cross-validation or bootstrapping.
experimental results on three real  world data sets from usenet, yahoo, and corporate web pages show improved  performance, with a reduction in error up to 29\% over the traditional flat  classifier.}, } @inproceedings{mccallum98c, author = {andrew mccallum and k.  nigam}, title = {
it is shown that foil usually forms classifiers with lower error rates and higher rates of precision and recall with a relational encoding than with a propositional encoding.
using k-nn, naive bayes and centroid-based  classifiers, the experimental results show that the multi-dimensional-based and  hierarchical-based classification performs better than the flat-based  classifications.}, } @inproceedings{thompson01, author = {paul thompson}, title  = {automatic categorization of case law},
editor = {nicholas j.  belkin and peter ingwersen and annelise mark pejtersen}, publisher = {acm  press, new york, us}, address = {kobenhavn, dk}, pages = {37--50}, year =
{jonathan furner and david harper}, address = {
{american  society for information science, washington, us}, editor = {everett h.  brenner}, year = {1978}, address = {new york, us}, pages = {152--155}, url =  {}, abstract = {}, } @article{hamill80, author = {
using insights gained from examining the way humans make fast decisions when classifying text documents, two new text classification algorithms are developed based on sequential sampling processes.
this paper studies the  ability of symbolic learning algorithms to perform a text categorization task.
using a machine learning approach, we build classifiers  that accept an audio file of conversational human speech as input, and output  an estimate of the topic being discussed.
the paper describes  how despite this fact the inner product can be efficiently evaluated by a  dynamic programming technique.
the profiles involved are quite small, typically 10k bytes for a  category training set, and less than 4k bytes for an individual document.
685--686", abstract =
besides, we generate several knowledge base instead of one knowledge base for the classification of new object, hoping that the combination of answers of the multiple knowledge bases result in better performance.
the mcnemar test shows that in most  categories the increases are very significant.
before  performing text classification, back data should be constructed.
in  order to accomplish this testing, we employ the em algorithm which helps  efficiently estimate parameters in a finite mixture model.
first, they relax some of the  independence assumptions of naive bayes—allowing a local markov chain  dependence in the observed variables—while still permitting efficient  inference and learning.
however, the use of a text-classification system on this is  a bit more problematic - in the most straight-forward approach each number  would be considered a distinct token and treated as a word.
phase ii consists of two  stages.
lim, joo hwee}, title = {learnable visual keywords for image classification}, booktitle = {proceedings of dl-99, 4th acm conference on digital libraries}, editor =
collaboration with friends of the earth allows us to test our ideas in a non-academic context involving high volumes of documents.
the main problem is to  identify what words are best suited to classify the documents in such a way as  to discriminate between them.
it employs a meta-learning phase using document feature characteristics.
the texts to be indexed are  abstracts written in english.
subjective human evaluation of the keyphrases generated by extractor suggests that about 80\% of the keyphrases are acceptable to human readers.
{proceedings of the 1st workshop on learning language in logic}, editor =
infoclas is a first step towards the understanding of documents proceeding to a classification-driven extraction of information.
{ieee computer society press, los alamitos, us}, editor = {amari, shun-ichi and giles, c. lee and gori, marco and piuri, vincenzo}, year = {2000}, address = {como, it}, volume = {5}, pages = {205--209}, url = {http://dlib.computer.org/conferen/ijcnn/0619/pdf/06193581.pdf}, abstract = {
"proceedings of the 29th european conference on information retrieval", year = "2007", month = "april", address =
the experiments show substantial  improvements over inductive methods, especially for small training sets,  cutting the number of labeled training examples down to a 20th on some tasks.
the computer knows only the number of categories.
"dijon, france", abstract =
in this paper, however, we show that in the case of text classification, term-frequency transformations have a larger impact on the performance of svm than the kernel itself.
this paper describes a series of automatic text categorization  experiments with case law documents.
phrases are  represented by an abstraction called head/modifier pairs.
thus, they do not fully make use of the weight information provided by standard term weighting methods.
this is achieved by the exploitation of the a priori domain knowledge available, that there are relatively homogeneous temporal segments in the data stream.
{cdm: an approach to learning in text  categorization}, booktitle = {
we also show that whirl can be efficiently used to select from a large pool of unlabeled items those that can be classified correctly with high confidence.}, } @article{cohen99, author = {william w. cohen and yoram singer}, title = {context-sensitive learning methods for text categorization}, journal = {acm transactions on information systems}, year = {1999}, volume = {17}, number = {2}, pages = {141--173}, url = {http://www.acm.org/pubs/articles/journals/tois/1999-17-2/p141-cohen/p141-cohen.pdf}, abstract = {two recently implemented machine-learning algorithms, ripper and sleeping-experts for phrases, are evaluated on a number of large text categorization problems.
moreover, the performance of these greedy  methods may be deteriorated when the reserved data dimension is extremely low.
published in the ``lecture notes in computer science'' series, number 2256}, url = {http://link.springer.de/link/service/series/0558/papers/2256/22560309.pdf}, abstract = {
{robert korfhage and edie rasmussen and peter willett}, publisher =
it is shown that foil usually forms classifiers with  lower error rates and higher rates of precision and recall with a relational  encoding than with a propositional encoding.
rijsbergen, cornelis j.}, publisher = {springer verlag, heidelberg, de}, year =
the co-training  setting applies to datasets that have a natural separation of their features  into two disjoint sets.
this work also proposes an algorithm for training tsvms efficiently, handling 10,000 examples and more.}, } @article{juan02, author = {juan, alfons and vidal, enrique}, title = {
hein ragas and cornelis h.  koster}, title = {four text classification algorithms compared on a dutch  corpus}, booktitle = {proceedings of sigir-98,
"shen, dou and  sun, jian-tao and yang, qiang and chen, zheng", title =
the results reveal that subjects are sensitive  to the manipulations of graphic and phonetic information and use the same  additive information integration rule in making poetic text categorization  judgements.
patrick caldon}, title =
we discuss the issues of incorporating prior knowledge using this rather general formulation.
in this paper, we combine a probabilistic model for the darmstadt indexing approach with logistic regression.
then it combines these predictions using a majority voting.
we  then show that a quantum leap in performance is achieved when we further modify  the algorithms to better address some of the specific characteristics of the  domain.
then the final output of the pca is combined with the feature vectors from the class-profile which contains the most regular words in each class.
{apt\'{e}, chidanand and damerau, fred j. and weiss, sholom m.}, title  = {automated learning of decision rules for text categorization}, journal =
{se{\'{a}}n slattery and mark craven}, title = {
to do this, we use techniques from  statistical pattern recognition to efficiently separate the feature words, or  discriminants, from thenoise words at each node of the taxonomy.
in this paper, a study on parameters of the rocchio text classifier has been carried out to achieve its maximal accuracy.
previous work on the  categorization of document images has relied on optical character recognition  (ocr) to provide the transformation between the image domain and a domain where  pattern recognition techniques are more readily applied.
published in the ``lecture notes in computer science'' series,  number 2857}, year = {2003}, address = {manaus, br}, pages = {183--196}, url =  {http://www.gia.ist.utl.pt/~acardoso/spire03.pdf}, abstract = {
automating survey coding by multiclass text categorization techniques}, journal =
{chade-meng tan and yuan-fang wang and chan-do lee}, title = {
as a practical matter, we also explain how the text classification system can be easily ported across domains.}, } @phdthesis{riloff94a, author = {ellen riloff}, title = {information extraction as a basis for portable text classification systems}, school = {department of computer science, university of massachusetts}, address = {
performance can be significantly improved by using  active learning to select high-quality initializations, and by using  alternatives to em that avoid low-probability local maxima.}, }  @inproceedings{nigam98, author = {kamal nigam and andrew k. mccallum and  sebastian thrun and tom m. mitchell}, title = {
{teytaud, olivier and jalam, radwan}, title = {kernel based text  categorization}, booktitle =
we present a set of small-scale but reasonable experiments in  text genre detection, author identification, and author verification tasks and  show that the proposed method performs better than the most popular  distributional lexical measures, i.e., functions of vocabulary richness and  frequencies of occurrence of the most frequent words.
a boosting machine learning approach is applied to classifying web chinese documents that share a topic hierarchy.
we demonstrate the potential of these networks using an 82,339 word  corpus from the reuters newswire, reaching recall and precision rates above  92\%.
here we show how the classification accuracy of foil on this task can be improved by discovering additional regularities on the test set pages that must be classified.
our system can be used in any application that  requires fast and easily adaptable text categorization in terms of  stylistically homogeneous categories.
this algorithm alleviates the problem of local minimum in the tsvm optimization procedure while also being computationally attractive.
we define parameters of categories that make it possible to acquire numerous datasets with desired properties, which in turn allow better control over categorization experiments.
a comparison of classifiers and document representations for the routing problem}, booktitle = {proceedings of sigir-95, 18th acm international conference on research and development in information retrieval}, editor =
"763--768" } @article{raghavan:2006:alf, author = {raghavan, hema and madani, omid and jones, rosie}, title = {active learning with feedback on features and instances}, journal = {journal of machine learning research}, volume = {7}, pages = {1655--1686}, year = {2006}, abstract = {
{10}, number = {1}, pages = {35--67}, year = {2007}, month = {january} }  @article{serrano:2007:eld, author = {serrano, j.i. and del castillo, m.d.},  title = {evolutionary learning of document categories}, journal = {information  retrieval}, volume = {10}, number = {1}, pages = {69--83}, year = {2007}, month  = {january} } @article{ceci:2007:cwd, author = {michelangelo ceci and donato
in this paper we  describe extensive experiments for semantic text routing based on classified  library titles and newswire titles.
the information retrieval community is becoming increasingly  interested in machine learning techniques, of which text categorization is an  application.
the authors  developed a data-driven constructive-induction method that uses multiple  operators to improve the representation space.
the  intuition that different text classifiers behave in qualitatively different  ways has long motivated attempts to build a better metaclassifier via some  combination of classifiers.
however, full-length documents available today in large quantities pose renewed interests in text classification.
the indexing dictionary can be derived automatically from a set of manually indexed documents.
{spie, the international society for optical engineering, washington, us}, url = {}, abstract = {
{126--139}, url = {}, abstract = {}, } @article{benkhalifa01, author =
categorizing web documents in hierarchical catalogues},  booktitle = {proceedings of ecir-01, 23rd european colloquium on information  retrieval research}, editor = {}, year = {2001}, address = {darmstadt, de},  publisher = {}, pages = {}, url =  {http://ls6-www.informatik.uni-dortmund.de/bib/fulltext/ir/frommholz:01a.pdf},  abstract = {automatic categorization of web documents (e.g. html documents)
{2000}, address = {san jose, us}, pages =
we are also given a training corpus of  documents already placed in one or more categories.
when a natural split does not exist, co-training algorithms that manufacture a feature split may outperform algorithms not using a split.
it  of applications, ranging from tracking usersõ products or about  political candidates as expressed forums, to customer relationship management.
{melbourne, us}, booktitle = {proceedings of sac-03, 18th acm symposium on  applied computing}, publisher = {acm press, new york, us}, pages = {798--802},  url = {http://www.math.unipd.it/~fabseb60/publications/sac03a.pdf}, abstract =  {\emph{survey coding} is the task of assigning a symbolic code from a  predefined set of such codes to the answer given in response to an open-ended  question in a questionnaire (aka \emph{survey}).
the specifics of our classifier is that it allows  accurate categorization of short messages containing only a few words.
in addition, the evaluation results given by  the kddcup 2005 organizer confirm the effectiveness of our proposed approaches.
{acm press, new york, us}, booktitle = {proceedings of the 1st international conference on autonomous agents}, address = {marina del rey, us}, year = {1997}, pages = {201--208}, url = {http://www.acm.org/pubs/articles/proceedings/ai/267658/p201-clack/p201-clack.pdf}, abstract = {
"acm press", url = "http://doi.acm.org/10.1145/1099554.1099687", abstract =
our simulation  results also show the effectiveness of idc in text categorization problems.
we performed a number of experiments with texts from a german  newspaper.
using the  same representation of documents, charade offers better performance than  earlier reported experiments with decision trees on the same corpus.
methods of information filtering and  fetching are then required.
lim, joo hwee}, title = {learnable visual  keywords for image classification}, booktitle = {proceedings of dl-99, 4th acm  conference on digital libraries}, editor =
we employ machine learning  techniques to create dynamic document categorizations based on the full-text of  articles that are retrieved in response to users' queries.
{http://www.acm.org/pubs/articles/journals/tois/1994-12-3/p252-yang/p252-yang.pdf},  abstract = {
"2005" } @inproceedings{xia:2005:fae, author =
then we compared it with one-class versions of  the algorithms prototype (rocchio), nearest neighbor, naive bayes, and finally  a natural one-class neural network classification method based on  ``bottleneck" compression generated filters.
experiments over real-world text corpus are carried out, which validates the  effectiveness and efficiency of the proposed approach.
"kumar,  ravi and punera, kunal and tomkins, andrew", title =
this allows the individual models for each topic on  the second level to focus on finer discriminations within the group.
a major challenge in indexing unstructured hypertext databases is to automatically extract meta-data that enables structured searching using topic taxonomies, circumvents keyword ambiguity and improves the quality of searching and profile-based routing and filtering.
"august 29--31, 2003", pages =  "33--38", isbn = "isbn 963-7154-17-5", }  @inproceedings{gabrilovich:2006:wikipedia, author =
zhi-qiang liu and ya-jun zhang}, title = {a competitive neural network approach to web-page categorization}, journal = {international journal of uncertainty, fuzziness and knowledge-based systems}, volume = {9}, number = {6}, pages = {731--741}, year = {2001}, } @inproceedings{liu02, author = {yan liu and yiming yang and jaime carbonell}, title = {
our work uses a different technology to provide this transformation.
empirical estimates of  weights (likelihood ratios) become unstable when counts are small.
to this end, several centroid-based classifiers are constructed with different term weightings.
after 8 cycles the computer is found to have formed 9 groups consisting of about 50 per cent of documents that were also lumped together by professional indexers on the basis of subject content.
textual information is processed by two  methods of analysis: a natural language analysis followed by a statistical  analysis.
our experiments  show that this method significantly outperforms the combination of single label  approach."
despite its simplicity, results of experiments  on web pages and tv closed captions demonstrate high classification accuracy.
we employ a linear least squares fit (llsf) technique to compute such  connections from a collection of queries and documents where the relevance is  assigned by humans, and then use these connections in the retrieval of  documents where the relevance is unknown.
in addition, we compare the algorithms on a larger collection of 1700 texts and describe an automated method for empirically deriving appropriate threshold values.
we conclude that even the most careful  term selection cannot overcome the differences in document frequency between  phrases and words, and propose the use of term clustering to make phrases more  cooperative.}, } @article{krier02, author = {marc krier and francesco  zacc{\`a}}, title = {automatic categorization applications at the european  patent office}, journal = {world patent information}, year = {2002}, volume =
the results obtained by ripper surpass those of the operational process.}, } @inproceedings{tong00, author = {simon tong and daphne koller}, title = {support vector machine active learning with applications to text classification}, booktitle = {proceedings of icml-00, 17th international conference on machine learning}, editor = {pat langley}, year = {2000}, address = {stanford, us}, pages = {999--1006}, publisher =
it is found that decision forest outperforms both c4.5 and  knn in all cases, and that category dependent term selection yields better  accuracies.
(the uncertainty sampling and random sampling results in that paper were correct.)
as a means of evaluating representation quality, a text retrieval test collection introduces a number of confounding factors.
this paper reports on a system that uses natural language text processing to derive keywords from free text news stories, separate these keywords into segments, and automatically build a segmented database.
using the intra- and extra-document statistics, both a simple posteriori  calculation on a small example and an experiment on a large reuters-21578  database demonstrate the advantage of the dlsi space-based probabilistic  classifier over the lsi space-based classifier in classification performance.},  } @inproceedings{chen04, author = {wenliang chen and jingbo zhu and honglin wu  and yao tianshun}, title = {automatic learning features using bootstrapping for  text categorization}, booktitle = {proceedings of cicling-04, 5th international  conference on computational linguistics and intelligent text processing}, year  = {2004}, editor = {alexander f. gelbukh}, publisher = {springer verlag,  heidelberg, de}, address = {seoul, ko}, note = {
we demonstrate that the classifiers perform  10-15\% better than relevance feedback via rocchio expansion for the trec-2 and  trec-3 routing tasks.
in this work, we developed an approach to automatically generate category themes and reveal the hierarchical structure among them.
the simple method of conducting  hypothesis testing over word-based distributions in categories suffers from the  data sparseness problem.
but  in cases where many features are highly redundant with each other, we must  utilize other means, for example, more complex dependence models such as  bayesian network classifiers.
when an existing document is used as an exemplar, the completeness and accuracy with which topically related documents are retrieved is comparable to that of the best existing systems.
we also  observe that dimensionality reduction techniques eliminate a large number of  ocr errors and improve categorization results.}, } @inproceedings{taira01,  author =
william b. cavnar and john m. trenkle}, title = {n-gram-based text  categorization}, booktitle = {proceedings of sdair-94, 3rd annual symposium on  document analysis and information retrieval}, publisher = {}, editor = {}, year  = {1994}, address = {las vegas, us}, pages = {161--175}, url =  {http://www.nonlineardynamics.com/trenkle/papers/sdair-94-bc.ps.gz}, abstract =  {text categorization is a fundamental task in doc-ument processing, allowing  the automated handling of enormous streams of documents in electronic form.
comparisons  with traditional rocchio's algorithm adapted for text categorization, as well  as flat neural network classifiers are provided.
while knn  and aram yield better performances than svm on small and clean data sets, svm  and aram significantly outperformed knn on noisy data.
"learning to classify email into ``speech acts''", booktitle = "emnlp'04", pages =
these improvements are reported for  two different classifiers, support vector machines (svm) and k-nearest  neighbours (knn), and two different text corpora.
when  categorizing magazine articles with broad subject descriptors, we study three  aspects of text classification: (1) effective selection of feature words and  proper names that reflect the main topics of the text; (2) learning algorithms;  and (3) improvement of the quality of the learned classifier by selection of  examples.
"proceedings of the 29th annual international acm sigir  conference on research and development in information retrieval", year =  "2006", pages =
we can say that the usage of machine learning techniques on text databases (usually referred to as text-learning) is an important part of the content-based approach.
rijsbergen, cornelis j. and ross wilkinson and justin zobel}, publisher = {acm  press, new york, us}, year = {1998}, address = {melbourne, au}, pages =
parameter  tuning through cross-validation becomes very difficult when the validation set  contains no or only a few examples of the classes in the evaluation set.
{aaai press, menlo park, us}, year = {1997}, pages = {591--596}, address = {providence, us}, url = {http://www.rdrop.com/~lierer/aaai97.ps}, abstract = {
moreover, a macro-averaged recall and precision was calculated: the former reported a 0.72, the latter a 0.79.
in reality, however, many information retrieval problems can be more accurately described as learning a binary classifier from a set of incompletely labeled examples, where we typically have a small number of labeled positive examples and a very large number of unlabeled examples.
although much research has been done on text  categorization, this algorithm is novel in that it is unsupervised, i.e., it  does not require pre-labeled training examples, and it can assign multiple  category labels to documents.
"jingyang li and maosong sun", title =
we investigate how best to resolve the training problems related to the attribution of multiple classification codes to each patent document.}, } @inproceedings{fangmeyer68, author = {hermann fangmeyer and gerhard lustig}, title = {
when choosing optimal pairs of metrics for each of the four performance goals, bns is consistently a member of the pair---e.g., for greatest recall, the pair bns + f1-measure yielded the best performance on the greatest number of tasks by a considerable margin.}, } @inproceedings{forman04, author = {
traditionally these tasks are  carried out individually in two distinct phases: the first is the global  feature selection during a corpus pre-processing and the second is the  application of the feature weighting model.
our work uses a  different technology to provide this transformation.
we built these profiles by selecting feature words and phrases from the training documents.
to  select the proper number of , we use assures the degree of our disappointment  in any differences between the true distribution over inputs and the learner's  prediction.
we discuss ways of using self-organizing maps for document classification.
bo pang and lillian lee and shivakumar  vaithyanathan}, title =
the use of reduced feature sets allows us to utilize  more complex (probabilistic) models, without encountering many of the standard  computational and robustness difficulties.}, } @inproceedings{kongovi02, author  = {madhusudhan kongovi and juan carlos guzman and venu dasigi}, title = {
"nadia ghamrawi and andrew mccallum", title =
based on the hierarchical structure, we propose a way of dividing  the problem into subproblems, each representing one of the categories included  in the yahoo hierarchy.
the approach of using a separate category tree  represents an extension of the standard relevance list, and provides a way to  refine the search on need, offering the user a non-imposing, but potentially  powerful tool for locating needed information quickly and efficiently.
for the hierarchical  approach, we found the same accuracy using a sequential boolean decision rule  and a multiplicative decision rule.
in  particular, whirl generally achieves lower generalization error than c4.5,  ripper, and several nearest-neighbor methods.
text categorization as an information retrieval task}, journal = {the south african computer journal}, year = {1999}, pages = {4--15}, volume = {21}, url = {}, abstract = {
when the complete feature set is available, the classifier learning algorithm can better relate to the suitable representation level the different complex features like linguistic ones (e.g. syntactic categories associated to words in the training material or terminological expressions).
the model is based on the concept of  `uncertainty sampling', a technique that allows for relevance feedback both on  relevant and nonrelevant documents.
in previous work, such "distributional clustering" of features has been found to achieve improvements over feature selection in terms of classification accuracy, especially at lower number of features [2, 28].
= {pat langley}, year = {2000}, address  = {
an object-oriented  design allows easy addition of new preprocessors, machine learning algorithms,  and classifier types.}, } @article{li02, author
the automatic categorisation of web documents is becoming crucial  for organising the huge amount of information available in the internet.
w. bruce croft and alistair moffat and van rijsbergen, cornelis j. and ross wilkinson and justin zobel}, publisher = {acm press, new york, us}, year = {1998}, address = {melbourne, au}, pages = {90--95}, url = {http://cobar.cs.umass.edu/pubfiles/ir-121.ps}, abstract = {several standard text-categorization techniques were applied to the problem of automated essay grading.
{581--586}, url =  {http://dlib.computer.org/conferen/ijcnn/0619/pdf/06193581.pdf}, abstract =  {one important task for text data mining is automatic text categorization,  which assigns a text document to some predefined category according to their  correlations.
the paper  examines some alternative ways to represent text based on syntactic and  semantic relationships between words (phrases, synonyms and hypernyms).
the number of documents in this set would be uniquely determined by the system's category-boundary predictor, and this set is likely to contain less than 5\% of the incoming stream of documents.}, } @inproceedings{liere97, author = {
the article  documents the author's participation in the filtering and routing tasks of  trec-6 with the commercial filtering system teklis.
the effectiveness of the  llsf mapping and the significant improvement over alternative approaches was  evident in the tests.}, } @article{yang94, author = {yiming yang and  christopher g. chute}, title = {
reprinted in karen sparck jones and peter willett (eds.), ``readings in information retrieval'', morgan kaufmann, san francisco, us, 1997, pp.\ 513--517.}, url = {http://www.acm.org/pubs/articles/proceedings/ir/62437/p333-biebricher/p333-biebricher.pdf}, abstract = {since october 1985, the automatic indexing system air/phys has been used in the input production of the physics data base of the fachinformationszentrum karlsruhe/west germany.
this study reports the results of a series of experiments in the  techniques of automatic document classifications.
previously, research has focused predominantly on developing or adopting statistical classification or inductive learning methods for automatically discovering text categorization patterns for a pre-defined set of categories.
to facilitate web  object searching and organizing, in this paper, we propose a novel approach to  web object indexing, by discovering its inherent structure information with  existed domain knowledge.
linear support vector machines (svms) are particularly promising because they are very accurate, quick to train, and quick to evaluate.} } @inproceedings{elyaniv01, author = {
an extensive empirical study of feature selection metrics for text classification}, journal = {
since tcfp algorithm is  very simple, its implementation and training process can be done very easily.
to demonstrate  the language independent and task independent nature of these classifiers, we  present experimental results on several text classification  problems—authorship attribution, text genre classification, and topic  detection—in several languages—greek, english, japanese and chinese.
we also report results of making actual use of the selected $n$-grams in the context of a linear classifier induced by means of the rocchio method.}, } @inproceedings{carreras01, author = {xavier carreras and llu\'{\i}s m\'arquez}, title = {
this work presents a system for the categorization of noisy texts.
this margin widened in tasks with  high class skew, which is rampant in text classification problems and is  particularly challenging for induction algorithms.
neural networks allow us to model higher-order interaction between document terms and to simultaneously predict multiple topics using shared hidden features.
sigir-02, 25th acm international conference on research and development in information retrieval}, editor = {micheline beaulieu and ricardo baeza-yates and sung hyon myaeng and kalervo j{\"{a}}rvelin}, publisher = {acm press, new york, us}, address = {
stanford, us}, pages = {431--438},  publisher = {morgan kaufmann publishers, san francisco, us}, url =  {http://www-ai.cs.uni-dortmund.de/dokumente/joachims_00a.pdf}, abstract = {
text  categorization is performed using the kullback-leibler distance between the  probability distribution of the document to classify and the probability  distribution of each category.
[no abstract]}, } @inproceedings{lewis98, author = {lewis, david  d.}, title = {naive (bayes) at forty: the independence assumption in  information retrieval.}, booktitle = {proceedings of ecml-98, 10th european  conference on machine learning}, publisher = {springer verlag, heidelberg, de},
the proposed method is tested on an svm text classifier.
"2004" } @inproceedings{schneider:2005:tip,  author =
{morgan kaufmann publishers, san francisco, us}, url =  {http://robotics.stanford.edu/users/sahami/papers-dir/ml96-mcmm.ps}, abstract =  {the paper introduces the use of the multiple cause mixture model for automatic  text category assignment.
we adopt a machine learning approach and the model parameters  are learned from a labeled training set of representative documents.
we show that our method is especially useful for classification tasks involving a large number of categories where co-training doesn't perform very well by itself and when combined with ecoc, outperforms several other algorithms that combine labeled and unlabeled data for text classification in terms of accuracy, precision-recall tradeoff, and efficiency.}, } @inproceedings{ghani02, author = {rayid ghani}, title = {
then, test  documents are scanned and categories ranked based on the presence of vocabulary  terms.
published in the ``lecture notes in  computer science'' series, number 2167}, url =  {http://link.springer.de/link/service/series/0558/papers/2167/21670121.pdf},
"xiaoyan  li and croft, w. bruce", title =
it is a traditional belief that in order to scale-up to more  effective retrieval and access methods modern information retrieval has to  consider more the text content.
{11}, number = {2}, pages = {138--151}, url =  {http://www.acm.org/pubs/articles/journals/jacm/1964-11-2/p138-borko/p138-borko.pdf},  abstract = {
the  projection of documents is performed following subsequent steps.
text categorization experiments demonstrates the ability of this representation  to catch information about the semantic content of the text.}, }  @inproceedings{viechnicki98, author = {peter viechnicki}, title = {
in this paper, we  describe a novel text classifier that can effectively cope with structured  documents.
the results is an original statistical classifier fed with linguistic (i.e. more complex) features and characterized by the novel feature selection and weighting model.
using the lessons learned from our previous crawler evaluation studies, we experiment with multiple versions of different classification schemes.
the system does not perform a complete semantic or syntactic analyses of the input stories.
to  this end, several centroid-based classifiers are constructed with different  term weightings.
the approach benefits from two existing, and  in our view complimentary, sets of categorization techniques: those based on  rocchio's algorithm and those belonging to the rule learning class of machine  learning algorithms.
the extension comprises the computation of the codes by a simulated  annealing algorithm and optimization of kullback-leibler (kl) category  distances within the code-words.
= {springer verlag, heidelberg, de}, pages = {328--342}, url = {http://www.springerlink.com/openurl.asp?genre=article&issn=0302-9743&volume=2734&spage=328}, note
we test  this approach on a large collection of personal e-mail messages, which we make  publicly available in "encrypted" form contributing towards standard  benchmarks.
we  focus on the robustness of these methods in dealing with a skewed category  distribution, and their performance as function of the training-set category  frequency.
these were applied to seven-class yahoo news groups (business, entertainment, health, international, politics, sports and technology) individually and in combination, we studied three classifier combination approaches: simple voting, dynamic classifier selection and adaptive classifier combination.
we then systematically study the key factors in the can model that can  influence the classification performance, and analyze the strengths and  weaknesses of the model.} } @article{makkonen:2004:sst, author = {juha makkonen  and helena ahonen-myka and marko salmenkivi}, title = {simple semantics in  topic detection and tracking}, journal = {information retrieval}, publisher =
the combination of these techniques significantly influences the overall  performance of text categorization.
the first is an ontology that defines the classes (e.g., company, person, employee, product) and relations (e.g., employed_by, produced_by) of interest when creating the knowledge base.
however, problem solving usually involves both a domain and a task.
text categorization: a symbolic approach}, booktitle = {proceedings of sdair-96, 5th annual symposium on document analysis and information retrieval}, publisher = {}, editor = {}, address = {las vegas, us}, year = {1996}, pages = {87--99}, url = {http://www-poleia.lip6.fr/~moulinie/sdair.ps.gz}, abstract = {recent research in machine learning has been concerned with scaling-up to large data sets.
link information alone is able to obtain gains of up to 46 points in f1, when compared to a traditional content-based classifier.
in the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories.
using web  structure for classifying and describing web pages}, booktitle = {proceedings  of www-02, international conference on the world wide web}, address =
a comparison of classifiers  and document representations for the routing problem}, booktitle = {proceedings  of sigir-95, 18th acm international conference on research and development in  information retrieval}, editor =
the use of bigrams to enhance text categorization}, journal = {information processing and management}, year = {2002}, volume = {38}, number = {4}, pages = {529--546}, url = {http://www.serve.com/cmtan/meng/ig_m.pdf}, abstract = {
booktitle = {proceedings of cikm-03, 12th acm  international conference on information and knowledge management}, publisher =
"text categorization using bibliographic records: beyond document content", booktitle =
in comparison to the previously proposed agglomerative  strategies our divisive algorithm achieves higher classification accuracy  especially at lower number of features.
"we enhance machine learning algorithms for text categorization with generated features based on domain-specific and common-sense knowledge.
this paper  proposes a new approach for classifying text documents into two disjoint  classes.
however, it significantly degrades precision when ambiguity arises,  i.e., when there exist more than one candidate category to which a document can  be assigned.
finally, the future of automatic classification and some of the practical problems to be faced are outlined.}, } @article{drucker99, author = {harris drucker and vladimir vapnik and dongui wu}, title = {support vector machines for spam categorization}, journal = {ieee transactions on neural networks}, year = {1999}, number = {5}, volume = {10}, pages = {1048--1054}, url = {http://www.monmouth.edu/~drucker/svm_spam_article_compete.pdf}, abstract = {
"transferring naive bayes  classifiers for text classification", booktitle =
we are particularly interested in domain transfer: how well the learned classifiers generalize from the training corpus to a new document corpus.
the process generates, for each  $c_{i}$ in a set $c=\{c_{1},\ldots,c_{m}\}$ of domains, a lexicon $l^{i}_{1}$,  bootstrapping from an initial lexicon $l^{i}_{0}$ and a set of documents  $\theta$ given as input.
these results are compared to an existing operational  process using boolean queries manually constructed by domain experts.
profile filters the netnews and uses a scale of 11 predefined values of relevance.
phrase-descriptor relations has been developed.
the {trec-7} filtering track: description and analysis}, booktitle = {proceedings of trec-7, 7th text retrieval conference}, publisher = {national institute of standards and technology, gaithersburg, us}, editor = {ellen m. voorhees and donna k. harman}, year = {1998}, address = {gaithersburg, us}, pages = {33--56}, url = {http://trec.nist.gov/pubs/trec7/papers/tr7filter/paper.ps}, abstract = {this article describes the experiments conducted in the trec-7 filtering track, which consisted of three subtasks: adaptive filtering, batch filtering, and routing.
our approach is to analyze the document cluster map to find  centroids of some super-clusters.
three dictionaries produced by autoslog for different domains performed well in the author`s text classification experiments.}, } @incollection{riloff99, author = {ellen riloff and jeffrey lorenzen}, title = {extraction-based text categorization: generating domain-specific role relationships}, booktitle = {natural language information retrieval}, editor = {tomek strzalkowski}, year = {1999}, pages = {167--196}, publisher =
the information retrieval community is becoming increasingly interested in machine learning techniques, of which text categorization is an application.
several sets of controlled experiments on the reuters-21578 corpus are conducted to investigate the robustness of these methods.
we describe and compare different sets of experiments.
an extended set of e-mail document features including structural characteristics and linguistic patterns were derived and, together with a support vector machine learning algorithm, were used for mining the e-mail content.
"in this paper we generalize the lars feature selection method to the linear svm model, derive an efficient algorithm for it, and empirically demonstrate its usefulness as a feature selection tool for text classification." } @inproceedings{ramakrishnan:2005:mha, author =
intuitively, one would like each of the two kernels to  contribute information that is not available to the other.
wang, wenxian and  meng, weiyi and yu, clement}, title = {concept hierarchy based text database  categorization in a metasearch engine environment}, booktitle = {proceedings of  wise-00, 1st international conference on web information systems engineering},  editor = {li, qing and ozsoyoglu, z. meral and wagner, roland and kambayashi,  yahiko and zhang, yanchun}, pages =
the  dimension of the feature vectors is then reduced by linear transformation,  keeping the essential information.
a lot of research is going on with the goal of  automating this time-consuming task.
comparison with manually assigned classes shows that link information enhances classification in data with sufficiently high link density, but is detrimental to performance at low link densities or if the quality of the links is degraded.
unlike conventional approaches to learning text classifiers,  which rely primarily on empirical evidence, this model explains why and when  svms perform well for text classification.
initial experimental results demonstrate that this approach can produce accurate recommendations.}, } @inproceedings{moschitti03, author = {alessandro moschitti}, title = {a study on optimal parameter tuning for rocchio text classifier}, booktitle = {proceedings of ecir-03, 25th
in this paper, we propose and evaluate several database  categorization algorithms.
this is expensive, requires a degree of sophistication about linguistics and classification, and makes it difficult to use combinations of weak predictors.
hence, different algorithms may  be employed for different categories.
the user interface helps users search in fields without requiring the knowledge of inquery query operators.
within this paradigm, a general inductive process automatically builds a classifier by ``learning'', from a set of previously classified documents, the characteristics of one or more categories; the advantages are a very good effectiveness, a considerable savings in terms of expert manpower, and domain independence.
it uses fisher's linear discriminant, a classical tool  from statistical pattern recognition, to project training instances to a  carefully selected low-dimensional subspace before inducing a decision tree on  the projected instances.
the goal of the work described in this paper is to automatically categorize web documents in order to enable effective retrieval of web information.
conference on automated learning and discovery},  editor = {}, publisher = {aaai press, menlo park, us}, year = {1998}, pages =
however, there is still much room for improving the effectiveness of these classifiers, and new models need to be examined.
} @inproceedings{couto:2006:csc, author  =
considerable improvement in the classification accuracies of two popular classification algorithms on standard labeled data-sets with and without artificially introduced noise, as well as in the presence and absence of unlabeled data, indicates that this may be a promising method to reduce the burden of manual labeling."
a tc system for chinese texts using words as features is implemented.
this reduced feature space is then  used to train a classifier over a larger training set because more documents  now fit into the same amount of memory.
% % % % references that are *not* considered pertinent are: % % % % * publications that discuss techniques in principle useful for % % atc (e.g. machine learning techniques, information retrieval % % techniques) but do not explicitly discuss their application % % to atc; % % % %
we found that it and other existing methods failed to produce good  results on an industrial text classification problem.
training algorithms are derived for both cases, and illustrated on real data by clustering news stories and categorising newsgroup messages.
empirical results indicate that our  approach outperforms the best published results on this reuters collection.
an effective information filtering system is one that provides the exact information that fulfills a user's interest with the minimum effort by the user to describe it.
four kinds of classifiers  were used in our experiments: naive bayes, rocchio, k-nn, and svm.
we describe a method for classifying news stories using memory  based reasoning (mbr) a k-nearest neighbor method), that does not require  manual topic definitions.
{697--700},  volume = {5}, url = {}, abstract = {
the determination of category themes and their hierarchical structures were most done by human experts.
in the domain of  terrorism, autoslog created a dictionary using a training corpus and five  person-hours of effort that achieved 98\% of the performance of a hand-crafted  dictionary that took approximately 1500 person-hours to build.
{text classification and named entities for new event detection}, booktitle =  {proceedings of sigir-04, 27th acm international conference on research and  development in information retrieval}, editor = {
these results demonstrate that this approach  can scale up to a large real-world task and show a lot of potential for text  classification.}, } @inproceedings{wermter99, author = {stefan wermter and  christo panchev and garen arevian}, title = {hybrid neural plausibility  networks for news agents}, booktitle = {proceedings of aaai-99, 16th conference  of the american association for artificial intelligence}, publisher =
a robust model for intelligent text classification}, booktitle =
a document is represented with a list of  keywords.
short sequences of  system calls have been used by others to characterize a program's normal  behavior before.
document classification is characterized by the large number of attributes involved in the objects (documents).
above 90\% correct recognition rates have been achieved for the major categories concerned.
rtcut, a new method combining the strength of category ranking and scoring, outperforms both pcut and rcut significantly.}, } @article{yang02, author = {yiming yang and se{\'{a}}n slattery and rayid ghani}, title = {a study of approaches to hypertext categorization}, journal = {journal of intelligent information systems}, year = {2002}, note = {special issue on automated text categorization}, volume = {18}, number = {2/3}, pages = {219--241}, url = {http://www.wkap.nl/article.pdf?391248}, abstract = {
these results help explain why co-training  algorithms are both discriminative in nature and robust to the assumptions of  their embedded classifiers.}, } @phdthesis{nigam01, author = {kamal nigam},  title = {using unlabeled data to improve text classification}, school =
however, the exponential  growth in the volume of on-line textual information makes it nearly impossible  to maintain such taxonomic organization for large, fast-changing corpora by  hand.
visual keywords can be constructed automatically from samples of visual data through supervised/unsupervised learning.
results show that for hierarchical techniques it is better to use  hierarchical training sets.}, } @inproceedings{cerny83, author = {barbara a.  cerny and anna okseniuk and j. dennis lawrence}, title = {
we augment naive bayes models with statistical n-gram language models to address short-comings of the standard naive bayes text classifier.
the results reveal that a new feature selection metric we call 'bi-normal separation' (bns), outperformed the others by a substantial margin in most situations.
we also carefully analyze the case of those documents whose categorization is not in accordance with the one provided by the human specialists.
maintaining catalogues manually is becoming increasingly difficult, due to the sheer amount of material on the web; it is thus becoming necessary to resort to techniques for the automatic classification of documents.
extensive experiments using two benchmarks and a large real-life collection are conducted.
the use of syntactic parsing to produce indexing phrases has been widely investigated as a possible route to better text representations.
most existing recommender systems use collaborative filtering methods that base recommendations on other users' preferences.
to save the storage space and computation time in text categorization, efficient and effective algorithms for reducing the data before analysis are highly desired.
we propose to solve a text categorization task using a new metric between documents, based on a priori semantic knowledge about words.
{kalervo j{\"{a}}rvelin and james allan and peter bruza and mark sanderson}, publisher
we found that lr performs strongly and robustly in optimizing t11su (a trec utility function) while rocchio is better for optimizing ctrk (the tdt tracking cost), a high-recall oriented objective function.
another reformulation of the method leads to an algorithm that can be applied to supervised multi-class categorization.
some computational experiments have investigated the effectiveness of the  ocat-based approach and compared it to the well-known vector space model (vsm).
the space has been bound by giving a  feature selection interpretation of the rocchio parameters.
we investigate several recent approaches for text categorization under the framework of similarity-based learning.
the test results show the hybrid method is better than the previous rough set-based approach.}, } @inproceedings{basili00, author = {roberto basili and alessandro moschitti and maria t. pazienza}, title = {language-sensitive text classification}, booktitle = {proceedings of riao-00, 6th international conference ``recherche d'information assistee par ordinateur''}, editor = {}, address = {paris, france}, year = {2000}, pages = {331--343}, url = {}, abstract =
text categorization (tc) and information extraction (ie) are two  important goals of natural language processing.
the task of the kddcup 2005 competition was to classify 800,000  internet user search queries into 67 predefined categories.
most previous studies  found that the majority of these features are relevant for classification, and  that the performance of text categorization with support vector machines peaks  when no feature selection is performed.
while most of the previous approaches decompose a multi-class  classification problem into multiple independent binary classification tasks,  the proposed approach enables direct multi-class classification.
{rio de janeiro, br}, year = {2001}, pages = {137--146}, url = {http://link.springer.de/link/service/series/0558/papers/2013/20130137.pdf}, abstract = {
probe, count, and classify:  categorizing hidden web databases}, booktitle = {proceedings of sigmod-01, acm  international conference on management of data}, editor =
we describe a text categorization task and an  experiment using documents from the reuters and ohsumed collections.
many methods  can be used to categorize texts once their words are known, but ocr can garble  a large proportion of words, particularly when low quality images are used.
text genre detection usign common  word frequencies}, booktitle = {proceedings of coling-00, the 18th  international conference on computational linguistics}, year = {2000}, editor =
we present a generative bayesian model for the modeling of structured (e.g. xml) documents.
in particular,  enhanced versions of the rocchio text classifier, characterized by high  performance, have been proposed.
we investigate the use of such information for representing web sites, and the effectiveness of different classifiers (naive bayes, nearest neighbor, and {\sc foil}) in exploiting those representations.
the former never consider  the negative features, which are quite valuable, while the latter cannot ensure  the optimal combination of the two kinds of features especially on imbalanced  data.
however, this  perspective is not appropriate in the case of many digital libraries that offer  as contents scanned and optically read books or magazines.
we argue that these  algorithms which categorize documents by learning a linear separator in the  feature space have a few properties that make them ideal for this domain.
the paper deals with a new and very efficient way of assessing this generalization performance.
the main idea is to consider a possible double aspect of the importance of a word: the local importance in a category, and the global importance in the rest of the categories.
"rome, italy" }  @inproceedings{demelo:2007:mtc, author = "de melo, gerard and siersdorfer,  stefan", title =
we show that the accuracy of a  naive bayes classifier over text classification tasks can be significantly  improved by taking advantage of the error-correcting properties of the code.
this is important because in many text classification  problems obtaining training labels is expensive, while large quantities of  unlabeled documents are readily available.
the  experimental results show that the system using a simple classifier achieved  95.31\% accuracy.
combining naive bayes  $n$-gram and language models for text classification}, booktitle = {proceedings  of ecir-03, 25th european conference on information retrieval}, publisher =
in particular, boosting methods such as  adaboost have shown good performance applied to real text data.
experimental results show that the methods are a significant improvement over previously used methods in a number of areas.
however, the algorithm is much more efficient (because the learner does not have to be invoiced at all) and thus solves model selection problems with as many as a thousand relevant attributes and 12000 examples.}, } @inproceedings{schneider03, author = {{karl-michael} schneider}, year = {2003}, title = {a comparison of event models for naive bayes anti-spam e-mail filtering}, pages = {}, address = {}, editor = {}, booktitle = {proceedings of eacl-03, 11th conference of the european chapter of the association for computational linguistics}, url = {http://www.phil.uni-passau.de/linguistik/mitarbeiter/schneider/pub/eacl2003.pdf}, abstract = {}, } @inproceedings{schutze95, author = {hinrich sch{\"{u}}tze and david a. hull and jan o. pedersen}, title = {
{ieee computer society press, los alamitos, us}, editor =
"cohen, william w. and carvalho, vitor r. and mitchell, tom m.", title =
the article documents the author's participation in the filtering and routing tasks of trec-6 with the commercial filtering system teklis.
{exploiting structural information for text classification on the www},  booktitle = {proceedings of ida-99, 3rd symposium on intelligent data  analysis}, publisher = {springer verlag, heidelberg, de}, note = {
this paper presents the design and evaluation of a text  categorization method based on the hierarchical mixture of experts model.
the proxy logs the user's activities and extracts the user's interests without user intervention.
we present an effectiveness measure based on utility, and two sampling strategies (pooling and stratified sampling) for estimating the utility of the submitted sets.
using  systematic cross-corpus parameter optimization with both methods, we obtained  the best results ever reported on tdt5, trec10 and trec11.
the rule base defines what  categories the application can assign to texts and contains rules that make the  categorization decisions for particular texts.
hang li and kenji  yamanishi}, title = {document classification using a finite mixture model},  booktitle = {proceedings of acl-97, 35th annual meeting of the association for  computational linguistics}, publisher = {morgan kaufmann publishers, san  francisco, us}, editor = {philip r. cohen and wolfgang wahlster}, year =
we  introduce a new algorithm that incrementally learns a linear-threshold  classifier for each node of the taxonomy.
this paper proposes six fast-feature techniques that  use only features available in the search result list, such as title, snippet,  and url, to categorize results into meaningful categories.
to demonstrate the language independent and task independent nature of these classifiers, we present experimental results on several text classification problemsauthorship attribution, text genre classification, and topic detectionin several languagesgreek, english, japanese and chinese.
we have tried our representation scheme for automatic document categorisation on the reuters' test set of documents.
their only potential  drawback is their training time and memory requirement.
"david  vogel and steffen bickel and peter haider and rolf schimpfky and peter siemen  and steve bridges and tobias scheffer", title =
we investigate several recent approaches for text categorization  under the framework of similarity-based learning.
the motivation  for the algorithms stems from recent advances in online learning algorithms.
the feature quantity: an information-theoretic perspective of tfidf-like measures}, booktitle = {proceedings of sigir-00, 23rd acm international conference on research and development in information retrieval}, editor = {nicholas j. belkin and peter ingwersen and mun-kew leong}, publisher = {acm press, new york, us}, address = {athens, gr}, year = {2000}, pages = {104--111}, url = {http://doi.acm.org/10.1145/345508.345556}, abstract = {
this system worked very well for language classification,  achieving in one test a 99.8\% correct classification rate on usenet newsgroup  articles written in different languages.
two different methods are  proposed for improving lsi representations for the topic spotting task.
therefore, an accurate  classifier is an essential component of a hypertext database.
in natural language tasks like text categorization, we usually have  an enormous amount of unlabeled data in addition to a small amount of labeled  data.
while most  other approaches rely on a relatively simple document representation and do not  make use of any background knowledge, rule induction algorithms offer a good  potential for improvements in both of these areas.
a direct computation of this feature vector would involve a prohibitive amount of computation even for modest values of k, since the dimension of the feature space grows exponentially with k.
document feature characteristics,  derived from the training document set, capture some inherent properties of a  particular category.
the approach of using a separate category tree represents an extension of the standard relevance list, and provides a way to refine the search on need, offering the user a non-imposing, but potentially powerful tool for locating needed information quickly and efficiently.
"supervised learning approaches to text classification are in practice often required to work with small and unsystematically collected training sets.
{providence, us}, url = {http://www.rdrop.com/~lierer/aaai97.ps}, abstract =  {
the input nodes of  the network are words in the training texts, the nodes on the intermediate  level are the training texts, and the output nodes are categories.
"coling", pages = "841--847", url =  "http://acl.ldc.upenn.edu/coling2004/main/pdf/121-637.pdf", abstract  =
rough set (rs) theory can be applied to reducing the  dimensionality of data used in if/ir tasks, by providing a measure of the  information content of datasets with respect to a given classification.
using multinomial naive bayes and regularized logistic regression as classifiers, our experiments show both great potential and actual merits of explicitly combining positive and negative features in a nearly optimal fashion according to the imbalanced data.}, } @inproceedings{zhou00, author = {shuigeng zhou and ye fan and jiangtao hua and fang yu and yunfa hu}, title = {hierachically classifying chinese web documents without dictionary support and segmentation procedure}, booktitle = {proceedings of waim-00, 1st international conference on web-age information management}, publisher = {springer verlag, heidelberg, de}, editor = {hongjun lu and aoying zhou}, note = {
thus, svm adapts efficiently in  dynamic environments that require frequent additions to the document  collection.
experiments on two real-world spidering tasks show a three-fold improvement in spidering efficiency over traditional breadth-first search, and up to a two-fold improvement over reinforcement learning with immediate reward only.}, } @article{ribeironeto01, author = {berthier ribeiro-neto and alberto h.f. laender and luciano r. {de lima}}, title = {
{843--848}, url = {}, abstract = {a language-independent means of gauging topical similarity in unrestricted text is described.
"493--500", abstract =
text categorization using character shape codes}, booktitle = {proceedings of  the 7th spie conference on document recognition and retrieval}, publisher =
we show that foil's performance  can be improved by relation selection, a first order analog of feature  selection.
furthermore, by combining these methods, we significantly  reduced the variance in performance of our event tracking system over different  data collections, suggesting a robust solution for parameter optimization.}, }  @inproceedings{yang00a, author = {yiming yang and thomas ault and thomas  pierce}, title = {
the overall  compression of an article with respect to different models can be compared to  see which one it fits most closely.
published in the  ``lecture notes in computer science'' series, number 1398}, editor = {claire  n{\'{e}}dellec and c{\'{e}}line rouveirol}, address = {chemnitz, de}, pages =
experiments on a real-world data set show a reduction in classification error by up to 66\% over the traditional naive bayes classifier.
"2005", month = "november", address =
{santiago de compostela, es}, year = {2005}, pages = {300--314}, url = {},  abstract = {compression-based text classification methods are easy to apply,  requiring virtually no preprocessing of the data.
in addition, we find  that some types of appraisal appear to be more significant for sentiment  classification than others."
scoring rules can further take advantage of the hierarchy by considering only second-level categories that exceed a threshold at the top level.
forthcoming}, } @incollection{debole04a, author = {franca debole and fabrizio sebastiani}, title = {supervised term weighting for automated text categorization}, year = {2004}, booktitle = {text mining and its applications}, editor = {spiros sirmakessis}, publisher =
in this  paper, we propose a neuro-genetic approach to feature selection in text  categorization.
this interesting finding of boosting on rare categories has not been reported before.}, } @inproceedings{liu03, author = {
the second concerns abis, an intelligent agent for supporting users in filtering data from distributed and heterogeneous archives and repositories.
this holds for ocr texts as well as for  correct ascii texts.}, } @article{kaban02, author = {
our experimental comparison of eleven feature scoring measures show that considering domain and algorithm characteristics significantly improves the results of classification.}, } @article{moens00, author = {marie-francine moens and jos dumortier}, title = {
[17]for the tc-won tasks.
{a comparison of word- and sense-based text categorization using several  classification algorithms}, author = {athanasios kehagias and vassilios  petridis and vassilis g. kaburlasos and pavlina fragkou}, journal = {journal of  intelligent information systems}, year = {2003}, volume = {21}, number = {3},  pages = {227--247}, url = {http://www.wkap.nl/article.pdf?391243}, abstract =  {most of the text categorization algorithms in the literature represent  documents as collections of words.
we present an algorithm for learning a value function that maps hyperlinks to future discounted reward using a naive bayes text classifier.
we suggest, for  text categorization, the integration of external wordnet lexical information to  supplement training data for a semi-supervised clustering algorithm which (i)  uses a finite design set of labeled data to (ii) help agglomerative  hierarchical clustering algorithms (ahc) partition a finite set of unlabeled  data and then (iii) terminates without the capacity to classify other objects.
with the recent dramatic increase in electronic access to  documents, text categorization-the task of assigning topics to a given  document-has moved to the center of the information sciences and knowledge  management.
text categorization, or the assignment of textual documents to one or more pre-defined categories based on their content, is an essential component of efficient management and retrieval of documents.
the relevance values are interpreted as subjective probabilities and hence are mapped into the real interval [0; 1].
"proceedings of the 14th {acm} international conference on information and knowledge management", year =
{159--194}, abstract = {data sparseness or overfitting is a serious problem in natural language processing employing machine learning methods.
a proposed purification process can effectively reduce the dimensionality of the feature space from 50,576 terms in the word-based approach to 19,865 terms in the unknown word-based approach.
we evaluate different measures of similarity -- five derived from the citation information of the collection, and three derived from the structural content -- and determine how they can be fused to improve classification effectiveness.
this paper describes a learning news agent hynet which uses hybrid neural  network techniques for classifying news titles as they appear on an internet  newswire.
a pitfall and solution in multi-class  feature selection for text classification}, booktitle = {proceedings of  icml-04, 21st international conference on machine learning}, editor = {carla e.  brodley}, year = {2004}, address = {banff, ca}, pages = {}, publisher =
for the comparison, we run a series of experiments using a digital library of computer science papers and a web directory.
even when multi-labels are sparse, the models improve subset classification  error by as much as 40%."
text  categorization}, editor = {alessandro zanasi}, year = {2005}, booktitle = {
= {australian computer society}, address = {
we then analyzed the  two maps to obtain the categories and the structure among them.
the experimental results show that the proposed methods of  incorporating prior knowledge is effective." }  @inproceedings{pekar:2004:cwp, author =
while most other approaches rely on a relatively simple document representation and do not make use of any background knowledge, rule induction algorithms offer a good potential for improvements in both of these areas.
we then categorize documents using this a priori knowledge of the definition of each category.
{association for computational linguistics, morristown, us}, editor = {lillian  lee and donna harman}, pages = {44--50}, address = {pittsburgh, us}, url =  {http://arxiv.org/pdf/cs/0106040}, abstract = {
it outperforms existing  systems by keeping most of their interesting properties (i.e. easy  implementation, low complexity and high scalability).
the process generates, for each $c_{i}$ in a set $c=\{c_{1},\ldots,c_{m}\}$ of domains, a lexicon $l^{i}_{1}$, bootstrapping from an initial lexicon $l^{i}_{0}$ and a set of documents $\theta$ given as input.
each of the methods makes a priori assumptions, which it employs, given the data, when searching for its hypothesis.
empirical evaluation on 19 datasets shows substantial improvements.}, } @inproceedings{forman04a, author = {george forman and ira cohen}, title = {
we employ machine learning techniques to create dynamic document categorizations based on the full-text of articles that are retrieved in response to users' queries.
with the continuing exponential growth of the internet and the more  recent growth of business intranets, the commercial world is becoming  increasingly aware of the problem of electronic information overload.
{american society for information science, washington, us}, year = {1997}, address = {washington, us}, pages = {59--72}, url = {http://www.cs.uiowa.edu/~mruiz/papers/sigcr97/sigcrfinal2.html}, abstract = {this paper presents the results obtained from a series of experiments in automatic text categorization of medline articles.
in this study, we proposed a mining-based category evolution (mice) technique to adjust document categories based on existing categories and their associated documents.
the tcs and  how it meets its design goals are described, and examples of applications built  with tcs are given.
the method maintains a  window on the training data.
five  methods were evaluated, including term selection based on document frequency  (df), information gain (ig), mutual information (mi), a 2 -test (chi), and term  strength (ts).
this paper presents an application of nonlinear neural networks to  topic spotting.
the results we have obtained significantly outperform the results  achieved by previous automated survey coding approaches.}, }  @inproceedings{glover02, author =
instead, the probability estimates must be  renormalized using logistic regression on the known relevance judgements.
previous researches on advanced representations for document  retrieval have shown that statistical state-of-the-art models are not improved  by a variety of different linguistic representations.
luc de raedt and peter a.  flach}, publisher = {springer verlag, heidelberg, de}, address = {freiburg,  de}, year = {2001}, pages = {454--465}, note = {
the number of common keywords between keywords from the document itself and representative keywords from back data classifies documents.
adaptive information filtering using evolutionary computation}, journal = {information sciences}, year = {2000}, volume = {122}, number = {2/4}, pages = {121--140}, url = {http://www.elsevier.nl/gej-ng/10/23/143/56/27/27/article.pdf}, abstract = {information filtering is concerned with filtering data streams in such a way as to leave only pertinent data (information) to be perused.
} @inproceedings{gliozzo:2005:dkt, author = {gliozzo, alfio and strapparava, carlo}, title = {domain kernels for text categorization}, booktitle = {proceedings of the ninth conference on computational natural language learning (conll-2005)}, month = {june}, year = {2005}, address = {ann arbor, michigan}, publisher = {association for computational linguistics}, pages = {56--63}, url = {http://www.aclweb.org/anthology/w/w05/w05-0608} } @inproceedings{fukumoto:2004:cna, author =
we can say that the usage of machine learning techniques on  text databases (usually referred to as text-learning) is an important part of  the content-based approach.
our analyses show that when the positive training data is not too under-sampled, svmc significantly outperforms other methods because svmc basically exploits the natural "gap" between positive and negative documents in the feature space, which eventually corresponds to improving the generalization performance.
in addition, we find that some types of appraisal appear to be more significant for sentiment classification than others."
it consists of the following components: layout analysis, pre-classification, ocr interface, fuzzy string matching, text categorization, lexical, syntactical and semantic analysis.
we find that on average,  labeling a feature takes much less time than labeling a document.
dell zhang and wee sun lee}, title = {question classification using support vector machines}, booktitle = {proceedings of sigir-03, 26th acm international conference on research and development in information retrieval}, editor = {jamie callan and gordon cormack and charles clarke and david hawking and alan smeaton}, publisher = {acm press, new york, us}, address = {toronto, ca}, year = {2003}, pages = {26--32}, url = {http://doi.acm.org/10.1145/860435.860443}, abstract = {question classification is very important for question answering.
we experimentally evaluated waknn on 52 document data sets from a variety of domains and compared its performance against several classification algorithms, such as c4.5, ripper, naive-bayesian, pebls and vsm.
{bratislava, sk}, pages = {65--79}, url = {http://link.springer.de/link/service/series/0558/papers/2435/24350065.pdf}, abstract =
however, the complexity of natural languages and the extremely high  dimensionality of the feature space of documents have made this classification  problem very difficult.
we present a generative bayesian model for the modeling of  structured (e.g. xml) documents.
classification of documents allow the automatic distribution or archiving of letters and is also an excellent starting point for higher-level document analysis.}, } @article{hoyle73, author = {w.g. hoyle}, title = {automatic indexing and generation of classification by algorithm}, journal = {information storage and retrieval}, year = {1973}, volume = {9}, number = {4}, pages = {233--242}, url = {}, abstract = {a system of automatic indexing based on bayes' theorem is described briefly.
they can be computed at essentially no extra cost immediately after training a single svm.
{tapio elomaa and heikki mannila and h. toivonen}, address  = {helsinki, fi}, pages =
the document organization and classification performance of our ica-based hierarchical classifier are evaluated in several encouraging experiments conducted on a journalistic-style text corpus for speech synthesis in catalan.}, } @inproceedings{shanahan03, author = {james g. shanahan and norbert roma}, title = {
additionally, the computation procedure can be improved to locate the sets of duplicate or plagiarised documents.
the advantages of hbc are experimentally verified from several viewpoints.
first, they relax some of the independence assumptions of naive bayesallowing a local markov chain dependence in the observed variableswhile still permitting efficient inference and learning.
support vector machines (svms) excel in binary classification, but the elegant theory behind large-margin hyperplane cannot be easily extended to multi-class text classification.
"proceedings of the international joint conference on artificial intelligence", year =
{simon tong and daphne koller}, title = {support vector machine active learning  with applications to text classification}, journal = {journal of machine  learning research}, volume = {2}, month = {november}, pages = {45--66}, year =
{303--310}, publisher = {
{choose your words carefully: an empirical study of feature  selection metrics for text classification}, booktitle = {proceedings of  pkdd-02, 6th european conference on principles of data mining and knowledge  discovery}, editor =
{information processing letters}, pages = {203--212}, year = {2003}, volume =
"we study the effects of feature selection and human feedback on features in active learning settings.
sven {meyer zu eissen} and benno stein}, title = {
published  in the ``lecture notes in computer science'' series, number 1925}, url = {},  abstract = {
eliminating high-degree biased character bigrams for dimensionality  reduction in chinese text categorization}, booktitle = {proceedings of ecir-04,  26th european conference on information retrieval research}, editor = {
each of  the methods makes a priori assumptions, which it employs, given the data, when  searching for its hypothesis.
different from the conventional techniques, the proposed mfom method attempts to integrate any performance metric of interest (e.g. accuracy, recall, precision, or f1 measure) into the design of any classifier.
"2007", month = "april", address =
"2006", pages =  "701--702", abstract =
in this work, we present a family of semi-supervised linear support vector classifiers that are designed to handle partially-labeled sparse datasets with possibly very large number of examples and features.
this paper presents an extension of the rocchio formula [11] as a feature weighting and selection model used as a basis for multilingual information extraction.
we introduce an algorithm for  learning from labeled and unlabeled documents based on the combination of  expectation-maximization (em) and a naive bayes classifier.
"using appraisal groups for sentiment analysis", pages =  "625--631", booktitle =
owing to the use of context-sensitive features, the classifier is very accurate.
} @inproceedings{olsson:2006:act, author =
}  @article{shen:2005:q2c, author =
in  this article, we apply ml to text-database analyses and knowledge acquisitions  from text databases.
"large scale  semi-supervised linear svms", booktitle =
in  addition, we are able to obtain tight bounds for the complexities by using the  power law to approximate category distributions over a hierarchy.
given seeker, the text retrieval system, we achieved these results in about two person-months.
"experiments with multilabel text  classifier on the {r}euters collection", booktitle = "proc. of the  ieee int.
this paper describes a unique example-based mapping method for document retrieval.
automated text classification is attractive because it frees organizations from the need of manually organizing document bases, which can be too expensive, or simply infeasible given the time constraints of the application or the number of documents involved.
the algorithm first  trains a classifier using the available labeled documents, and  probabilistically labels the unlabeled documents.
{1992}, url = {http://www.research.att.com/~lewis/papers/lewis92b.ps}, abstract = {syntactic phrase indexing and term clustering have been widely explored as text representation techniques for text retrieval.
using text classifiers for numerical classification}, booktitle =
for  example, in email classification, it is possible to use instance  representations that consider not only the text of each message, but also  numerical-valued features such as the length of the message or the time of day  at which it was sent.
{1998}, address = {melbourne, au}, pages = {369--370}, url =  {http://www.acm.org/pubs/articles/proceedings/ir/290941/p369-ragas/p369-ragas.pdf},  abstract = {we describe an experiment in applying text classification  algorithms to dutch texts.
"prague, czech republic" }  @inproceedings{li:2007:eci, author =
moreover, dimension reduction, which  is usually imperative, now becomes optional.
"email classification for automated service handling", booktitle =
the technique of periodical updates improves the routing accuracy ranging from 20\% to 100\% but incurs runtime overhead.
these scores were combined with several other summary text measures using linear regression.
in this paper we describe experiments that investigate the effects of ocr errors on text categorization.
a major knowledge-engineering bottleneck for information extraction systems is the process of constructing an appropriate dictionary of extraction patterns.
{acquaintance: a novel vector-space n-gram technique for document categorization}, booktitle = {proceedings of trec-3, 3rd text retrieval conference}, publisher = {national institute of standards and technology, gaithersburg, us}, editor = {donna k. harman}, year = {1994}, address = {gaithersburg, us}, pages = {305--310}, url = {}, abstract = {acquaintance is the name of a novel vector-space n-gram technique for categorizing documents.
it is a traditional belief that in order to scale-up to more effective retrieval and access methods modern information retrieval has to consider more the text content.
rtcut, a new method combining the strength of category ranking and scoring,  outperforms both pcut and rcut significantly.}, } @article{yang02, author =  {yiming yang and se{\'{a}}n slattery and rayid ghani}, title = {
few research works have been carried out on finding better usages of time information.
text classification using esc-based stochastic decision  lists}, journal = {information processing and management}, pages = {343--361},  year = {2002}, number = {3}, volume = {38}, url = {}, abstract = {
learning for  text categorization and information extraction with ilp}, booktitle =
"categorizing web  search results into meaningful and stable categories using fast-feature  techniques", booktitle =
{proceedings of trec-4, 4th text retrieval conference}, publisher = {national  institute of standards and technology, gaithersburg, us}, editor = {donna k.  harman and ellen m. voorhees}, year = {1995}, address = {gaithersburg, us},  pages = {359--371}, url = {http://trec.nist.gov/pubs/trec4/papers/nsa.ps.gz},  abstract = {acquaintance is the name of a novel vector-space n-gram for  categorizing documents.
"unsupervised text classification using kohonens self organizing network", booktitle = "computational linguistics and intelligent text processing", year =
this has  encouraged interest in developing agents/softbots that can act as electronic  personal assistants and can develop and adapt representations of users  information needs, commonly known as profiles.
"international series in intelligent technologies", number = 20, isbn =
experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 33\%.}, } @inproceedings{oh00, author = {hyo-jung oh and sung hyon myaeng and mann-ho lee}, title = {a practical hypertext categorization method using links and incrementally available class information}, booktitle = {
the strategy is generic and takes advantage of  training errors to successively refine the classification model of a base  classifier.
in this paper we show how performance on new event detection (ned) can be improved by the use of text classification techniques as well as by using named entities in a new way.
"the kdd-cup 2005 competition was held in conjunction with the eleventh acm sigkdd international conference on knowledge discovery and data mining.
in our opinion, more  sophisticated natural language processing techniques need to be developed  before better text representations can be produced for classification.}, }  @inproceedings{sebastiani00, author = {fabrizio sebastiani and alessandro  sperduti and nicola valdambrini}, title = {
given an unlabeled document d, we need to find its class label, ci, using the mapping function f where f(d) =
ludovic denoyer  and patrick gallinari}, title = {bayesian network model for semi-structured  document classification}, journal = {information processing and management},  year = {2004}, volume = {40}, number = {5}, pages = {807--827}, url = {},  abstract = {}, } @article{devel01, author = {olivier y. {de vel} and alison  anderson and malcolm corney and george m. mohay}, title = {mining email content  for author identification forensics}, journal = {sigmod record}, year = {2001},  volume = {30}, number = {4}, pages = {55--64}, url = {}, abstract = {
we also show that features found to be useful in one corpus do not transfer well to other corpora with different genres." } @article{vogel:2005:kddcup2005, author =
moreover, the system can then save such document organizations in user profiles  which can then be used to help classify future query results by the same user.
"17", pages =  "1223--1232", url =  "http://doi.ieeecomputersociety.org/10.1109/tkde.2005.149", abstract  =
these include feature sets that are constructed  by latent semantic indexing, `local dictionaries' in the form of the words that  score highest in frequency in positive class examples and feature sets that are  constructed by relevance feedback strategies such as j.j. rocchio's (1971)
based on the idea of distilling the characteristics of how we estimate the merits of each component algorithm, we propose three different strategies for the linear combination approach.
our novel classifier can take advantage of  the information in the structure of document that conventional, purely  term-based classifiers ignore.
the layout of a document contains a significant amount of information that can be used to classify it by type in the absence of domain-specific models.
in this paper, we describe an active learning method that  uses a committee of learners to reduce the number of training examples required  for learning.
then we compared it with one-class versions of the algorithms prototype (rocchio), nearest neighbor, naive bayes, and finally a natural one-class neural network classification method based on ``bottleneck" compression generated filters.
in addition, nlp systems can increase the information contained in keyword fields by separating keywords into segments, or distinct fields that capture certain discriminating content or relations among keywords.
in this paper, we present a hierarchical text classifier based on independent component analysis (ica), which is capable of (i) organizing the contents of the corpus in a hierarchical manner and (ii) classifying the texts to be synthesized according to the learned structure.
one of our main  results is the identification of a single classifier with good performance  (relative to our classifier space) across all subdialogue lengths.}, }  @inproceedings{nardiello03, author = {pio nardiello and fabrizio sebastiani and  alessandro sperduti}, title = {
all of these  methods showed significant improvement (up to 71\% reduction in weighted error  rates) over the performance of the original knn algorithm on tdt benchmark  collections, making knn among the top-performing systems in the recent tdt3  official evaluation.
furthermore, the use of shape coding is particularly advantageous over ocr in the processing of page images of poor quality.
four of the six criteria indicate the hypothesis  holds, and two point to no effect.
a modular  system is proposed which allows the integration of this technique with a large  variety of different if/ir approaches.
it produces inferior results because  it is insensitive to subtle differences between articles that belong to a  category and those that do not.
compared with other active learning algorithms, the proposed representative sampling explicitly addresses the problem of selecting more than one unlabeled documents.
papers from the 1996 aaai spring symposium}, institution = {americal association for artificial intelligence}, address = {
however, these methods of indexing have two major drawbacks: first, they must  be laboriously assigned by human indexers.
our novel classifier can take advantage of the information in the structure of document that conventional, purely term-based classifiers ignore.
published in the ``lecture notes in computer science'' series, number 1642}, editor = {david j. hand and joost n. kok and michael r. berthold}, address = {amsterdam, nl}, year = {1999}, pages = {513--524}, url = {http://link.springer.de/link/service/series/0558/papers/1642/16420513.pdf}, abstract = {
we use information from a pre-existing taxonomy in order to supervise the creation of a set of related clusters, though with some freedom in defining and creating the classes.
our experiments show that the  lower dimensional spaces computed by our algorithm consistently improve the  performance of traditional algorithms such as c4.5, k-nearest-neighbor, and
a linear least squares fit (llsf)
pcut copes better with rare categories and exhibits a smoother trade-off in recall versus precision, but is not suitable for online decision making.
mh$^kr$} is based on the idea to build, at every iteration of the learning phase, not a single classifier but a sub-committee of the $k$ classifiers which, at that iteration, look the most promising.
a machine-aided indexing (mai) system is described that indexes one million words of text per hour of cpu time.
enhancing text categorization with encyclopedic knowledge", booktitle1 =
on feature distributional clustering for text categorization}, booktitle = {proceedings of sigir-01, 24th acm international conference on research and development in information retrieval}, editor = {croft, w. bruce and harper, david j. and kraft, donald h. and zobel, justin}, publisher = {acm press, new york, us}, address = {new orleans, us}, year = {2001}, pages = {146--153}, url = {http://www.cs.huji.ac.il/labs/learning/papers/sigir.ps.gz}, abstract = {
bayes can be improved using locally weighted learning.
kalervo j{\"{a}}rvelin and james allan  and peter bruza and mark sanderson}, publisher = {acm press, new york, us},  address = {sheffield, uk}, year = {2004}, pages = {234--241}, url =  {http://doi.acm.org/10.1145/1008992.1009034}, abstract = {this paper explores  feature scoring and selection based on weights from linear classification  models.
this study benchmarks the performance of twelve feature selection metrics  across 229 text classification problems drawn from reuters, ohsumed, trec, etc.
this paper seeks a principled approach to providing the answers.
a combination of different classifiers produced better results than any single type of classifier.
"2006", pages = "701--702", abstract =
in general, based on the current recall and precision performance, as well as the detailed analysis, we show that recurrent plausibility networks hold a lot of potential for developing learning and robust newswire agents for the internet.}, } @inproceedings{wibowo02, author = {wahyu wibowo and hugh e. williams}, title = {simple and accurate feature selection for hierarchical categorisation}, booktitle = {proceedings of the 2002 acm symposium on document engineering}, publisher = {acm press, new york, us}, editor = {}, year = {2002}, address = {mclean, us}, pages = {111--118}, url = {http://doi.acm.org/10.1145/585058.585079}, abstract = {categorisation of digital documents is useful for organisation and retrieval.
a  comparative study on feature selection in text categorization}, booktitle =  {proceedings of icml-97, 14th international conference on machine learning},  editor = {
{available as {\tt  http://www.daviddlewis.com/resources/testcollections/reuters21578/readme.txt}},  url =  {http://www.daviddlewis.com/resources/testcollections/reuters21578/readme.txt},  abstract = {
we show that it can effectively select  an appropriate window size in a robust way.}, } @inproceedings{knorz82, author  = {knorz, gerhard}, title = {a decision theory approach to optimal automated  indexing}, booktitle = {proceedings of sigir-82, 5th acm international  conference on research and development in information retrieval}, year =
the motivation is  that there are statistical problems associated with natural language text when  it is applied as input to existing machine learning algorithms (too much noise,  too many features, skewed distribution).
on-line information services generally depend on keyword indices  rather than other methods of retrieval, because of the practical features of  keywords for storage, dissemination, and browsing as well as for retrieval.
evaluating  the performance of a two-level implementation on the reuters-22173 testbed of  newswire articles shows the most significant improvement for rare classes.}, }  @article{weiss99, author = {sholom m. weiss and chidanand apt\'{e} and fred j.  damerau and david e. johnson and frank j. oles and thilo goetz and thomas  hampp}, title = {maximizing text-mining performance}, journal = {ieee  intelligent systems}, year = {1999}, number = {4}, volume = {14}, pages =
{marti a. hearst and fredric gey and richard tong}, publisher = {acm  press, new york, us}, address = {berkeley, us}, year = {1999}, pages =
for text categorization (tc) are available fewer and less definitive studies on the use of advanced document representations as it is a relatively new research area (compared to document retrieval).
finally, the generative model may be used to derive a fisher kernel expressing similarity between documents.}, } @article{gentili01, author = {g.l. gentili and mauro marinilli and alessandro micarelli and filippo sciarrone}, title = {
hence, the creation of new directories or the modification of existing ones require strong investments.
we show that judicious use of a hierarchy can significantly improve both the speed and effectiveness of the categorization process.
we focus on domains with  many features that also have a highly unbalanced class distribution and  asymmetric misclassification costs given only implicitly in the problem.
this  article describes the experiments conducted in the trec-7 filtering track,  which consisted of three subtasks: adaptive filtering, batch filtering, and  routing.
this has important implications for document  classification when a hierarchic ordering of topics exists.
text classification using small number of features}, booktitle  = {proceedings of mldm-05, 4th international conference on machine learning and  data mining in pattern recognition}, year = {2005}, pages = {580--589}, address  = {leipzig, germany}, series = {lecture notes in artificial intelligence},  volume = {3587}, publisher =
the results show that the use of the hierarchical structure improves text categorization performance significantly.}, } @inproceedings{ruiz99a, author = {miguel e. ruiz and padmini srinivasan}, title = {
{4}, pages = {719--736}, url =  {http://www.jucs.org/jucs_4_9/categorisation_by_context}, abstract =  {assistance in retrieving of documents on the world wide web is provided either  by search engines, through keyword based queries, or by catalogues, which  organise documents into hierarchical collections.
experiments with the purely theoretical approach and with several heuristic variations show that heuristic assumptions may yield significant improvements.}, } @article{fuhr94, author = {norbert fuhr and ulrich pfeifer}, title = {probabilistic information retrieval as combination of abstraction inductive learning and probabilistic assumptions}, journal = {acm transactions on information systems}, year = {1994}, number = {1}, volume = {12}, pages = {92--115}, url = {http://ls6-www.informatik.uni-dortmund.de/bib/fulltext/ir/fuhr_pfeifer:94.ps.gz},
we review methods for analyzing novelty and then describe newsjunkie,  a system that personalizes news for users by identifying the novelty of stories  in the context of stories they have already reviewed.
nevertheless,  tc provides many challenges to machine learning.
using the intra- and extra-document statistics, both a simple posteriori calculation on a small example and an experiment on a large reuters-21578 database demonstrate the advantage of the dlsi space-based probabilistic classifier over the lsi space-based classifier in classification performance.}, } @inproceedings{chen04, author = {wenliang chen and jingbo zhu and honglin wu and yao tianshun}, title = {automatic learning features using bootstrapping for text categorization}, booktitle = {proceedings of cicling-04, 5th international conference on computational linguistics and intelligent text processing}, year = {2004}, editor = {alexander f. gelbukh}, publisher = {springer verlag, heidelberg, de}, address = {seoul, ko}, note = {
the simple tfidf classifier is chosen to train sample data  and to classify other new data.
performance is usually, though not  always, less good here, but the term weighting functions implicit in the  resulting ranking functions are intriguing, and the approach could easily be  adapted to mixtures of textual and nontextual data.}, }  @inproceedings{jacobs92, author = {paul s. jacobs}, title = {
we show that it can effectively select an appropriate window size in a robust way.}, } @inproceedings{knorz82, author = {knorz, gerhard}, title = {a decision theory approach to optimal automated indexing}, booktitle = {proceedings of sigir-82, 5th acm international conference on research and development in information retrieval}, year = {1982}, editor = {gerard salton and hans-jochen schneider}, pages = {174--193}, address = {berlin, de}, publisher = {springer verlag, heidelberg, de}, note = {published in the ``lecture notes in computer science'' series, number 146}, url = {}, abstract = {}, } @inproceedings{ko00, author = {youngjoong ko and jungyun seo}, title = {automatic text categorization by unsupervised learning},
a theoretical analysis and experiments show that the new  method can effectively estimate the performance of svm text classifiers in an  efficient way.}, } @inproceedings{joachims01b, author = {thorsten joachims and  nello cristianini and john shawe-taylor}, title = {composite kernels for  hypertext categorisation}, booktitle = {proceedings of icml-01, 18th  international conference on machine learning}, editor = {carla brodley and  andrea danyluk}, address = {williams college, us}, year = {2001}, pages =  {250--257}, publisher =
a machine-aided  indexing (mai) system is described that indexes one million words of text per  hour of cpu time.
existing classification schemes which ignore the hierarchical structure and treat the topics as separate classes are often inadequate in text classification where the there is a large number of classes and a huge number of relevant features needed to distinguish between them.
a system called autoslog is presented which automatically constructs dictionaries for information extraction, given an appropriate training corpus.
since  uncertainties in the labeling are taken into account, the model provides an  elegant mechanism to deal with noisy labels.
recently, probabilistic relational models, a relational version of bayesian networks, were used to define a joint probabilistic model for a collection of related entities.
for solving these kinds of problems, neural networks have the advantage of extracting the underlying relationships between the input data and the output classes automatically.
in  particular, we show that in our environment, ocr errors have no effect on  categorization when we use a classifier based on the naive bayes model.
booktitle = {proceedings of icml-03, 20th international conference on machine learning}, editor = {}, year = {2003}, address = {washington, dc}, pages = {}, publisher =
to our knowledge, this work is the first to report  performance results with the entire new reuters corpus.}, } @article{craven00,  author = {mark craven and dan dipasquo and dayne freitag and andrew k. mccallum  and tom m. mitchell and kamal nigam and se{\'{a}}n slattery}, title = {
though our experiments with words yielded good results, we found  instances where the phrase-based approach produced more effectiveness.
for more than a half of testing examples a correct category is among the 3 categories with the highest predicted probability.}, } @inproceedings{mladenic98b, author = {
in this paper, we propose the use of a non-relevant information profile to reduce the mistaken retrieval of non-relevant documents.
= {}, editor = {}, year = {1994}, address = {las vegas, us}, pages = {161--175}, url = {http://www.nonlineardynamics.com/trenkle/papers/sdair-94-bc.ps.gz}, abstract = {text categorization is a fundamental task in doc-ument processing, allowing the automated handling of enormous streams of documents in electronic form.
to facilitate research in this area, the task description, data,  answer set, and related information of this kdd-cup are published at the kddcup  2005 web site: http://www.acm.org/sigs/sigkdd/kdd2005/kddcup.html."
we adopt an established statistical technique called shrinkage that  smooths parameter estimates of a data-sparse child with its parent in order to  obtain more robust parameter estimates.
when few examples are available, we observe that accuracy is sensitive to k and that best k tends to increase with training size.
the architecture  relies on hidden markov models whose emissions are bag-of-words resulting from  a multinomial word event model, as in the generative portion of the naive bayes  classifier.
this task is usually carried  out in order to group respondents according to a predefined scheme based on  their answers.
our approach has practical advantages for problem  solving by introducing the viewpoint of tasks to achieve higher performance.},  } @inproceedings{mccallum98, author = {andrew k. mccallum and kamal nigam},  title = {
to test the effectiveness of the proposed model, experiments were conducted using a subset of the reuters-22173 test collection for text categorization.
experimental results confirm the predictions of the theory in the hypertext domain.}, } @inproceedings{joachims01c, author = {thorsten joachims}, title = {a statistical learning model of text classification with support vector machines}, booktitle = {proceedings of sigir-01, 24th acm international conference on research and development in information retrieval},
we identify several new issues to be addressed when the assumption is  removed, and formulate the web unit mining problem.
{http://doi.acm.org/10.1145/956863.956910}, abstract = {many real-world  problems involve a combination of both text- and numerical-valued features.
the choice of the kernel  function is crucial to most applications of support vector machines.
the \textsf{reuters-21578} test collection, together with its earlier variants, has been such a standard benchmark for the text categorization (tc) task throughout the last ten years.
we develop an  automatic text categorization approach and investigate its application to text  retrieval.
this paper studies how link information can be used to improve classification results for web collections.
we found that pages in some  genres change rarely if at all and can be used in present-day research  experiments without requiring an updated version.
{150--162}, year = {2002}, publisher = {
in this paper, we  propose a more general formulation of text categorization, allowing documents  to be organized as \textit{sequences} of pages.
in our classifier, web documents are represented by n-grams (n$\leq 4$) that are easy to be extracted.
the input nodes of the network are words in the training texts, the nodes on the intermediate level are the training texts, and the output nodes are categories.
{2000}, number = {6}, volume = {36}, url = {}, abstract = {automatic text  categorization is an important research area and has a potential for many  text-based applications including text routing and filtering.
stretch offers ease of use and application programming and the ability to dynamically adapt to new types of documents.
detecting concept drift with support vector machines},  booktitle = {proceedings of icml-00, 17th international conference on machine  learning}, editor = {pat langley}, year = {2000}, address = {stanford, us},  pages = {487--494}, publisher = {morgan kaufmann publishers, san francisco,  us}, url =  {http://www-ai.cs.uni-dortmund.de/dokumente/klinkenberg_joachims_2000a.pdf.gz},  abstract = {
text classification using string kernels}, journal = {journal of machine  learning research}, volume = {2}, pages = {419--444}, year = {2002}, url =  {http://www.ai.mit.edu/projects/jmlr/papers/volume2/lodhi02a/lodhi02a.pdf},  abstract =
"little  work to date in sentiment analysis (classifying texts by `positive' or  `negative' orientation) has attempted to use fine-grained semantic distinctions  in features used for classification.
in order to make boosting practical for a real learning domain of thousands of words, several ways of accelerating the algorithm by reducing the feature space are studied.
the technique for machine-aided indexing (mai) developed at the defense documentation center (ddc) is illustrated on a randomly chosen abstract.
the rapid  expansion of multimedia digital collections brings to the fore the need for  classifying not only text documents but their embedded non-textual parts as  well.
we extend a multi-class categorization scheme proposed by dietterich and bakiri 1995 for binary classifiers, using error correcting codes.
we  describe {\sc adaboost.
a statistical learning approach to assigning controlled index terms  is presented.
examples are agents for locating information on world wide web and usenet news filtering agents.
= {ivan bratko and saso  dzeroski}, year = {1999}, address = {bled, sl}, publisher =
a series of experiments indicates that the  use of senses does not result in any significant categorization improvement.},  } @inproceedings{kessler97, author = {brett kessler and geoff nunberg and  hinrich sch{\"{u}}tze}, title = {automatic detection of text genre},  booktitle = {proceedings of acl-97, 35th annual meeting of the association for  computational linguistics}, publisher = {morgan kaufmann publishers, san  francisco, us}, editor =
while there are many aspects to managing corporate knowledge, one key issue is how to organize corporate documents into categories of interest.
in this work we investigate the usefulness of  $n$-grams in tc independently of any specific learning algorithm.
this combination is based on the notion of classifier reliability  and presented gains of up to 14\% in micro-averaged f1 in the web collection.
using a relatively large amount of real personal e-mail data, a comprehensive comparative study was conducted using the two classifiers.
{3}, pages = {421--439}, url = {}, abstract = {}, } @inproceedings{peng03,  author = {fuchun peng and dale schuurmans}, title = {
we explore the subsequent risk that k tuned on partitions will be suboptimal after aggregation and re-training.
proceeding of cscsi-03, 16th conference of the canadian society for computational studies of intelligence}, editor = {
} @inproceedings{seki:2005:atc,  author =
extending whirl with background knowledge for improved text classification}, journal = {information retrieval}, volume = {10}, number = {1}, pages = {35--67}, year = {2007}, month = {january} } @article{serrano:2007:eld, author = {
we describe here an n-gram-based approach to text categorization that is tolerant of textual errors.
this paper shows that the accuracy of text classifiers trained with a small number of labeled documents can be improved by augmenting this small training set with a large pool of unlabeled documents.
we show that our method is especially useful for text classification tasks involving a large number of categories and outperforms other semi-supervised learning techniques such as em and co-training.
the baldwin effect concerns the tradeoffs between learning and evolution.
in many  information retrieval and data mining applications, linear classifiers are  strongly preferred because of their ease of implementation, interpretability  and empirical performance.
experiments show that the models outperform their single-label counterparts on standard text corpora.
we attempt to  explain through experiments what factors contribute to the improvement.}, }  @inproceedings{ontrup01, author = {j{\"{o}}rg ontrup and helge ritter},  title = {text categorization and semantic browsing with self-organizing maps on  non-euclidean spaces}, booktitle = {proceedings of pkdd-01, 5th european  conference on principles and practice of knowledge discovery in databases},  editor = {
text categorization based on {k}-nearest neighbor approach for web site classification}, journal = {information processing and management}, year = {2003}, volume = {39}, number = {1}, pages = {25--44}, url = {}, abstract = {}, } @inproceedings{kwon99, author =
the measurements of performance evaluation are: classification rate,  correctness rate, and classified correctness rate.}, }  @inproceedings{joachims00, author = {thorsten joachims}, title = {
the case of contiguous subsequences is also considered for comparison with the subsequences kernel with different decay factors.
when applied to text classification, these learning algorithms lead to svms with excellent precision but poor recall.
experiments with the muc corpus suggest that case-based text classification can achieve very high levels of precision and outperforms our previous algorithms based on relevancy signatures.}, } @article{riloff94, author = {ellen riloff and wendy lehnert}, title = {information extraction as a basis for high-precision text classification}, journal = {acm transactions on information systems}, year = {1994}, number = {3}, volume = {12}, pages = {296--333}, url = {http://www.cs.utah.edu/~riloff/psfiles/single-acm.ps}, abstract = {
the bases of the cdm are research results about the way that humans learn categories and concepts vis-a-vis contrasting concepts.
comparisons with traditional rocchio's algorithm adapted for text categorization, as well as flat neural network classifiers are provided.
for these reasons, tcfp can be a useful classifier in the areas, which need a fast and high-performance text categorization task.}, } @inproceedings{ko02a, author =
"scalable term selection for text categorization", booktitle  =
analyzing predictive opinions on the web", booktitle =
the new approach is both theoretically well-founded as well as effective and efficient in practice.
{probabilistic learning for selective  dissemination of information}, journal = {information processing and  management}, pages = {633--654}, year = {1999}, number = {5}, volume = {35},  url = {http://www.cs.strath.ac.uk/~fabioc/papers/99-ipem.pdf}, abstract = {new  methods and new systems are needed to filter or to selectively distribute the  increasing volume of electronic information being produced nowadays.
this knowledge is represented using publicly available ontologies that contain hundreds of thousands of concepts, such as the open directory; these ontologies are further enriched by several orders of magnitude through controlled web crawling.
{2004}, url = {http://doi.acm.org/10.1145/967900.968026}, abstract = {
= {iterative double clustering for unsupervised and semi-supervised learning}, booktitle = {proceedings of ecml-01, 12th european conference on machine learning}, editor =
the pattern extraction is aimed at providing descriptions (in the form of two logical expressions) of the two classes of positive and negative examples.
however, there has been little firm evidence to confirm the utility of link information.
empirical evaluation indicates that the error rate (as obtained by running a plain naive bayes classifier on isolated page) can be roughly reduced by half if contextual information is incorporated.}, } @article{frasconi02, author = {paolo frasconi and giovanni soda and alessandro vullo}, title = {
combining multiple learning strategies for effective cross-validation}, booktitle = {proceedings of icml-00, 17th international conference on machine learning}, editor =
the word-pairs are selected  automatically using a technique based on frequencies of n-grams (sequences of  characters), which takes into account both the frequencies of word-pairs as  well as the context in which they occur.
"stein, sterling stuart and  argamon, shlomo and frieder, opher", title =
this article describes an approach to tc based on the integration of a training collection (reuters-21578) and a lexical database (wordnet 1.6) as knowledge sources.
both svm and knn  are tested and compared on the 20-newsgroups database.
"alm, cecilia ovesdotter and roth, dan and sproat, richard", title =
"gamon, michael", title =
we discuss  applications where our system can improve searching and filtering  capabilities.}, } @article{chakrabarti99, author = {soumen chakrabarti and  byron e. dom and s. ravi kumar and prabhakar raghavan and sridhar rajagopalan  and andrew tomkins and david gibson and jon kleinberg}, title = {
these metrics are shown to be good predictors of categorization accuracy that  can be achieved on a dataset, and serve as efficient heuristics for generating  datasets subject to user's requirements.
in this paper, we show how inverted indexes can be used  for scalable training in categorisation, and propose novel heuristics for a  fast, accurate, and memory efficient approach.
"soo-min kim and eduard hovy", title = "crystal: analyzing  predictive opinions on the web", booktitle =
"el{\'i}as f. combarro and elena monta{\~n}{\'e}s and irene d{\'i}az and jos{\'e} ranilla and ricardo mones", journal = "ieee trans. knowl.
although such learning models succeed in  exploiting relational knowledge, they are highly demanding in terms of labeled  examples, because the number of categories is related to the dimension of the  corresponding hierarchy.
systematic experiments and their results are reported and analyzed.}, } @inproceedings{chuang00, author = {wesley t. chuang and asok tiyyagura and jihoon yang and giovanni giuffrida}, title = {a fast algorithm for hierarchical text classification}, booktitle = {proceedings of dawak-00, 2nd international conference on data warehousing and knowledge discovery}, editor = {yahiko kambayashi and mukesh mohania and a.min tjoa}, year = {2000}, publisher = {springer verlag, heidelberg, de}, note = {
{seattle, us}, pages = {256--263}, url =  {http://www.cs.cmu.edu/~yiming/papers.yy/sigir95.ps}, abstract = {the paper  studies noise reduction for computational efficiency improvements in a  statistical learning method for text categorization, the linear least squares  fit (llsf) mapping.
our technique also adapts gracefully to the fraction of neighboring documents having known topics.
booktitle = {proceedings of  sigir-95, 18th acm international conference on research and development in  information retrieval}, editor =
profile is a filtering system for the netnews which uses this model with a scale of 11 predefined values of relevance.
text categorization is an interesting area for evaluating and quantifying the impact of linguistic information.
in this paper, we propose a different approach.
document classification is characterized by the large  number of attributes involved in the objects (documents).
pisa, it}, year = {2003}, pages =
{vasant honavar and giora slutzki},  year = {1998}, pages = {244--256}, publisher = {springer verlag, heidelberg,  de}, note = {
"janyce wiebe and rada mihalcea", title = "word sense and subjectivity", booktitle = "proceedings of the 21st international conference on computational linguistics and 44th annual meeting of the association for computational linguistics", year =
we conjecture that this dual feature set approach can be generalized to improve the performance of subject classification as well.}, } @inproceedings{lee02a, author = {michael d. lee}, title = {fast text classification using sequential sampling processes}, booktitle = {
one way to achieve this aim is by applying machine learning techniques to training data containing the various senses of the ambiguous words.
alexios  chouchoulas and qiang shen}, title = {rough set-aided keyword reduction for  text categorization}, journal = {applied artificial intelligence}, pages =
the algorithm first  trains a classifier using the available labeled documents, and  probabilistically labels the unlabeled documents; it then trains a new  classifier using the labels for all the documents, and iterates to convergence.
these methods share the similarity by finding hyperplanes that approximately separate a class of document vectors from its complement.
{probabilistic learning for information filtering}, booktitle = {proceedings of  riao-97, 1st international conference ``recherche d'information assistee par  ordinateur''}, editor =
"kybernetika", year = 2003, volume = 39,  number = 5, pages =
the model is evaluated on the reuters test collection and compared to the multinomial naive bayes model.
"guillaume cleuziou and celine  poudat", title =
this paper investigates automatic hierarchical categorisation and, specifically, the role of features in the development of more effective categorisers.
the  importance of text mining stems from the availability of huge volumes of text  databases holding a wealth of valuable information that needs to be mined.
technical documentation % required: title % optional: author, organization, address, edition, month, year, note % %
in this  paper schapire and singer's adaboost.
ellen m. voorhees and donna k. harman}, year = {1997}, address =  {gaithersburg, us}, pages = {619--621}, url =  {http://trec.nist.gov/pubs/trec6/papers/siemens.ps.gz}, abstract = {
the method combines information derived from  n-grams (consecutive sequences of n characters) with a simple vector-space  technique that makes sorting, categorization, and retrieval feasible in a large  multilingual collection of documents.
experimental comparisons of the performance of the kernel compared with a standard word feature space kernel (joachims, 1998) show positive results on modestly sized datasets.
however, a first-order  representation seems to be advantageous when high-precision classifiers are  desirable.}, } @inproceedings{cohen96a, author = {william w. cohen and yoram  singer}, title = {context-sensitive learning methods for text categorization},  booktitle = {proceedings of sigir-96, 19th acm international conference on  research and development in information retrieval}, editor =
once the optimal number of is selected, for eac ter, the procedure  is repeated.
classification of documents allow the automatic distribution or  archiving of letters and is also an excellent starting point for higher-level  document analysis.}, } @article{hoyle73, author = {w.g. hoyle}, title =  {automatic indexing and generation of classification by algorithm}, journal =
we propose an approach  that utilizes the hierarchical topic structure to decompose the classification  task into a set of simpler problems, one at each node in the classification  tree.
"157--158",  abstract = "automatic text classification is an important operational  problem in digital library practice.
the article describes a pilot version of a commercial application of natural language processing techniques to the problem of categorizing new stories into broad topic categories.
{843--848}, url = {},  abstract = {a language-independent means of gauging topical similarity in  unrestricted text is described.
it not only approaches and  sometimes exceeds svm accuracy, but also beats svm running time by orders of  magnitude.
{new orleans, us}, year = {2001}, pages = {128--136}, url =  {http://www.cs.cornell.edu/people/tj/publications/joachims_01a.pdf}, abstract =  {this paper develops a theoretical learning model of text classification for  support vector machines (svms).
the links between nodes are computed based on statistics of the word distribution and the category distribution over the training set.
proceedings of the 14th australian joint conference on artificial intelligence}, editor =
results obtained by using a set of 2,344 medline documents are presented and discussed.}, } @inproceedings{ruiz99, author =
in this paper, we describe an active learning method based on query by committee (qbc) that reduces the number of labeled training examples (text documents) required for learning by 1-2 orders of magnitude.}, } @inproceedings{lim99, author = {
we present such an approach - a sparse network of linear separators, utilizing the winnow learning algorithm - and show how to use it in a variety of ambiguity resolution problems.
kristina toutanova and francine chen and kris popat and thomas hofmann}, title = {
based on this understanding, we  present solutions inspired by round-robin scheduling that avoid this pitfall,  without resorting to costly wrapper methods.
using the same representation of categories, experiments show a significant improvement when the above mentioned method is used.
preliminary experiments with 1998 darpa bsm audit data show that the knn classifier can effectively detect intrusive attacks and achieve a low false positive rate.}, } @article{liddy94, author = {elizabeth d. liddy and woojin paik and edmund s. yu}, title = {
in this paper, a bayesian inference network model for automatic  indexing with index terms (descriptors) from a prescribed vocabulary is  presented.
the paper  demonstrates good performance of context-group discrimination for a sample of  natural and artificial ambiguous words.}, } @mastersthesis{scott98, author =
in order to verify our methods, we test a large  body of tagged japanese newspaper articles created by rwcp.
"the utility of information extraction in the classification of books", booktitle =
these algorithms make  extremely fast decisions, because they need to examine only a small number of  words in each text document.
support vector machines  provide the best accuracy on test data.}, } @article{skarmeta00, author =
textual information is processed by two methods of analysis: a natural language analysis followed by a statistical analysis.
we describe in detail an implementation, called boostexter, of the new boosting algorithms for text categorization tasks.
the vast majority of them represent cases that can only be fully categorized with the assistance of a human subject (because, for instance, they require specific knowledge of a given pathology).
this can be considered as the effective combination of documents with no topic or class labels (unlabeled data), labeled documents, and prior domain knowledge (in the form of the known hierarchic structure), in providing enhanced document classification performance.}, } @inproceedings{vinot03, author = {
the chain augmented naive bayes classifiers we propose have two advantages over standard naive bayes classifiers.
parameters empirically, whereas our previous approach required the  specification of two parameters (beta and gamma).
the world wide web is a vast repository of information, but the sheer volume makes it difficult to identify useful documents.
neural networks were developed and examined for this topic since they  support robustness and learning in noisy unrestricted real-world texts.
with the knn classifier, the  frequencies of system calls are used to describe the program behavior.
this paper focuses on the application of mixtures of multivariate bernoulli distributions to binary data.
the investigation includes  different attribute and distance-weighting schemes, and studies on the effect  of the neighborhood size, the size of the attribute set, and the size of the  training corpus.
surprisingly, this unsupervised procedure can be competitive with a (supervised) svm trained with a small training set.
however, problem solving usually  involves both a domain and a task.
text categorization for a comprehensive time-dependent benchmark}, journal = {information processing and management}, year = {2004}, volume = {40}, number = {2}, pages = {209--221}, url = {}, abstract = {}, } @article{dasigi01, author = {dasigi, venu and mann, reinhold c. and protopopescu, vladimir a.}, title = {information fusion for text classification: an experimental comparison}, journal = {pattern recognition}, year = {2001}, volume = {34}, number = {12}, pages = {2413--2425}, url = {}, abstract = {
we also analyse the relationships of news headlines and their contents of the new reuters corpus by a series of experiments.
we then present a fast, divisive algorithm that monotonically decreases this objective function value, thus converging to a local minimum.
the previous works in this area have used a large  number of labeled training documents for supervised learning.
experimental results for the pure theoretical model as well as for heuristic variants are given.
we argue that this evaluation measure is also  very well suited for text categorization tasks.
while hand-crafting rules for  both tasks has a long tradition, learning approaches used to gain much interest  in the past.
this approach builds on previous thresholding work based upon the beta-gamma algorithm.
it combines a well known feature selection criterion, the information gain, and a new algorithm that selects and adds a feature to a bag-of-words if it does not occur too often with the features already in a small set composed of the best features selected so far for their high information gain.
a text is  represented as a set of cases and we classify a text as relevant if any of its  cases is deemed to be relevant.
owing to the use of context-sensitive features,  the classifier is very accurate.
in this paper we address the problem of exploiting  the potential of weighted representations in the context of  \textsc{adaboost}-like algorithms by discretizing the continuous attributes  through the application of entropy-based discretization methods.
the feature sets are based on the ``latent semantics'' of a reference library - a collection of documents adequately representing the desired concepts.
we compare, for text categorization, two partially  supervised (or semi-supervised) clustering algorithms: the semi-supervised  agglomerative hierarchical clustering (ssahc) algorithm (a. amar et al., 1997)  and the semi-supervised fuzzy-c-means (ssfcm) algorithm (m. amine et al.,  1996).
in this paper, we introduce a new weighting method based on statistical estimation of the importance of a word for a specific categorization problem.
they allow us to construct compact feature sets  with few elements, with which a satisfactory genre diversi- fication is  achieved.
the feature quantity, a quantitative representation of specificity introduced in this paper, is based on an information theoretic perspective of co-occurrence events between terms and documents.
we view  this result as a confirmation of the usefulness of classifiers that represent  contextual information.}, } @inproceedings{crammer02, author = {koby crammer  and yoram singer}, title = {a new family of online algorithms for category  ranking}, booktitle = {proceedings of sigir-02, 25th acm international  conference on research and development in information retrieval}, editor =
yan liu and jaime carbonell and rong jin}, title = {a new pairwise ensemble approach for text classification}, booktitle = {proceedings of ecml-03, 14th european conference on machine learning}, publisher = {}, editor = {}, year = {2003}, address = {dubrovnik, hk}, pages = {}, url = {}, abstract = {}, } @article{lodhi02, author = {huma lodhi and craig saunders and john shawe-taylor and nello cristianini and chris watkins}, title =
published in the ``lecture notes in  computer science'' series, number 2291}, pages = {248--267}, url =  {http://link.springer.de/link/service/series/0558/papers/2291/22910248.pdf},  abstract =
{59--72}, url =  {http://www.cs.uiowa.edu/~mruiz/papers/sigcr97/sigcrfinal2.html}, abstract =  {this paper presents the results obtained from a series of experiments in  automatic text categorization of medline articles.
a problem in the use of both algorithms is that they require documents to be represented by binary vectors, indicating presence or absence of the terms in the document.
kalervo j{\"{a}}rvelin and james allan and peter bruza and mark sanderson}, publisher = {acm press, new york, us}, address = {sheffield, uk}, year = {2004}, pages = {234--241}, url = {http://doi.acm.org/10.1145/1008992.1009034}, abstract = {this paper explores feature scoring and selection based on weights from linear classification models.
"adapting the naive bayes classifier to rank procedural texts", booktitle =
proceedings of icml-00, 17th international conference on machine learning}, editor =
the links  between nodes are computed based on statistics of the word distribution and the  category distribution over the training set.
{proceedings of cikm-99, 8th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, editor = {}, year = {1999}, address = {kansas city, us}, pages = {114--121}, url = {http://www.acm.org/pubs/articles/proceedings/cikm/319950/p114-hsu/p114-hsu.pdf},
previous researches have investigated the use of $n$-grams (or some variant of them) in the context of specific learning algorithms, and thus have not obtained general answers on their usefulness for tc.
based on the  darmstadt indexing approach, the indexing task is divided into a description  step and a decision step.
in addition, more than 80\% of automatically extracted terms are meaningful.
hwee t. ng and wei b. goh  and kok l. low}, title = {feature selection, perceptron learning, and a  usability case study for text categorization}, booktitle = {proceedings of  sigir-97, 20th acm international conference on research and development in  information retrieval}, editor = {nicholas j. belkin and a. desai narasimhalu  and peter willett}, publisher = {acm press, new york, us}, year = {1997},  address = {philadelphia, us}, pages = {67--73}, url =  {http://www.acm.org/pubs/articles/proceedings/ir/258525/p67-ng/p67-ng.pdf},  abstract = {
we extracted the words around the gene occurrences and used them to represent the gene for go domain code annotation.
in our experiments we compare the effectiveness of the svm -based feature selection with that of more traditional feature selection methods, such as odds ratio and information gain, in achieving the desired tradeoff between the vector sparsity and the classification performance.
"yiming yang and shinjae yoo and jian zhang and bryan kisiel", title =
exploiting thesaurus knowledge in rule induction for text classification}, booktitle = {
luc de raedt and peter a. flach}, publisher = {springer verlag, heidelberg, de}, address = {freiburg, de}, year = {2001}, pages = {121--132}, note = {
upon close examination of the  algorithm, we concluded that the algorithm is most successful in correctly  classifying more positive documents, but may cause more negative documents to  be classified incorrectly.}, } @inproceedings{taskar01, author = {benjamin  taskar and eran segal and daphne koller}, title = {probabilistic classification  and clustering in relational data}, booktitle = {
ah-hwee tan}, title = {predictive self-organizing networks for text  categorization}, booktitle = {proceedings of pakdd-01, 5th pacific-asia  conferenece on knowledge discovery and data mining}, editor = {david cheung and  qing li and graham williams}, year = {2001}, publisher =
a classifier was constructed applying the above features to complement the knn classifier.
abis minimizes user's effort in selecting the huge amount of available documents.
the focus is on aggressive dimensionality reduction.
seattle, us}, year = {2001}, pages = {870--878}, url =  {http://robotics.stanford.edu/~btaskar/pubs/ijcai01.ps}, abstract = {supervised  and unsupervised learning methods have traditionally focused on data consisting  of independent instances of a single type.
we provide experimental results on a webpage classification task, showing that accuracy can be significantly improved by modeling relational dependencies.}, } @article{tauritz00, author = {daniel r. tauritz and joost n. kok and ida g. sprinkhuizen-kuyper}, title = {
our evaluation using the 1996 reuters corpus which consists of  806,791 documents shows that automatically constructing hierarchy improves  classification accuracy."
they use credible  knowledge resources, including a us government organizational hierarchy, a  thematic hierarchy from the open directory project (odp) web directory, and  personal browse histories, to add valuable metadata to search results.
in this paper we study the use of a semi-supervised agglomerative hierarchical clustering (ssahc) algorithm to text categorization, which consists of assigning text documents to predefined categories.
"viktor pekar, richard evans and ruslan mitkov", title =
"tuning  jensen-renyi divergences with statistically similar examples for unsupervised  document categorization via iterative denoising trees", booktitle =  "proceedings of the annual conference of the north american chapter of the  association for computational linguistics", year = "2007", month  = "april", address =
since this criterion provides a good upper bound of the generalization error.
however the existing clustering, techniques are agglomerative in nature and result in (i) suboptimal word clusters and (ii) high computational cost.
experimental results indicate  that the mfom classifier gives improved f1 and enhanced robustness over the  conventional one.
this knowledge-engineering bottleneck must be addressed before  knowledge-based systems will be practical for real-world applications.
{267--268}, url =  {http://www.acm.org/pubs/articles/proceedings/ir/312624/p267-lewis/p267-lewis.pdf},  abstract = {numerous systems for ranked retrieval on text databases have been  implemented by both information retrieval researchers and in the commercial  sector.
"generalized {lars} as an effective feature selection tool for text classification with {svm}s", booktitle =
after investigating two similarity-based  classifiers (k-nn and rocchio) and three common thresholding techniques (rcut,  pcut, and scut), we describe a new learning algorithm known as the keyword  association network (kan) and a new thresholding strategy (rinscut) to improve  performance over existing techniques.
"xiaoguang  qi and brian davison", title =
in this paper we  describe how the lsi approach can be implemented in a kernel-defined feature  space.
our technique also adapts  gracefully to the fraction of neighboring documents having known topics.
and shiang chen liew}, title = {
we  see genre classification as a powerful instrument to bring web-based search  services closer to a user's information need.
"{ocfs}: optimal  orthogonal centroid feature selection for text categorization", pages =  "122--129", booktitle =
the approach presented here represents a compromise between keyword-based techniques and in-depth natural language processing.
overall, we show that by  using a few terms, categorisation accuracy can be improved substantially:  unstructured leaf level categorisation can be improved by up to 8.6\%, while  top-down hierarchical categorisation accuracy can be improved by up to 12\%.
in previous research, lsi has produced a small improvement in retrieval performance.
we use the classification on keywords as the baseline, which we compare with the contribution of the pure hm pairs to classification accuracy, and the incremental contributions from heads and modifiers.
"combined syntactic and semanitc kernels for text classification", booktitle = "proceedings of the 29th european conference on information retrieval", year =
we show that the results  outperform competing methods.
in many cases, users would like to search for information of a certain  'object', rather than a web page containing the query terms.
"transferring naive bayes classifiers for text classification", booktitle =
instead of replacing knowledge-based methods with statistics, statistical training replaces knowledge engineering.
techreport % = a report published by a school or other institution, % usually numbered within a series % required: author, title, institution, year % optional: type, number, address, month, note % % unpublished % = a document with an author and title, but not formally published % required: author, title, note % optional: month, year %
these search engines are, however, unsuited for a wide range of equally important tasks.
the indexing dictionary can be derived  automatically from a set of manually indexed documents.
the former approach has been favored because of the tendency of people in the computer field to strive for new methods of dealing with the literature -- methods which do not resemble those of traditional libraries.
although most research on mixture models has concentrated on mixtures for continuous data, emerging pattern recognition applications demand extending research efforts to other data types.
we provide background,  present procedures for building metaclassifiers that take into consideration  both reliability indicators and classifier outputs, and review a set of  comparative studies undertaken to evaluate the methodology.}, }  @inproceedings{bickel04, author = {steffen bickel and tobias scheffer}, title =
"computational linguistics and intelligent text processing (lecture notes in computer science, vol. 2945)", year =
however, when lsi is used is conduction with statistical  classification, there is a dramatic improvement in performance.} }  @inproceedings{hull96, author = {david a. hull and jan o. pedersen and hinrich  sch{\"u}tze}, title = {method combination for document filtering},  booktitle = {proceedings of sigir-96, 19th acm international conference on  research and development in information retrieval}, editor = {hans-peter frei  and donna harman and peter sch{\"{a}}uble and ross wilkinson}, publisher =
in addition, we investigate alternative classification and evaluation methods, and the effect that a secondary feature can have on indoor/outdoor classification.
when user's interests change, pva, in not only the contents, but also in the structure of user profile, is modified to adapt to the changes.
experimental results indicate that, at the same level of vector sparsity, feature selection based on svm normals yields better classification performance than odds ratio- or information gainbased feature
categorical points to each category are computed by summing the  frequency of each keyword from back data, or the number of documents from it.
alexander f. gelbukh}, publisher = {springer verlag, heidelberg, de}, address = {seoul, ko}, note = {published in the ``lecture notes in computer science'' series, number 2945}, pages = {559--570}, url = {}, abstract = {}, } @inproceedings{guthrie94, author = {louise guthrie and elbert walker and joe a. guthrie}, title = {document classification by machine: theory and practice}, booktitle = {proceedings of coling-94, 15th international conference on computational linguistics}, publisher = {},
however, it does show that tfidf conversion and document length normalization are important.
dunja mladeni{\'{c}}}, title = {
the results obtained by ripper surpass those of the  operational process.}, } @inproceedings{tong00, author = {simon tong and daphne  koller}, title = {support vector machine active learning with applications to  text classification}, booktitle = {proceedings of icml-00, 17th international  conference on machine learning}, editor = {pat langley}, year = {2000}, address  = {stanford, us}, pages = {999--1006}, publisher =
page variation is more prodigious than the  data's raw scale:
in the test of the expert network method on cacm documents, for example, an 87\% removal of unique words reduced the vocabulary of documents from 8,002 distinct words to 1,045 words, which resulted in a 63\% time saving and a 74\% memory saving in the computation of category ranking, with a 10\% precision improvement, on average, over not using word removal.
integrating linguistic resources in tc through wsd},  journal = {computers and the humanities}, year = {2001}, number = {2}, volume =  {35}, pages = {215--230}, url = {http://www.wkap.nl/article.pdf?266250},  abstract = {information access methods must be improved to overcome the  information overload that most professionals face nowadays.
the category discrimination method (cdm) is a new learning algorithm designed for text categorization.
{acm press, new york, us}, editor = {henrique paques and ling liu and david grossman}, year = {2001}, address = {atlanta, us}, pages = {365--370}, url = {http://doi.acm.org/10.1145/502585.502647}, abstract = {
therefore, this method can be used in areas where low-cost text categorization is needed.
experiments with simulated concept drift scenarios based on real-world text data compare the new method with other window management approaches.
we carry out experiments over two different corpora and find that  the proposed measures perform better than the existing ones."
% % % % % originally created by % % %
{learning rules for large vocabulary word sense disambiguation}, booktitle =  {proceedings of ijcai-99, 16th international joint conference on artificial  intelligence}, editor = {thomas dean}, publisher = {
"s. sathiya keerthi", title =
k-nearest-neighbour, relevance feedback, and bayesian independence classifiers were applied individually and in combination.
"proceedings of the 29th european conference on information  retrieval", year =
the encyclopedia of database technologies and applications}, publisher =
one problem is that it is difficult to create the labeled training documents.
"we demonstrate the value of using context in a new-information  detection system that achieved the highest precision scores at the text  retrieval conference's novelty track in 2004.
we evaluate the algorithms on the reuters-21578 corpus and the new corpus released by reuters in 2000.
kalervo j{\"{a}}rvelin and james allan and peter bruza and mark  sanderson}, publisher = {acm press, new york, us}, address = {sheffield, uk},  year = {2004}, pages = {250--257}, url =  {http://www.cs.technion.ac.il/~gabr/papers/accio.pdf}, abstract = {
our experiments demonstrate that this new approach is able to learn more accurate classifiers than either of its constituent methods alone.}, } @inproceedings{craven98, author = {mark craven and dan dipasquo and dayne freitag and andrew k. mccallum and tom m. mitchell and kamal nigam and se{\'{a}}n slattery}, title = {learning to extract symbolic knowledge from the world wide web}, booktitle = {proceedings of aaai-98, 15th conference of the american association for artificial intelligence}, publisher = {aaai press, menlo park, us}, year = {1998}, pages = {509--516}, address = {madison, us}, note
european conference on information retrieval}, publisher = {springer verlag}, editor = {fabrizio sebastiani}, address = {pisa, it}, year = {2003}, pages = {408--419}, url = {http://link.springer.de/link/service/series/0558/papers/2633/26330408.pdf}, abstract = {
in this  categorization process recall is considered more important than precision.
cc and or are one-sided metrics  while ig and chi are two-sided.
our approach to wsd is also based on the integration of two linguistic resources: a training collection (semcor and reuters-21578) and a lexical database (wordnet 1.6).}, } @inproceedings{vert01, author = {jean-philippe vert}, title = {
"text classification by labeling  words", booktitle =
we use support vector machine (svm) classifiers, which have been shown to be efficient and effective for classification, but not previously explored in the context of hierarchical classification.
{las vegas, us}, pages = {81--93}, url =  {http://www.research.att.com/~lewis/papers/lewis94b.ps}, abstract = {
= {kansas city, us}, pages = {475--482}, url = {http://ls6-www.informatik.uni-dortmund.de/ir/publications/1999/goevert_etal:99.html}, abstract = {
for the svm implementation we used both a version of schoelkopf et al.
in this paper, we describe an automated learning approach to text  categorization based on perceptron learning and a new feature selection metric,  called correlation coefficient.
editor = {}, year  = {2003}, address = {grenoble, fr}, pages = {118--120}, url =  {http://doi.acm.org/10.1145/958220.958242}, abstract = {
improving performance of text categorization by combining filtering and  support vector machines}, journal = {journal of the american society for  information science and technology}, year = {2004}, volume = {55}, number =
the first is an ontology that defines the classes (e.g., company,  person, employee, product) and relations (e.g., employed_by, produced_by) of  interest when creating the knowledge base.
"{sigkdd}  explorations", pages =
we describe the email classification experiments we have carried out and discuss the development of customer services based on automatic email classification."
combining link-based and content-based methods for web document classification}, booktitle = {proceedings of cikm-03, 12th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, editor = {}, year = {2003}, address = {new orleans, us}, pages = {394--401}, url = {http://doi.acm.org/10.1145/956863.956938}, abstract = {
{2}, pages = {209--221}, url = {}, abstract = {}, } @article{dasigi01, author =
we then analyzed the two maps to obtain the categories and the structure among them.
classification is a function that matches a new object with one of the predefined classes.
in addition, topic hierarchies can be utilized to overcome the sparseness problem in text categorization with a large number of categories, which is the main focus of this paper.
furthermore, the online approach offers the advantage of continuous learning in the batch-adaptive text filtering task.}, } @inproceedings{chakrabarti02, author = {soumen chakrabarti and shourya roy and mahesh soundalgekar}, title = {fast and accurate text classification via multiple linear discriminant projections}, booktitle = {proceedings of vldb-02, 28th international conference on very large data bases}, publisher = {}, editor = {}, year = {2002}, address = {hong kong, cn}, pages = {658--669}, url = {http://www.vldb.org/conf/2002/s19p01.pdf}, abstract = {support vector machines (svms) have shown superb performance for text classification tasks.
experimental results show that (1) using an enhanced representation of web documents is crucial for an effective categorisation of web documents, and (2) a theoretical interpretation of the k-nearest neighbour classifier gives us improvement over the standard k-nearest neighbour classifier.}, } @inproceedings{goldberg95, author = {goldberg, jeffrey l.}, title = {cdm: an approach to learning in text categorization}, booktitle = {proceedings of ictai-95, 7th international conference on tools with artificial intelligence}, publisher = {ieee computer society press, los alamitos, us}, editor = {}, address = {herndon, us}, year = {1995}, pages = {258--265}, url = {}, note = {an extended version appears as~\cite{goldberg96}}, abstract = {
with the knn classifier, the frequencies of system calls are used to describe the program behavior.
"acm press", url =  "http://doi.acm.org/10.1145/1099554.1099591", abstract =
this set of relevant features varies widely throughout the hierarchy, so  that, while the overall relevant feature set may be large, each classifier only  examines a small subset.
forn samples.}, } @inproceedings{ciravegna99, author = {fabio ciravegna and alberto lavelli and nadia mana and johannes matiasek and luca gilardoni and silvia mazza and william j. black and fabio rinaldi}, title = {facile: classifying texts integrating pattern matching and information extraction}, booktitle = {proceedings of ijcai-99, 16th international joint conference on artificial intelligence}, editor = {thomas dean}, publisher = {morgan kaufmann publishers, san francisco, us}, year = {1999}, pages = {890--895}, address = {stockholm, se}, url = {http://ecate.itc.it:1024/lavelli/lavelli-papers/ijcai99/ijcai99.ps.gz}, abstract = {
previous research on automated text categorization has mixed machine learning and knowledge engineering methods, making it difficult to draw conclusions about the performance of particular methods.
application of feature subset selection techniques improves the performance.
the algorithm is automatic and unsupervised  in both training and application: senses are induced from a corpus without  labeled training instances or other external knowledge sources.
in this paper, we focus on using user assisted text classification in conjunction with a web portal, multiple document management systems and an ontology, to provide a powerful solution for organizing information about a company's technology.
dunja mladeni{\'{c}} and marko grobelnik}, title = {
{105--113}, url = {http://www.cs.utah.edu/~riloff/psfiles/cikm93-w-addend.ps},  abstract = {research on text classification has typically focused on keyword  searches and statistical techniques.
this has important implications for document classification when a hierarchic ordering of topics exists.
a relatively high degree of accuracy was achieved by the  supervised method, however, classification accuracy varied across classes.
results on a real-world  text datasets show that these learners may substantially benefit from using a  large amount of unlabeled documents in addition to some labeled documents.}, }  @inproceedings{larkey96, author = {leah s. larkey and w. bruce croft}, title =
in a candidate feature set consisting of chinese character bigrams, there exist a number of bigrams which are high-degree biased according to character frequencies.
"bremen, germany", publisher = "acm press", url = "http://doi.acm.org/10.1145/1099554.1099734", abstract =
the second concerns the infoagent, a system for supporting users in retrieving data in distributed and heterogeneous archives and repositories.
this chapter will outline the fundamental traits of the technologies involved, of the applications that can feasibly be tackled through text classification, and of the tools and resources that are available to the researcher and developer wishing to take up these technologies for deploying real-world applications.}, } @incollection{sebastiani05a, author = {fabrizio sebastiani}, title = {
{gongde guo and hui wang and  david a. bell and yaxin bi and kieran greer}, title = {an knn model-based  approach and its application in text categorization}, booktitle = {proceedings  of cicling-04, 5th international conference on computational linguistics and  intelligent text processing}, year = {2004}, editor = {alexander f. gelbukh},  publisher =
this objective raises two questions: (1) what are useful genres when searching the www?
via giovanni battista belzoni, 7 - 35131 padova, italy % % http://www.math.unipd.it/~fabseb60/ %
in  the first approach, the content (eg., text) plays an important role, while in  the second approach, the existence of several knowledge sources (eg., several  users) is required.
we evaluate the  algorithms on the basis of two test sets from the muc-4 corpus.
proceedings of iea/aie-96, 9th international conference in industrial and engineering applications of artificial intelligence and expert systems}, editor
text learning  and related intelligent agents: a survey}, journal = {ieee intelligent  systems}, year = {1999}, number = {4}, volume = {14}, pages = {44--54}, url =  {http://www-ai.ijs.si/dunjamladenic/papers/pww/agentoverieee.ps.gz}, abstract =  {analysis of text data using intelligent information retrieval, machine  learning, natural language processing or other related methods is becoming an  important issue for the development of intelligent agents.
this paper investigates the use of supervised clustering in order to create sets of categories for classification of documents.
we experimentally evaluate the quality of the lower dimensional spaces both in the context of document categorization and improvements in retrieval performance on a variety of different document collections.
it employs a combination of technologies that takes the results of queries to networked information sources and, in real-time, automatically retrieve, parse and organize these documents into coherent categories for presentation to the user.
profile allows the user to update on-line his  profile and to check the discrepancy between his assessment and the prediction  of relevance of the system.
{analia amandi and ricardo zunino}, year = {1999}, address = {buenos aires,  ar}, pages = {7--35}, url =  {http://www.math.unipd.it/~fabseb60/publications/asai99.pdf}, note = {
because text domains present much irrelevant information, effective feature reduction is essential to improve classifiers' effectiveness and efficiency.
{morgan kaufmann publishers, san francisco, us}, url = {http://www.cs.cmu.edu/~rayid/mypapers/ecoc-icml.ps}, abstract = {we report the results of a study on topic spotting in conversational speech.
the algorithm is based on the multiperturbation shapley analysis, a framework which relies on game theory to estimate usefulness.
{209--228}, url =  {http://trec.nist.gov/pubs/trec1/papers/17.txt}, abstract = {describes an  approach to document routing on the trec corpus that employs a technique for  the automatic construction of classification trees.
using text classifiers for numerical classification},  booktitle =
{acm press, new york, us}, editor = {georges gardarin and james c. french and  niki pissinou and kia makki and luc bouganim}, year = {1998}, address =
non-textual information is processed by a symbolic learning technique.
{st-malo, fr}, pages = {}, year = {2002}, url =  {http://www.cavi.univ-paris3.fr/lexicometrica/jadt/jadt2002/pdf-2002/gomez_debuenaga_urena_martin_garcia.pdf},  abstract = {automatic text categorization (atc) is an important task in the  field of information access.
it requires an indexing dictionary with rules mapping terms of the respective subject field onto descriptors and inverted lists for terms occurring in a set of documents of the subject field and descriptors manually assigned to these documents.
we conclude by examining factors  that make the sentiment classification problem more challenging.}, }  @article{park04, author = {seong-bae park and byoung-tak zhang}, title =  {co-trained support vector machines for large scale unstructured document  classification using unlabeled data and syntactic information}, journal =
the algorithm's predictive accuracy is competitive with  other recently introduced hierarchical multi-category or multilabel  classification learning algorithms."
we use a simple perceptron to optimize the relative emphasis of each semantic class in the tracking and detection decisions.
we  benchmark several widely used supervised learning methods on rcv1-v2,  illustrating the collection's properties, suggesting new directions for  research, and providing baseline results for future studies.
such a scheme has several potential advantages because it does not require any pre-processing of the input text.
drawing on interviews with reuters personnel and access to reuters documentation, we describe the coding policy and quality control procedures used in producing the rcv1 data, the intended semantics of the hierarchical category taxonomies, and the corrections necessary to remove errorful data.
the model is based on the concept of ``uncertainty sampling'', a technique that allows for relevance feedback both on relevant and non relevant documents.
two problems arise from this basic  approach.
using synthetically generated data we empirically demonstrate that whenever the dc procedure is successful in recovering some of the structure hidden in the data, the extended idc procedure can incrementally compute a dramatically better classification, with minor additional computational resources.
{pittsburgh, us}, year = {2001}, url =  {http://www-2.cs.cmu.edu/~knigam/papers/thesis-nigam.pdf}, abstract = {one key  difficulty with text classification learning algorithms is that they require  many hand-labeled examples to learn accurately.
training algorithms for  linear text classifiers}, booktitle = {proceedings of sigir-96, 19th acm  international conference on research and development in information retrieval},  editor =
tc is the  classification of documents into a predefined set of categories.
= {gang wang and frederick h. lochovsky}, title = {feature selection with conditional mutual information maximin in text categorization}, booktitle = {proceedings of cikm-04, 13th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, address = {washington, us}, editor = {david a. evans and luis gravano and otthein herzog and chengxiang zhai and marc ronthaler}, year = {2004}, pages = {342--349}, url = {}, abstract = {}, } @inproceedings{wang99, author = {hui wang and nguyen h. son}, title = {
a new selection technique, scar, is proposed for k-dnf (disjunctive normal form) learners and evaluated on the reuters financial data set.
"kardkovacs, z. t. and tikk, d. and bansaghi, z.", title =
} @article{koppel:2007:mdu, author = {
in our system, the user navigates through the query response not as a flat unstructured list, but embedded in the familiar taxonomy, and annotated with document signatures computed dynamically with respect to where the user is located at any time.
compared to a previously tested naive  bayes filter, the memory-based filter performs on average better, particularly  when the misclassification cost for non-spam messages is high.}, }  @inproceedings{sasaki98, author = {minoru sasaki and kenji kita}, title =
we propose a novel fs technique, based on a simplified variant of the $\chi^2$ statistics.
"automatic learning  features using bootstrapping for text categorization", booktitle =  "computational linguistics and intelligent text processing (lecture notes  in computer science, vol. 2945)", year =
"dell zhang and xi chen and lee, wee sun", title =
text  categorization with support vector machines: learning with many relevant  features}, booktitle = {proceedings of ecml-98, 10th european conference on  machine learning}, publisher = {springer verlag, heidelberg, de}, note =
{38}, number = {4}, pages = {583--604}, url = {}, abstract = {
in two experiments, the research method of  information integration theory was employed in order to test two hypotheses  relating to the radical conventionalist and traditional positions on the role  of specific formal textual features in the categorization of poetic texts.
we describe an investigation into e-mail content mining for author identification, or authorship attribution, for the purpose of forensic investigation.
"november", address =
the effects of these considerations are examined in the  experiments using reuters-21578 and ntcir-j1 standard test collections.}, }  @inproceedings{alias02, author = {francesc al{\'i}as and ignasi iriondo and  pere barnola}, title = {multi-domain text classification for unit selection  text-to-speech synthesis}, booktitle = {proceedings of icphs-03, 15th
year = {2004}, editor = {
tc has been an application for many learning approaches.
"valerio  freschi and andrea seraghiti and alessandro bogliolo", title =
we introduce a new algorithm that incrementally learns a linear-threshold classifier for each node of the taxonomy.
can models have two advantages over standard naive bayes classifiers.
this paper presents the design and evaluation of a text categorization method based on the hierarchical mixture of experts model.
we adopt an established statistical technique called shrinkage that smooths parameter estimates of a data-sparse child with its parent in order to obtain more robust parameter estimates.
"recomputation of class relevance scores for improving text classification", booktitle =
{published in the ``lecture notes in computer science'' series, number 1293}, url = {}, abstract = {
we implemented our classification scheme using decision  tree classifiers and self-organizing maps.}, } @inproceedings{siersdorfer04,  author = {stefan siersdorfer and sergej sizov and gerhard weikum}, title =  {goal-oriented methods and meta methods for document classification and their  parameter tuning}, booktitle = {proceedings of cikm-04, 13th acm international  conference on information and knowledge management}, publisher = {acm press,  new york, us}, address = {washington, us}, editor = {david a. evans and luis  gravano and otthein herzog and chengxiang zhai and marc ronthaler}, year =
using  movie reviews as data, we find that standard machine learning techniques  definitively outperform human-produced baselines.
however, the effect of boosting for rare categories varies across classifiers: for svms and decision trees, we achieved a 13-17\% performance improvement in macro-averaged f1 measure, but did not obtain substantial improvement for the other two classifiers.
booktitle = {proceedings of wcnn-93, world congress on neural networks}, publisher = {}, editor = {}, year = {1993}, address = {portland, us}, pages =
this paper introduces a multistrategy learning approach to the  categorization of text documents.
{215--230}, url = {http://www.wkap.nl/article.pdf?266250}, abstract = {information access methods must be improved to overcome the information overload that most professionals face nowadays.
{63--69}, url = {http://www.research.ibm.com/dar/papers/pdf/ieee99_mtmp.pdf},  abstract = {
the text categorization system teklis at {trec-6}}, booktitle = {proceedings of trec-6, 6th text retrieval conference}, publisher = {national institute of standards and technology, gaithersburg, us}, editor = {
we use a simple perceptron to optimize the relative emphasis of each  semantic class in the tracking and detection decisions.
this study indicates that, while some document  categorization algorithms could be adopted for database categorization,  algorithms that take into consideration the special characteristics of  databases may be more effective.
this paper presents an examination of the effect of thresholding strategies on the performance of a classifier under various conditions.
in addition, in this paper, we describe a new threshold relaxation  algorithm.
in this paper, a study  on parameters of the rocchio text classifier has been carried out to achieve  its maximal accuracy.
link information  alone is able to obtain gains of up to 46 points in f1, when compared to a  traditional content-based classifier.
"using the  essence of texts to improve document classification", booktitle =  "recent advances in natural language processing", year =  "2005", month =
we evaluate the inequality me models on text categorization datasets, and demonstrate their advantages over standard me estimation, similarly motivated gaussian map estimation of me models, and support vector machines (svms), which are one of the state-of-the-art methods for text categorization.} } @article{kim:2005:drt, author = {
we identify  document genre is an important factor in retrieving useful documents and focus  on the novel document genre dimension of subjectivity.
the average f1 value of our two submitted  solutions is 94.4% higher than the average f1 value from all other submitted  solutions."
it is built on top of a text-categorization paradigm where text articles are annotated with keywords organized in a hierarchical structure.
"august", address = "bonn, germany", url = "http://www.machinelearning.org/proceedings/icml2005/papers/086_handlingapproximate_ramakrishanetal.pdf", abstract =
"ganesh ramakrishnan and chitrapura, krishna prasad and raghu krishnapuram and pushpak bhattacharyya", title =
booktitle = {proceedings  of ranlp-97, 2nd international conference on recent advances in natural  language processing}, publisher = {}, editor = {ruslan milkov and nicolas  nicolov and nilokai nikolov}, address = {tzigov chark, bl}, pages = {202--207},  year = {1997}, url = {http://www.dfki.uni-kl.de/~junker/download/ranlp97.ps},  abstract = {systems for learning text classifiers recently gained considerable  interest.
we propose to use the  following techniques to improve the classification performance of the baseline  method: (1) use routing (classification) accuracy and the similarity values to  refine the training set; (2) update the underlying term structures periodically  during testing; and (3) apply k-means clustering to partition the newsgroup  articles and represent each newsgroup by k vectors.
"sentiment classification is a recent subdiscipline classification which  is concerned not with the topic is about, but with the opinion it expresses.
the euratom automatic indexing project}, booktitle = {proceedings of  the ifip congress (booklet j)}, publisher = {}, editor = {}, year = {1968},  address = {edinburgh, uk}, pages = {66--70}, url = {}, abstract = {}, }  @inproceedings{fangmeyer70, author
text classification, the  grouping of texts into several clusters, has been used as a means of improving  both the efficiency and the effectiveness of text retrieval/categorization.
instead of replacing knowledge-based methods with statistics, statistical  training replaces knowledge engineering.
such a knowledge base  would enable much more effective retrieval of web information, and promote new  uses of the web to support knowledge-based inference and problem solving.
the subsequences are weighted by an exponentially decaying factor  of their full length in the text, hence emphasising those occurrences that are  close to contiguous.
feature selection techniques are then needed to identify these words.
text  classification using esc-based stochastic decision lists}, booktitle =
experimental results show that our system can achieve satisfactory performance, which is comparable with other traditional classifiers.}, } @inproceedings{zhou03, author = {shuigeng zhou and tok wang ling and jihong guan and jiangtao hu and aoying zhou}, title = {fast text classification: a training-corpus pruning based approach}, booktitle = {proceedings of dasfaa-03, 8th ieee international conference on database advanced systems for advanced application}, editor = {}, publisher = {ieee computer society press, los alamitos, us}, year = {2003}, address = {kyoto, jp}, pages = {127--136}, url = {}, abstract = {}, } @inproceedings{zu03, author = {guowei zu and wataru ohyama and tetsushi wakabayashi and fumitaka kimura}, title = {accuracy improvement of automatic text classification based on feature transformation}, booktitle = {proceedings of doceng-03, acm symposium on document engineering}, publisher = {acm press, new york, us},
"adapting the naive bayes classifier to rank  procedural texts", booktitle =
the encyclopedia of language and linguistics},  publisher = {elsevier science publishers}, address = {amsterdam, nl}, pages =
{springer verlag, heidelberg, de}, address = {dublin, ie}, pages = {23--30},  year = {1994}, note = {
a re-examination of text  categorization methods}, booktitle = {proceedings of sigir-99, 22nd acm  international conference on research and development in information retrieval},
the {trec-7}  filtering track: description and analysis}, booktitle = {proceedings of trec-7,  7th text retrieval conference}, publisher = {national institute of standards  and technology, gaithersburg, us}, editor = {ellen m. voorhees and donna k.  harman}, year = {1998}, address = {gaithersburg, us}, pages = {33--56}, url =  {http://trec.nist.gov/pubs/trec7/papers/tr7filter/paper.ps}, abstract = {
on integrating catalogs},  booktitle = {proceedings of www-01, 10th international conference on the world  wide web}, publisher = {acm press, new york, us}, editor = {}, year = {2001},  address = {hong kong, cn}, pages = {603--612}, url =  {http://doi.acm.org/10.1145/371920.372163}, abstract = {
since uncertainties in the labeling are taken into account, the model provides an elegant mechanism to deal with noisy labels.
however, in carefully controlled experiments using syntactic phrases produced by church's stochastic bracketer, in conjunction with reciprocal nearest neighbor clustering, term clustering was found to produce essentially no improvement in the properties of the phrasal representation.
we describe a classifier of email queries, which executes text categorization by topic.
surprisingly, this unsupervised procedure can be competitive with a  (supervised) svm trained with a small training set.
our second set of results examines the behavior  of rankboost when it has to learn not only a ranking function but also all  aspects of term weighting from raw data.
preliminary results show  improved accuracy, as well as reduced cost, resulting from these automated  techniques.}, } @inproceedings{rennie03, author = {jason rennie and lawrence  shih and jaime teevan and david karger}, title = {
published in the  ``lecture notes in computer science'' series, number 1777}, pages = {365--379},  url = {http://www.almaden.ibm.com/cs/people/ragrawal/papers/athena.ps},  abstract =
we do so by applying feature selection to the pool of all $k$-grams ($k\leq n$), and checking how many $n$-grams score high enough to be selected in the top $\sigma$ $k$-grams.
and i % % will substitute the link.
while regular support vector machines (svms) try to induce a  general decision function for a learning task, tsvms take into account a  particular test set and try to minimize misclassifications of just those  particular examples.
"australian journal of intelligent information processing systems", volume = 8, number = 3, year = 2004, pages = "123--131", issn = "1321-2133" } @incollection{isumabook05, author =
however, compression-based classification methods have  drawbacks (such as slow running time), and not all such methods are equally  effective.
de m{\'{a}}ntaras and enric plaza}, address = {barcelona, es}, pages =
{1998}, address = {melbourne, au}, pages = {96--103}, url =  {http://www.cs.cmu.edu/~mccallum/papers/clustering-sigir98.ps.gz}, abstract =  {we describe the application of distributional clustering to document  classification.
this approach is illustrated for the case of indexing with a controlled vocabulary.
since it does not require complicated parameterization, it is simpler to use and more robust than comparable heuristics.
two recently implemented machine-learning algorithms, ripper and  sleeping-experts for phrases, are evaluated on a number of large text  categorization problems.
our  analyses show that when the positive training data is not too under-sampled,  svmc significantly outperforms other methods because svmc basically exploits  the natural "gap" between positive and negative documents in the  feature space, which eventually corresponds to improving the generalization  performance.
further experiments using two less orthodox categorizers are also presented  which suggest that combining text categorizers can be successful, provided the  essential element of 'difference' is considered.}, } @article{urena01, author =
experimental results show that our system can effectively and efficiently classify chinese web documents.}, } @inproceedings{zhou02, author = {shuigeng zhou and jihong guan}, title = {an approach to improve text classification efficiency}, booktitle = {proceedings of adbis-02, 6th east-european conference on advances in databases and information systems}, publisher = {springer verlag, heidelberg, de}, editor = {yannis manolopoulos and pavol n{\'a}vrat}, year = {2002}, address =
we find that adding the words in the linked neighborhood to the page having those links (both inlinks and outlinks) were helpful for all our classifiers on one data set, but more harmful than helpful for two out of the three classifiers on the remaining datasets.
"a novel refinement approach for text categorization", booktitle =
this procedure involves a complex hierarchical taxonomy, within which we classify documents into 114 classes and 451 subclasses.
pva consists of three parts: a {\it proxy}, {\it personal  view constructor}, and {\it personal view maintainer}.
as a by-product, we can compute for each document a set of terms that occur significantly more often in it than in the classes to which it belongs.
we also incorporate a batch updating scheme to periodically do maintenance on  the term structures of the news database after training.
semi-automated methods were used to build a lexicon of appraising adjectives and their modifiers.
"ling yin and richard  power", title =
ts compares favorably  with the other methods with up to 50\% vocabulary reduction but is not  competitive at higher vocabulary reduction levels.
in this paper, instead of overhauling the classifier itself, we propose mechanisms to detect misclassification and take appropriate actions.
pisa,  it}, pages = {185--196}, year = {2004}, publisher = {springer verlag,  heidelberg, de}, note = {
the baseline method  combines the terms of all the articles of each newsgroup in the training set to  represent the newsgroups as single vectors.
{129--141}, publisher = {springer verlag, heidelberg, de}, note = {
this task arises in the construction of search engines and  web knowledge bases.
we take a radically new stand, and formulate the problem of automated survey coding as a \emph{text categorization} problem, i.e.\ as the problem of learning, by means of supervised machine learning techniques, a model of the association between answers and codes from a training set of pre-coded answers, and applying the resulting model to the classification of new answers.
it uses fisher's linear discriminant, a classical tool from statistical pattern recognition, to project training instances to a carefully selected low-dimensional subspace before inducing a decision tree on the projected instances.
boosting to correct the inductive bias for text classification}, booktitle = {proceedings of cikm-02, 11th acm international conference on information and knowledge management}, publisher = {acm press, new york, us}, editor = {}, year = {2002}, address = {mclean, us}, pages =
we present an analysis which characterizes the expected  generalization error of the hypothesis with least training error in terms of  the distribution of error rates of the hypotheses in the model.
using these, we build a multilevel classifier.
to  accomplish this task, each substantive word in a text is first categorized  using a feature set based on the semantic subject field codes (sfcs) assigned  to individual word senses in a machine-readable dictionary.
experimental results show that modulating the structure of user profile does increase the accuracy of personalization systems.}, } @article{chen02, author = {
evaluating cost-sensitive unsolicited bulk email categorization}, booktitle = {proceedings of sac-02, 17th acm symposium on applied computing}, editor = {}, address = {madrid, es}, pages = {615--620}, year = {2002}, url = {http://doi.acm.org/10.1145/508791.508911}, abstract = {
we find that error correcting codes perform better than 1-of-n coding with increasing code length.
in the past few years, researchers investigated various forms of semi-supervised learning to reduce the burden of manual labeling.
second, they are inaccurate, because of mistakes made by these indexers as well as the difficulties users have in choosing keywords for their queries, and the ambiguity a keyword may have.
given seeker, the text retrieval system, we  achieved these results in about two person-months.
the second set of experiments applies the  genex algorithm to the task.
tc has been an  application for many learning approaches.
the svms' results common to both filtering  are that 1) the optimal number of features differed completely across  categories, and 2) the average performance for all categories was best when all  of the words were used.
the test  results show the hybrid method is better than the previous rough set-based  approach.}, } @inproceedings{basili00, author = {roberto basili and alessandro  moschitti and maria t. pazienza}, title = {language-sensitive text  classification}, booktitle = {proceedings of riao-00, 6th international  conference ``recherche d'information assistee par ordinateur''}, editor = {},  address = {paris, france}, year = {2000}, pages = {331--343}, url = {},
computer programs scan  text in a document and apply a model that assigns the document to one or more  prespecified topics.
a problem in the use of both algorithms is that they  require documents to be represented by binary vectors, indicating presence or  absence of the terms in the document.
author detection with svms on full word forms was remarkably robust even if the author wrote about different topics.}, } @inproceedings{dinunzio03, author = {giorgio m. {di nunzio} and alessandro micarelli}, title = {does a new simple gaussian weighting approach perform
{van rijsbergen, cornelis j.}, pages = {391--408}, address = {cambridge, uk}, url = {}, abstract = {}, } @inproceedings{fuhr85, author = {fuhr, norbert}, title = {
{thorsten joachims}, title = {transductive inference for text classification
finally, we show empirically that this categorization system utilizing a machine-derived taxonomy performs as well as a manual categorization process, but at a far lower cost.}, } @inproceedings{agrawal00, author = {rakesh agrawal and roberto j. bayardo and ramakrishnan srikant}, title = {{\sc athena}: mining-based interactive management of text databases}, booktitle = {proceedings of edbt-00, 7th international conference on extending database technulogy},
we use information from a pre-existing taxonomy in  order to supervise the creation of a set of related clusters, though with some  freedom in defining and creating the classes.
again, boosting compares favourably to the other benchmark algorithms.}, } @article{fall03, author = {c. j. fall and a. t{\"o}rcsv{\'a}ri and k. benzineb and g. karetka}, title = {automated categorization in the {international patent classification}}, journal = {sigir forum}, year = {2003}, pages = {10--25}, volume = {37}, number = {1}, url = {http://www.acm.org/sigir/forum/s2003/cjf_manuscript_sigir.pdf}, abstract = {a new reference collection of patent documents for training and testing automated categorization systems is established and described in detail.
technology from machine learning (ml) will offer efficient tools for the intelligent analyses of the data using generalization ability.
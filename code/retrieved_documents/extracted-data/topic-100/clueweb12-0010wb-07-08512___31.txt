for example, the entire financial system hinges on trust, and the level of trust in financial instruments is not known to be particularly stable.
five years ago, in what is web 2.0, tim o'reilly said that "data is the next intel inside."
all of this data would be  useless if we couldn't store it, and that's where moore's law comes in.
the explosion of text data (distinct from audio, video, etc.) is wholly due to kiddies such as google who didn't want to build bcnf datastores because they're (the kiddies, not the data) just too dense.
do you really care if you have 1,010 or 1,012 twitter followers?
they  accept all data formats, including the most messy, and their schemas evolve as  the understanding of the data changes.
most recently discussed related topics related events strata 2012, february 28-march 1, santa clara, calif.
another example that comes to mind is pandora.
it then branched out  incrementally.
however, data from  o'reilly research shows a steady year-over-year increase in hadoop and  cassandra job listings, which are good proxies for the "data science"  market as a whole.
however, data from o'reilly research shows a steady year-over-year increase in hadoop and cassandra job listings, which are good proxies for the "data science" market as a whole.
to understand what the  numbers mean, the stories they are really telling, you need to generate a  graph.
almost any e-commerce  application is a data-driven application.
point-of-sale devices and  frequent-shopper's cards make it possible to capture all of your retail  transactions, not just the ones you make online.
while there are two dozen or so products available (almost all  of them open source), a few leaders have established themselves: cassandra: developed at facebook, in production use at twitter, rackspace,  reddit, and other large sites.
ben hoyle [ 7 june 2010 03:32 am] great article.
the first step of any data analysis  project is "data conditioning," or getting data into a state where  it's usable.
hiring trends for data science
save 30% on registration with the  code stn11rad google is a master at creating data products.
the two physical phenomena are often confused, but please don't further this problem!
these  recommendations are "data products" that help to drive amazon's more  traditional retail business.
many sources of "wild data" are extremely messy.
suitable for extremely large databases (billions of rows, millions of columns), distributed across thousands of nodes.
on the subject of visualization, stanford's protovis is also probably worthy of considerationhttp://vis.stanford.edu/protovis/ i've only started trying it out, but protovis is just javascript and svg, and that's a big plus for me.
google popularized themapreduce approach, which is  basically a divide-and-conquer strategy for distributing an extremely large  problem across an extremely large computing cluster.
yahoo's claim that they had built the world's largest production  hadoop application, with 10,000 cores running linux, brought it onto center  stage.
they aren't well-behaved xml files with all the metadata nicely in place.
scripting languages, such asperl and python, are essential.
almost any e-commerce application is a data-driven application.
as well as whether or not to keep it.
the need  to define a schema in advance conflicts with reality of multiple, unstructured  data sources, in which you may not know what's important until after you've  analyzed the data.
data is everywhere: your government, your web server, your business partners,even your body.
hadoop processes data as it arrives, and delivers intermediate results in (near) real-time.
i just stumbled here in your blog and thanks a lot for this great post.:d ahmet ardal [29 june 2011 03:57 am] thanks for this excellent article.
in the last few years, there has been an explosion in the amount of data  that's available.
in the "map"  stage, a programming task is divided into a number of identical subtasks, which  are then distributed across many processors; the intermediate results are then  combined by a single reduce task.
machine learning is another essential tool for the data scientist.
i am stunned... ken [ 2 june 2010 11:16 pm]
data scientists data science requires skills ranging from traditional computer science to  mathematics to art.
edward tufte'svisual display of quantitative information is the classic for data visualization, and a foundational text for anyone practicing data science.
it has a 5 mb capacity
data  science isn't just about the existence of data, or making guesses about what  that data might mean; it's about testing hypotheses and making sure that the  conclusions you're drawing from the data are valid.
cddb is a great example of data jiujitsu: identifying music by  analyzing an audio stream directly is a very difficult problem (though not  unsolvable --
this is really an issue booten gurg [ 5 december 2010 11:27 am] does wikileaks apply here?
having said that, this is a great article.
marc [ 4 june 2010 03:54
we are seeing more data in formats that are easier to consume: atom data feeds, web services, microformats, and other newer technologies provide data in formats that's directly machine-consumable.
the errors in the r and stat books are such to make it obvious your authors (you, too) have an understanding of inferential statistics (the only variety of any value) about as deep as the iso-9000 and six sigma folks.
google has just announced theirprediction api, which exposes their machine learning algorithms for public use via a restful interface.
but what does that statement mean?
hadoop is essentially a batch system, but hadoop online prototype (hop) is an experimental project that enables stream processing.
i also am a bit skeptical about the fetish-izing of 'big data'.
i.e. bringing a pot of water to boil is not caused by switching on the stove.
it really encouraged and motivated me a lot as a newbie data scientist.
extracting information from data is more like cooking, an art.
that's a bit of a stretch!
i'm looking at the future, thank you for teaching and sharing!
faster  computations make it easier to test different assumptions, different datasets,  and different algorithms.
think in terms of the larger scale of it -- the electricity required (both in generating the electricity itself & paying for use), plus the machines that must be purchased, and the humans who must be paid to manage the data through its lifecycle.
where data comes from working with data at scale making data tell its story data scientists download free pdf versionwe've all heard it: according to hal varian, statistics is the next sexy job.
usurp ones "observe" and "orient" functions in their closed-loop decision processes, and you can then predict an individual's "decide" and "act" part of the loop (as described by john boyd).
statistics further only produce correlation but not causation.
storing data is only part of building a data platform, though.
before it does anything else, itunes reads the length of every track, sends it to cddb, and gets back the track titles.
data scientists combine entrepreneurship with patience, the willingness to  build data products incrementally, the ability to explore, and the ability to  iterate over a solution.
if the problem involves human language, understanding the data adds another  dimension to the problem.
o'reilly publications related to data science r in a nutshell a quick and practical reference to learn what is  becoming the standard for developing statistical software.
scientists also know how to break large problems up into smaller problems.
whether  humans or software decided to ignore anomalous data, it appears that data was  ignored.
it would have been easy to turn this into a high-ceremony development  project that would take thousands of hours of developer time, plus thousands of  hours of computing time to do massive correlations across linkedin's  membership.
what do you think "intent harvesting" and "intent generation" is about?
the thread that ties most of these applications together is that data  collected from users provides added value.
megan [ 5 june 2010 01:33 pm] kudos for encouraging thoughtfulness about data, and interdisciplinary approaches to data analysis problems.
but hadoop (and particularly elastic mapreduce) make it easy to build clusters that can perform computations on long datasets quickly.
science is a process for discovering truth (or at least discarding falsity).
it's easer to consult with clients to figure out whether you're asking the right questions, and it's possible to pursue intriguing possibilities that you'd otherwise have to drop for lack of time.
the web is full of "data-driven apps."
" @technontologist jorn bettin [ 4 june 2010 03:16 pm] very nice article.
the developers of cddb realized that any cd had a unique signature, based on the exact length (in samples) of each track on the cd.
we're increasingly finding data in the wild, and data scientists are involved with gathering data, massaging it into a tractable form, making it tell its story, and presenting that story to others.
there's a database behind a web front end, and middleware that talks to a number of other databases and data services (credit card processing companies, banks, and so on).
brian ahier [ 2 june 2010 11:08 pm] this is the best post on data i have ever read.
the most popular open source implementation of mapreduce is the hadoop project.
all of this data would be useless if we couldn't store it, and that's where moore's law comes in.
cassandra is designed for high performance, reliability, and automatic replication.
how does a young guy with strong communications skills experience in statistics, computing, math, and data get into this field?
strata conference new york 2011, being held sept. 22-23, covers the latest and best tools and technologies for data science -- from gathering, cleaning, analyzing, and storing data to communicating data intelligence effectively.
filtering the relevant stuff is not easy and mostly an intuitive human skill as long as the model can be grasped.
according to martin wattenberg (@wattenberg , founder offlowing media), visualization is key to data conditioning: if you  want to find out just how bad your data is, try plotting it.
do that across enough people in the public domain, and guess what you have.
oil companies, telecommunications companies, and other data-centric industries have had huge datasets for a long time.
data science is cool.
storage, is indeed, cheap, and getting cheaper.
the most meaningful definition i've heard:"big data" is when the size of the data itself becomes part of the problem.
so, how do we make that data useful?
they group together  fundamentally dissimilar products by telling you what they aren't.
brian shaler [ 4 june 2010 04:44 pm] i want to clarify on my last comment, when i said "all these" i was not referring to all data-related projects in general.
tracking links made google searches much more useful, and pagerank has been a key ingredient to the company's success.
i'm aware that tools have been available to manipulate data for decades.
eric christensen [ 3 june 2010 07:23 pm] data is cool.
the article would be even better if it didn't contain simple calculation errors like "ram has moved from $1,000/mb to roughly $25/gb -- a price reduction of about 4000".
while i haven't stressed traditional statistics, building statistical models plays an important role in any data analysis.
no one in the nascent data industry is trying to  build the 2012 nissan stanza or office 2015; they're all trying to find new  products.
in contrast, a 32 gb microsd card measures around 5/8 x 3/8 inch and weighs about 0.5 gram.
we don't yet know what those products are, but we do know that the  winners will be the people, and the companies, that find those products.
does a successful retail chain spread like an  epidemic, and if so, does that give us new insights into how economies work?
and it's not just companies using their own data, or the data contributed by their users.
in software development, "agile practices" are associated with faster product cycles, closer interaction between developers and consumers, and testing.
but google has made huge strides by using the voice data they've collected, and has been able tointegrate voice search into their core search engine.
facebook and linkedin use patterns of friendship relationships to suggest other people you may know, or should know, with sometimes frightening accuracy.
if there's a single tool that provides an end-to-end solution for statistics work, r is it.
the question facing every company today, every startup, every non-profit,  every project site that wants to attract a community, is how to use data  effectively -- not just their own data, but all the data that's available and  relevant.
near real-time data analysis enables features liketrending topics on sites like twitter.
while all these hip new projects have been cropping up during the last five years, we're still at the beginning of this conversation.
since  the early '80s, processor speed has increased from10 mhz to 3.6 ghz -- an  increase of 360 (not counting increases in word length and number of cores).
you  need some creativity for when the story the data is telling isn't what you  think it's telling.
the turk is an excellent way to develop training sets.
http://www.nytimes.com/2010/12/05/world/05restrict.html?hp larry (ieor tools)
this graph shows the increase in cassandra jobs, and the  companies listing cassandra positions, over time.
am] your graph of job listings vs time displays no indication of its vertical scale.
once you've parsed the data, you can start thinking about the quality of  your data.
according to martin wattenberg (@wattenberg , founder offlowing media), visualization is key to data conditioning: if you want to find out just how bad your data is, try plotting it.
ellen o'neal [23 august 2010 08:41 am] this stuff fascinates me!
i won't even go into the already mentioned issues of privacy and misuse of the collected data by governments and big business.
"data mashups in r" analyzes mortgage foreclosures in philadelphia county by taking a public report from the county sheriff's office, extracting addresses and using yahoo to convert the addresses to latitude and longitude, then using the geographical data to place the foreclosures on a map (another data source), and group them by neighborhood, valuation, neighborhood per-capita income, and other socio-economic factors.
data science isn't just about the existence of data, or making guesses about what that data might mean; it's about testing hypotheses and making sure that the conclusions you're drawing from the data are valid.
" also important is the ability to figure out how to tap into existing sources of data, rather than collecting your own.
they come about because amazon understands that a book isn't just a book, a camera isn't just a camera, and a customer isn't just a customer; customers generate a trail of "data exhaust" that can be mined and put to use, and a camera is a cloud of data that can be correlated with the customers' behavior, the data they leave every time they visit the site.
if data is missing, do you simply ignore the missing points?
it's the kind of question we now ask routinely.
it incorporateshdfs, a distributed filesystem designed for the performance and reliability requirements of huge datasets; the hbase database;hive, which lets developers explore hadoop datasets using sql-like queries; a high-level dataflow language calledpig; and other components.
although r is an odd and quirky language, particularly to someone with a background in computer science, it comes close to providing "one stop shopping" for most statistical work.
the thread that ties most of these applications together is that data collected from users provides added value.
statisticians have been thinking hard about data for many, many years.
what's less obvious is that mapreduce has proven to be widely applicable to many large data problems, ranging from search to machine learning.
data science requires skills ranging from traditional computer science to mathematics to art.
according to hilary mason (@hmason), data scientist  atbit.ly, it's possible to precompute much of the calculation, then use one of  the experiments in real-time mapreduce to get presentable results.
the models of global warming and climate change are complete rubbish and so will be many of the data models that try to make sense of all the random data collected.
their business is fundamentally different from selling music, sharing  music, or analyzing musical tastes (though these can also be "data  products").
it depends to a large extent on the nature of the product: we demand a different level of trust for financial products (and that trust has obviously been shaken badly over the past two years) than we do for facebook or linkedin recommendations.
while rock-solid consistency is crucial to many applications, it's not really necessary for the kind of analysis we're discussing here.
vector [ 4 december 2010 10:46 am] thanks for the good and quite useful article.
1956 disk drive one of the first commercial disk drives from ibm.
data are our future.
tell us it's not relevant doesn't cut it.
try usinggoogle trends to figure out  what's happening with thecassandra database or the python language, and you'll  get a sense of the problem.
whether you look at bits per gram, bits per dollar, or raw  capacity, storage has more than kept pace with the increase of cpu speed.
sites likeinfochimps and factual provide  access to many large datasets, including climate data, myspace activity  streams, and game logs from sporting events.
amazon'selastic mapreduce makes it much easier to  put hadoop to work without investing in racks of linux machines, by providing  preconfigured hadoop images for its ec2 clusters.
here's a few examples: google's breakthrough was realizing that a search engine could use input other than the text on the page.
the need to define a schema in advance conflicts with reality of multiple, unstructured data sources, in which you may not know what's important until after you've analyzed the data.
but merely using data  isn't really what we mean by "data science."
roger magoulas, who runs the data analysis group at  o'reilly, was recently searching a database for apple job listings requiring  geolocation skills.
and if that isn't possible, why bother.
oil companies, telecommunications companies, and other  data-centric industries have had huge datasets for a long time.
describing the data science group he put together at facebook (possibly the first data science group at a consumer-oriented web property), jeff hammerbacher said: ... on any given day, a team member could author a multistage processing pipeline in python, design a hypothesis test, perform a regression analysis over data samples with r, design and implement an algorithm for some data-intensive product or service in hadoop, or communicate the results of our analyses to other members of the organization3
data conditioning can involve cleaning up messy html with tools like  beautiful soup, natural language processing to parse plain text in english and  other languages, or even getting humans to do the dirty work.
google has just  announced theirprediction api, which exposes their machine learning algorithms  for public use via a restful interface.
in software development, "agile practices" are associated with faster  product cycles, closer interaction between developers and consumers, and  testing.
it is made for high throughput batch processing, not quick turn around.
data analysis with open source tools this book shows you how to think  about data and the results you want to achieve with it.
and that problem is  showing up more and more frequently.
if you've ever used itunes to rip a cd, you've taken  advantage of this database.
according to jeff  hammerbacher2 (@hackingdata), we're trying to build information platforms or  dataspaces.
the more storage is available, the more data you will find to put into it.
22-23, covers the latest and best tools and technologies for data science --  from gathering, cleaning, analyzing, and storing data to communicating data  intelligence effectively.
all these moves are driven by the goal of usurping ones ooda loop.
according to hilary mason (@hmason), data scientist atbit.ly, it's possible to precompute much of the calculation, then use one of the experiments in real-time mapreduce to get presentable results.
as with the number of followers on twitter,  a "trending topics" report only needs to be current to within five  minutes -- or even an hour.
however there is still a long way to go for data products.
data is only useful if you can do something with it, and enormous datasets present computational problems.
they were the vanguard, but newer companies like bit.ly are following their path.
for example, if you're looking at job listings, and want to know which originated with apple, you can have real people do the classification for roughly $0.01 each.
disambiguation is never an easy task, but tools like thenatural language toolkit library can make it simpler.
please, consider to include http://mytaskhelper.com in your review.
i'm a librarian and our lot faces a handful of data as well.
if there's a single tool that provides an  end-to-end solution for statistics work, r is it.
don't forget that all models are wrong but some are useful (einstein).
i've just fixed the reference to the ozone layer.
making data tell its story isn't just a matter of  presenting results; it involves making connections, then going back to other  data sources to verify them.
there was insufficient computing power, the data was all locked up in proprietary sources, and the tools for working with the data were insufficient.
http://www.joeandmotorboat.com/2010/05/31/beyond-bigdata/ robert richards [ 2 june 2010 01:10 pm] thanks for this post.
we now expect web and mobile applications to incorporate recommendation engines, and building a recommendation engine is a quintessential artificial intelligence problem.
it looks like the big companies such as google may know a little bit too much about us... rissy
alan howlett aka @technontologist [ 4 june 2010 07:19 am] i see lots of commonality with richard saul wurman's definition of information architect[ure] (not the co-opted web developer version), ""information architect.
provide data in formats that's directly machine-consumable.
i think by using mapreduce one can find out the hidden patterns in unstructured data.
data expands to fill the space you have to store it.
second, of course we have, or will soon have, the algorithms available to process this data.
most of the organizations that have built data platforms have found it  necessary to go beyond the relational database model.
jf robert young [ 3 june 2010 08:03 am] first (or most recently, at least)
we often need to pull content from external public websites (usually government and npo's), which don't have any form of exportable data formats.
i wish i'd written it.
and this is one place where "art" comes in: not just the  aesthetics of the visualization itself, but how you understand it.
let's not discard them with a flippant comment about actuaries and the false belief that anyone can run a few programs in r and correctly analyze data.
thanks for pointing it out--it's been corrected.
visualization is crucial to each stage of the data scientist.
the nasa article denies this, but also says that in 1984, they decided that the low values (whch went back to the 70s) were "real."
which is why managing a business with bpm workflows and predictive analytics is for idiots who don't understand nature and science.
thanks for pointing that out.
hilary  mason came to the same conclusion.
i'm looking forward to seeing where the industry goes in the next five years and hope to play a part!
you don't have to look at many modern web applications to see  classification, error detection, image matching (behindgoogle goggles and  snaptell) and even face detection -- an ill-advised mobile application lets you  take someone's picture with a cell phone, and look up that person's identity  using photos available online.
the part of  hal varian's quote that nobody remembers says it all: the ability to take data -- to be able to understand it, to process  it, to extract value from it, to visualize it, to communicate it -- that's  going to be a hugely important skill in the next decades.
one of my favorites is this animation of thegrowth of walmart  over time.
google has indexed many, many websites about large  snakes.
further it depends how the data is collected: how it is timed, how accurate it is, how it is filtered, which questions were asked or points that were measured, and a few more elements that rely on model assumption.
suitable for extremely large databases (billions of rows, millions of  columns), distributed across thousands of nodes.
you wouldn't use mapreduce like this in a low-latency query situation.
i think we need to be careful not to lose sight of the drivers of science and where much of science comes from - intuition and great ideas .. after which we create experiments to test hypotheses which inevitably involve data of some kind.
near  real-time data analysis enables features liketrending topics on sites like  twitter.
computing a signature based on track lengths, and then looking up that  signature in a database, is trivially simple.
flu trends google was able to spot trends in the swine flu epidemic roughly two weeks before the center for disease control by analyzing searches that people were making in different regions of the country.
do if not process lots of (sensory) data extremely cleverly.
now, why is o'reilly et all not discussing this critical aspect of the technologies and strategies that are being pushed on to the tech community for deployment?
traditional data analysis has been hampered by extremely long  turn-around times.
the value of data analysis and representation critically depends on access to trustworthy source data.
it has excellent graphics facilities;  cran includes parsers for many kinds of data; and newer extensions extend r  into distributed computing.
muhammad mudssar [ 3 june 2010 10:45 pm] good article about data and its its importance specially importance of massive unstructured data.
then  you might like to join the cornell alumni group.
why do we  suddenly care about statistics and about data?
facebook and  linkedin use patterns of friendship relationships to suggest other people you  may know, or should know, with sometimes frightening accuracy.
think of all the other causal elements that are needed including air pressure and saline content.
so far it's worked out pretty well as we've been able to "scrape" raw data as structured information from hundreds of public sources.
you're likely to  be dealing with an array of data sources, all in different forms.
the web has people spending more time  online, and leaving a trail of data wherever they go.
there was  insufficient computing power, the data was all locked up in proprietary  sources, and the tools for working with the data were insufficient.
the data exhaust you leave behind whenever you surf the web, friend someone on facebook, or make a purchase in your local supermarket, is all carefully collected and analyzed.
if data is  incongruous, do you decide that something is wrong with badly behaved data  (after all, equipment fails), or that the incongruous data is telling its own  story, which may be more interesting?
spell checking isn't a terribly difficult problem, but by suggesting corrections to misspelled searches, and observing what the user clicks in response, google made it much more accurate.
whether we're talking about web server logs, tweet streams, online transaction records, "citizen science," data from sensors, government data, or some other source, the problem isn't finding data, it's figuring out what to do with it.
they can think outside the box to come up with new ways to  view the problem, or to work with very broadly defined problems: "here's a  lot of data, what can you make from it?" the future belongs to the companies who figure out how to collect and use  data successfully.
whether  it's mining your personal biology, building maps from the shared experience of  millions of travellers, or studying the urls that people pass to others, the  next generation of successful businesses will be built around data.
patil's first flippant  answer to "what kind of person are you looking for when you hire a data  scientist?" was "someone you would start a company with."
save 20% with code radar20 archives © 2012, o'reilly media, inc. (707) 827-7019(800) 889-8969 all trademarks and registered trademarks appearing on oreilly.com are the property of their respective owners.
i've been writing about this for years.
more to the  point, it's easy to notice that one advertisement forr in a nutshell generated 2 percent more conversions than another.
mike loukides [ 5 june 2010 06:17 am] ouch.
a picture may or may not be worth a thousand words, but a picture is certainly worth a thousand numbers.
using data effectively requires something different from traditional statistics, where actuaries in business suits perform arcane but fairly well-defined kinds of analysis.
but old-stylescreen scraping hasn't died, and isn't going to die.
point-of-sale devices and frequent-shopper's cards make it possible to capture all of your retail transactions, not just the ones you make online.
they can think outside the box to come up with new ways to view the problem, or to work with very broadly defined problems: "here's a lot of data, what can you make from it?" the future belongs to the companies who figure out how to collect and use data successfully.
in this post, i examine the many sides of data science -- the technologies, the companies and the unique skill sets.
this data was presented as an html file that was probably generated automatically from a spreadsheet.
it has a very flexible data model.
but google has made huge strides by using the voice data they've  collected, and has been able tointegrate voice search into their core search  engine.
and as storage  capacity continues to expand, today's "big" is certainly tomorrow's  "medium" and next week's "small."
what are we trying to do with data that's different?
of course data science is described as a sexy gig when you're bosses are control freaks.
information platforms are similar to traditional data warehouses, but different.
we are reaching a stage where the great limiting factor to significant progress is our intelligent processing systems, we are beginning to have the data but we don't yet have suitably developed systems to intelligently process that data (both in a human and non-human manner).
factual enlists users to update  and improve its datasets, which cover topics as diverse as endocrinologists to  hiking trails.
if data is incongruous, do you decide that something is wrong with badly behaved data (after all, equipment fails), or that the incongruous data is telling its own story, which may be more interesting?
increased storage capacity demands increased sophistication in the analysis and use of that data.
when natural language processing fails, you can replace artificial  intelligence with human intelligence.
cddb arises entirely from viewing a musical problem as a data problem.
that's the foundation of data science.
patil described the process of creating the group recommendation feature at  linkedin.
but the cddb staff used data creatively  to solve a much more tractable problem that gave them the same result.
gnuplot is very effective; r incorporates a fairly comprehensive graphics package; casey reas' and ben fry'sprocessing is the state of the art, particularly if you need to create animations that show how things change over time.
beautiful data learn from the best data practitioners in the field about how wide-ranging -- and beautiful -- working with data can be.
the nasa article denies this, but also says that in 1984, they decided  that the low values (whch went back to the 70s) were "real."
the  developers of cddb realized that any cd had a unique signature, based on the  exact length (in samples) of each track on the cd.
but it takes statistics to know whether this difference is significant, or just a random fluctuation.
the part of hal varian's quote that nobody remembers says it all: the ability to take data -- to be able to understand it, to process it, to extract value from it, to visualize it, to communicate it -- that's going to be a hugely important skill in the next decades.
a friend of mine wrote an article that i think may be interesting to fellow readers: http://info.livelogic.net/customer-intelligence-project-success/bid/47060/pandora-s-jar-and-the-art-of-the-cross-sell thanks for the post.
while this sounds simple enough, it's  revolutionary: cddb views music as data, not as audio, and creates new value in  doing so.
hadoop goes far beyond a simple mapreduce implementation (of which there are several); it's the key component of a data platform.
one of the earlier data products on the web was the cddb database.
they are inherently interdiscplinary.
they can tackle all aspects of a problem, from initial data collection and data conditioning to drawing conclusions.
during the swine flu epidemic of 2009, google was able to track the  progress of the epidemicby following searches for flu-related topics.
"data mashups in r" analyzes  mortgage foreclosures in philadelphia county by taking a public report from the  county sheriff's office, extracting addresses and using yahoo to convert the  addresses to latitude and longitude, then using the geographical data to place  the foreclosures on a map (another data source), and group them by  neighborhood, valuation, neighborhood per-capita income, and other  socio-economic factors.
head first data analysis learn how to collect your data, sort the distractions from the truth, and find meaningful patterns.
for example, if you're looking at job listings, and want to  know which originated with apple, you can have real people do the  classification for roughly $0.01 each.
anyway, this is government 2.0 at work !
relational databases are designed for consistency, to  support complex transactions that can easily be rolled back if any one of a  complex set of operations fails.
the first step of any data analysis project is "data conditioning," or getting data into a state where it's usable.
andrew ng's machine learning course is one of the most popular courses in computer science at stanford, with hundreds of students (this video is highly recommended).
i hope this movement will play a big role in the future as there is so much benefits to leverage data from the public and private sectors.
cassandra is designed for high performance,  reliability, and automatic replication.
in  contrast, a 32 gb microsd card measures around 5/8 x 3/8 inch and weighs about  0.5 gram.
whether we're talking about web server logs, tweet streams,  online transaction records, "citizen science," data from sensors,  government data, or some other source, the problem isn't finding data, it's  figuring out what to do with it.
once you've collected your training  data (perhaps a large collection of public photos from twitter), you can have  humans classify them inexpensively -- possibly sorting them into categories,  possibly drawing circles around faces, cars, or whatever interests you.
using data effectively requires something different from traditional  statistics, where actuaries in business suits perform arcane but fairly  well-defined kinds of analysis.
this is the heart of what patil calls "data jiujitsu" -- using smaller auxiliary problems to solve a large, difficult problem that appears intractable.
if you start a calculation, it might not finish for hours,  or even days.
these are frequently called nosql databases, or non-relational databases, though neither term is very useful.
in an article on the importance of data, that leaves me wondering how much of the content is hype and how much is well thought out.
it's reported that the discovery of ozone  layer depletion was delayed becauseautomated data collection tools discarded  readings that were too low 1.
at some point, traditional techniques for  working with data run out of steam.
alex tolley [ 2 june 2010 07:21 pm] good article.
and you're right, in finance there's a marketing industry that's designed to manufacture trust entirely aside from the quality of the data or the analysis.
the result was a valuable data product that analyzed a huge database -- but it was never conceived as such.
while i haven't stressed traditional statistics, building statistical  models plays an important role in any data analysis.
you have to make it tell its story.
it's an  excellent way to classify a few thousand data points at a cost of a few cents  each.
and it's stored in a cabinet roughly the size of a luxury refrigerator.
disk drive on display at ibm almaden research much of the data we currently work with is the direct consequence of web 2.0,  and of moore's law applied to data.
or the spread of a flu virus through a population?
while there are many commercial statistical packages, the open source r  language -- and its comprehensive package library, cran -- is an essential  tool.
yahoo's claim that they had built the world's largest production hadoop application, with 10,000 cores running linux, brought it onto center stage.
data analysis with open source tools this book shows you how to think about data and the results you want to achieve with it.
we converted it as a formidable tool and gave a face to understand it better by user's.
amazon saves your  searches, correlates what you search for with what other users search for, and  uses it to create surprisingly appropriate recommendations.
but that's not really what concerns us here.
hitachi made the first gigabyte disk drives in 1982, weighing in at roughly 250 pounds; now terabyte drives are consumer equipment, and a 32 gb microsd card weighs about half a gram.
we've started using a tool called feedity -http://feedity.com for that purpose.
it doesn't matter whether we get the image of the golden ring, or the network centric system, as both images represent identical ends.
many of these databases are the logical descendants of google'sbigtable and amazon's dynamo, and are designed to be distributed across many nodes, to provide "eventual consistency" but not absolute consistency, and to have very flexible schema.
it's easy to distribute a search across thousands of processors, and then combine the results into a single set of answers.
physicists have a strong mathematical background, computing skills, and come  from a discipline in which survival depends on getting the most from the data.
g. boyd [ 6 june 2010 09:20 pm] data science exists in order to usurp an individual's ooda loop without their knowledge, as described by former air force pilot john boyd.
i'm excited to see the future of predictive analytics, as my company calls it.
as to r, yes, it is useful, but sas still rules the job listings.
visualization is  also frequently the first step in analysis.
whether that data is search terms,  voice samples, or product reviews, the users are in a feedback loop in which  they contribute to the products they use.
these features only require soft real-time; reports on trending topics  don't require millisecond accuracy.
both can also be thought of as overlapping with ai and brain science; what does the brain (human, fly etc.)
many of the key hadoop developers have found a home atcloudera, which provides commercial support.
statistical processing does however not replicate reality, but is always built on a model illusion.
put out the feelers and collect more data (figure out what to do with it later).
does it look like the spread of cancer throughout a body?
fred beringer [28 june 2010 03:04 am] excellent article and a good summary of where data is headed.
joe [ 2 june 2010 12:33 pm] good post, definitely where things are going.
but the process worked quite differently: it started out with a  relatively small, simple program that looked at members' profiles and made  recommendations accordingly.
beautiful visualization this book demonstrates why visualizations are beautiful not only for their aesthetic design, but also for elegant layers of detail.
according to jeff hammerbacher2 (@hackingdata), we're trying to build information platforms or dataspaces.
it's increasingly common to mashup data from a number of sources.
we're discussing data problems ranging from gigabytes to petabytes of data.
you don't have to look at many modern web applications to see classification, error detection, image matching (behindgoogle goggles and snaptell) and even face detection -- an ill-advised mobile application lets you take someone's picture with a cell phone, and look up that person's identity using photos available online.
[22 august 2010 05:17 pm] fascinating article mike.
if you've ever seen the html that's generated  by excel, you know that's going to be fun to process.
and as storage capacity continues to expand, today's "big" is certainly tomorrow's "medium" and next week's "small."
brandyn [ 2 june 2010 08:06 pm] "in hindsight, mapreduce seems like an obvious solution to google's biggest problem, creating large searches.
along with hadoop, commercial  support is provided bycloudera.
it might be useful to note that data science has been recognized as a discipline since at least 2001; that one of the first scholars to use "data science" in the sense used in this post was dr. william s. cleveland in his 2001 article, "data science: an action plan for expanding the technical areas of the field of statistics," http://j.mp/a8mqep ; that people working in this discipline have organized professional associations, including codata, the international council for science: committee on data for science and technologyhttp://www.codata.org/ ; that there are conferences devoted to this discipline, including the international codata conferencehttp://www.codata2010.com/ ; and that there are journals that publish data science research, including journal of data science http://www.jds-online.com/ (founded in 2003), and data science journal http://j.mp/9yyh5r (founded in 2002).
that's an important insight: we're entering the era of products that are built on data.
and who can possibly advise us about the practical realities of data science?
photo: mike loukides.
mining large data sets to death is very similar to "spreadsheet-itis" that spread quickly in te 1980's once spreadsheets became widely available.
you're likely to be dealing with an array of data sources, all in different forms.
andrew ng's machine learning course is one of the  most popular courses in computer science at stanford, with hundreds of students  (this video is highly recommended).
whether it's mining your personal biology, building maps from the shared experience of millions of travellers, or studying the urls that people pass to others, the next generation of successful businesses will be built around data.
the turk is an  excellent way to develop training sets.
recently a meteorological link was discovered whereby a the increased uvb radiation due to the depleted ozone layer does affect air circulation patterns in the antarctic, but this is basically a reflection of the fact that almost any aspects of the earth system if you look hard enough.
seemidomi, for example).
so they build, using xml to compound the insult, massively redundant flat files, which in turn demand vast amounts of hdd and cpu to process.
according to dj patil, chief  scientist atlinkedin (@dpatil), the best data scientists tend to be "hard  scientists," particularly physicists, rather than computer science majors.
working with data at scale making data tell its story data scientists download free pdf version we've all heard it: according to hal varian, statistics is the next  sexy job.
[20 february 2011 12:05 am] mike, great research on a relevant subject.
it's easy to distribute a search across thousands of processors, and then combine the results into a single set of answers."
that's  an important insight: we're entering the era of products that are built on  data.
during the swine flu epidemic of 2009, google was able to track the progress of the epidemicby following searches for flu-related topics.
data scientists combine entrepreneurship with patience, the willingness to build data products incrementally, the ability to explore, and the ability to iterate over a solution.
it would be  nice if there was a standard set of tools to do the job, but there isn't.
another source of data you briefly mentions are the millions of sensors we're going to see in our life in the next 5-10 years, with the so called internet of things.
google'spagerank algorithm was among the first  to use data outside of the page itself, in particular, the number of links  pointing to a page.
along with hadoop, commercial support is provided bycloudera.
or the spread of a flu virus  through a population?
to do it well you need to understand the grammatical structure of a job posting; you need to be able to parse the english.
that's where services like amazon's mechanical turk come in.
2 "information platforms as dataspaces," by jeff hammerbacher (in beautiful data) 3 "information platforms as dataspaces," by jeff hammerbacher (in beautiful data) tags: data, data science, data scientist tweet comments: 52 tim o'reilly [ 2 june 2010 12:25 pm] this is a really important and articulate post, mike.
[15 november 2010 06:53 pm] great post !
this data was presented as an html file that was probably generated  automatically from a spreadsheet.
head first data analysis learn how to collect your data, sort the  distractions from the truth, and find meaningful patterns.
nothing wrong with using spreadsheets, except that they tended to focus attention on the questions where data that was available, rather than the questions that really needed answers.
we're increasingly  finding data in the wild, and data scientists are involved with gathering data,  massaging it into a tractable form, making it tell its story, and presenting  that story to others.
it was an agile, flexible process that built toward its goal incrementally, rather than tackling a huge mountain of data all at once.
hbase: part of the apache hadoop project, and modelled on google's bigtable.
and that problem is showing up more and more frequently.
gracenote built a database of track lengths, and coupled it to a database of album metadata (track titles, artists, album titles).
at o'reilly, we frequently combine publishing industry data fromnielsen bookscan with our own sales data, publicly available amazon data, and even job data to see what's happening in the publishing industry.
even a relatively large job only costs a few hundred dollars.
while rock-solid consistency is crucial to  many applications, it's not really necessary for the kind of analysis we're  discussing here.
the number of references & links to languages, platforms/frameworks, tools, and projects was very impressive!
her job as scientist at bit.ly is really to investigate the data that bit.ly is generating, and find out how to build interesting products from it.
they look for predictability where there is none.
brand niemann [ 3 june 2010 03:38 am] i think i have always been a "data scientist" as coined by jeff hammerbacher in facebook 2007, combining data analyst & research scientist: http://twitter.com/bniemannsr/status/15207340783 please see my data science library in the cloud: http://ondemand.spotfire.com/public/library.aspx?folder=users/useevl-4372/public juan moya [ 3 june 2010 07:54
g. boyd [ 7 june 2010 10:42 am] @ben hoyle, first, information engineering is a means to an end, this end which can be referred to as "social engineering", or the designing of society and its participants.
then at books members had in  their libraries.
the web has people spending more time online, and leaving a trail of data wherever they go.
in the "map" stage, a programming task is divided into a number of identical subtasks, which are then distributed across many processors; the intermediate results are then combined by a single reduce task.
jim revitol [31 may 2011 03:05 pm] a very interesting and thorough article!
yes, some of it will be useful as an aha moment, but it certainly won't predict the future or allow us to control things better.
hilary mason says that when she gets a new data set, she starts by making a dozen or more scatter plots, trying to get a sense of what might be interesting.
we don't yet know what those products are, but we do know that the winners will be the people, and the companies, that find those products.
disk drive on display at ibm almaden researchmuch of the data we currently work with is the direct consequence of web 2.0, and of moore's law applied to data.
hitachi made the first gigabyte disk drives in 1982, weighing in at roughly 250 pounds; now  terabyte drives are consumer equipment, and a 32 gb microsd card weighs about  half a gram.
we've haven't seen anything yet !
amazon saves your searches, correlates what you search for with what other users search for, and uses it to create surprisingly appropriate recommendations.
certainly sometimes data sets are necessarily large, but arguably better data, acquired through better questions, is preferable.
managing sharding and replication across a horde of database servers is difficult and slow.
while a little scatterbrained, it covers an amazingly broad array of subjects within the realm of data science.
if you  have a cd that's not in the database (including a cd you've made yourself), you  can create an entry for an unknown album.
it incorporateshdfs, a  distributed filesystem designed for the performance and reliability  requirements of huge datasets; the hbase database;hive, which lets developers  explore hadoop datasets using sql-like queries; a high-level dataflow language  calledpig; and other components.
we've all heard the joke  that eating pickles causes death, because everyone who dies has eaten pickles.
according tomike driscoll ( @dataspora), statistics is the "grammar of data science."
machine learning  almost always requires a "training set," or a significant body of  known data with which to develop and tune the application.
what's less obvious is that mapreduce has  proven to be widely applicable to many large data problems, ranging from search  to machine learning.
what differentiates data science from  statistics is that data science is a holistic approach.
this book teaches statistics through puzzles, stories, visual aids, and real-world examples.
google is a master at creating data products.
these are frequently called nosql databases, or non-relational  databases, though neither term is very useful.
if you want to create new information from existing, read codd.
a data application  acquires its value from the data itself, and creates more data as a result.
statistics in a nutshell an introduction and reference for anyone with  no previous background in statistics.
while we aren't drowning in a sea of data, we're finding that almost everything can (or has) been instrumented.
ben fry'sprocessing is the state of the art, particularly if you need to  create animations that show how things change over time.
on the where data comes from, you don't mentions the opendata movement i'm mentioning in my article about the future of data and predictive analytics (http://www.fredberinger.com/musings-on-the-future-of-data-and-predictive-analytics-3/).
but merely using data isn't really what we mean by "data science."
casey cheshire [ 2 september 2010 06:40
once you've parsed the data, you can start thinking about the quality of your data.
manufacturing and publicising statements that are designed to influence the level of trust in financial instruments is an entire industry in its own right.
whether that data is search terms, voice samples, or product reviews, the users are in a feedback loop in which they contribute to the products they use.
programming collective intelligence learn how to build web  applications that mine the data created by people on the internet.
before it does anything else, itunes reads the  length of every track, sends it to cddb, and gets back the track titles.
in this post, i examine the many sides of data science -- the technologies,  the companies and the unique skill sets.
in addition to looking at profiles, linkedin's data scientists  started looking at events that members attended.
what is data science?
they expose rich apis, and are designed for exploring and  understanding the data rather than for traditional analysis and reporting.
once you've gotten some hints at what the data might be saying, you can follow it up with more detailed analysis.
ram has moved from $1,000/mb to roughly $25/gb -- a price reduction of about 40000, to say nothing of the reduction in size and increase in speed.
we've been writing some java and ruby code lately for in-house data analysis and even some basic segmentation of our huge inventory.
there are many libraries available for machine learning: pybrain in python, elefant, weka in java, and mahout (coupled to hadoop).
that's the beginning of data science.
but given petabytes or yottabytes of data, if you aren't using 90% of what you are storing, does it make sense to keep paying to store, migrate, etc., the data (barring legal requirements to do so)?
then you might like to join the cornell alumni group.
where do you find the people this versatile?
as with the number of followers on twitter, a "trending topics" report only needs to be current to within five minutes -- or even an hour.
one of my favorites is this animation of thegrowth of walmart over time.
physicists have a strong mathematical background, computing skills, and come from a discipline in which survival depends on getting the most from the data.
although r is an odd and quirky language, particularly to someone with a  background in computer science, it comes close to providing "one stop  shopping" for most statistical work.
kirk [ 2 june 2010 11:02 pm] thanks mike, "terabyte drives are consumer equipment" are the cord wood to my campfire.
vassilis nikolopoulos
programming collective intelligence learn how to build web applications that mine the data created by people on the internet.
i guess retweets are the new comments.
at some point, traditional techniques for working with data run out of steam.
where data comes from data is everywhere: your government, your web server, your business  partners,even your body.
they have to think about the big picture, the big problem.
five years ago, in what is web 2.0, tim o'reilly said that "data  is the next intel inside."
machine learning almost always requires a "training set," or a significant body of known data with which to develop and tune the application.
that's not a question we could even have asked a few years ago.
sites likeinfochimps and factual provide access to many large datasets, including climate data, myspace activity streams, and game logs from sporting events.
the problem with most data analysis algorithms is that they generate a set of numbers.
beautiful visualization this book demonstrates why visualizations are  beautiful not only for their aesthetic design, but also for elegant layers of  detail.
errors like this one have a direct impact on trustworthiness.
to understand what the numbers mean, the stories they are really telling, you need to generate a graph.
cost-benefit analysis is an important component.
flu trends google was able to spot trends in the swine flu epidemic roughly two weeks  before the center for disease control by analyzing searches that people were  making in different regions of the country.
while there are many commercial statistical packages, the open source r language -- and its comprehensive package library, cran -- is an essential tool.
i think you mean that nasa's data screening delayed discovery of ozone depletion (as your links discuss), not global warming.
2) a person who creates the structure or map of information which allows others to find their personal paths to knowledge.
i do question the term data 'science', as in "data science enables the creation of data products.".
while we aren't drowning in a sea of data, we're  finding that almost everything can (or has) been instrumented.
at ibm'smany eyes, many of the visualizations are full-fledged interactive applications.
many of  these databases are the logical descendants of google'sbigtable and amazon's  dynamo, and are designed to be distributed across many nodes, to provide  "eventual consistency" but not absolute consistency, and to have very  flexible schema.
the foreclosure data used in "data mashups in r " was posted on a public website by the philadelphia county sheriff's  office.
speech recognition has always been a hard problem, and it remains difficult.
they've built a dictionary of common misspellings, their corrections, and the contexts in which they occur.
it is crucial to "making data speak coherently."
that isn't always possible.
[23 december 2010 05:50 am] this is a great article about how getting information from data is going to be so important in the future.
a data application acquires its value from the data itself, and creates more data as a result.
to do it well you need to understand the grammatical structure of a  job posting; you need to be able to parse the english.
managing sharding and  replication across a horde of database servers is difficult and slow.
does a successful retail chain spread like an epidemic, and if so, does that give us new insights into how economies work?
but the process worked quite differently: it started out with a relatively small, simple program that looked at members' profiles and made recommendations accordingly.
it has excellent graphics facilities; cran includes parsers for many kinds of data; and newer extensions extend r into distributed computing.
while that sounds like a simple task, the trick was disambiguating "apple" from many job postings in the growing apple industry.
i'm surprised that there are no comments, but 123 retweets already.
when you've just  spent a lot of grant money generating data, you can't just throw the data out  if it isn't as clean as you'd like.
they aren't well-behaved xml files with all the  metadata nicely in place.
at ibm'smany eyes,  many of the visualizations are full-fledged interactive applications.
at o'reilly, we  frequently combine publishing industry data fromnielsen bookscan with our own  sales data, publicly available amazon data, and even job data to see what's  happening in the publishing industry.
if anything can be called a one-stop information platform, hadoop is it.
in data science, what you have is frequently all  you're going to get.
what differentiates data science from statistics is that data science is a holistic approach.
and they (and you) pat them on the back as being oh so clever.
why do we suddenly care about statistics and about data?
o'reilly publications related to data science r in a nutshell a quick and practical reference to learn what is becoming the standard for developing statistical software.
amazon'selastic mapreduce makes it much easier to put hadoop to work without investing in racks of linux machines, by providing preconfigured hadoop images for its ec2 clusters.
their business is fundamentally different from selling music, sharing music, or analyzing musical tastes (though these can also be "data products").
the importance of moore's law as applied to data isn't just geek  pyrotechnics.
( something like putting relativity theory in simple words e.g. linux for dummies .
strata conference new york 2011, being held sept.
it's an excellent way to classify a few thousand data points at a cost of a few cents each.
we've all heard the joke that eating pickles causes death, because everyone who dies has eaten pickles.
and it's not just companies using their own  data, or the data contributed by their users.
precision has an allure, but in most data-driven applications  outside of finance, that allure is deceptive.
a second best can be data sampling to reduce the analyzable data set to more tractable proportions.
cddb is a great example of data jiujitsu: identifying music by analyzing an audio stream directly is a very difficult problem (though not unsolvable -- seemidomi, for example).
beautiful data learn from the best data practitioners in the field  about how wide-ranging -- and beautiful -- working with data can be.
relational databases are designed for consistency, to support complex transactions that can easily be rolled back if any one of a complex set of operations fails.
it's easer to consult with clients to figure out  whether you're asking the right questions, and it's possible to pursue  intriguing possibilities that you'd otherwise have to drop for lack of time.
it would have been easy to turn this into a high-ceremony development project that would take thousands of hours of developer time, plus thousands of hours of computing time to do massive correlations across linkedin's membership.
to do data conditioning, you have to be ready for whatever comes, and be willing to use anything from ancient unix utilities such asawk to xml parsers and machine learning libraries.
it's increasingly common to  mashup data from a number of sources.
once you've gotten some hints at  what the data might be saying, you can follow it up with more detailed analysis.
the question facing every company today, every startup, every non-profit, every project site that wants to attract a community, is how to use data effectively -- not just their own data, but all the data that's available and relevant.
in hindsight, mapreduce seems like an obvious solution to google's biggest problem, creating large searches.
whether humans or software decided to ignore anomalous data, it appears that data was ignored.
in many ways "data science" and "information engineering" are synonymous; both deal with a large amount of unstructured data and form conclusions, inferences and predictions.
i just wrote about this and related topics over the long weekend.
information platforms are similar to traditional data warehouses,  but different.
it's not easy to get a handle on jobs in data science.
but hadoop (and particularly elastic mapreduce) make it easy to  build clusters that can perform computations on long datasets quickly.
but the cddb staff used data creatively to solve a much more tractable problem that gave them the same result.
i have posted this article on cloud secuirty alliance group in linked in.
edward tufte'svisual display of quantitative information is the classic  for data visualization, and a foundational text for anyone practicing data  science.
mike loukides [28 october 2010 11:14 am] there's been a good discussion on quora on how to become a data scientist: http://www.quora.com/how-do-i-become-a-data-scientist
you need some creativity for when the story the data is telling isn't what you think it's telling.
that's what relations do.
if anything can be called a one-stop  information platform, hadoop is it.
rukmal fernando [ 2 june 2010 10:25 pm] great read!
faster computations make it easier to test different assumptions, different datasets, and different algorithms.
should we consider creating data science programs?
loss of privacy and the simultaneous rise of data engineering is no accident.
spell checking isn't a terribly difficult problem, but by suggesting  corrections to misspelled searches, and observing what the user clicks in  response, google made it much more accurate.
data is frequently missing or incongruous.
asking things like, did you go to cornell?
it's usually impossible to get "better" data,  and you have no alternative but to work with the data at hand.
for those commenting that this is the best article they've read about data, please allow me to introduce you to w. edwards deming (http://en.wikipedia.org/wiki/w._edwards_deming).
google isn't the only company that knows how to use data.
1) the individual who organizes the patterns inherent in data, making the complex clear.
then at books members had in their libraries.
mobile applications leave  an even richer data trail, since many of them are annotated with geolocation,  or involve video or audio, all of which can be mined.
hadoop processes data  as it arrives, and delivers intermediate results in (near) real-time.
we're talking about generating individual profiles on the 'feedback' side of the loop such that 'controls' can be implemented on other half of the loop.
it's the  kind of question we now ask routinely.
i love how "data science" is really taking on a life of its own now.
when you've just spent a lot of grant money generating data, you can't just throw the data out if it isn't as clean as you'd like.
if the problem involves human language, understanding the data adds another dimension to the problem.
they were the  vanguard, but newer companies like bit.ly are following their path.
the problem with most data analysis  algorithms is that they generate a set of numbers.
as far image data sets are concerned , being multidimensional has a total mathematical basis for its processing and result retrievals.
quantification of trustworthiness is one of the harder problems.
traditional relational database systems stop being effective at this scale.
the more  storage is available, the more data you will find to put into it.
it has a 5 mb capacity and it's stored in a cabinet roughly the size of a luxury refrigerator.
michael r. bernstein [ 3 june 2010 08:11 am] "the future belongs to the companies who figure out how to collect and use data successfully.
what can we do to offer better more relevant education to individuals who may be pursuing or using data science?
your account by mike loukides | @mikeloukides | +mike loukides | comments: 52 | 2 june 2010 tweet what is data science?
if you can split your task up into a large number of subtasks that are easily described, you can use mechanical turk's marketplace for cheap labor.
i think data along with better processing techniques is the future.
[22 march 2011 03:38 am] while data science is incredibly beneficial and helping to improve the standard of living of humanity, i'm not so sure that i go so far as to agree with hal varian that statistics is the next sexy job.
statistics plays a role in everything from traditional business intelligence (bi) to understanding how google's ad auctions work.
they can tackle  all aspects of a problem, from initial data collection and data conditioning to  drawing conclusions.
in addition to being physicists, mathematicians, programmers, and  artists, they're entrepreneurs.
precision has an allure, but in most data-driven applications outside of finance, that allure is deceptive.
gracenote built a database  of track lengths, and coupled it to a database of album metadata (track titles,  artists, album titles).
andrew walkingshaw [ 2 june 2010 02:42 pm] as one of the three founders of timetric (http://timetric.com), all of whom have a background in the physical sciences, i'm biased: but we couldn't agree more.
mechanical turk is also an important part of the toolbox.
information engineering is a process to extract the useful knowledge or result from a set of information which can be data and sometime intangibles also or perceived ones.
do you really care if you have 1,010 or 1,012 twitter  followers?
the most popular open source implementation of mapreduce is the hadoop  project.
it started small, and added value iteratively.
m s prasad [ 9 june 2010 01:40 am] great knowledge filled post and good coverage of tehnology.
if you have already reduced the set to 10,000 postings with the word "apple," paying humans $0.01 to classify them only costs $100.
google has indexed many, many websites about large snakes.
most data analysis is comparative: if you're asking whether sales to northern europe are increasing faster than sales to southern europe, you aren't concerned about the difference between 5.92 percent annual growth and 5.93 percent.
entrepreneurship is another piece of the puzzle.
in addition to being physicists, mathematicians, programmers, and artists, they're entrepreneurs.
does google's new machine transcription service change the game?
according to dj patil, chief scientist atlinkedin (@dpatil), the best data scientists tend to be "hard scientists," particularly physicists, rather than computer science majors.
[20 june 2011 12:45 am] very interesting post..i get a lot of information and insights here..
in hindsight, mapreduce seems like an obvious  solution to google's biggest problem, creating large searches.
that arithmetic error really hurts.
it then branched out incrementally.
the foreclosure data used in "data mashups in r " was posted on a public website by the philadelphia county sheriff's office.
the point is how one can get benefit from unstructured massive amount of data.
statistics plays a role in  everything from traditional business intelligence (bi) to understanding how  google's ad auctions work.
i want to know more about how video data is being analyzed.
mobile applications leave an even richer data trail, since many of them are annotated with geolocation, or involve video or audio, all of which can be mined.
if you've ever seen the html that's generated by excel, you know that's going to be fun to process.
once you've collected your training data (perhaps a large collection of public photos from twitter), you can have humans classify them inexpensively -- possibly sorting them into categories, possibly drawing circles around faces, cars, or whatever interests you.
many of the key hadoop developers have found a home atcloudera, which  provides commercial support.
head first statistics
if you have already reduced the set to  10,000 postings with the word "apple," paying humans $0.01 to  classify them only costs $100.
but despite my nitpicking, this a good article.
visualization is crucial  to each stage of the data scientist.
yall made up web 2.0.
data is indeed the new intel inside.
it isn't superseded by newer techniques from machine learning and other disciplines; it complements them.
this book teaches statistics through puzzles,  stories, visual aids, and real-world examples.
max j. pucher - chief architect isis papyrus software
if you can split your task up into a large number of  subtasks that are easily described, you can use mechanical turk's marketplace  for cheap labor.
hadoop is essentially a batch system, but hadoop online prototype (hop) is  an experimental project that enables stream processing.
google, amazon, facebook, and linkedin have all tapped into their datastreams and made that the core of their success.
yes the web is full of data-driven apps but semantic data-driven apps is the future.
data science  enables the creation of data products.
but it takes statistics to  know whether this difference is significant, or just a random fluctuation.
hadoop goes far beyond a simple mapreduce implementation (of which there  are several); it's the key component of a data platform.
while this sounds simple enough, it's revolutionary: cddb views music as data, not as audio, and creates new value in doing so.
if data is missing, do  you simply ignore the missing points?
describing the data science group he put together at  facebook (possibly the first data science group at a consumer-oriented web  property), jeff hammerbacher said: ... on any given day, a team member could author a multistage processing  pipeline in python, design a hypothesis test, perform a regression analysis  over data samples with r, design and implement an algorithm for some  data-intensive product or service in hadoop, or communicate the results of our  analyses to other members of the organization3
speech recognition has always been a hard problem, and it remains  difficult.
they group together fundamentally dissimilar products by telling you what they aren't.
it isn't  superseded by newer techniques from machine learning and other disciplines; it  complements them.
mike loukides [ 5 june 2010 05:37 am] i'm glad you liked the article.
data science enables the creation of data products.
they come about because amazon understands that a  book isn't just a book, a camera isn't just a camera, and a customer isn't just  a customer; customers generate a trail of "data exhaust" that can be  mined and put to use, and a camera is a cloud of data that can be correlated  with the customers' behavior, the data they leave every time they visit the  site.
more to the point, it's easy to notice that one advertisement forr in a nutshell generated 2 percent more conversions than another.
for computer vision, theopencv library is a de-facto standard.
data science , as i understand has been a forte of statistics & mathematicians for long .
in data science, what you have is frequently all you're going to get.
google'spagerank algorithm was among the first to use data outside of the page itself, in particular, the number of links pointing to a page.
nathan yau's flowingdata blog is a great place to look for creative visualizations.
her job as scientist at bit.ly is really to  investigate the data that bit.ly is generating, and find out how to build  interesting products from it.
statistics in a nutshell an introduction and reference for anyone with no previous background in statistics.
it's usually impossible to get "better" data, and you have no alternative but to work with the data at hand.
these recommendations are "data products" that help to drive amazon's more traditional retail business.
ram  has moved from $1,000/mb to roughly $25/gb -- a price reduction of about 40000,  to say nothing of the reduction in size and increase in speed.
to do  data conditioning, you have to be ready for whatever comes, and be willing to  use anything from ancient unix utilities such asawk to xml parsers and machine  learning libraries.
joel [ 2 june 2010 09:25 pm] ...and still, to date, the best tool to mess around with large data sets is sas - been around for ages.
if you have a cd that's not in the database (including a cd you've made yourself), you can create an entry for an unknown album.
[21 july 2010 07:47 am] excellent article on new data management trends... traditional approaches to db and storage / analytics have to be changed, in order to cope with this huge evolution on data and stream decision analysis... olap, bi and traditional data mining have to be "updated" with new r&d tips... david alan christopher
great post by the way.
this is facebook's play in the display advertising game, with their launch of the "i like" feature.
factual enlists users to update and improve its datasets, which cover topics as diverse as endocrinologists to hiking trails.
google popularized themapreduce approach, which is basically a divide-and-conquer strategy for distributing an extremely large problem across an extremely large computing cluster.
these features only require soft real-time; reports on trending topics don't require millisecond accuracy.
hbase: part of the apache hadoop project, and modelled on google's  bigtable.
igor [ 3 june 2010 01:14 am] great article!
nathan yau's flowingdata blog is a great place to look for creative  visualizations.
seems that as the web matures, so too does the tool sets and methodologies it uses to extract value.
computing a signature based on track lengths, and then looking up that signature in a database, is trivially simple.
in the last few years, there has been an explosion in the amount of data that's available.
visualization is also frequently the first step in analysis.
brian shaler [ 4 june 2010 04:40 pm] i'm going to cite the hell out of this article.
this is really getting real !
it is  crucial to "making data speak coherently."
making data tell its story a picture may or may not be worth a thousand words, but a picture is  certainly worth a thousand numbers.
many sources of "wild  data" are extremely messy.
the result was a valuable data product that analyzed a huge  database -- but it was never conceived as such.
the importance of moore's law as applied to data isn't just geek pyrotechnics.
you can allocate and de-allocate processors as needed, paying only for the time you use them.
the most meaningful  definition i've heard:"big data" is when the size of the data  itself becomes part of the problem.
does it look  like the spread of cancer throughout a body?
it would be nice if there was a standard set of tools to do the job, but there isn't.
[23 october 2010 03:08 pm] great article.
but we've seen much bigger increases in storage capacity, on every level.
this graph shows the increase in cassandra jobs, and the companies listing cassandra positions, over time.
no one in the nascent data industry is trying to build the 2012 nissan stanza or office 2015; they're all trying to find new products.
no, that isn't an attaboy.
traditional data analysis has been hampered by extremely long turn-around times.
here's a few examples: google's breakthrough was realizing that a search engine could use input  other than the text on the page.
this is the heart of what patil calls "data jiujitsu" -- using  smaller auxiliary problems to solve a large, difficult problem that appears  intractable.
while there are two dozen or so products available (almost all of them open source), a few leaders have established themselves: cassandra: developed at facebook, in production use at twitter, rackspace, reddit, and other large sites.
it's not just an application with data; it's a data product.
does the maximum vertical point on the graph represent 3 listings or 300,000?
it's reported that the discovery of ozone layer depletion was delayed becauseautomated data collection tools discarded readings that were too low 1.
to get a sense for what skills are required, let's look at the data lifecycle: where it comes from, how you use it, and where it goes.
there's a database behind a web front  end, and middleware that talks to a number of other databases and data services  (credit card processing companies, banks, and so on).
they accept all data formats, including the most messy, and their schemas evolve as the understanding of the data changes.
if you start a calculation, it might not finish for hours, or even days.
google, amazon, facebook, and linkedin have all tapped into  their datastreams and made that the core of their success.
we are seeing more data in formats that are easier to consume:  atom data feeds, web services, microformats, and other newer technologies
[16 june 2010 08:59 am] an incredibly well written article...
if you've ever used itunes to rip a cd, you've taken advantage of this database.
but old-stylescreen  scraping hasn't died, and isn't going to die.
they've built a dictionary of  common misspellings, their corrections, and the contexts in which they occur.
a new startup,riptano, provides commercial support.
aw heck, i hate to say this mike, but "data are", not "data is".
jewel ward [21 june 2010 12:53 pm] i would add to this list of aspects of data science the ability to determine the best way to migrate, store, & archive data.
fred leo irakliotis [ 5 july 2010 08:22 pm] as i keel reading and reading the article, the academic inside me keeps repeating the same question: do we (academics) do a good, or even adequate job, at educating future data scientists?
with the skills you list, it sounds like you're a good part of the way there already... mike steve ardire
a  new startup,riptano, provides commercial support.
making data tell its story isn't just a matter of presenting results; it involves making connections, then going back to other data sources to verify them.
since the early '80s, processor speed has increased from10 mhz to 3.6 ghz -- an increase of 360 (not counting increases in word length and number of cores).
3) the emerging 21st century professional occupation addressing the needs of the age focused upon clarity, human understanding, and the science of the organization of information.
and this is one place where "art" comes in: not just the aesthetics of the visualization itself, but how you understand it.
martin king [20 june 2010 10:46 am] there are many aspects to science much of it consists of the daily grind with data and the tools of science.
patil's first flippant answer to "what kind of person are you looking for when you hire a data scientist?" was "someone you would start a company with."
2  "information platforms as dataspaces," by jeff hammerbacher (in beautiful data) 3  "information platforms as dataspaces," by jeff hammerbacher (in beautiful data) tags: data, data science, data scientist tweet
try usinggoogle trends to figure out what's happening with thecassandra database or the python language, and you'll get a sense of the problem.
they expose rich apis, and are designed for exploring and understanding the data rather than for traditional analysis and reporting.
now you make up data science.
the problem of trust is interesting.
(http://www.fredberinger.com/is-the-internet-of-things-ready-for-prime-time/)
most data analysis is  comparative: if you're asking whether sales to northern europe are increasing  faster than sales to southern europe, you aren't concerned about the difference  between 5.92 percent annual growth and 5.93 percent.
cddb arises entirely from viewing a musical problem as a data  problem.
there are many libraries available for machine learning: pybrain in python,  elefant, weka in java, and mahout (coupled to hadoop).
to store huge datasets effectively, we've seen a new breed of databases  appear.
i bet this space will only grow as data becomes more and more important for organizations world-wide.
we're discussing data problems ranging  from gigabytes to petabytes of data.
data is only  useful if you can do something with it, and enormous datasets present  computational problems.
the data  exhaust you leave behind whenever you surf the web, friend someone on facebook,  or make a purchase in your local supermarket, is all carefully collected and  analyzed.
roger magoulas, who runs the data analysis group at o'reilly, was recently searching a database for apple job listings requiring geolocation skills.
humans tend to misinterpret many effects as causes because the effect can be seen while the cause doesn't exist as a singular event but is a complex web if feeding stimuli that need a complex web of receptive context to actually make something happen.
to get a sense for what skills are required, let's look at the data  lifecycle: where it comes from, how you use it, and where it goes.
all human activity cannot be causally interpreted because social activity must be seen as complex adaptive systems that undergo continuous change that is hardly ever reflected in the data model and processing.
i love the examples of facebook, linkedin and amazon using patterns to suggest other relationships and cross sell.
whether you look at bits per gram, bits per dollar, or raw capacity, storage has more than kept pace with the increase of cpu speed.
one of the best articles i've ever read.
they share the fact that their principal drivers are forms of pollution from human activities, but they are not the same in their impacts or the actions needed to mitigate them.
i probably should have phrased it more like, "with all these hip new projects cropping up [...]" jessie wells [ 4 june 2010 08:04 pm] hi mike, thanks for a really interesting article.
however, i must make two points: 1) actuarial science !=
it started small, and added  value iteratively.
hilary mason came to the same conclusion.
it's easy to  distribute a search across thousands of processors, and then combine the  results into a single set of answers.
it was an agile, flexible process that built toward its goal  incrementally, rather than tackling a huge mountain of data all at once.
disambiguation is never an easy task, but tools like thenatural  language toolkit library can make it simpler.
traditional relational  database systems stop being effective at this scale.
while that sounds like a simple task, the trick was  disambiguating "apple" from many job postings in the growing apple  industry.
data conditioning can involve cleaning up messy html with tools like beautiful soup, natural language processing to parse plain text in english and other languages, or even getting humans to do the dirty work.
i do feel a little uneasy, though, reading about the ability to extract all the types of information from raw data.
hadoop has been instrumental in enabling "agile" data analysis.
we now  expect web and mobile applications to incorporate recommendation engines, and  building a recommendation engine is a quintessential artificial intelligence  problem.
most of the organizations that have built data platforms have found it necessary to go beyond the relational database model.
michael f. martin [ 2 june 2010 01:33 pm] accountants are a kind of data scientist too.
to store huge datasets effectively, we've seen a new breed of databases appear.
statistics has become a basic skill.
in addition to looking at profiles, linkedin's data scientists started looking at events that members attended.
when natural language processing fails, you can replace artificial intelligence with human intelligence.
increased storage capacity demands increased sophistication in the  analysis and use of that data.
that joke doesn't work if you understand what correlation means.
hilary mason says that when she  gets a new data set, she starts by making a dozen or more scatter plots, trying  to get a sense of what might be interesting.
gnuplot is very  effective; r incorporates a fairly comprehensive graphics package; casey reas'  and
tracking links made google searches much more useful, and  pagerank has been a key ingredient to the company's success.
you can allocate and  de-allocate processors as needed, paying only for the time you use them.
working with data at scale we've all heard a lot about "big data," but "big" is  really a red herring.
for computer vision, theopencv library  is a de-facto standard.
we've all heard a lot about "big data," but "big" is really a red herring.
cddb is a great success because their data model is simple and the data are really very limited, a few million rows.
let me however add that much of the data that is collected is irrelevant.
there are many packages for plotting and presenting data.
patil described the process of creating the group recommendation feature at linkedin.
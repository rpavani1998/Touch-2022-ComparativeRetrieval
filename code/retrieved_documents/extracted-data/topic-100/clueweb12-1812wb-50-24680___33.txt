i should also say that many of  our comparisons apply to computational science in general, not just climate  science, although we used climate modeling for many specific examples.
for example: more interestingly, we talked about what happens when these contextual factors change over time.
i started off with a quick overview of the three research themes we identified at the oopsla workshop in the fall: inevitably, we spent most of our time this week talking about the first topic – software engineering of computational models, as that’s the closest to the existing expertise of the group, and the most obvious place to start.
you mention that for the climate models the “developers are domain experts – they do not delegate programming tasks to programmers, which means they avoid the misunderstandings of the requirements common in many software projects”.
this is not universally true, but where it is not, it causes coordination problems.
even things likescipy.
and it comes with “batteries included.”
being smart is only an advantage if you design and build simple software.
or even, in the case of health care, when change in the  regulatory framework relaxes a constraint – such as the recent us  healthcare bill, under which it (presumably) becomes easier to share health  records among medical professionals if knowledge of pre-existing conditions is  no longer a critical privacy concern.
(edit: oops, i just noticed the last of these is what you already linked to; maybe you should contact the authors and see what they’ve done more recently).
this is used as an  organising principle both for the code modules, and also for the teams of  scientists who contribute code.
do these contextual factors really explain observed  differences in se practices, and if so how?
fortran is the language of choice and the reason has nothing to do with legacy code.
i see more of “one should use what one is most productive in, whether that is because of familiarity or technique.”
we also speculated about some other contextual factors that might  distinguish different software engineering species, not necessarily related to  our analysis of computational science software.
or even, in the case of health care, when change in the regulatory framework relaxes a constraint – such as the recent us healthcare bill, under which it (presumably) becomes easier to share health records among medical professionals if knowledge of pre-existing conditions is no longer a critical privacy concern.
for climate modeling centers, it would be almost impossible to discard the existing models, or to port them to another language.
which means there’s really no push to  discard fortran.
generally the resolution required to show ‘convergence’ depends on the functional; for functionals like the global mean (of anything), i’d think that youcould show convergence (in fact one of those papers i linked mentions that some things converge and some things don’t).
i started off with a quick overview of the three research  themes we identified at the oopsla workshop in the fall: climate modeling, which we could characterize as a kind of end-user  software development, embedded in a scientific process; global collective decision-making, which involves creating the software  infrastructure for collective curation of sources of evidence in a highly  charged political atmosphere; green software engineering, including carbon accounting for the software  systems lifecycle (development, operation and disposal), but where we have no  existing no measurement framework, and tendency to to make unsupported claims  (aka greenwashing).
but we felt  we might have some difficulty recruiting subjects for such a study (unless we  concealed our intent), and we would probably get into trouble once we tried to  publish the results
i guess it is possible that systematic errors are being made  by many different climate modeling groups in the same way, but these  wouldn’t be coding errors – they would be errors in the  understanding of the physical processes and how best to represent them in a  model.
as a person who has spent decades doing technical/engineering programming in areas unrelated to climate modeling, i find the post very interesting.
i do make use of polymorphism, which i think has helped in achieving a reasonably clean design in my case.
they merely lack the skills and/or time to improve the code through refactoring, and our understandably apprehensive about change.
on architectures, we appear to have different definitions of the word “architecture”.
like to climate modeling community the mesoscale model development community is done at the local university and national labs.
in practice, optimization, where it  is done, tends to be an afterthought.
the software developers are very smart people.
close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading... close previewloading...
i’ve added a link here (http://www.easterbrook.ca/steve/?p=974) to the draft version which is a little longer (we had to condense it for page limits).
interestingly, in my experience, most modelers are to some degree aware that there is a problem, and are even open to assistance in reducing the source code “entropy”.
audris pointed out that given the  volumes of data being handled (terrabytes per day), there are almost certainly  errors introduced in storage and retrieval (i.e. bits getting flipped), and  enough that standard error correction would still miss a few.
in contrast, most other  types of software engineering focus instead on unit testing, with elaborate  test harnesses to test pieces of the software in isolation from the rest of the  system.
in our  discussions at the workshop, we came up with many insights for mainstream  software engineering, which means this is a two-way street: plenty of  opportunity for re-examination of mainstream software engineering, as well as  learning how to study se for climate science.
steve thanks for sharing from your reading list.
an  example used by pretty much every climate model ismpi.
software architecture (at least the way software engineers use the term) is different from choice of implementation algorithm.
now imagine we have already characterized this space of species of se.
i’m writing a talk for next week on software infrastructures for earth system modelling.
this is interesting, because one  might expect climate modelers to put a huge amount of effort into optimization,  given that century-long climate simulations still take weeks/months on some of  the world’s fastest supercomputers.
i was trying to say that it’s an important success factor at the uk met office; i’m now exploring this assertion by looking at other models for which development is more distributed.
programming is also an art.
we ended up discussing three closely related issues: how do we characterize/distinguish different points in this space  (different species of software engineering)?
but we couldn’t think of  any existing empirical evidence to support such a claim.
if  there was something better they would use it.
the underlying methods are very different.
i mentioned this above, but  want to add something here.
allgcms have the same conceptual  architecture, and it is unchanged since modeling began, because it is derived  from the natural boundaries in physical processes being simulated [edit: i mean  the top level organisation of the code, not the choice of numerical methods,  which do vary across models - see bob's comments below].
we think there are lots of interesting studies to be done of what happens to the software development processes for different species of software when such contextual factors change.
in my experience (when i was a climate modeller, talking to climate modellers), many who had had *enough* exposure to object orientation, would cheerfully admit that o-o codes could well have made some model tasks easier, but given that the job was already done in fortran … … so my experience is precisely the opposite of bob’s.
you haven’t talked to many meteorologists/physicist/engineers have you?
you’re talking about the implementation choices for the core numerical equations in an atmospheric and/or ocean model.
as a good engineer, i am automatically a good programmer.
i decided to refactor the fortran implementation, and was very pleased by the results.
the reason is that the model is changed  so frequently that hand optimization of any particular model version is not  useful.
imho, the most serious consequence of a climate software being defective would be to then use it to make a defective political decision costing trillions of dollars to society.
as a good scientist, i am automatically a good engineer.
it seems reasonable, as you say, that the changes only matter for more regional things, but we really don’t know until we converge a solution (and aren’t the regional things where we can make a much more direct connection to the things people care about?).
most open source projects are built by people who  want a tool for their own use, but that others might find useful too.
even a computer science degree does not make you a good programmer.
pcm from ncar is a merger of multiple model types from universities into a single framework model.
it has a particularly simple object model, that you can completely ignore if you want to.
fortran is the language used because it allows you to express the mathematics and physics in a very clear succinct fashion.
again, these are often features of modern software of all types.
farmers are domain experts too.
the se research community hasn’t really tackled the question of how the different contexts in which software development occurs might affect software development practices, nor when and how it’s appropriate to attempt to generalize empirical observations across different contexts.
the modules and integrated system each have independent lives,  owned by different communities.
for example, the emergence of a competitor where there was none previously, or the creation of a new regulatory framework where none existed.
march  24th, 2010 sme leave a comment go to comments
there areglimmers of hope, but the results were not encouraging.
object-oriented programming is the answer to a question that nobody has ever felt the need to ask.
indeed, i’ve used python for modeling; not a climate-related model, but a relatively complex model in the software engineering domain itself (as yet unpublished).
the reasons are simple: there is a huge body of  legacy fortran code, everyone in the community knows and understands fortran  (and for many of them, only fortran), and fortran is ideal for much of the work  of coding up the mathematical formulae that represent the physics.
[openmp doesn't seem to have any  bigger fanclub].
and frankly, for longer term, the climate problem just doesn’t seem to be susceptible to upscaling issues (at least for the global mean)
a lot of our discussion was focussed on the observation that climate  modeling (and software for computational science in general) is a very  different kind of software engineering than most of what’s discussed in  the se literature.
the fact is that fortran is the language of choice, and there are many good reasons why this is so
the whole language choice seems always to degenerate into a religious war, and like all religious wars, the sides tend to be chosen on the basis of where one grew up.
hi steve, thanks for your insights into the minds of climate modelers.
for example, a particular ocean model might be  used uncoupled by a large community, and also be integrated into several  different coupled climate models at different labs.
i will get on to the kinds of correctness/validation questions you’re asking about, but need to do some more groundwork first.
once we’ve identified what we think are important distinguishing  traits (or constraints), how do we investigate whether these are indeed salient  contextual factors?
what would be the impact of  software errors in climate models?
there are very few resources available for software infrastructure .
the single site development thing is interesting, and i grossly oversimplified.
the only reasonable path to assessing productivity that we can think  of must focus on time-to-results, or time-to-publication, rather than on  software development and delivery.
i wonder if this preference is due to differences in the domain, or in problem solving conventions shaped by the languages themselves.
with over 30 years of scientific programming behind me, i really don’t see any universal hatred of any language or methodology such as oo.
it’s like we’ve identified a new species of  software engineering, which appears to be a an outlier (perhaps an entirely new phylum?).
(parameterisations do have “fudge” factors in them, but we like to call those empirical adjustments.
a second miss is “a stable architecture, defined by physical processes: atmosphere, ocean, sea ice, land scheme,….
we pondered how  often hardware errors occur in supercomputer installations, and whether if they  did it would affect the software.
and if none of the traditional software  engineering metrics (e.g. for quality, productivity, …) can be used for  cross-species comparison, how can we do such comparisons?
more research needed.
an example from climate modeling:  software that was originally developed as part of a phd project intended for  use by just one person eventually grows into a vast legacy system, because it  turns out to be a really useful model for the community to use.
because  there are many other modeling groups, and scientific results are filtered  through processes of replication, and systematic assessment of the overall  scientific evidence, the impact of software errors on, say, climate policy is  effectively nil.
i’d really like to see some reports on grid convergence results for the models used in the ipcc’s write-ups).
the spherical geometry was in a separate module, as were the bits of logic for accessing files to get offline wind fields and implementing periodicity.
inevitably, we spent most of our time this week talking about the first  topic – software engineering of computational models, as that’s the  closest to the existing expertise of the group, and the most obvious place to  start.
further, many of the folk i know choose to use o-o languages, particularly python, to do data analysis, so far from hating it … it’s an analysis language of choice.
a good parallel to climate models are computational chemistry codes such as gaussian, spartan, gamess, molpro, etc.
what kinds of study are needed to investigate these  contextual factors?
even if we could do that, we could only afford to run one instance of the model anyway, and that wouldn’t be very interesting … certainly not for decadal and near term prediction, and frankly, for longer term, the climate problem just doesn’t seem to be susceptible to upscaling issues (at least for the global mean).
at the very least the next developer would have a much easier time introducing for example an adaptive rk scheme.
other “success factors” you mention, such as: highly tailored process, single site development, shared ownership, commitment to quality, openness, and benchmarking are all attributes commonly found in successful software projects, regardless of ilk.
home > climate modeling, research methods >  what makes software engineering for climate models different?
arbitrary schedules merely mean arbitrary functionality.
(disclosure: i still consider myself a dynamicist, my day job used to be the science of gravity waves and their parameterisation in models.
the dimensions that matter for the correctness of pde solvers are pretty well established (see roache’s early work, also more recently oberkampf/trucano/et al out of sandia and a host of various others scattered about); the dimensions that matter for usefulness in decision support are probably a little more of an open research area (but there’s still plenty of work in this area that you guys seem to be ignoring, maybe this is too far away from the development process to interest you?).
however, the modelers don’t necessarily  derive some of the usual benefits of stable software architectures, such as  information hiding and limiting the impacts of code changes, because the  modules have very complex interfaces between them.
it seems to have just enough in the way of functional language structures that succinctly writing model equations is not too much of a chore.
(in another language, i’m sure my code would run a lot faster, but that’s not my principal concern.)
i’m more than prepared to admit that the costs would be counterproductive, but i want to be convinced.
an error may affect some of the experiments  performed on the model, with perhaps the most serious consequence being the  need to withdraw published papers (although i know of no cases where this has  happened because ofsoftware errors rather than methodological errors).
software development activities are completely entangled with a  wide set of other activities: doing science.
how should the contextual factors be taken into account in  other empirical studies?
but such is not the case with the climate software.
testing focuses almost exclusively on integration testing.
when we discussed this in the group, we  all agreed this is a very significant factor, and that you don’t need  much (formal) process with very smart people.
thirdly “the programming language of choice is fortran, and is unlikely to change for very good reasons.
all far easier to understand.
it’s as complicated as people can make it.
this discovery (and the resulting comparisons) seems to tell  us a lot about the other species that we thought we already understood.
consensus software engineering practices should be entirely sufficient when developing climate software.
the amr framework, chombo, is all c++, as is the vorpal em/accelerator/plasma framework.
the only common feature in terms of programming tools amongst modelers is a universal hatred of object-oriented programming languages, particularly python.
then there is: “the software developers are very smart people.”
the synergia beam modeling framework is python linking in modules in fortran and c++.
it was a select gathering, with many great  people, which meant lots of fascinating discussions, and not enough time to  type up all the ideas we’ve been bouncing around.
i read the links, quickly, so sorry if i go off on the wrong tack.
i’ve some papers on my “to read” pile that might answer some of your questions, josh.
this is all the more odd to me since the science upon which climate software is based is mostly multidisciplinary.
my meteorology majors will graduate with a second degree in either mathematics or computer science.
nearly all modelers that i know are fluent not only in fortran, but c, c++, and perl as well.
miroc3.2, ingv-sxg, echam5/mpi-om are examples of university generated models.
are they real or imagined?
there really has not been any  serious consideration of these various contextual factors and their impact on  software practices in the literature, and hence we might need to re-think a lot  of the ways in which claims for generality are handled in empirical software  engineering studies.
so i fail to understand what is so distinguishing about such traits.
a question would be, how are these common attributes measured and documented for climate software?
in our discussions at the workshop, we came up with many insights for mainstream software engineering, which means this is a two-way street: plenty of opportunity for re-examination of mainstream software engineering, as well as learning how to study se for climate science.
yes, there are limits to what can be done with a complex mathematical relationship expressed in source code, but there is little excuse for much of the implementation quality in the remaining portions of the code.
it’d be surprising if there weren’t exceptions.
experience with trying to measure defect rates in  climate models suggests that it is much harder to make valid comparisons than  is generally presumed in the software literature.
the software is used to build a product line, rather than an  individual product.
finally, we talked a bit about the challenge of finding metrics that are  valid across the vastly different contexts of the various software engineering  species we identified.
ncar coordinates how these multiple communities contribute to the coupled model, with a team dedicated to managing coordination of the various community contributions, folding externally contributed changes into the coupled model.
my experience is that different scientific communities have made different choices, even though they are all coding up equations.
in contrast, climate models are absolutely  central to the scientific work on which the climate scientists’ job  performance depends.
and another: the move from single site development (which is how nearly all climate models were developed) to geographically distributed development, now that it’s getting increasingly hard to get all the necessary expertise under one roof, because of the increasing diversity of science included in the models.
i think you’ve just given me a few slides – which will of course get due attribution.
i’ve no idea of the answer to the first  question, but the second is readily handled by the checkpoint and restart  features built into all climate models.
finally, we talked a bit about the challenge of finding metrics that are valid across the vastly different contexts of the various software engineering species we identified.
the idea here is that a craftsman has many tools in his tool chest the amateur believes everything is a nail.
i’ve been meaning to do a post on the different convergence behavior of a weather-like functional compared to a climate-like functional using the lorenz ‘63 system, but it keeps getting pushed down my queue… parameterisations do have “fudge” factors in them, but we like to call those empirical adjustments.
for example: existence of competitors; whether software is developed for single-person-use versus intended for  broader user base; need for certification (and different modes by which certification might  be done, for example where there are liability issues, and the need to  demonstrate due diligence) whether software is expected to tolerate and/or compensate for hardware  errors.
the tools  are built on the side (i.e. not part of the developers’ main job  performance evaluations) but most such tools aren’t critical to the  developers’ regular work.
there are alsoframeworks for structuring climate models and  coupling the different physics components (more on these in asubsequent post).
hence, the  inter-dependence has to be continually re-negotiated.
then there is the statement: “the software has huge societal importance, but the impact of software errors is very limited.”
and another:  the move from single site development (which is how nearly all climate models  were developed) to geographically distributed development, now that it’s  getting increasingly hard to get all the necessary expertise under one roof,  because of the increasing diversity of science included in the models.
finite differencing vs spectral methods vs finite element vs analytical solutions all have significantly different methods for solving the same problem.
sure; i understand that, nobody’s (at least i’m not) asking for dns or molecular dynamics simulations of atmospheric and ocean flows over centuries, but how solid is the empirical basis for you parameter choice when it is used as a fix for a grid you know is under-resolved?
you ask what makes software engineering for climate models different.
a lot of our discussion was focussed on the observation that climate modeling (and software for computational science in general) is a very different kind of software engineering than most of what’s discussed in the se literature.
you are correct the models are constrained by physical processes, but the models do not have the same conceptual architecture and certainly have changed significantly.
we spent some time talking about the specific case of defect measurements, but i’ll save that for a future post.
we discussed how much this directly  impacts the climate modellers, and i have to admit i don’t really know.
however, i also think the situation is  changing rapidly, especially in the last few months, and climate scientists of  all types are starting to feel more exposed.
we identified lots of contextual factors that seem to matter.
the communities who care  about the ocean model on its own will have different needs and priorities than  each of communities who care about the coupled models.
what measures of software quality attributes (e.g. defect rates, productivity,  portability, changeability…) are robust enough to allow us to make valid  comparisons between species of se?
even in the hard-core numerical portions, much can often be done to improve the situation.
at the workshop we identified many more distinguishing traits, any of which  might be important: a stable architecture, defined by physical processes: atmosphere,  ocean, sea ice, land scheme,….
we think there are lots of interesting studies to be done of what happens  to the software development processes for different species of software when  such contextual factors change.
maybe we should have farmers design and build tractors.
my sense is that all of the modelers i’ve interviewed are shielded to a  large extend from the political battles (i never asked them about this).
i once worked on a project where i was helping some scientists translate some idl code into fortran (for performance and portability).
internal constraints are those that are imposed on the software team  by itself, for example, choices of working style, project schedule, etc.
those  scientists who have been directly attacked (e.g. mann, jones,  santer) tend to be scientists more involved in creation and analysis of  datasets, rather than gcm developers.
hence, we described them as mission-critical, but only in  a personal kind of way.
programming is a domain in its own right.
most fusion codes are fortran, but many are c, and the facets framework is c++ that links in modules written in c++, fortran, python, and c. much viz work and data analysis is now done with python/matplotlib/scipy, while other is done within the visit c++ application.
steve] bob pasken : fortran is the language of choice and the reason has nothing to do with legacy code.
the middle two papers are a little older, and seem to date from the time when atmosphere models still needed flux adjustments.
how can something be of great importance whether or not it is correct?
in embedded software, the testing environment usually needs to  simulate the operational environment; the most extreme case i’ve seen is  the software for the international space station, where the only end-to-end  software integration was the final assembly in low earth orbit.
many of the mesoscale models and climate models were initial developed at universities then migrated to the larger computing facilities over time.
i was invited to run a working group on the challenges to empirical software engineering posed by climate change.
in the end, the top level rk routine looked almost like something you would see in a text book.
we scientists have become all too accustomed to 1000+ line procedures with short variable names, and are generally unaware of what clean, understandable code can look like.
i ran across a rk routine and could tell that a runge-kutta scheme was lurking in the 200-300 lines of code.
there is a relationship between software schedule and the software’s functionality.
but i also was able to discover that the treatment of the lat-lon grid was terribly inaccurate near the poles and was able to introduce a more correct geometric treatment that allowed 10x larger step sizes.
all gcms have the same conceptual architecture, and it is unchanged since modeling began, because it is derived from the natural boundaries in physical processes being simulated.”
…the whole point is that we haven’t yet studied which of these dimensions matter… you, however, seem to be proceeding from an assumption that if they don’t use standard se processes, then there must be something wrong with their software.
for example, ncar’s ccsm is a genuine community model, with some of the submodels managed at other sites.
unfortunately, i have encountered the following argument a distressing number of times: 1.
rather than compare the practice of climate modelers to standard software development practice, it might be more instructive to compare their procedures to those of other computational physicists.
for now, i’m still exploring the broader context in which the coding takes place, and how activities are organised.
my list was: at the workshop we identified many more distinguishing traits, any of which might be important: we also speculated about some other contextual factors that might distinguish different software engineering species, not necessarily related to our analysis of computational science software.
then you mention distinguishing traits such as stable architecture and a modular/integrated/framework (component?, oo?) design.
optimization doesn’t help.
external constraints are things like resource limitations, or  particular characteristics of customers or the environment where the software  must run.
i should also say that many of our comparisons apply to computational science in general, not just climate science, although we used climate modeling for many specific examples.
plus the code has to remain very understandable,
however,  there’s enough noise in the data that in general, such things probably go  unnoticed, although we speculated what would happen when the most significant  bit gets flipped in some important variable.
which metrics can be applied in a consistent  way across vastly different contexts?
the other thing to realize is that fortran (modern 90/95 etc) is a high level language for scientific computing.
the se research community hasn’t really tackled the question of how  the different contexts in which software development occurs might affect  software development practices, nor when and how it’s appropriate to  attempt to generalize empirical observations across different contexts.
most of the funding is concentrated on the frontline science (and the costs  of buying and operating supercomputers).
and why does it matter?
this discovery (and the resulting comparisons) seems to tell us a lot about the other species that we thought we already understood.
the key point being that they are empirical.)
i like python, but my python tends to look like fortran, and i mostly call stuff written in fortran and wrapped with f2py (like bob, i’m not climate modeler either, my entire domain might reach 300 meters, but the software doesn’t care about the scale, everything is non-dimensionalized anyway).
we looked for  external and internal constraints on the software development project that seem  important.
as it turns out, there was also a fair bit of spherical geometry and other indirectly related code that was one “long chunk of sequential code implementing a series of equations”.
mm5, rams and wrf are examples of mesoscale models that began at universities.
i would recommend studying the domain before making such grand assumptions.
experience with trying to measure defect rates in climate models suggests that it is much harder to make valid comparisons than is generally presumed in the software literature.
programming in an object-oriented language is like kicking a dead whale down the beach this comment interests me.
an example from climate modeling: software that was originally developed as part of a phd project intended for use by just one person eventually grows into a vast legacy system, because it turns out to be a really useful model for the community to use.
but they continue to bear the costs of this entropy (often called “code debt”) through (1) steep learning curve for new scientists, (2) increased difficulty in extending the model, and (3) increased difficulty in debugging the model when it breaks.
the uk met office is an extreme example of single site development; other places have a more federated approach.
the software is developed by users for their own use, and this software is mission-critical for them.
a meteorology graduate student without a math or cs backgeound won’t be able to graduate in a reasonable length of time (m.s. <= 4 years) because they will have to take the time to fill in the missing pieces.
it’s very hard to divert any of  this funding to software engineering support, so development of the software  infrastructure is sidelined and sporadic.
there really has not been any serious consideration of these various contextual factors and their impact on software practices in the literature, and hence we might need to re-think a lot of the ways in which claims for generality are handled in empirical software engineering studies.
the first is “single site development – virtually all climate models are developed at a single site, usually a government lab as universities don’t have the resources;” although i am not directly involved in climate modeling, i am involved in mesoscale modeling (1000 -> 300 meter horizontal resolution).
programming in an object-oriented language is like kicking a dead whale down the beach bob: thanks for your comments – that’s very helpful feedback.
i come away from the post with the feeling the climate software community is insular.
i don’t think anyone is jumping to the conclusion that “there’s necessarily something wrong with the code” (were you george?), but the conclusion that “we don’t know if there’s anything wrong (and neither do you)” is pretty well supported based on the sort of process you describe which does not include formal code verification, nor calculation verification (maybe you’re leaving those parts out because they are just a given?
this luxury has nothing to do with quality per se.
how can you write the code to process the data in real-time from a ship-borne radar that is rolling, pitching and yawing.
i was invited to run a  working group on the challenges to empirical software engineering posed by  climate change.
though python lacks the specialised libraries of r, it makes architectural design easier.
we spent some time talking about the specific case of  defect measurements, but i’ll save that for a future post.
i haven’t read ‘em yet, though, so no guarantees: http://dx.doi.org/10.1109/mcise.2002.1032431 http://www.gfdl.noaa.gov/bibliography/related_files/ih9401.pdf http://www.osti.gov/energycitations/product.biblio.jsp?osti_id=518379 http://ams.allenpress.com/perlserv/?doi=10.1175%2fmwr2788.1&request=get-document&ct=1 the first looks like a good overview; the last gets into the guts of testing the numerical routines.
george [george: read the post again.
that this is not the case (e.g., “without following a standard software engineering process”) is, imho, a significant issue.
i would recommend studying the domain before making such grand assumptions. -
existence and use of shared infrastructure and frameworks.
there is *no way* we can run models at high enough resolution to have grids converge in the way i think was suggested as required.
so the answer is “it depends” (isn’t that always the answer?).
…and last but not least, a very politically charged  atmosphere.
but this isn’t about software engineering, it’s about the science that we are encoding in our models.
maybe george and i are just interested in slightly different questions than your research community so we’re talking past each other a bit.
i guess the interesting question is whether these variations are clumped in particular communities or whether there is a broad distribution of behaviours.
write down pseudo-code for an algorithm involving lots of operations on vectors and it looks pretty much the same after you translate it into fortran90 or matlab.
how can you write the code necessary to process the data in real-time coming from fore and aft pointing aircraft doppler radars without knowing how interrupts are preocessed.
this makes it almost impossible to  assess software productivity in the usual way, and even impossible to estimate  the total development cost of the software.
on fortran, i completely agree with the reasons you give for fortran being used; i disagree with the comment that it has nothing to do with legacy code.
what makes software engineering for climate models different?
several climate scientists have pointed out to me that it  probably doesn’t matter what language they use, the bulk of the code  would look pretty much the same – long chunks of sequential code  implementing a series of equations.
so i fail to understand the “universal hatred” modelers have about it.
use of frameworks is an internal constraint that will distinguish some species  of software engineering, although i’m really not clear how it will relate  to choices of software development process.
so we speculated that  we needed a multi-case case study, with some cases representing software built  by very smart people (e.g. climate models, the linux kernel, apache, etc), and  other cases representing software built by ….
it’s like we’ve identified a new species of software engineering, which appears to be a an outlier (perhaps an entirely new phylum?).
which is all to say that any generalisations are just that.
why tell you this, because a lot of what i did was because we knew the grids didn’t converge, and i wouldn’t want you to think i was hiding that).
the bright ideas are due to  the group (vic basili, lionel briand, audris mockus, carolyn seaman and claes  wohlin), while the mistakes in presenting them here are all mine.
the programming language of choice is fortran, and is unlikely to  change for very good reasons.
for example, the emergence of a competitor where  there was none previously, or the creation of a new regulatory framework where  none existed.
i think you miss by a mile on a couple of points.
we focussed particularly on how  climate modeling is different from other forms of se, but we also attempted to  identify factors that would distinguish other species of se from one another.
unconstrained release schedule – as there is no external  customer, software releases are unhurried, and occur only when the software is  considered stable and tested enough.
or, as i like to put it, i am 95% confident nobody can write great software.
the answer should be — nothing.
btw bryan points out my paper is behind a paywall.
people in any of these communities are doing good work.
this is a set of hypotheses from a workshop brainstorming - the whole point is that we haven't yet studied which of these dimensions matter - we're trying to figure out ways of measuring them.
anecdotal though it may be, it has been my experience that only about 5% of “smart people” could ever actually write great software.
@george crews george you say you mention that for the climate models the “developers are domain experts – they do not delegate programming tasks to programmers, which means they avoid the misunderstandings of the requirements common in many software projects”.
we tried this as a thought  experiment at the hadley centre, and quickly gave up: there is no sensible way  of drawing a boundary to distinguish some set of activities that could be  regarded as contributing to the model development, from other activities that  could not.
however, i strongly disagree that the current situation with regard to software engineering is any where near an optimum for this community.
i don’t see how it can be both ways.
i have also seen that some communities are more conservative, others more avant garde.
in the extreme case, the uk met office  produces several operational weather forecasting models and several research  climate models from the same unified codebase, although this is unusual for a  climate modeling group.
guess it means i haven’t ever really modeled anything using python?
as a good programmer, i am automatically a good software quality assurance analyst (whatever that is, nothing significant i would guess).
so, here’s a summary of our discussions.
tags: very timely steve.
more interestingly, we talked about what happens when these contextual  factors change over time.
i look forward to steve telling us (because most of us down our own rabbit holes have no time to pop our heads up and survey the landscape).
my list was: highly tailored software development process – software  development is tightly integrated into scientific work; single site development – virtually all coupled climate  models aredeveloped at a single site, managed and coordinated at a single site,  once they become sufficiently complex [edited - see bob's comments below],  usually a government lab as universities don’t have the resources; software developers are domain experts – they do not  delegate programming tasks to programmers, which means they avoid the  misunderstandings of the requirements common in many software projects; shared ownership and commitment to quality, which means that the  software developers are more likely to make contributions to the project that  matter over the long term (in contrast to, say, offshored software development,  where developers are only likely to do the tasks they are immediately paid for); openness – the software is freely shared with a broad  community, which means that there are plenty of people examining it and  identifying defects; benchmarking – there are many groups around the world  building similar software, with regular, systematic comparisons on the same set  of scenarios, throughmodel inter-comparison projects (this trait could be  unique – we couldn’t think of any other type of software for which  this is done so widely).
again, usually a strong fortran base, written by domain experts and increasingly commercialized.
that’s great, since software development is the most complicated thing people do.
in my study of the climate modelers at the uk met office hadley centre, i  had identified a list of potential success factors that might explain why the  climate modelers appear to be successful (i.e. to the extent that we are able  to assess it, they appear to build good quality software with low defect rates,  without following a standard software engineering process).
for example, for automotive software, much of the complexity comes from  building fault-tolerance into the software because correcting hardware problems  introduced in design or manufacture is prohibitively expense.
i strongly doubt that this particular bit of code is unique in terms of the opportunity it provided.
validation will remain a fundamental problem for climate modeling, but that’s probably another discussion (and a fruitful area for new research btw).
some other forms of  software have this feature too: audris mentioned voice response systems in  telecoms, which can be used stand-alone, and also in integrated call centre  software; lionel mentioned some types of embedded control systems onboard  ships, where the modules are used indendently on some ships, and as part of a  larger integrated command and control system on others.
instead the focus is on very extensive integration tests, with  daily builds, overnight regression testing, anda rigorous process of comparing  the output from runs before and after each code change.
i haven’t used fortran, but have used python and r for data analysis in my own work.
indeed the primary point of having multiple models is that don’t have the same conceptual architecture (unless you mean simulate the climate).
this week i attended a dagstuhl seminar on new frontiers for empirical software engineering.
so very clever  designed-in optimizations tend to be counter-productive.
you, however, seem to be proceeding from an assumption that if they don't use standard se processes, then there must be something wrong with their software.
we could debate the relative impact of the reasons each of us gives, but it would be pointless – the fact is that fortran is the language of choice, and there are many good reasons why this is so.
it’s the most complicated domain there is because if people could make their programs any more complicated they would.
first, a contrast: for automotive  software, a software error can immediately lead to death, or huge expense,  legal liability, etc,  as cars are recalled.
this week i attended a dagstuhl seminar on new frontiers for  empirical software engineering.
the bright ideas are due to the group (vic basili, lionel briand, audris mockus, carolyn seaman and claes wohlin), while the mistakes in presenting them here are all mine.
btw, python is my favorite programming language.
oh, and  performance matters enough that the overhead of object oriented languages makes  them unattractive.
imho, climate software developers are not as different as they may think.
the key point being that they are empirical.
i did consider building my model in r, but ultimately chose python, almost entirely on the basis that itis object-oriented.
the software has huge societal importance, but the impact of  software errors is very limited.
it was a select gathering, with many great people, which meant lots of fascinating discussions, and not enough time to type up all the ideas we’ve been bouncing around.
a more appropriate analogy would be the the chief engineer at john deere retiring to a farm and designing a new tractor for his hilly farm @bob pasken hi bob, you say: a more appropriate analogy would be the the chief engineer at john deere retiring to a farm and designing a new tractor for his hilly farm.
both this, and the years of experience in using fortran in the community act as a strong constraint on language choice.
i never would have even attempted that change in the original code.
i’m talking about the top level structure of the code in a coupled model, and the corresponding organisation of teamwork.
we need to consider how we would  determine this empirically.
it is absolutely an issue for the regional scale.
as someone who at least has a foot (well maybe a toe) in both communities, i agree with many of the observations about the the unique aspects of developing scientific models.
i could go on, but i think i’ve made my point.
however, unlike fortran,  which is generally liked (if not loved), everyone universally hates mpi.
you mentioned a “unconstrained release schedule” for the climate software.
all the main climate models have a number of different  model configurations, representing different builds from the codebase (rather  than say just different settings).
but you see the problem with this?
a large number of people actively seek to undermine the  science, and to discredit individual scientists, for political (ideological) or  commercial (revenue protection) reasons.
i think i could claim that in general, once a coupled gcm becomes sufficiently complex, there is a tendency to migrate to a single site.
that’s because, in general, if people could program anything any more complicated, and the software still work, they would.
we ended up discussing three closely related issues: in my study of the climate modelers at the uk met office hadley centre, i had identified a list of potential success factors that might explain why the climate modelers appear to be successful (i.e. to the extent that we are able to assess it, they appear to build good quality software with low defect rates, without following a standard software engineering process).
in  climate modeling, there is very little unit testing, because it’s hard to  specify an appropriate test for small units in isolation from the full  simulation.
the reasons are simple: there is a huge body of legacy fortran code, everyone in the community knows and understands fortran (and for many of them, only fortran), and fortran is ideal for much of the work of coding up the mathematical formulae that represent the physics.’
typically with  phds in physics or related geosciences.
i really wish that these costs could be quantified so that we could judge whether it was in the long term interests of the organization to hire more professional software developers to improve long term scientific productivity.
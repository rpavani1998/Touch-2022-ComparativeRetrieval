i should also say that many of our comparisons apply to computational science in general, not just climate science, although we used climate modeling for many specific examples.
for example: more interestingly, we talked about what happens when these contextual factors change over time.
i started off with a quick overview of the three research themes we identified at the oopsla workshop in the fall: inevitably, we spent most of our time this week talking about the first topic – software engineering of computational models, as that’s the closest to the existing expertise of the group, and the most obvious place to start.
you mention that for the climate models the “developers are domain experts – they do not delegate programming tasks to programmers, which means they avoid the misunderstandings of the requirements common in many software projects”.
being smart is only an advantage if you design and build simple software.
or even, in the case of health care, when change in the regulatory framework relaxes a constraint – such as the recent us healthcare bill, under which it (presumably) becomes easier to share health records among medical professionals if knowledge of pre-existing conditions is no longer a critical privacy concern.
fortran is the language of choice and the reason has nothing to do with legacy code.
we also speculated about some other contextual factors that might distinguish different software engineering species, not necessarily related to our analysis of computational science software.
or even, in the case of health care, when change in the regulatory framework relaxes a constraint – such as the recent us healthcare bill, under which it (presumably) becomes easier to share health records among medical professionals if knowledge of pre-existing conditions is no longer a critical privacy concern.
for climate modeling centers, it would be almost impossible to discard the existing models, or to port them to another language.
i started off with a quick overview of the three research themes we identified at the oopsla workshop in the fall: climate modeling, which we could characterize as a kind of end-user software development, embedded in a scientific process; global collective decision-making, which involves creating the software infrastructure for collective curation of sources of evidence in a highly charged political atmosphere; green software engineering, including carbon accounting for the software systems lifecycle (development, operation and disposal), but where we have no existing no measurement framework, and tendency to to make unsupported claims (aka greenwashing).
i guess it is possible that systematic errors are being made by many different climate modeling groups in the same way, but these wouldn’t be coding errors – they would be errors in the understanding of the physical processes and how best to represent them in a model.
as a person who has spent decades doing technical/engineering programming in areas unrelated to climate modeling, i find the post very interesting.
i do make use of polymorphism, which i think has helped in achieving a reasonably clean design in my case.
they merely lack the skills and/or time to improve the code through refactoring, and our understandably apprehensive about change.
like to climate modeling community the mesoscale model development community is done at the local university and national labs.
the software developers are very smart people.
interestingly, in my experience, most modelers are to some degree aware that there is a problem, and are even open to assistance in reducing the source code “entropy”.
audris pointed out that given the volumes of data being handled (terrabytes per day), there are almost certainly errors introduced in storage and retrieval (i.e. bits getting flipped), and enough that standard error correction would still miss a few.
in contrast, most other types of software engineering focus instead on unit testing, with elaborate test harnesses to test pieces of the software in isolation from the rest of the system.
in our discussions at the workshop, we came up with many insights for mainstream software engineering, which means this is a two-way street: plenty of opportunity for re-examination of mainstream software engineering, as well as learning how to study se for climate science.
an example used by pretty much every climate model ismpi.
i’m writing a talk for next week on software infrastructures for earth system modelling.
this is interesting, because one might expect climate modelers to put a huge amount of effort into optimization, given that century-long climate simulations still take weeks/months on some of the world’s fastest supercomputers.
programming is also an art.
if there was something better they would use it.
allgcms have the same conceptual architecture, and it is unchanged since modeling began, because it is derived from the natural boundaries in physical processes being simulated [edit: i mean the top level organisation of the code, not the choice of numerical methods, which do vary across models - see bob's comments below].
in my experience (when i was a climate modeller, talking to climate modellers), many who had had *enough* exposure to object orientation, would cheerfully admit that o-o codes could well have made some model tasks easier, but given that the job was already done in fortran … … so my experience is precisely the opposite of bob’s.
as a good engineer, i am automatically a good programmer.
the reason is that the model is changed so frequently that hand optimization of any particular model version is not useful.
imho, the most serious consequence of a climate software being defective would be to then use it to make a defective political decision costing trillions of dollars to society.
as a good scientist, i am automatically a good engineer.
it seems reasonable, as you say, that the changes only matter for more regional things, but we really don’t know until we converge a solution (and aren’t the regional things where we can make a much more direct connection to the things people care about?).
most open source projects are built by people who want a tool for their own use, but that others might find useful too.
even a computer science degree does not make you a good programmer.
it has a particularly simple object model, that you can completely ignore if you want to.
fortran is the language used because it allows you to express the mathematics and physics in a very clear succinct fashion.
again, these are often features of modern software of all types.
farmers are domain experts too.
the modules and integrated system each have independent lives, owned by different communities.
for example, the emergence of a competitor where there was none previously, or the creation of a new regulatory framework where none existed.
the reasons are simple: there is a huge body of legacy fortran code, everyone in the community knows and understands fortran (and for many of them, only fortran), and fortran is ideal for much of the work of coding up the mathematical formulae that represent the physics.
and frankly, for longer term, the climate problem just doesn’t seem to be susceptible to upscaling issues (at least for the global mean)
hi steve, thanks for your insights into the minds of climate modelers.
for example, a particular ocean model might be used uncoupled by a large community, and also be integrated into several different coupled climate models at different labs.
the single site development thing is interesting, and i grossly oversimplified.
the only reasonable path to assessing productivity that we can think of must focus on time-to-results, or time-to-publication, rather than on software development and delivery.
with over 30 years of scientific programming behind me, i really don’t see any universal hatred of any language or methodology such as oo.
an example from climate modeling: software that was originally developed as part of a phd project intended for use by just one person eventually grows into a vast legacy system, because it turns out to be a really useful model for the community to use.
because there are many other modeling groups, and scientific results are filtered through processes of replication, and systematic assessment of the overall scientific evidence, the impact of software errors on, say, climate policy is effectively nil.
i’d really like to see some reports on grid convergence results for the models used in the ipcc’s write-ups).
the spherical geometry was in a separate module, as were the bits of logic for accessing files to get offline wind fields and implementing periodicity.
inevitably, we spent most of our time this week talking about the first topic – software engineering of computational models, as that’s the closest to the existing expertise of the group, and the most obvious place to start.
further, many of the folk i know choose to use o-o languages, particularly python, to do data analysis, so far from hating it … it’s an analysis language of choice.
a good parallel to climate models are computational chemistry codes such as gaussian, spartan, gamess, molpro, etc.
even if we could do that, we could only afford to run one instance of the model anyway, and that wouldn’t be very interesting … certainly not for decadal and near term prediction, and frankly, for longer term, the climate problem just doesn’t seem to be susceptible to upscaling issues (at least for the global mean).
at the very least the next developer would have a much easier time introducing for example an adaptive rk scheme.
however, the modelers don’t necessarily derive some of the usual benefits of stable software architectures, such as information hiding and limiting the impacts of code changes, because the modules have very complex interfaces between them.
it seems to have just enough in the way of functional language structures that succinctly writing model equations is not too much of a chore.
an error may affect some of the experiments performed on the model, with perhaps the most serious consequence being the need to withdraw published papers (although i know of no cases where this has happened because ofsoftware errors rather than methodological errors).
software development activities are completely entangled with a wide set of other activities: doing science.
when we discussed this in the group, we all agreed this is a very significant factor, and that you don’t need much (formal) process with very smart people.
thirdly “the programming language of choice is fortran, and is unlikely to change for very good reasons.
the amr framework, chombo, is all c++, as is the vorpal em/accelerator/plasma framework.
then there is: “the software developers are very smart people.”
the synergia beam modeling framework is python linking in modules in fortran and c++.
it was a select gathering, with many great people, which meant lots of fascinating discussions, and not enough time to type up all the ideas we’ve been bouncing around.
i read the links, quickly, so sorry if i go off on the wrong tack.
i’ve some papers on my “to read” pile that might answer some of your questions, josh.
this is all the more odd to me since the science upon which climate software is based is mostly multidisciplinary.
my meteorology majors will graduate with a second degree in either mathematics or computer science.
nearly all modelers that i know are fluent not only in fortran, but c, c++, and perl as well.
in our discussions at the workshop, we came up with many insights for mainstream software engineering, which means this is a two-way street: plenty of opportunity for re-examination of mainstream software engineering, as well as learning how to study se for climate science.
yes, there are limits to what can be done with a complex mathematical relationship expressed in source code, but there is little excuse for much of the implementation quality in the remaining portions of the code.
experience with trying to measure defect rates in climate models suggests that it is much harder to make valid comparisons than is generally presumed in the software literature.
the software is used to build a product line, rather than an individual product.
finally, we talked a bit about the challenge of finding metrics that are valid across the vastly different contexts of the various software engineering species we identified.
ncar coordinates how these multiple communities contribute to the coupled model, with a team dedicated to managing coordination of the various community contributions, folding externally contributed changes into the coupled model.
my experience is that different scientific communities have made different choices, even though they are all coding up equations.
in contrast, climate models are absolutely central to the scientific work on which the climate scientists’ job performance depends.
i’ve no idea of the answer to the first question, but the second is readily handled by the checkpoint and restart features built into all climate models.
finally, we talked a bit about the challenge of finding metrics that are valid across the vastly different contexts of the various software engineering species we identified.
the idea here is that a craftsman has many tools in his tool chest the amateur believes everything is a nail.
the tools are built on the side (i.e. not part of the developers’ main job performance evaluations) but most such tools aren’t critical to the developers’ regular work.
there are alsoframeworks for structuring climate models and coupling the different physics components (more on these in asubsequent post).
then there is the statement: “the software has huge societal importance, but the impact of software errors is very limited.”
finite differencing vs spectral methods vs finite element vs analytical solutions all have significantly different methods for solving the same problem.
you ask what makes software engineering for climate models different.
you are correct the models are constrained by physical processes, but the models do not have the same conceptual architecture and certainly have changed significantly.
however, i also think the situation is changing rapidly, especially in the last few months, and climate scientists of all types are starting to feel more exposed.
the communities who care about the ocean model on its own will have different needs and priorities than each of communities who care about the coupled models.
even in the hard-core numerical portions, much can often be done to improve the situation.
at the workshop we identified many more distinguishing traits, any of which might be important: a stable architecture, defined by physical processes: atmosphere, ocean, sea ice, land scheme,….
my sense is that all of the modelers i’ve interviewed are shielded to a large extend from the political battles (i never asked them about this).
i once worked on a project where i was helping some scientists translate some idl code into fortran (for performance and portability).
internal constraints are those that are imposed on the software team by itself, for example, choices of working style, project schedule, etc.
those scientists who have been directly attacked (e.g. mann, jones,  santer) tend to be scientists more involved in creation and analysis of datasets, rather than gcm developers.
hence, we described them as mission-critical, but only in a personal kind of way.
most fusion codes are fortran, but many are c, and the facets framework is c++ that links in modules written in c++, fortran, python, and c. much viz work and data analysis is now done with python/matplotlib/scipy, while other is done within the visit c++ application.
in embedded software, the testing environment usually needs to simulate the operational environment; the most extreme case i’ve seen is the software for the international space station, where the only end-to-end software integration was the final assembly in low earth orbit.
in the end, the top level rk routine looked almost like something you would see in a text book.
we scientists have become all too accustomed to 1000+ line procedures with short variable names, and are generally unaware of what clean, understandable code can look like.
i ran across a rk routine and could tell that a runge-kutta scheme was lurking in the 200-300 lines of code.
but i also was able to discover that the treatment of the lat-lon grid was terribly inaccurate near the poles and was able to introduce a more correct geometric treatment that allowed 10x larger step sizes.
all gcms have the same conceptual architecture, and it is unchanged since modeling began, because it is derived from the natural boundaries in physical processes being simulated.”
for example, ncar’s ccsm is a genuine community model, with some of the submodels managed at other sites.
rather than compare the practice of climate modelers to standard software development practice, it might be more instructive to compare their procedures to those of other computational physicists.
my list was: at the workshop we identified many more distinguishing traits, any of which might be important: we also speculated about some other contextual factors that might distinguish different software engineering species, not necessarily related to our analysis of computational science software.
i should also say that many of our comparisons apply to computational science in general, not just climate science, although we used climate modeling for many specific examples.
however, there’s enough noise in the data that in general, such things probably go unnoticed, although we speculated what would happen when the most significant bit gets flipped in some important variable.
the other thing to realize is that fortran (modern 90/95 etc) is a high level language for scientific computing.
i like python, but my python tends to look like fortran, and i mostly call stuff written in fortran and wrapped with f2py (like bob, i’m not climate modeler either, my entire domain might reach 300 meters, but the software doesn’t care about the scale, everything is non-dimensionalized anyway).
as it turns out, there was also a fair bit of spherical geometry and other indirectly related code that was one “long chunk of sequential code implementing a series of equations”.
experience with trying to measure defect rates in climate models suggests that it is much harder to make valid comparisons than is generally presumed in the software literature.
an example from climate modeling: software that was originally developed as part of a phd project intended for use by just one person eventually grows into a vast legacy system, because it turns out to be a really useful model for the community to use.
but they continue to bear the costs of this entropy (often called “code debt”) through (1) steep learning curve for new scientists, (2) increased difficulty in extending the model, and (3) increased difficulty in debugging the model when it breaks.
the software is developed by users for their own use, and this software is mission-critical for them.
a meteorology graduate student without a math or cs backgeound won’t be able to graduate in a reasonable length of time (m.s. <= 4 years) because they will have to take the time to fill in the missing pieces.
it’s very hard to divert any of this funding to software engineering support, so development of the software infrastructure is sidelined and sporadic.
the first is “single site development – virtually all climate models are developed at a single site, usually a government lab as universities don’t have the resources;” although i am not directly involved in climate modeling, i am involved in mesoscale modeling (1000 -> 300 meter horizontal resolution).
programming in an object-oriented language is like kicking a dead whale down the beach bob: thanks for your comments – that’s very helpful feedback.
i come away from the post with the feeling the climate software community is insular.
this luxury has nothing to do with quality per se.
how can you write the code to process the data in real-time from a ship-borne radar that is rolling, pitching and yawing.
though python lacks the specialised libraries of r, it makes architectural design easier.
there is *no way* we can run models at high enough resolution to have grids converge in the way i think was suggested as required.
…and last but not least, a very politically charged atmosphere.
maybe george and i are just interested in slightly different questions than your research community so we’re talking past each other a bit.
how can you write the code necessary to process the data in real-time coming from fore and aft pointing aircraft doppler radars without knowing how interrupts are preocessed.
this makes it almost impossible to assess software productivity in the usual way, and even impossible to estimate the total development cost of the software.
several climate scientists have pointed out to me that it probably doesn’t matter what language they use, the bulk of the code would look pretty much the same – long chunks of sequential code implementing a series of equations.
use of frameworks is an internal constraint that will distinguish some species of software engineering, although i’m really not clear how it will relate to choices of software development process.
so we speculated that we needed a multi-case case study, with some cases representing software built by very smart people (e.g. climate models, the linux kernel, apache, etc), and other cases representing software built by ….
why tell you this, because a lot of what i did was because we knew the grids didn’t converge, and i wouldn’t want you to think i was hiding that).
for example, the emergence of a competitor where there was none previously, or the creation of a new regulatory framework where none existed.
we focussed particularly on how climate modeling is different from other forms of se, but we also attempted to identify factors that would distinguish other species of se from one another.
unconstrained release schedule – as there is no external customer, software releases are unhurried, and occur only when the software is considered stable and tested enough.
or, as i like to put it, i am 95% confident nobody can write great software.
btw bryan points out my paper is behind a paywall.
people in any of these communities are doing good work.
anecdotal though it may be, it has been my experience that only about 5% of “smart people” could ever actually write great software.
@george crews george you say you mention that for the climate models the “developers are domain experts – they do not delegate programming tasks to programmers, which means they avoid the misunderstandings of the requirements common in many software projects”.
however, i strongly disagree that the current situation with regard to software engineering is any where near an optimum for this community.
i have also seen that some communities are more conservative, others more avant garde.
in the extreme case, the uk met office produces several operational weather forecasting models and several research climate models from the same unified codebase, although this is unusual for a climate modeling group.
as a good programmer, i am automatically a good software quality assurance analyst (whatever that is, nothing significant i would guess).
tags: very timely steve.
i look forward to steve telling us (because most of us down our own rabbit holes have no time to pop our heads up and survey the landscape).
my list was: highly tailored software development process – software development is tightly integrated into scientific work; single site development – virtually all coupled climate models aredeveloped at a single site, managed and coordinated at a single site, once they become sufficiently complex [edited - see bob's comments below], usually a government lab as universities don’t have the resources; software developers are domain experts – they do not delegate programming tasks to programmers, which means they avoid the misunderstandings of the requirements common in many software projects; shared ownership and commitment to quality, which means that the software developers are more likely to make contributions to the project that matter over the long term (in contrast to, say, offshored software development, where developers are only likely to do the tasks they are immediately paid for); openness – the software is freely shared with a broad community, which means that there are plenty of people examining it and identifying defects; benchmarking – there are many groups around the world building similar software, with regular, systematic comparisons on the same set of scenarios, throughmodel inter-comparison projects (this trait could be unique – we couldn’t think of any other type of software for which this is done so widely).
again, usually a strong fortran base, written by domain experts and increasingly commercialized.
that’s great, since software development is the most complicated thing people do.
in my study of the climate modelers at the uk met office hadley centre, i had identified a list of potential success factors that might explain why the climate modelers appear to be successful (i.e. to the extent that we are able to assess it, they appear to build good quality software with low defect rates, without following a standard software engineering process).
for example, for automotive software, much of the complexity comes from building fault-tolerance into the software because correcting hardware problems introduced in design or manufacture is prohibitively expense.
i strongly doubt that this particular bit of code is unique in terms of the opportunity it provided.
some other forms of software have this feature too: audris mentioned voice response systems in telecoms, which can be used stand-alone, and also in integrated call centre software; lionel mentioned some types of embedded control systems onboard ships, where the modules are used indendently on some ships, and as part of a larger integrated command and control system on others.
instead the focus is on very extensive integration tests, with daily builds, overnight regression testing, anda rigorous process of comparing the output from runs before and after each code change.
indeed the primary point of having multiple models is that don’t have the same conceptual architecture (unless you mean simulate the climate).
so very clever designed-in optimizations tend to be counter-productive.
it’s the most complicated domain there is because if people could make their programs any more complicated they would.
first, a contrast: for automotive software, a software error can immediately lead to death, or huge expense, legal liability, etc,  as cars are recalled.
oh, and performance matters enough that the overhead of object oriented languages makes them unattractive.
imho, climate software developers are not as different as they may think.
the key point being that they are empirical.
the software has huge societal importance, but the impact of software errors is very limited.
it was a select gathering, with many great people, which meant lots of fascinating discussions, and not enough time to type up all the ideas we’ve been bouncing around.
i’m talking about the top level structure of the code in a coupled model, and the corresponding organisation of teamwork.
as someone who at least has a foot (well maybe a toe) in both communities, i agree with many of the observations about the the unique aspects of developing scientific models.
all the main climate models have a number of different model configurations, representing different builds from the codebase (rather than say just different settings).
i think i could claim that in general, once a coupled gcm becomes sufficiently complex, there is a tendency to migrate to a single site.
that’s because, in general, if people could program anything any more complicated, and the software still work, they would.
in climate modeling, there is very little unit testing, because it’s hard to specify an appropriate test for small units in isolation from the full simulation.
the reasons are simple: there is a huge body of legacy fortran code, everyone in the community knows and understands fortran (and for many of them, only fortran), and fortran is ideal for much of the work of coding up the mathematical formulae that represent the physics.’
typically with phds in physics or related geosciences.
i really wish that these costs could be quantified so that we could judge whether it was in the long term interests of the organization to hire more professional software developers to improve long term scientific productivity.
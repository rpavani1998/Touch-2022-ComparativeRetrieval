because there are many other modeling groups, and scientific results are filtered through processes of replication, and systematic assessment of the overall scientific evidence, the impact of software errors on, say, climate policy is effectively nil.the software is developed by users for their own use, and this software is mission-critical for them.in my study of the climate modelers at the uk met office hadley centre, i had identified a list of potential success factors that might explain why the climate modelers appear to be successful (i.e. to the extent that we are able to assess it, they appear to build good quality software with low defect rates, without following a standard software engineering process).the software has huge societal importance, but the impact of software errors is very limited.an error may affect some of the experiments performed on the model, with perhaps the most serious consequence being the need to withdraw published papers (although i know of no cases where this has happened because ofsoftware errors rather than methodological errors).the only reasonable path to assessing productivity that we can think of must focus on time-to-results, or time-to-publication, rather than on software development and delivery.i guess it is possible that systematic errors are being made by many different climate modeling groups in the same way, but these wouldn’t be coding errors – they would be errors in the understanding of the physical processes and how best to represent them in a model.they merely lack the skills and/or time to improve the code through refactoring, and our understandably apprehensive about change.experience with trying to measure defect rates in climate models suggests that it is much harder to make valid comparisons than is generally presumed in the software literature.those scientists who have been directly attacked (e.g. mann, jones,  santer) tend to be scientists more involved in creation and analysis of datasets, rather than gcm developers.though python lacks the specialised libraries of r, it makes architectural design easier.my list was: highly tailored software development process – software development is tightly integrated into scientific work; single site development – virtually all coupled climate models aredeveloped at a single site, managed and coordinated at a single site, once they become sufficiently complex [edited - see bob's comments below], usually a government lab as universities don’t have the resources; software developers are domain experts – they do not delegate programming tasks to programmers, which means they avoid the misunderstandings of the requirements common in many software projects; shared ownership and commitment to quality, which means that the software developers are more likely to make contributions to the project that matter over the long term (in contrast to, say, offshored software development, where developers are only likely to do the tasks they are immediately paid for); openness – the software is freely shared with a broad community, which means that there are plenty of people examining it and identifying defects; benchmarking – there are many groups around the world building similar software, with regular, systematic comparisons on the same set of scenarios, throughmodel inter-comparison projects (this trait could be unique – we couldn’t think of any other type of software for which this is done so widely).all gcms have the same conceptual architecture, and it is unchanged since modeling began, because it is derived from the natural boundaries in physical processes being simulated.”you are correct the models are constrained by physical processes, but the models do not have the same conceptual architecture and certainly have changed significantly.this is interesting, because one might expect climate modelers to put a huge amount of effort into optimization, given that century-long climate simulations still take weeks/months on some of the world’s fastest supercomputers.
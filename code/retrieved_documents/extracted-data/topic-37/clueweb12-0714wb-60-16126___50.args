the hd graphics 2000 is not as impressive.
in ivy bridge chips more login intel to integrate directx 11
the main point of intel graphics is it is cheap.
that's high-end only, and won't be playable on their hardware, even though it's 'supported'.
it will include directx 11 *and* theoretically be twice as fast as sandy bridge.
it's a built-in graphics card on the cpu.
in ivy bridge chips 199 posted by kdawson on monday january 10 2011, @12:15pm from the keeping-up-with-the-jonses dept.
content producers want true end to end drm for obvious reasons.
i'd rather they made their integrated graphics fast than simply support new directx capabilities.
gp didn't mention gamers.
the sandy bridge chips are the first in which intel has combined a graphics processor and cpu on a single piece of silicon.
the "rest of us" includes business users, htpc users, and casual gamers.
i realize that intel gma is not meant to excel at anything at all save for ripping some additional cash from my hand, but there's no need to integrate brain damaged graphics or wireless to achieve this.
give us that level of performance and then i'll be happy.
not unless minecraft improves the features they are using.
if you believe, as many do, that the drm is inherently bad for society in general, then it is important to go far beyond simply avoiding it yourself.
amd has already implemented directx 11 in its fusion low-power chips.
(i'm sorry)share twitter facebook i'd rather they made their integrated graphics fast than simply support new directx capabilities.
for anyone who likes their games to run at 30fps at 1024x768 with low graphics settings.
because sufficiently generic hardware is not sufficiently fast at the desired task, graphics computation.
this was not a happy discovery given that the machine cost $19,995 and was meant to excel at graphics.
" 100 of 199 comments loaded twitter facebook graphics amd hardware browser exploit kits using built-in java feature submission: intel to integrate directx 11 in ivy bridge chips early ivy bridge benchmark: graphics performance greatly improved some wikileaks contributions to public discourse
direct x is a microsoft product direct x isn't really a product (you can't buy it and never have been able to).
the next version of windows will run on arm.
i thought intel already did this a while ago with the newer atom chips: http://en.wikipedia.org/wiki/intel_atom#second_generation_cores [wikipedia.org]share twitter facebook i'm sure the article was thinking mainstream x86 line, but failed to say it.
if they change the directx api, intel changes the driver.
actually it is obvious you haven't used windows in awhile, because thanks to unified driver arch they "just work" and have for quite awhile now, as long as you stay away from the bleeding edge beta stuff.
once they tack on one or two more isa extension you'll be able to have 100% of your code avoid the x86 path.
my laptop's graphics card supports dx10, but if i enable the dx10 engine in any game i own that has one then the frame rate halves.
parent share twitter facebook better than that.
mean it was bad enough when there are chunks of the video chip they can't get thanks to hdmi, thus giving them
share twitter facebook the 3d text mode in outlook 2012 is pretty cool.
you can't meaningfully accelerate blits to frame buffers any faster than they already are.
either you live with the reduced functionality, or you put in a new video card, assuming your motherboard has a graphics card slot.
most significantly these days 3d graphics.
that only works if you don't like closed content for purely selfish reasons.
it's not like intel is going to include a top-end gpu on every cpu just in case you happen to need it either.
amd has already implemented directx 11 in its fusion low-power chips.
that said, boycotting closed media is likely to be just as effective as boycotting hardware that supports it; probably more so, as it is somewhat more direct.
though i'm more concerned on what this will do to the future of directx. wh opengl has to please a large group with more uses than just games; it is done with input from the wide range of developers that use it.
the hardware is, in fact, generic graphics hardware, at least in the sense i think you mean. because directx sounds cooler to marketing?
yeah, that's exactly why i had to put in the qualifier about the driver, unfortunately.
because things change, and dx11 will soon enough be the low end.
ibm cell chips integrate a vector chip on the cpu.
a better graphics card, or better graphics driver, will render minecraft better.
that is quite a lot of bandwidth to be consuming 100% of the time.
that driver can be modified/optimized later.
or you can just attach an hdfury2 to the hdmi and pipe the resulting component video into a hauppauge hd pvr.
the news here is (more of) the directx11 api will be in hw.
you have to accelerate higher level graphics abstractions.
parent share twitter facebook the chip's instruction set will be designed around the shading languages used in 3d graphics, it won't be very generic.
so chances are, most any new feature is going to be implemented on linux first.
those new texture mapping algorithms will really make outlook load fast.
i'm sure the people i know who do that will be glad to know that they won't have to work around chip bugs anymore.
that low bar is still quite low, but it's a lot higher than it used to be.
integrated gpu/cpu will always be lower performance than discrete.
if you want a barebones low graphics computer you buy integrated, which intel regularly develops, mostly for use in laptops (which add the bonus of power savings).
what i really want to see from ivy bridge and beyond is the ability to compete with $70 gpus.
nvidia is making arm cpus.
i'd say it is definitely intel's fault if they load the chip with drm bs that can only be used by signing ndas and agreeing not to share your work, which from the looks of the bridge chips really wouldn't surprise me.
it makes common algorithms run faster.
well, considering dx11 has been out for a while and has been generally tested for bugs already - the idea is that you won't have a bug if it's in the hardware - theres no where for the variables to change values based on a different cpu build or other factors if the calculations are specifically designed to run on that piece of hardware.
any sufficiently advanced technology is indistinguishable from a rigged demo.
you can find sandy bridge gpu benchmarks at http://www.anandtech.com/show/4083/the-sandy-bridge-review-intel-core-i7-2600k-i5-2500k-core-i3-2100-tested/11 [anandtech.com] "intel's hd graphics 3000 makes today's $40-$50 discrete gpus redundant.
what i was getting at is that if the chip is designed specifically for directx11, you shouldn't have directx11 bugs.
ivy bridge will succeed the recently announced core i3, i5, and i7 chips, which are based on intel's sandy bridge microarchitecture.
parent share twitter facebook you can get to the vendor specific features in directx also.
it's a really primitive design, there's almost no way any existing card isn't rendering what minecraft puts out at maximum quality.
if you bought that laptop for the dx10 you should return it and get one that works.
sandy bridge's integrated gpu beats most discrete graphics cards under $50.
there will be a driver.
parent share twitter facebook almost all boycotts are quixotic.
i'm not willing to pay more so that every cpu and/or motherboard is suitable for high end gaming.
i don't really see the point of supporting certain features if the whole thing is going to be slow.
at 1920x1080 with 32-bit color, the framebuffer is close to 64mib. this will typically be refreshed at 60hz, requiring 3.7gib/s of memory bandwidth.
disclaimer: i am a total n00b when it comes to discussing processor architectures, so i could be wrong about something.
the problem there is we've never been happy with $40-$50 discrete gpus for anything but htpc use.
yes, chip bugs definately do exist, but i would think (though i have no proof) that when a piece of hardware is designed for a specific task, it generally preforms that one task better and has issues elsewhere.
even with the optimization intel has put into this, they'll be more than an order of magnitude of graphics performance behind the dedicated solutions of their competitors.
they want to sell hardware, and being a full generation or more behind their competitors, have no reason to hold back any secrets of their implementation.
it just allows some new drm'ed protocol to be developed; one that only works on recent intel processors.
that way your not limited software versions for your hardware.
it isn't hard-wired for dx11.
no new comments can be posted.
if they could do everything ass-backwards without a speed loss just to make it extremely hard to port to/from opengl directx would do that.
the lack of dx11 is acceptable for snb consumers but it's—again—not moving the industry forward.
the directx dictatorship is faster and likely more efficient (in a way)
nope, hell they don't even sell linux on netbooks anymore.
but in either case, that's definitely the ugly way to write code.
high end gamers buy discrete graphics cards (or specialized notebooks), period.
" note: all sandy bridge laptop cpu have intel hd graphics 3000parent share twitter facebook i'd rather they made their integrated graphics fast than simply support new directx capabilities.
anything with an hdmi output has to support drm so people can't record the signal.
without javascript enabled, you might want toturn on classic discussion system in your preferences instead.
we are not responsible for them in any way.
in this context it's perfectly reasonble to ask whether those features will be exposed to other operating systems.
all graphics cards incorporate "hard-wired directx".
it can't encumber anything that presently exists.
by theoretically i mean it will have twice as many stream processors.
it's ironic that no one ever had the slightest intention of trying to record a digital monitor signal anyway.
so why bother implementing dx11 at all (instead of, for example, making dx10 faster, possibly enough faster to play high end dx10 titles), when it won't be usably fast for any actual dx11 software?
angry tapir writes "intel will integrate directx 11 graphics technology in its next generation of laptop and desktop chips based on the ivy bridge architecture, a company executive revealed at ces.
then (if it weren't for marketing) maybe it would make sense to implement directx11 in the next generation, or the one after that, when they can actually make directx11 content usable.
that's a problem in the minecraft client, not in the hardware that displays it.
it will still accelerate everything it was accelerating before.
minecraft uses lwjgl, the lightweight java game library, which in turn uses opengl.
however what intel delivers on their igp chips are typically the low bar of performance, like what i might get if you tried playing a game on a work laptop which obviously wasn't bought for gaming.
dx11 titles are so high-end, that no one would find them playable with the capabilities of intel hw.
it's like using -moz-border-radius, -webkit-border-radius and -khtml-border-radius to get css3 rounded borders long before css3 is officially released, and yet css3 won't be beholden to any one browser's implementation.
but will instead support all the hardware features that directx 11 needs.
opengl and direct3d do mostly the same things so it's not much of a hardship for the driver writers.
or more likely, written by someone who doesn't care about the platforms atom is aimed at, and therefore didn't know.
but it comes at a price that wiser people are not willing to pay.
performance comparable to the lesser nvidia and amd chips, e.g., amd 5400 series, nvidia 410 and 420 (possibly 430) series, is not considered slow by anyone except high end gamers.
dx11 is a bit ahead of ogl in hardware requirements/capabilities, so full support for dx11 means it has everything ogl needs also.
next year dx12 will be the meme.
it makes sense to implement as many of them in hardware as you can.
the graphics chip runs code which is uploaded when the machine boots.
if you are going to have graphics accelerators, they have to accelerate graphics.
hdmi is rated at 10.2 gigabits.
that graphics card has all the hardware necessary to support the directx 11 api.
the blocks should be more blocky but look less blocky.
and if you're a linux zealot, you can compile your kernel for whatever target hardware you want.
[anandtech.com]parent share twitter facebook the "intel graphics are slow" meme is dead.
but when idiots talk about drm, they lose contact with reality.
a lot of newer games are playable e 1.
if they really just wanted to move faster, they co the idea is that you won't have a bug if it's in the hardware
the same thing that happens when you install directx 10 on a dx9 card: the dx9 subset of dx10 is hardware accelerated, the dx10 parts are run in software.
but i actually do feel kinda sorry for them right now, as it looks like they are gonna get a butt raping that made the gma 500 look linux friendly.i
any software, such as opengl, can (and does) tap into the more well chosen of those abstractions.
the gpu on sandy bridge consumes die area approximately equivalent to two cpu cores [bit-tech.net].
this reduces the utility of open source software, which almost universally is unable to take advantage of this kind of system due to protection measures that typically require signed trusted code.
so, yes, it's a waste of time but intel is contractually bound to support the drm if they want to have hdmi output)
opengl programmers are always ahead of directx, even in this case where the hardware directly targets future directx specs.
using outlook always felt like someone was poking me in the eye.
incidentally, i recall that on my old sgi o2 r10k, it surprised me to find that algorithms touching only the cpu and memory ran a third slower at maximum resolution vs at 800x600.
sandy bridge will have drm in it (though they don't call it that for some weird reason), and sandy bridge isdirectly related to ivy bridge [wikimedia.org], so therefore it could possibly inherit the drm features of sandy bridge.
the ivy bridge solution will be even faster.
i am not a gamer but i would love to see more programs use the gpu for trans-coding and other none game play uses.
if you want bleeding-edge, open your wallet.
a lot more older games will run at good performance.
nickname: password:public terminal 100 of 199 comments loaded twitter facebookgraphics amd hardware nickname: password:public terminal the fine print: the following comments are owned by whoever posted them.
i have no idea about code quality or upkeep, so i will say nothing except i know they add regularly.
it's generally faster than what we had with clarkdale, but it's not exactly moving the industry forward.
intel hw indeed rules integrated graphics (until fusion is on the street), but no one plays high end dx10 titles, much less dx11 titles on such hardware.
i've worked with directx at a low level a bit, but no i've never actually to develop the hardware or the drivers for such devices.
its worth noting that linux now has a long tradition with intel at receiving support first because the code base is readily available for development, experimentation, and testing.
i would gladly pay for additional l3 cache or another cpu core or two.
putting graphics processing in hw instead of doing it in sw is always better, and intel currently rule in hw speed for mainstream chips.
intel should just do away with the 6 eu version, or at least give more desktop skus the 3000 gpu.
so i actually replied twice depending on which level of generic they wanted.
none of these chips execute 'direct3d' or 'opengl' directly, they remap the functions to an internal 3d api.
so why not integrate like the old altvec of ppc a vector co-processor.
they actually may, seeing that the entire gui frontend of everything in vista and windows 7 is basically a multithreaded version of direct 3d.
the "intel graphics are slow" meme is dead.
anybody who has a clue is more interested in decrypting the blu-ray files (quite a trick, but that genie is decidedly out of the bottle).
gtx 400 isn't integrated onto a cpu, which i think was the point.
i believe intel does want to take graphics seriously, but i need to see more going forward.
unified memory architecture is an elegant thing, but it does require storing the framebuffer in main memory.
[techdirt.com] as someone up in the discussion mentioned, it may have something other than tpm.
opengl 4.1 incorporates pretty-much everything in dx11 and more, not forgetting that ogl can then have extensions added taking it even further ahead.
the entire minecraft world should be a dynamic fractal, with the shape of each individual block mirroring the structure of the whole.
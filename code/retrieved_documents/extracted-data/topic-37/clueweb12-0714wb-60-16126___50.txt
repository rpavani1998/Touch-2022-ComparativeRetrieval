i want tessellated blocks.
the hd graphics 2000 is not as impressive.
in ivy bridge chips  more login intel to integrate directx 11
what the hell intel?parent share twitter facebook
parent share twitter facebook
he's babbling about drm.
if you don't like closed content, just don't use it!
that's what "support" means when talking about graphics.
the main point of intel graphics is it is cheap.
and show me where exactly is linux doing good in desktops?
that's high-end only, and won't be playable on their hardware, even though it's 'supported'.
it will include directx 11 *and* theoretically be twice as fast as sandy bridge.
that's 76.5 gigabytes per minute!
share twitter facebook
it's a built-in graphics card on the cpu.
dx11 has been out for over a year.
in ivy bridge chips 199 posted by  kdawson on monday january 10 2011, @12:15pm from the  keeping-up-with-the-jonses dept.
content producers want true end to end drm for obvious reasons.
if you don't like closed content, just don't use it!parent share twitter facebook
i'd rather they made their integrated graphics fast than simply support new directx capabilities.
at least, thats the theory.
gp didn't mention gamers.
the sandy bridge chips are the first in which intel has combined a graphics processor and cpu on a single piece of silicon.
they are not going to hardwire in directx
you shouldn't bother paying for something that doesn't work for you.
the "rest of us" includes business users, htpc users, and casual gamers.
i realize that intel gma is not meant to excel at anything at all save for ripping some additional cash from my hand, but there's no need to integrate brain damaged graphics or wireless to achieve this.
give us that level of performance and then i'll be happy.
why not use a generic chip designed for that type of instruction set?
not unless minecraft improves the features they are using.
i think it may actually meet your definition.
can the hardware be upgraded?
if you mean strict risc, it was too late the day the term was coined.
if you believe, as many do, that the drm is inherently bad for society in general, then it is important to go far beyond simply avoiding it yourself.
amd has already implemented directx 11 in its fusion low-power chips.
- andy finkel, computer guy
(i'm sorry)share twitter facebook i'd rather they made their integrated graphics fast than simply support new directx capabilities.
for anyone who likes their games to run at 30fps at 1024x768 with low graphics settings.
because sufficiently generic hardware is not sufficiently fast at the desired task, graphics computation.
this was not a happy discovery given that the machine cost $19,995 and was meant to excel at graphics.
" 100 of 199 comments loaded twitter  facebook graphics amd hardware browser exploit kits using built-in java feature submission: intel to integrate directx 11 in ivy bridge chips early ivy bridge benchmark: graphics performance greatly improved some wikileaks contributions to public discourse
as has nvidia in gtx 400 [wikipedia.org].
those "reflections" on the edges of the window frame?
direct x is a microsoft product direct x isn't really a product (you can't buy it and never have been able to).
either replace it, or live with a subset of current functionality.
the next version of windows will run on arm.
the words are practically poking you in the eyeballs!
i thought intel already did this a while ago with the newer atom chips: http://en.wikipedia.org/wiki/intel_atom#second_generation_cores [wikipedia.org]share twitter facebook i'm sure the article was thinking mainstream x86 line, but failed to say it.
do you have the slightest idea?
i suppose it's easier to implement something than it is to implement it well.
programmers have used this to get at the latest features of chipsets long before they're standardized.
if they change the directx api, intel changes the driver.
assuming someone writes the driver.
actually it is obvious you haven't used windows in awhile, because thanks to unified driver arch they "just work" and have for quite awhile now, as long as you stay away from the bleeding edge beta stuff.
once they tack on one or two more isa extension you'll be able to have 100% of your code avoid the x86 path.
go read the slashdot [slashdot.org] article on sandy bridge i take the sentiment back.
my laptop's graphics card supports dx10, but if i enable the dx10 engine in any game i own that has one then the frame rate halves.
parent share twitter facebook better than that.
mean it was bad enough when there are chunks of the video chip they can't get thanks to hdmi, thus giving them
and textures require mapping.
this is the next generation.
that's exactly what he said he was going to do, so it seems you're the one who's babbling.
share twitter facebook the 3d text mode in outlook 2012 is pretty cool.
before you jump, though, you must decide on what is the longest word size your computer will address and what is the smallest unit it will address.
you can't meaningfully accelerate blits to frame buffers any faster than they already are.
either you live with the reduced functionality, or you put in a new video card, assuming your motherboard has a graphics card slot.
nobody expects "built in" graphics to be comparable to high end discrete graphics.
most significantly these days 3d graphics.
parent share twitter facebook and just how were you planning to write the drivers without documentation?
that only works if you don't like closed content for purely selfish reasons.
it's not like intel is going to include a top-end gpu on every cpu just in case you happen to need it either.
amd has already implemented directx 11 in its fusion low-power  chips.
that said, boycotting closed media is likely to be just as effective as boycotting hardware that supports it; probably more so, as it is somewhat more direct.
though i'm more concerned on what this will do to the future of directx. wh opengl has to please a large group with more uses than just games; it is done with input from the wide range of developers that use it.
the hardware is, in fact, generic graphics hardware, at least in the sense i think you mean. because directx sounds cooler to marketing?
yeah, that's exactly why i had to put in the qualifier about the driver, unfortunately.
http://www.anandtech.com/show/4083/the-sandy-bridge-review-intel-core-i7-2600k-i5-2500k-core-i3-2100-tested/11
so it's hard to tell what you're saying.
because things change, and dx11 will soon enough be the low end.
sure, just like gma 500
ibm cell chips integrate a vector chip on the cpu.
a better graphics card, or better graphics driver, will render minecraft better.
yes, yes it's not exactly a gamer's gpu.
that is quite a lot of bandwidth to be consuming 100% of the time.
that driver can be modified/optimized later.
or you can just attach an hdfury2 to the hdmi and pipe the resulting component video into a hauppauge hd pvr.
the news here is (more of) the directx11 api will be in hw.
you have to accelerate higher level graphics abstractions.
parent share twitter facebook the chip's instruction set will be designed around the shading languages used in 3d graphics, it won't be very generic.
if you're buying high-end software, why are you expecting to play it on low-end hardware?
so chances are, most any new feature is going to be implemented on linux first.
those new texture mapping algorithms will really make outlook load fast.
i'm sure the people i know who do that will be glad to know that they won't have to work around chip bugs anymore.
have you seen performance numbers for sandy bridge's on chip graphics?
that low bar is still quite low, but it's a lot higher than it used to be.
i love the way it bump-mapped the bumped post on 4chan.
integrated gpu/cpu will always be lower performance than discrete.
if you want a barebones low graphics computer you buy integrated, which intel regularly develops, mostly for use in laptops (which add the bonus of power savings).
there may be more comments in this discussion.
what i really want to see from ivy bridge and beyond is the ability to compete with $70 gpus.
nvidia is making arm cpus.
it wasn't at all clear how 'generic' the grandparent wanted ...
i'd say it is definitely intel's fault if they load the chip with drm bs that can only be used by signing ndas and agreeing not to share your work, which from the looks of the bridge chips really wouldn't surprise me.
it makes common algorithms run faster.
well, considering dx11 has been out for a while and has been generally tested for bugs already - the idea is that you won't have a bug if it's in the hardware - theres no where for the variables to change values based on a different cpu build or other factors if the calculations are specifically designed to run on that piece of hardware.
any sufficiently advanced technology is indistinguishable from a rigged demo.
i can tell you've never developed graphics hardware or drivers...
you can find sandy bridge gpu benchmarks at http://www.anandtech.com/show/4083/the-sandy-bridge-review-intel-core-i7-2600k-i5-2500k-core-i3-2100-tested/11 [anandtech.com] "intel's hd graphics 3000 makes today's $40-$50 discrete gpus redundant.
worse what happens when directx 12 comes along?
this just gives them a way to realize that.
the answer of course is marketing.
widespread deployment of systems that allow closed content are likely to encourage content providers who are releasing content using current unprotected or insecure systems to switch to a more secure closed system.
what i was getting at is that if the chip is designed specifically for directx11, you shouldn't have directx11 bugs.
ivy bridge will succeed the recently announced  core i3, i5, and i7 chips, which are based on intel's sandy bridge  microarchitecture.
does it still contain the drm restrictions capability ?, because intel can forget all about cpu sales from us and from any of our customers until its removed i dont care if it promises a free pony contains drm==no sale periodshare twitter facebook what the heck are you babbling about?
parent share twitter facebook you can get to the vendor specific features in directx also.
it's a really primitive design, there's almost no way any existing card isn't rendering what minecraft puts out at maximum quality.
if you bought that laptop for the dx10 you should return it and get one that works.
and if you don't like the cpus that support the creation of the closed content, just don't buy them!
it's not really hard-wired hardware these days.
i hope they support opencl as well.
not much to complain about there.
sandy bridge's integrated gpu beats most discrete graphics cards under $50.
there will be a driver.
in ivy bridge chips archived discussion  load all comments full  abbreviated  hidden
no. directx has certain hardware requirements.
parent share twitter facebook almost all boycotts are quixotic.
i'm not willing to pay more so that every cpu and/or motherboard is suitable for high end gaming.
i don't really see the point of supporting certain features if the whole thing is going to be slow.
at 1920x1080 with 32-bit color, the framebuffer is close to 64mib. this will typically be refreshed at 60hz, requiring 3.7gib/s of memory bandwidth.
but i use os/x, linux, and windows
disclaimer: i am a total n00b when it comes to discussing processor architectures, so i could be wrong about something.
the problem there is we've never been happy with $40-$50 discrete gpus for anything but htpc use.
so yes, opengl should be fully supported, assuming someone writes the driver.
yes, chip bugs definately do exist, but i would think (though i have no proof) that when a piece of hardware is designed for a specific task, it generally preforms that one task better and has issues elsewhere.
even with the optimization intel has put into this, they'll be more than an order of magnitude of graphics performance behind the dedicated solutions of their competitors.
they want to sell hardware, and being a full generation or more behind their competitors, have no reason to hold back any secrets of their implementation.
it just allows some new drm'ed protocol to be developed; one that only works on recent intel processors.
hence, it is something that should be discouraged.
that way your not limited software versions for your hardware.
it isn't hard-wired for dx11.
itself is an interfaces supplied by windows for various things gaming related.
no new comments can be posted.
will this hinder future versions of directx or are they backwards compatible in a way that there would be large chunks in hardware and new changes made as firmware revisions or software implementations?share twitter facebook the hardware has all the features necessary to support dx11.
a "directx 11 card" means a card that implements all the features required by directx 11.
if they could do everything ass-backwards without a speed loss just to make it extremely hard to port to/from opengl directx would do that.
the lack of dx11 is acceptable for snb consumers but it's—again—not moving the industry forward.
the directx dictatorship is faster and likely more efficient (in a way)
dx11 does support gpgpu
nope, hell they don't even sell linux on netbooks anymore.
but in either case, that's definitely the ugly way to write code.
high end gamers buy discrete graphics cards (or specialized notebooks), period.
is the hardware useless?
because this is what the gp was referencing.
and a lot of the isa has been changed over too.
so why not do it generically?
" note: all sandy bridge laptop cpu have intel hd graphics 3000parent share twitter facebook i'd rather they made their integrated graphics fast than simply support new directx capabilities.
on the plus side, x86 has been pretty much risc internally for a long time now.
anything with an hdmi output has to support drm so people can't record the signal.
i'll take slow freedom.
without javascript enabled, you might want toturn on classic discussion system in your preferences instead.
we are not responsible for them in any way.
in this context it's perfectly reasonble to ask whether those features will be exposed to other operating systems.
for the foreseeable future you can have your pick of arm and x86.
all graphics cards incorporate "hard-wired directx".
it can't encumber anything that presently exists.
by theoretically i mean it will have twice as many stream processors.
do the "rest of us" constantly carp that nvidia igp graphics are slow, amd igp graphics are slow, and amd fusion graphics (will be) slow?
it's ironic that no one ever had the slightest intention of trying to record a digital monitor signal anyway.
can i pick up a linux desktop in walmart?
so why bother implementing dx11 at all (instead of, for example, making dx10 faster, possibly enough faster to play high end dx10 titles), when it won't be usably fast for any actual dx11 software?
http://intellinuxgraphics.org/ [intellinuxgraphics.org] all intel drivers are open source on linux.
angry tapir writes "intel will  integrate directx 11 graphics technology in its next generation of laptop and  desktop chips based on the ivy bridge architecture, a company executive  revealed at ces.
it's not what you think.
then (if it weren't for marketing) maybe it would make sense to implement directx11 in the next generation, or the one after that, when they can actually make directx11 content usable.
that's a problem in the minecraft client, not in the hardware that displays it.
it will still accelerate everything it was accelerating before.
minecraft uses lwjgl, the lightweight java game library, which in turn uses opengl.
however what intel delivers on their igp chips are typically the low bar of performance, like what i might get if you tried playing a game on a work laptop which obviously wasn't bought for gaming.
this  discussion has been archived.
(we have the master key
dx11 titles are so high-end, that no one would find them playable with the capabilities of intel hw.
it's like using -moz-border-radius, -webkit-border-radius and -khtml-border-radius to get css3 rounded borders long before css3 is officially released, and yet css3 won't be beholden to any one browser's implementation.
do you know some other way to do it?
if, otoh, you mean a nearly orthogonal architecture that is general purpose (plus the ability to call on specialized functions from attached processor chips), that seem, to me, a real possibility.
its open, more democratic.
but will instead support all the hardware features that directx 11 needs.
intel to integrate directx 11
opengl and direct3d do mostly the same things so it's not much of a hardship for the driver writers.
or more likely, written by someone who doesn't care about the platforms atom is aimed at, and therefore didn't know.
intel and amd both have video chips integrated into the cpu.
but it comes at a price that wiser people are not willing to pay.
performance comparable to the lesser nvidia and amd chips, e.g., amd 5400 series, nvidia 410 and 420 (possibly 430) series, is not considered slow by anyone except high end gamers.
dx11 is a bit ahead of ogl in hardware requirements/capabilities, so full support for dx11 means it has everything ogl needs also.
next year dx12 will be the meme.
the very idea is insane.
it makes sense to implement as many of them in hardware as you can.
these days each version of directx specifies a set of required features.
it really depends on what you mean.
the graphics chip runs code which is uploaded when the machine boots.
if you are going to have graphics accelerators, they have to accelerate graphics.
but yeah - this does nothing if you typically aren't running windows.
hdmi is rated at 10.2 gigabits.
that graphics card has all the hardware necessary to support the directx 11 api.
the blocks should be more blocky but look less blocky.
and if you're a linux zealot, you can compile your kernel for whatever target hardware you want.
[anandtech.com]parent share twitter facebook the "intel graphics are slow" meme is dead.
exactly the same thing happens with these cpus.
which is exactly what 95% of people are quite happy with if it means they save $50.
but when idiots talk about drm, they lose contact with reality.
a lot of newer games are playable e 1.
if they really just wanted to move faster, they co the idea is that you won't have a bug if it's in the hardware
actually, on rereading your post ...
the same thing that happens when you install directx 10 on a dx9 card: the dx9 subset of dx10 is hardware accelerated, the dx10 parts are run in software.
so i want standards support.
but i actually do feel kinda sorry for them right now, as it looks like they are gonna get a butt raping that made the gma 500 look linux friendly.i
at least that is a coherent discussion, which i haven't seen elsewhere.
directx11 is merely the latest iteration of directx, and the first to get consideration as i suppose it's easier to implement something than it is to implement it well.
any software, such as opengl, can (and does) tap into the more well chosen of those abstractions.
will this in any way benefit opengl? 2.
fixing a bug is usually just a driver update.
in what way do you mean?
your expectations are unrealistic.
the gpu on sandy bridge consumes die area approximately equivalent to two cpu cores [bit-tech.net].
intel expects to start shipping ivy bridge chips with directx 11 support  to pc makers late this year.
now maybe others will be able to relate.
you know, i may not be a linux guy
this reduces the utility of open source software, which almost universally is unable to take advantage of this kind of system due to protection measures that typically require signed trusted code.
so, yes, it's a waste of time but intel is contractually bound to support the drm if they want to have hdmi output)
opengl programmers are always ahead of directx, even in this case where the hardware directly targets future directx specs.
using outlook always felt like someone was poking me in the eye.
what the heck are you babbling about?
directx is a standard for a set of common algorithms.
in opengl, you say "give me this vendor-specific feature" you get it.
incidentally, i recall that on my old sgi o2 r10k, it surprised me to find that algorithms touching only the cpu and memory ran a third slower at maximum resolution vs at 800x600.
the rest of us find that kind of slow actually.
that's all directx is - an abstraction of higher level graphics operations.
sandy bridge will have drm in it (though they don't call it that for some weird reason), and sandy bridge isdirectly related to ivy bridge [wikimedia.org], so therefore it could possibly inherit the drm features of sandy bridge.
dx11 is generally a superset of what opengl can do.
what happens to your nvidia 580 card when dx 12 comes along?
it is necessary to convince as many others as possible about the problems drm creates for us all.
but at least i have a slight idea what he's ranting about.
the ivy bridge solution will be even faster.
i am not a gamer but i would love to see more programs use the gpu for trans-coding and other none game play uses.
if you want bleeding-edge, open your wallet.
so why is intel bothering to support dx11?
http://www.techdirt.com/articles/20110107/10153912573/intel-claims-drmd-chip-is-not-drm-its-just-copy-protection.shtml
a lot more older games will run at good performance.
nickname: password:public terminal 100 of 199 comments loaded twitter facebookgraphics amd hardware nickname: password:public terminal the fine print: the following comments are owned by whoever posted them.
i believe he's babbling about this [techdirt.com].
i have no idea about code quality or upkeep, so i will say nothing except i know they add regularly.
it's generally faster than what we had with clarkdale, but it's not exactly moving the industry forward.
and if you think "growing by leaps and bounds" is a piddly 1% of the desktop, less than 30% of the servers and falling last i checked, and goes to 11!
intel hw indeed rules integrated graphics (until fusion is on the street), but no one plays high end dx10 titles, much less dx11 titles on such hardware.
i've worked with directx at a low level a bit, but no i've never actually to develop the hardware or the drivers for such devices.
its worth noting that linux now has a long tradition with intel at receiving support first because the code base is readily available for development, experimentation, and testing.
i would gladly pay for additional l3 cache or another cpu core or two.
the larger (and the smaller) you go, the harder the task wi
and what use is it when a bug is found in directx, you can change software, but hardware?
putting graphics processing in hw instead of doing it in sw is always better, and intel currently rule in hw speed for mainstream chips.
intel should just do away with the 6 eu version, or at least give more desktop skus the 3000 gpu.
the "rest of us" is broader than that.
so i actually replied twice depending on which level of generic they wanted.
none of these chips execute 'direct3d' or 'opengl' directly, they remap the functions to an internal 3d api.
so why not integrate like the old altvec of ppc a vector co-processor.
what that has to do with this intel chip?
they actually may, seeing that the entire gui frontend of everything in vista and windows 7 is basically a multithreaded version of direct 3d.
the "intel graphics are slow" meme is dead.
anybody who has a clue is more interested in decrypting the blu-ray files (quite a trick, but that genie is decidedly out of the bottle).
gtx 400 isn't integrated onto a cpu, which i think was the point.
i believe intel does want to take graphics seriously, but i need to see more going forward.
what's the point of supporting dx11 if the game is unplayable?
graphics processing is all about taking some piece of over-used software and putting it in hardware so that it consumes a few hundred picoseconds instead of a several dozen nanoseconds per iteration.
unified memory architecture is an elegant thing, but it does require storing the framebuffer in main memory.
[techdirt.com] as someone up in the discussion mentioned, it may have something other than tpm.
opengl 4.1 incorporates pretty-much everything in dx11 and more, not forgetting that ogl can then have extensions added taking it even further ahead.
the entire minecraft world should be a dynamic fractal, with the shape of each individual block mirroring the structure of the whole.
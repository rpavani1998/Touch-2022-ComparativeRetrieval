<!doctype html>
<meta charset="utf-8">
<title>Intel To Integrate DirectX 11 In Ivy Bridge Chips - Slashdot</title>
<body>
<br>
<br>
<br>
Slashdot Stories <br>
Slash Boxes <br>
Comments <br>
<br>
Join 
Log In Submit Story Jobs Newsletter Library <br>
 &nbsp; 
<p> Nickname: </p> 
<p> Password: </p>  Public Terminal <br>
Forgot your password? <br>
<br>
<br>

Close <br>
binspam dupe notthebest offtopic slownewsday stale stupid fresh funny
insightful interesting maybe offtopic flamebait troll redundant overrated 
insightful interesting informative funny underrated descriptive typo dupe error 
<ul> 
<li> Stories </li> 
<li> Recent </li> 
<li> Popular </li> </ul> 
<h4> Slashdot </h4> 
<ul> 
<li></li> 
<li> Ask Slashdot </li> 
<li> Book Reviews </li> 
<li> Games </li> 
<li> Idle </li> 
<li> YRO </li> 
<li></li> 
<li> Cloud </li> 
<li> Hardware </li> 
<li> Linux </li> 
<li> Management </li> 
<li> Mobile </li> 
<li> Science </li> 
<li> Security </li> 
<li> Storage </li> </ul> <br>
18529894 story 
<h2> Intel To Integrate DirectX 11 In Ivy Bridge Chips 199 </h2>  Posted by 
kdawson on Monday January 10 2011, @12:15PM <br>
from the 
keeping-up-with-the-jonses dept. angry tapir writes <i>&quot;Intel will 
integrate DirectX 11 graphics technology in its next generation of laptop and 
desktop chips based on the Ivy Bridge architecture, a company executive 
revealed at CES. AMD has already implemented DirectX 11 in its Fusion low-power 
chips. Intel expects to start shipping Ivy Bridge chips with DirectX 11 support 
to PC makers late this year. Ivy Bridge will succeed the recently announced 
Core i3, i5, and i7 chips, which are based on Intel's Sandy Bridge 
microarchitecture.&quot;</i> 
<p> <strong>100</strong> of <strong>199</strong> comments loaded twitter 
facebook </p> graphics amd hardware <br>

<h3>&larr;</h3> 
<h3>Related Links</h3> 
<h3>&rarr;</h3> Browser Exploit Kits Using Built-In Java Feature <br>

<h3>Submission: Intel to integrate DirectX 11 in Ivy Bridge chips</h3>
<h3>Early Ivy Bridge Benchmark: Graphics Performance Greatly Improved</h3> <br>
Some WikiLeaks Contributions To Public Discourse <br>
<br>
<br>
 This 
discussion has been archived. No new comments can be posted.<br>

<h4> Intel To Integrate DirectX 11 In Ivy Bridge Chips  More Login </h4> 
<h2>Intel To Integrate DirectX 11 In Ivy Bridge Chips</h2> Archived Discussion 
Load All Comments <br>
&nbsp;Full &nbsp;Abbreviated &nbsp;Hidden <br>
 /Sea 
Score: <br>
 5 <br>
 4 <br>
 3 <br>
 2 <br>
 1 <br>
 0 <br>
 -1 <br>
 More Login
<br>  &nbsp; 
<p> Nickname: </p> 
<p> Password: </p>  Public Terminal <br>
Forgot your password? <br>
<br>
<br>

Close <br>
Close <br>
<br>
Search 199 Comments Log In/Create an Account <br>

Comments Filter: 
<ul> 
<li>All</li> 
<li>Insightful</li> 
<li>Informative</li> 
<li>Interesting</li> 
<li>Funny</li> </ul> 
<p> <strong>The Fine Print:</strong> The following comments are owned by 
whoever posted them. We are not responsible for them in any way. </p> <br>

<ul> 
<li> <br>

<h4>also includes DRM ?  (Score:5, Insightful)</h4> by Anonymous Coward  
writes:  on Monday January 10 2011, @12:19PM (#34824864) <br>

<p>does it still contain the DRM restrictions capability ?,</p> 
<p>because Intel can forget all about CPU sales from us and from any of our 
customers until its removed</p> 
<p>i dont care if it promises a free pony<br>
contains DRM==No sale</p> 
<p>period</p> Share twitter facebook <br>

<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:2, Funny)</h4> by fnj (64210)  writes: <br>

<p>What the heck are you babbling about? Do you have the slightest idea?</p> 
<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:also includes DRM ?  (Score:5, Informative)</h4> by supersloshy 
(1273442)  writes:  on Monday January 10 2011, @12:46PM (#34825218) <br>

<p></p> 
<p>What the heck are you babbling about? Do you have the slightest idea?</p> 
<p>I believe he's babbling about this [techdirt.com]. Sandy Bridge will have 
DRM in it (though they don't call it that for some weird reason), and Sandy 
Bridge isdirectly related to Ivy Bridge [wikimedia.org], so therefore it could 
possibly inherit the DRM features of Sandy Bridge.</p> 
<p>Disclaimer: I am a total n00b when it comes to discussing processor 
architectures, so I could be wrong about something.</p> Parent Share twitter 
facebook <br>

<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:also includes DRM ?  (Score:4, Interesting)</h4> by fnj (64210)  writes:
 on Monday January 10 2011, @01:08PM (#34825472) <br>

<p>At least that is a coherent discussion, which I haven't seen elsewhere. But 
when idiots talk about DRM, they lose contact with reality. Content producers 
want true end to end DRM for obvious reasons. This just gives them a way to 
realize that. It can't encumber anything that presently exists. It just allows 
some new DRM'ed protocol to be developed; one that only works on recent Intel 
processors.</p> 
<p>So what? If you don't like closed content, just don't use it!</p> Parent 
Share twitter facebook <br>

<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:also includes DRM ?  (Score:5, Insightful)</h4> by julesh (229690)  
writes:  on Monday January 10 2011, @01:32PM (#34825792) <br>

<p></p> 
<p>So what? If you don't like closed content, just don't use it!</p> 
<p>Widespread deployment of systems that allow closed content are likely to 
encourage content providers who are releasing content using current unprotected 
or insecure systems to switch to a more secure closed system. This reduces the 
utility of open source software, which almost universally is unable to take 
advantage of this kind of system due to protection measures that typically 
require signed trusted code. Hence, it is something that should be discouraged.
</p> 
<p>That said, boycotting closed media is likely to be just as effective as 
boycotting hardware that supports it; probably more so, as it is somewhat more 
direct.</p> Parent Share twitter facebook <br>

<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:2)</h4> by fnj (64210)  writes: <br>

<p>Almost all boycotts are quixotic.</p> 
<ul> 
<li></li> </ul> </li> </ul></li> 
<li> <br>

<h4>and if...  (Score:2)</h4> by aeoo (568706)  writes: <br>

<p></p> 
<p>So what? If you don't like closed content, just don't use it!</p> 
<p>And if you don't like the CPUs that support the creation of the closed 
content, just don't buy them!</p> 
<ul> 
<li></li> </ul> </li> 
<li> <br>

<h4>Re:  (Score:2)</h4> by internettoughguy (1478741)  writes: <br>

<p></p> 
<p>So what? If you don't like closed content, just don't use it!</p> 
<p>That's exactly what he said he was going to do, so it seems you're the one 
who's babbling.</p> 
<ul> 
<li></li> </ul> </li> 
<li> <br>

<h4>Re:  (Score:3)</h4> by Jah-Wren Ryel (80510)  writes: <br>

<p></p> 
<p>So what? If you don't like closed content, just don't use it!</p> 
<p>That only works if you don't like closed content for purely selfish reasons.
<br>If you believe, as many do, that the DRM is inherently bad for society in 
general, then it is important to go far beyond simply avoiding it yourself. It 
is necessary to convince as many others as possible about the problems DRM 
creates for us all.</p> 
<ul> 
<li></li> </ul> </li> </ul></li> </ul></li> 
<li> <br>

<h4>Re:  (Score:2)</h4> by Monkeedude1212 (1560403)  writes: <br>

<p>He's babbling about DRM.</p> 
<p>What that has to do with this Intel Chip? I don't know. But at least I have 
a SLIGHT idea what he's ranting about.</p> 
<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:3)</h4> by Joce640k (829181)  writes: <br>

<p>Anything with an HDMI output has to support DRM so people can't record the 
signal.</p> 
<p>(We have the master key so, yes, it's a waste of time but Intel is 
contractually bound to support the DRM if they want to have HDMI output)</p> 
<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:3)</h4> by fnj (64210)  writes: <br>

<p>It's ironic that no one ever had the slightest intention of trying to 
record a digital monitor signal anyway. The very idea is insane. HDMI is rated 
at 10.2 gigabits. That's 76.5 gigabytes per MINUTE! Anybody who has a clue is 
more interested in decrypting the Blu-Ray files (quite a trick, but that genie 
is decidedly out of the bottle).</p> 
<p>Or you can just attach an HDFury2 to the HDMI and pipe the resulting 
component video into a Hauppauge HD PVR.</p> 
<ul> 
<li></li> </ul> </li> </ul></li> </ul></li> </ul></li> 
<li> <br>

<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:2)</h4> by JonySuede (1908576)  writes: <br>

<p>go read the slashdot [slashdot.org] article on sandy bridge</p> 
<ul> 
<li></li> </ul> </li> 
<li> <br>

<h4>Re:also includes DRM ?  (Score:4, Interesting)</h4> by Shikaku (1129753)  
writes:  on Monday January 10 2011, @12:54PM (#34825318) <br>

<p>I take the sentiment back.</p> 
<p>
http://www.techdirt.com/articles/20110107/10153912573/intel-claims-drmd-chip-is-not-drm-its-just-copy-protection.shtml
 [techdirt.com]</p> 
<p>As someone up in the discussion mentioned, it may have something other than 
TPM.</p> 
<p>What the hell Intel?</p> Parent Share twitter facebook <br>

<ul> 
<li></li> </ul> </li> </ul></li> </ul></li> 
<li> <br>

<h4>Other OSes ?  (Score:5, Interesting)</h4> by SirGeek (120712)  writes:  
&lt;sirgeek-slashdot&amp;mrsucko,org&gt; on Monday January 10 2011, @12:19PM (
#34824866) Homepage <br>
 Will Intel provide documentation so that other OSes 
will be able to make use of this feature ? Share twitter facebook <br>

<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:Other OSes ?  (Score:4, Insightful)</h4> by Surt (22457)  writes:  on 
Monday January 10 2011, @12:40PM (#34825144) Homepage Journal <br>

<p>Almost certainly. They want to sell hardware, and being a full generation 
or more behind their competitors, have no reason to hold back any secrets of 
their implementation.</p> Parent Share twitter facebook <br>

<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:3)</h4> by citizenr (871508)  writes: <br>

<p></p> 
<p>Almost certainly. They want to sell hardware, and being a full generation 
or more behind their competitors, have no reason to hold back any secrets of 
their implementation.</p> 
<p>sure, just like GMA 500</p> 
<ul> 
<li></li> </ul> </li> 
<li> <br>

<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:Other OSes ?  (Score:4, Insightful)</h4> by Surt (22457)  writes:  on 
Monday January 10 2011, @12:45PM (#34825212) Homepage Journal <br>

<p>Yes. Assuming someone writes the driver. DX11 is a bit ahead of OGL in 
hardware requirements/capabilities, so full support for dx11 means it has 
everything OGL needs also.</p> Parent Share twitter facebook <br>

<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:Other OSes ?  (Score:5, Informative)</h4> by kyz (225372)  writes:  on 
Monday January 10 2011, @01:15PM (#34825556) Homepage <br>

<p>Better than that. In OpenGL, you say &quot;give me this vendor-specific 
feature&quot; you get it. Programmers have used this to get at the latest 
features of chipsets long before they're standardized.</p> 
<p>OpenGL programmers are always ahead of DirectX, even in this case where the 
hardware directly targets future DirectX specs.</p> 
<p>It's like using -moz-border-radius, -webkit-border-radius and 
-khtml-border-radius to get CSS3 rounded borders long before CSS3 is officially 
released, and yet CSS3 won't be beholden to any one browser's implementation.
</p> Parent Share twitter facebook <br>

<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:3)</h4> by Surt (22457)  writes: <br>

<p>You can get to the vendor specific features in directx also. But in either 
case, that's definitely the ugly way to write code.</p> 
<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:3)</h4> by SplashMyBandit (1543257)  writes: <br>
 &gt; You 
can get to the vendor specific features in directx also. But in either case, 
that's definitely the ugly way to write code.<br>
<br>
 lol. Some folks still 
don't get it. Direct X is 'vendor specific' no matter what manufacturer's 
chipset is supported. That's why the guys doing OpenGL (ES) can write for 
Android, and iPhone/iPad, and Linux, and Solaris, and Max OS X, *AND* Windows.
<br> <br>
 Incidentally, your &quot;DX11 is a bit ahead of OGL in hardware 
requirements/capabilities&quot; is incorrect (used to be true for a while n 
<ul> 
<li></li> </ul> </li> </ul></li> </ul></li> 
<li> <br>

<h4>Re:  (Score:3)</h4> by beelsebob (529313)  writes: <br>

<p></p> 
<p>Yes. Assuming someone writes the driver. DX11 is a bit ahead of OGL in 
hardware requirements/capabilities, so full support for dx11 means it has 
everything OGL needs also.</p> 
<p>Not true at all. OpenGL 4.1 incorporates pretty-much everything in DX11 and 
more, not forgetting that OGL can then have extensions added taking it even 
further ahead.</p> 
<ul> 
<li></li> </ul> </li> </ul></li> </ul></li> </ul></li> 
<li> <br>

<h4>Linux will be definitively be supported  (Score:3)</h4> by feranick 
(858651)  writes: <br>
 Linux will receive support directly from Intel for Ivy 
Bridge, with better timing than for Sandy Bridge (whose support for Linux was 
notably very late):http://www.phoronix.com/scan.php?page=news_item&amp;px=ODk3Nw
 [phoronix.com]<br>

<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:3)</h4> by GooberToo (74388)  writes: <br>

<p>Its worth noting that Linux now has a long tradition with Intel at 
receiving support first because the code base is readily available for 
development, experimentation, and testing. So chances are, most any new feature 
is going to be implemented on Linux first.</p> 
<ul> 
<li></li> </ul> </li> </ul></li> 
<li> <br>

<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:Other OSes ?  (Score:5, Informative)</h4> by petermgreen (876956)  
writes:  &lt;plugwash@p1STRAW0link.net minus berry&gt; on Monday January 10 
2011, @01:13PM (#34825518) Homepage <br>

<p><i>Direct X is a Microsoft product</i><br>
Direct X isn't really a product 
(you can't buy it and never have been able to). DirectX itself is an interfaces 
supplied by windows for various things gaming related. Most significantly these 
days 3D graphics.</p> 
<p>These days each version of directx specifies a set of required features. A 
&quot;DirectX 11 card&quot; means a card that implements all the features 
required by DirectX 11. In this context it's perfectly reasonble to ask whether 
those features will be exposed to other operating systems.</p> Parent Share 
twitter facebook <br>

<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Interesting possibilities  (Score:3)</h4> by TiggertheMad (556308)  writes:
<br> <i>These days each version of directx specifies a set of required 
features. A &quot;DirectX 11 card&quot; means a card that implements all the 
features required by DirectX 11. In this context it's perfectly reasonble to 
ask whether those features will be exposed to other operating systems.</i> <br>

<br> this is a kind of a interesting line of thought to follow. One would 
suppose that the DX11 chip will be proprietary hardware acceleration that will 
integrate with the API. Now, because this is being baked into chips by Intel, 
they w 
<ul> 
<li></li> </ul> </li> </ul></li> 
<li> <br>

<h4>Re:  (Score:2)</h4> by Jeremy Erwin (2054)  writes: <br>

<p>And just how were you planning to write the drivers without documentation?
</p> 
<ul> 
<li></li> </ul> </li> </ul></li> 
<li> <br>

<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:3)</h4> by hairyfeet (841228)  writes: <br>

<p>I'd say it is definitely Intel's fault if they load the chip with DRM BS 
that can only be used by signing NDAs and agreeing not to share your work, 
which from the looks of the Bridge chips really wouldn't surprise me. You know, 
I may not be a Linux guy but I actually do feel kinda sorry for them right now, 
as it looks like they are gonna get a butt raping that made the GMA 500 look 
Linux friendly.i mean it was bad enough when there are chunks of the video chip 
they can't get thanks to HDMI, thus giving them </p> 
<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:3)</h4> by hairyfeet (841228)  writes: <br>

<p>Actually it is obvious you haven't used Windows in awhile, because thanks 
to unified driver arch they &quot;just work&quot; and have for quite awhile 
now, as long as you stay away from the bleeding edge beta stuff. And show me 
where exactly is Linux doing good in desktops? Can I pick up a Linux desktop in 
Walmart? Best Buy? Nope, hell they don't even sell Linux on netbooks anymore.
</p> 
<p>And if you think &quot;growing by leaps and bounds&quot; is a piddly 1% of 
the desktop, less than 30% of the servers and falling last I checked, and </p> 
<ul> 
<li></li> </ul> </li> </ul></li> </ul></li> </ul></li> </ul></li> 
<li> <br>

<h4>DirectX  (Score:4, Funny)</h4> by Anonymous Coward  writes:  on Monday 
January 10 2011, @12:21PM (#34824910) <br>

<p>Goes to 11!</p> 
<p>(I'm sorry)</p> Share twitter facebook <br>

<ul> 
<li></li> </ul> </li> 
<li> <br>

<h4>Intel integrated graphics  (Score:2, Insightful)</h4> by node 3 (115640)  
writes: <br>

<p>I'd rather they made their integrated graphics fast than simply support new 
DirectX capabilities. I don't really see the point of supporting certain 
features if the whole thing is going to be slow. I suppose it's easier to 
implement something than it is to implement it well.</p> 
<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:2)</h4> by Shikaku (1129753)  writes: <br>

<p>The main point of Intel graphics is it is cheap. If you want a barebones 
low graphics computer you buy integrated, which Intel regularly develops, 
mostly for use in laptops (which add the bonus of power savings).</p> 
<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:2)</h4> by UnknowingFool (672806)  writes: <br>
 Yes but my 
understanding is you don't get that choice for some models of the CPU. For the 
mobile i3, i5, and i7 series now, the Intel GPU is integrated into the chipset. 
So if you get a new i7 and a discrete GPU, the Intel GPU is just disabled. 
Apple has done some work so that both the Intel and the discrete both operate 
depending on the on-demand video requirements. The new Ivy Bridge will be 
integrated into the CPU and not just the chipset. 
<ul> 
<li></li> </ul> </li> </ul></li> 
<li> <br>

<h4>Re:  (Score:2)</h4> by Late Adopter (1492849)  writes: <br>
 Then you 
don't want Intel graphics. The point to their hardware is to make it cheap: low 
power usage and low die size. Features are just engineering time, and that's 
something Intel has a lot of. 
<ul> 
<li></li> </ul> </li> 
<li> <br>

<h4>Re:  (Score:2)</h4> by blair1q (305137)  writes: <br>

<p>That's what &quot;support&quot; means when talking about graphics. Graphics 
processing is all about taking some piece of over-used software and putting it 
in hardware so that it consumes a few hundred picoseconds instead of a several 
dozen nanoseconds per iteration. It makes common algorithms run faster.</p> 
<p>DirectX is a standard for a set of common algorithms. It makes sense to 
implement as many of them in hardware as you can. DirectX11 is merely the 
latest iteration of DirectX, and the first to get consideration as </p> 
<ul> 
<li></li> </ul> </li> 
<li> <br>

<h4>Re:  (Score:2)</h4> by Monkeedude1212 (1560403)  writes: <br>

<p></p> 
<p>I suppose it's easier to implement something than it is to implement it 
well.</p> 
<p>80/20 rule.</p> 
<ul> 
<li></li> </ul> </li> 
<li> <br>

<h4>Re:  (Score:3)</h4> by divisionbyzero (300681)  writes: <br>

<p></p> 
<p>I'd rather they made their integrated graphics fast than simply support new 
DirectX capabilities. I don't really see the point of supporting certain 
features if the whole thing is going to be slow. I suppose it's easier to 
implement something than it is to implement it well.</p> 
<p>It will include DirectX 11 *and* theoretically be twice as fast as Sandy 
Bridge. Not much to complain about there.</p> 
<p>P.S. By theoretically I mean it will have twice as many stream processors.
</p> 
<ul> 
<li></li> </ul> </li> 
<li> <br>

<h4>Intel integrated graphics at anandtech.com  (Score:5, Informative)</h4> by 
IYagami(136831)  writes:  on Monday January 10 2011, @12:53PM (#34825300) <br>

<p>You can find Sandy Bridge GPU benchmarks at 
http://www.anandtech.com/show/4083/the-sandy-bridge-review-intel-core-i7-2600k-i5-2500k-core-i3-2100-tested/11
 [anandtech.com]</p> 
<p>&quot;Intel's HD Graphics 3000 makes today's $40-$50 discrete GPUs 
redundant. The problem there is we've never been happy with $40-$50 discrete 
GPUs for anything but HTPC use. What I really want to see from Ivy Bridge and 
beyond is the ability to compete with $70 GPUs. Give us that level of 
performance and then I'll be happy.</p> 
<p>The HD Graphics 2000 is not as impressive. It's generally faster than what 
we had with Clarkdale, but it's not exactly moving the industry forward. Intel 
should just do away with the 6 EU version, or at least give more desktop SKUs 
the 3000 GPU. The lack of DX11 is acceptable for SNB consumers but 
it's&mdash;again&mdash;not moving the industry forward. I believe Intel does 
want to take graphics seriously, but I need to see more going forward.&quot;</p>
<p>Note: all Sandy Bridge laptop CPU have Intel HD Graphics 3000</p> Parent 
Share twitter facebook <br>

<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:2)</h4> by CrashNBrn (1143981)  writes: <br>
 Yet, you still 
need an i7 + intel integrated graphics and an i7 compatible motherboard to get 
the performance of a $~50 dedicated GPU. Pricewise, you could go with an AMD 
solution and a dedicated GPU in the $75-$100 range from Nvidia or AMD and still 
pay half as much for better 3D performance.<br>
<br>
 The numbers look even 
worse for Intel if you grab an &quot;off-the-shelf&quot; dedicated GPU thats 
one generation older, e.g. a 1GB Radeon 4670 for ~$65.<br>
<br>
 AMD also has 
Hybrid graphics, first introduced with the Puma or Spider 
<ul> 
<li></li> </ul> </li> </ul></li> 
<li> <br>

<h4>Re:Intel integrated graphics  (Score:4, Insightful)</h4> by 
TheTyrannyOfForcedRe(1186313)  writes:  on Monday January 10 2011, @12:58PM (
#34825368) <br>

<p></p> 
<p>I'd rather they made their integrated graphics fast than simply support new 
DirectX capabilities. I don't really see the point of supporting certain 
features if the whole thing is going to be slow. I suppose it's easier to 
implement something than it is to implement it well.</p> 
<p>Have you seen performance numbers for Sandy Bridge's on chip graphics? The 
&quot;Intel graphics are slow&quot; meme is dead. Sandy Bridge's integrated gpu 
beats most discrete graphics cards under $50. The Ivy Bridge solution will be 
even faster.</p> 
<p> 
http://www.anandtech.com/show/4083/the-sandy-bridge-review-intel-core-i7-2600k-i5-2500k-core-i3-2100-tested/11
 [anandtech.com]</p> Parent Share twitter facebook <br>

<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:Intel integrated graphics  (Score:5, Insightful)</h4> by 0123456 
(636235)  writes:  on Monday January 10 2011, @01:38PM (#34825868) <br>

<p></p> 
<p>The &quot;Intel graphics are slow&quot; meme is dead.</p> 
<p>For anyone who likes their games to run at 30fps at 1024x768 with low 
graphics settings. The rest of us find that kind of slow actually.</p> Parent 
Share twitter facebook <br>

<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:3)</h4> by beelsebob (529313)  writes: <br>

<p></p> 
<p>For anyone who likes their games to run at 30fps at 1024x768 with low 
graphics settings. The rest of us find that kind of slow actually.</p> 
<p>Which is exactly what 95% of people are quite happy with if it means they 
save $50.</p> 
<ul> 
<li></li> </ul> </li> 
<li> <br>

<h4>Re:Intel integrated graphics  (Score:4, Informative)</h4> by DRJlaw 
(946416)  writes:  on Monday January 10 2011, @03:33PM (#34827700) <br>

<blockquote> 
<blockquote> 
<p>The &quot;Intel graphics are slow&quot; meme is dead.</p> </blockquote> 
<p>For anyone who likes their games to run at 30fps at 1024x768 with low 
graphics settings. The rest of us find that kind of slow actually.</p> 
</blockquote> 
<p>Do the &quot;rest of us&quot; constantly carp that Nvidia IGP graphics are 
slow, AMD IGP graphics are slow, and AMD Fusion graphics (will be) slow? 
Because this is what the GP was referencing. Nobody expects &quot;built 
in&quot; graphics to be comparable to high end discrete graphics. Performance 
comparable to the lesser Nvidia and AMD chips, e.g., AMD 5400 series, Nvidia 
410 and 420 (possibly 430) series, is not considered slow by anyone except high 
end gamers. High end gamers buy discrete graphics cards (or specialized 
notebooks), period. The &quot;rest of us&quot; is broader than that. The 
&quot;rest of us&quot; includes business users, HTPC users, and casual gamers.
</p> 
<p>GP didn't mention gamers. I'm not willing to pay more so that every CPU 
and/or motherboard is suitable for high end gaming. Your expectations are 
unrealistic. Good day.</p> Parent Share twitter facebook <br>

<ul> 
<li></li> </ul> </li> 
<li> <br>

<h4>Re:  (Score:3)</h4> by Kjella (173770)  writes: <br>

<p>Yes, yes it's not exactly a gamer's GPU. It's not like Intel is going to 
include a top-end GPU on every CPU just in case you happen to need it either. 
However what Intel delivers on their IGP chips are typically the low bar of 
performance, like what I might get if you tried playing a game on a work laptop 
which obviously wasn't bought for gaming. That low bar is still quite low, but 
it's a lot higher than it used to be. A lot more older games will run at good 
performance. A lot of newer games are playable e </p> 
<ul> 
<li></li> </ul> </li> </ul></li> </ul></li> </ul></li> 
<li> <br>

<h4>Two Questions  (Score:4, Interesting)</h4> by chill (34294)  writes:  on 
Monday January 10 2011, @12:24PM (#34824960) Journal <br>

<p>1. Will this in any way benefit OpenGL?</p> 
<p>2. Will this hinder future versions of DirectX or are they backwards 
compatible in a way that there would be large chunks in hardware and new 
changes made as firmware revisions or software implementations?</p> Share 
twitter facebook <br>

<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:3)</h4> by Surt (22457)  writes: <br>

<p>The hardware has all the features necessary to support dx11. dx11 is 
generally a superset of what opengl can do. So yes, opengl should be fully 
supported, assuming someone writes the driver.</p> 
<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:2)</h4> by Burnhard (1031106)  writes: <br>
 I read that 
Intel's drivers are notoriously shite for OpenGL. Indeed, my own 
experimentation showed them to be shite at D3D as well. The device I was using 
claimed to support PS 3.0 (in its caps), but point-blank refused to run some of 
my shaders (they ran ok with ATI and NVIDIA cards). I won't be supporting Intel 
Graphics, that's for sure. 
<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:2)</h4> by Surt (22457)  writes: <br>

<p>Yeah, that's exactly why I had to put in the qualifier about the driver, 
unfortunately.</p> 
<ul> 
<li></li> </ul> </li> </ul></li> </ul></li> 
<li> <br>

<h4>Re:  (Score:2)</h4> by Tr3vin (1220548)  writes: <br>
 In theory, OpenGL 4 
could take advantage of the new hardware, but Intel would have to write good 
OpenGL drivers. Future versions of DirectX may require new hardware. We won't 
know until there is a spec. If it does require new hardware, then people would 
have to replace their DX11 cards anyway. 
<ul> 
<li></li> </ul> </li> </ul></li> 
<li> <br>

<h4>First Intel CPU + GPU on die?  (Score:3)</h4> by TeknoHog (164938)  writes:
 on Monday January 10 2011, @12:27PM (#34824992) Homepage Journal <br>
 FTA: 
<p></p> 
<p>The Sandy Bridge chips are the first in which Intel has combined a graphics 
processor and CPU on a single piece of silicon.</p> 
<p> I thought Intel already did this a while ago with the newer Atom chips: 
</p> 
<p> http://en.wikipedia.org/wiki/Intel_atom#Second_generation_cores 
[wikipedia.org]</p> Share twitter facebook <br>

<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:2)</h4> by Surt (22457)  writes: <br>

<p>I'm sure the article was thinking mainstream x86 line, but failed to say 
it. Or more likely, written by someone who doesn't care about the platforms 
atom is aimed at, and therefore didn't know.</p> 
<ul> 
<li></li> </ul> </li> 
<li> <br>

<h4>Re:  (Score:2)</h4> by blair1q (305137)  writes: <br>

<p>They had. The news here is (more of) the DirectX11 API will be in HW.</p> 
<ul> 
<li></li> </ul> </li> </ul></li> 
<li> <br>

<h4>Great!  (Score:5, Funny)</h4> by TechyImmigrant (175943) *  writes:  on 
Monday January 10 2011, @12:29PM (#34825000) Journal <br>

<p>Those new texture mapping algorithms will really make outlook load fast.</p>
Share twitter facebook <br>

<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:2)</h4> by Surt (22457)  writes: <br>

<p>The 3d text mode in outlook 2012 is pretty cool. The words are practically 
poking you in the eyeballs!</p> 
<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:3)</h4> by sgt scrub (869860)  writes: <br>

<p>cool! using outlook always felt like someone was poking me in the eye. now 
maybe others will be able to relate.</p> 
<ul> 
<li></li> </ul> </li> </ul></li> 
<li> <br>

<h4>Re:  (Score:2)</h4> by Monkeedude1212 (1560403)  writes: <br>

<p>I love the way it bump-mapped the bumped post on 4chan.</p> 
<ul> 
<li></li> </ul> </li> 
<li> <br>

<h4>They actually may  (Score:2)</h4> by melted (227442)  writes: <br>

<p>They actually may, seeing that the entire GUI frontend of EVERYTHING in 
Vista and Windows 7 is basically a multithreaded version of Direct 3D. Those 
&quot;reflections&quot; on the edges of the window frame? They're textures. And 
textures require mapping.</p> 
<ul> 
<li></li> </ul> </li> </ul></li> 
<li> <br>

<h4>But will it improve Minecraft's graphics?  (Score:5, Funny)</h4> by 
digitaldc(879047) *  writes:  on Monday January 10 2011, @12:31PM (#34825032) 
<br>  That's what I am worried about, I want my Minecraft landscapes to be 
rendered better. Share twitter facebook <br>

<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:3)</h4> by Surt (22457)  writes: <br>

<p>No. That's a problem in the minecraft client, not in the hardware that 
displays it.</p> 
<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:3)</h4> by kyz (225372)  writes: <br>

<p>Minecraft uses LWJGL, the lightweight Java game library, which in turn uses 
OpenGL.</p> 
<p>A better graphics card, or better graphics driver, will render Minecraft 
better.</p> 
<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:2)</h4> by Surt (22457)  writes: <br>

<p>Not unless minecraft improves the features they are using. It's a really 
primitive design, there's almost no way any existing card isn't rendering what 
minecraft puts out at maximum quality.</p> 
<ul> 
<li></li> </ul> </li> 
<li> <br>

<h4>Re:  (Score:2)</h4> by SheeEttin (899897)  writes: <br>
 I suspect that 
there is little optimization done in the world, so each cube gets two tris per 
(visible) face, even if it's part of a larger polygon.<br>
 So no, I don't 
think a new graphics card will help that much. (You should playCube 2: 
Sauerbraten [sauerbraten.org], anyway.) <br>

<ul> 
<li></li> </ul> </li> </ul></li> </ul></li> 
<li> <br>

<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:2)</h4> by TheL0ser (1955440)  writes: <br>
 The blocks should 
be more blocky but look less blocky. 
<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:3)</h4> by Colonel Korn (1258968)  writes: <br>

<p></p> 
<p>The blocks should be more blocky but look less blocky.</p> 
<p>I want tessellated blocks. The entire Minecraft world should be a dynamic 
fractal, with the shape of each individual block mirroring the structure of the 
whole.</p> 
<ul> 
<li></li> </ul> </li> </ul></li> </ul></li> </ul></li> 
<li> <br>

<h4>AMD has already implemented DirectX 11 in its F...  (Score:2)</h4> by 
drinkypoo(153816)  writes: <br>

<p></p> 
<p>AMD has already implemented DirectX 11 in its Fusion low-power chips.</p> 
<p>As has nVidia in GTX 400 [wikipedia.org].</p> 
<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:3)</h4> by Surt (22457)  writes: <br>

<p>gtx 400 isn't integrated onto a cpu, which I think was the point.</p> 
<ul> 
<li></li> </ul> </li> </ul></li> 
<li> <br>

<h4>I hope ....  (Score:2)</h4> by PPH (736903)  writes: <br>
 ... Intel will 
make it easy to re-flash the chipsets when DirectX 12 comes out. Or to install 
OpenGL firmware instead. 
<ul> 
<li></li> </ul> </li> 
<li> <br>

<h4>is this the best use of die space &amp; RAM bandwi  (Score:3)</h4> by 
hxnwix(652290)  writes:  on Monday January 10 2011, @03:53PM (#34827992) Journal
<br> 
<p>The GPU on sandy bridge consumes die area approximately equivalent to two 
CPU cores [bit-tech.net].</p> 
<p>Unified memory architecture is an elegant thing, but it does require 
storing the framebuffer in main memory. At 1920x1080 with 32-bit color, the 
framebuffer is close to 64MiB. This will typically be refreshed at 60Hz, 
requiring 3.7GiB/s of memory bandwidth. That is quite a lot of bandwidth to be 
consuming 100% of the time. Incidentally, I recall that on my old SGI O2 R10k, 
it surprised me to find that algorithms touching only the CPU and memory ran a 
third slower at maximum resolution vs at 800x600. This was not a happy 
discovery given that the machine cost $19,995 and was meant to excel at 
graphics.</p> 
<p>I realize that Intel GMA is not meant to excel at anything at all save for 
ripping some additional cash from my hand, but there's no need to integrate 
brain damaged graphics or wireless to achieve this. I would gladly pay for 
additional L3 cache or another CPU core or two.</p> Share twitter facebook <br>

<ul> 
<li></li> </ul> </li> 
<li> <br>

<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:2)</h4> by Monkeedude1212 (1560403)  writes: <br>

<p></p> 
<p>And what use is it when a bug is found in DirectX, you can change software, 
but hardware?</p> 
<p>Well, considering DX11 has been out for a while and has been generally 
tested for bugs already - the idea is that you won't HAVE a bug if it's in the 
hardware - theres no where for the variables to change values based on a 
different CPU build or other factors if the calculations are specifically 
designed to run on that piece of hardware. At least, thats the theory.</p> 
<p>But yeah - this does nothing if you typically aren't running Windows. 
Though I'm more concerned on what this will do to the future of DirectX. Wh </p>
<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>DirectX isn't open  (Score:2)</h4> by bussdriver (620565)  writes: <br>

<p>OpenGL has to please a large group with more uses than just games; it is 
done with input from the wide range of developers that use it. Its open, more 
democratic.<br>
The DirectX dictatorship is faster and likely more efficient 
(in a way) but it comes at a price that wiser people are not willing to pay.</p>
<p>I'll take slow freedom.</p> 
<p>If they could do everything ass-backwards without a speed loss just to make 
it extremely hard to port to/from OpenGL DirectX would do that. If they really 
just wanted to move faster, they co </p> 
<ul> 
<li></li> </ul> </li> 
<li> <br>

<h4>Re:  (Score:2)</h4> by 0123456 (636235)  writes: <br>

<p></p> 
<p>the idea is that you won't HAVE a bug if it's in the hardware</p> 
<p>I can tell you've never developed graphics hardware or drivers... I'm sure 
the people I know who do that will be glad to know that they won't have to work 
around chip bugs anymore.</p> 
<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:2)</h4> by Monkeedude1212 (1560403)  writes: <br>

<p>I've worked with directX at a low level a bit, but no I've never actually 
to develop the hardware or the drivers for such devices.</p> 
<p>What I was getting at is that if the Chip is designed specifically for 
DirectX11, you shouldn't have DirectX11 bugs. Yes, chip bugs definately do 
exist, but I would think (though I have no proof) that when a piece of hardware 
is designed for a specific task, it generally preforms that one task better and 
has issues elsewhere.</p> 
<ul> 
<li></li> </ul> </li> </ul></li> 
<li> <br>

<h4>Re:  (Score:2)</h4> by Joce640k (829181)  writes: <br>

<p>It's not really hard-wired hardware these days. The graphics chip runs code 
which is uploaded when the machine boots. Fixing a bug is usually just a driver 
update.</p> 
<ul> 
<li></li> </ul> </li> </ul></li> 
<li> <br>

<h4>Re:  (Score:2)</h4> by LWATCDR (28044)  writes: <br>

<p>ahhh... No.<br>
DirectX has certain hardware requirements. They are not 
going to hardwire in DirectX but will instead support all the hardware features 
that DirectX 11 needs.<br>
I hope they support OpenCL as well.<br>
I am not a 
gamer but I would love to see more programs use the GPU for trans-coding and 
other none game play uses.<br>
DX11 does support GPGPU but I use OS/X, Linux, 
and Windows so I want standards support.<br>
 &nbsp;</p> 
<ul> 
<li></li> </ul> </li> 
<li> <br>

<h4>Re:  (Score:2)</h4> by Surt (22457)  writes: <br>

<p>It's not what you think. It's a built-in graphics card on the CPU. That 
graphics card has all the hardware necessary to support the directx 11 api. If 
they change the directx API, intel changes the driver.</p> 
<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:3)</h4> by peragrin (659227)  writes: <br>

<p>So why not do it generically? IBM Cell chips integrate a Vector chip on the 
CPU. Intel and AMD both have video chips integrated into the CPU. So why not 
integrate like the old Altvec of PPC a Vector co-processor.</p> 
<p>Why not use a generic chip designed for that type of instruction set? That 
way your not limited software versions for your hardware.</p> 
<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:Hard-wired DirectX?  (Score:5, Informative)</h4> by Surt (22457)  
writes:  on Monday January 10 2011, @12:48PM (#34825252) Homepage Journal <br>

<p></p> 
<p>So why not do it generically? IBM Cell chips integrate a Vector chip on the 
CPU. Intel and AMD both have video chips integrated into the CPU. So why not 
integrate like the old Altvec of PPC a Vector co-processor.</p> 
<p>Why not use a generic chip designed for that type of instruction set? That 
way your not limited software versions for your hardware.</p> 
<p>Because sufficiently generic hardware is not sufficiently fast at the 
desired task, graphics computation. Even with the optimization intel has put 
into this, they'll be MORE than an order of magnitude of graphics performance 
behind the dedicated solutions of their competitors.</p> Parent Share twitter 
facebook <br>

<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:2)</h4> by Joce640k (829181)  writes: <br>

<p>The chip's instruction set will be designed around the shading languages 
used in 3D graphics, it won't be very generic.</p> 
<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:2)</h4> by Surt (22457)  writes: <br>

<p>Yeah exactly ... it wasn't at all clear how 'generic' the grandparent wanted
... so I actually replied twice depending on which level of generic they wanted.
</p> 
<ul> 
<li></li> </ul> </li> </ul></li> </ul></li> 
<li> <br>

<h4>Re:  (Score:2)</h4> by Surt (22457)  writes: <br>

<p>Actually, on rereading your post ... I think it may actually meet your 
definition. It isn't hard-wired for dx11. There will be a driver. That driver 
can be modified/optimized later. The hardware is, in fact, generic graphics 
hardware, at least in the sense I think you mean.</p> 
<ul> 
<li></li> </ul> </li> 
<li> <br>

<h4>Re:  (Score:2)</h4> by sgt scrub (869860)  writes: <br>

<p>because DirectX sounds cooler to marketing?</p> 
<ul> 
<li></li> </ul> </li> </ul></li> </ul></li> 
<li> <br>

<h4>Re:  (Score:2)</h4> by Shikaku (1129753)  writes: <br>

<p>Use Linux?</p> 
<p>http://intellinuxgraphics.org/ [intellinuxgraphics.org]</p> 
<p>All Intel drivers are open source on Linux. I have no idea about code 
quality or upkeep, so I will say nothing except I know they add regularly.</p> 
<ul> 
<li></li> </ul> </li> 
<li> <br>

<h4>What other kind of DirectX do you think there is?  (Score:3)</h4> by fnj 
(64210)  writes: <br>

<p>Do you know some other way to do it? All graphics cards incorporate 
&quot;hard-wired DirectX&quot;. If you are going to have graphics accelerators, 
they have to accelerate graphics. You can't meaningfully accelerate blits to 
frame buffers any faster than they already are. You have to accelerate higher 
level graphics abstractions. That's all DirectX is - an abstraction of higher 
level graphics operations. Any software, such as OpenGL, can (and does) tap 
into the more well chosen of those abstractions.</p> 
<ul> 
<li></li> </ul> </li> 
<li> <br>

<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:Hard-wired DirectX?  (Score:4, Insightful)</h4> by Anonymous Coward  
writes:  on Monday January 10 2011, @12:34PM (#34825072) <br>

<p></p> 
<p>Worse what happens when directX 12 comes along? is the hardware useless? 
can the hardware be upgraded?</p> 
<p>1) The same thing that happens when you install DirectX 10 on a DX9 card: 
the DX9 subset of DX10 is hardware accelerated, the DX10 parts are run in 
software.<br>
<br>
 2) No. It's not useless. It will still accelerate 
everything it was accelerating before.<br>
<br>
 3) Probably not. But who 
cares? Either replace it, or live with a subset of current functionality.</p> 
Parent Share twitter facebook <br>

<ul> 
<li></li> </ul> </li> 
<li> <br>

<h4>Re:  (Score:3)</h4> by Surt (22457)  writes: <br>

<p>What happens to your nvidia 580 card when dx 12 comes along? Exactly the 
same thing happens with these cpus. Either you live with the reduced 
functionality, or you put in a new video card, assuming your motherboard has a 
graphics card slot.</p> 
<ul> 
<li></li> </ul> </li> </ul></li> </ul></li> 
<li> <br>

<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:2)</h4> by Surt (22457)  writes: <br>

<p>For the foreseeable future you can have your pick of ARM and x86.<br>
On 
the plus side, x86 has been pretty much RISC internally for a long time now. 
And a lot of the ISA has been changed over too. Once they tack on one or two 
more ISA extension you'll be able to have 100% of your code avoid the x86 path.
</p> 
<ul> 
<li></li> </ul> </li> 
<li> <br>

<h4>Re:  (Score:2)</h4> by sexconker (1179573)  writes: <br>

<p>Nvidia is making ARM CPUs.<br>
The next version of Windows will run on ARM.
</p> 
<p>So, yes.</p> 
<p>And if you're a Linux zealot, you can compile your kernel for whatever 
target hardware you want.</p> 
<ul> 
<li></li> </ul> </li> 
<li> <br>

<h4>Re:RISC please  (Score:5, Insightful)</h4> by the linux geek (799780)  
writes:  on Monday January 10 2011, @01:54PM (#34826060) <br>
 Why? What RISC 
architecture provides the same price/power/performance ratio that x86 provides?
<br> <br>
 POWER is fast and has an excellent power/performance, but 
entry-level systems cost ~$3500 after discounts.<br>
 Itanium is fast, but 
expensive and power-hungry.<br>
 MIPS is fast and power-efficient, but none of 
the players in the high-performance MIPS market have any interest in anything 
but network processors.<br>
 SPARC gives you two options - SPARC64 (slow, 
expensive, power-inefficient) and SPARC T-series (fast, but only for 
throughput-driven workloads; expensive; fairly power-hungry)<br>
 ARM has good 
power and price characteristics, but is slow compared to any production x86 
chip except the Atoms and ULV stuff.<br>
<br>
 Basically, I'm not seeing a 
credible alternative to x86 for the market that it thrives in. If you want to 
pay up and get a nice fast RISC system, they're out there; alternatively, if 
you want a somewhat slower one for cheap, ARM is always available. Parent Share 
twitter facebook <br>

<ul> 
<li></li> </ul> </li> 
<li> <br>

<h4>Re:  (Score:3)</h4> by HiThere (15173)  writes: <br>

<p>It really depends on what you mean. If you mean strict RISC, it was too 
late the day the term was coined. If, OTOH, you mean a nearly orthogonal 
architecture that is general purpose (plus the ability to call on specialized 
functions from attached processor chips), that seem, to me, a real possibility.
</p> 
<p>Before you jump, though, you must decide on what is the longest word size 
your computer will address and what is the smallest unit it will address. The 
larger (and the smaller) you go, the harder the task wi </p> 
<ul> 
<li></li> </ul> </li> </ul></li> 
<li> <br>

<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:2)</h4> by blair1q (305137)  writes: <br>

<p>In what way do you mean?</p> 
<p>Putting graphics processing in HW instead of doing it in SW is always 
better, and Intel currently rule in HW speed for mainstream chips.</p> 
<p>So it's hard to tell what you're saying.</p> 
<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:2)</h4> by Surt (22457)  writes: <br>

<p>DX11 titles are so high-end, that no one would find them playable with the 
capabilities of intel HW. Intel HW indeed rules integrated graphics (until 
fusion is on the street), but no one plays high end dx10 titles, much less dx11 
titles on such hardware. So why bother implementing dx11 at all (instead of, 
for example, making dx10 faster, possibly enough faster to play high end dx10 
titles), when it won't be usably fast for any actual dx11 software? The answer 
of course is marketing.</p> 
<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:2)</h4> by blair1q (305137)  writes: <br>

<p>If you're buying high-end software, why are you expecting to play it on 
low-end hardware?</p> 
<p>Integrated GPU/CPU will always be lower performance than discrete. If you 
want bleeding-edge, open your wallet.</p> 
<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:2)</h4> by Surt (22457)  writes: <br>

<p>Precisely. So why is Intel bothering to support dx11? That's high-end only, 
and won't be playable on their hardware, even though it's 'supported'.</p> 
<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:2)</h4> by blair1q (305137)  writes: <br>

<p>Because things change, and DX11 will soon enough be the low end.</p> 
<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:2)</h4> by Surt (22457)  writes: <br>

<p>Then (if it weren't for marketing) maybe it would make sense to implement 
directx11 in the next generation, or the one after that, when they can actually 
make directx11 content usable.</p> 
<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:2)</h4> by blair1q (305137)  writes: <br>

<p>This is the next generation.</p> 
<p>DX11 has been out for over a year.</p> 
<p>Next year DX12 will be the meme.</p> 
<ul> 
<li></li> </ul> </li> </ul></li> </ul></li> </ul></li> 
<li> <br>

<h4>Re:  (Score:2)</h4> by 0123456 (636235)  writes: <br>

<p></p> 
<p>If you're buying high-end software, why are you expecting to play it on 
low-end hardware?</p> 
<p>What's the point of supporting DX11 if the game is unplayable?</p> 
<p>My laptop's graphics card supports DX10, but if I enable the DX10 engine in 
any game I own that has one then the frame rate halves. So why bother?</p> 
<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:  (Score:2)</h4> by blair1q (305137)  writes: <br>

<p>You shouldn't bother paying for something that doesn't work for you. If you 
bought that laptop for the DX10 you should return it and get one that works.</p>
<ul> 
<li></li> </ul> </li> </ul></li> </ul></li> </ul></li> </ul></li> </ul></li> 
<li> <br>

<ul> 
<li></li> </ul> 
<ul> 
<li> <br>

<h4>Re:DirectX who?  (Score:5, Informative)</h4> by Burnhard (1031106)  writes:
 on Monday January 10 2011, @01:00PM (#34825390) <br>
 Given that DX is driving 
innovation in graphics cards at the moment and that GL is playing catch-up, the 
answer has to be &quot;yes&quot;. Parent Share twitter facebook <br>

<ul> 
<li></li> </ul> </li> 
<li> <br>

<h4>Re:  (Score:2)</h4> by Joce640k (829181)  writes: <br>

<p>None of these chips execute 'Direct3D' or 'OpenGL' directly, they remap the 
functions to an internal 3D API.</p> 
<p>OpenGL and Direct3D do mostly the same things so it's not much of a 
hardship for the driver writers.</p> 
<ul> 
<li></li> </ul> </li> </ul></li> 
<li></li> </ul> 
<p><b>There may be more comments in this discussion. Without JavaScript 
enabled, you might want toturn on Classic Discussion System in your preferences 
instead.</b></p> <br>
<br>
<br>
Slashdot <br>
Archived Discussion Moderate 
Moderator Help Delete 
<ul> 
<li> Get more comments </li> 
<li>100 of 199 loaded</li> </ul> 
<ul> 
<li>Submit Story</li> </ul> <br>

<blockquote> 
<p>Any sufficiently advanced technology is indistinguishable from a rigged 
demo. - Andy Finkel, computer guy</p> </blockquote> 
<ul> 
<li>FAQ</li> 
<li>Story Archive</li> 
<li>Hall of Fame</li> 
<li>Advertising</li> 
<li>Terms</li> 
<li>Privacy</li> 
<li>About</li> 
<li> Feedback  If you have problems or questions with Slashdot, email us at 
<em>feedback</em> at <em>this domain</em>. </li> 
<li>Slashdot Japan</li> </ul>  Trademarks property of their respective owners. 
Comments owned by the poster.&copy; 2012 All Rights Reserved. Geeknet, Inc. <br>
Close 
<h3> Slashdot <br>
</h3> Working... <br>

</body>
<!doctype html>
<meta charset="utf-8">
<title></title>
<body>
<br>

<h1>4&nbsp;&nbsp;&nbsp;Writing Structured Programs (Extras)</h1> 
<p>This chapter introduces concepts in algorithms, data structures, program 
design, and applied Python programming. It also contains review of the basic 
mathematical notions of set, relation, and function, and illustrates them in 
terms of Python data structures. It contains many working program fragments 
that you should try yourself.</p> 
<h2>4.1&nbsp;&nbsp;&nbsp;The Practice of Software Development</h2> 
<p>http://www.jwz.org/doc/worse-is-better.html</p> 
<p>http://c2.com/cgi/wiki?DontRepeatYourself</p> 
<p>import this</p> 
<ul> 
<li>Pages on Python pitfalls.</li> </ul> 
<h2>4.2&nbsp;&nbsp;&nbsp;Abstract Data Types</h2> 
<h3>Stacks and Queues</h3> 
<p>Lists are a versatile data type. We can use lists to implement so-called 
abstract data types such as stacks and queues. A stack is a container that has 
a last-in-first-out (or LIFO) policy for adding and removing items (seeFigure 
4.1).</p> 
<p>Figure 4.1: Stacks and Queues</p> 
<p>Stacks are used to keep track of the current context in computer processing 
of natural languages (and programming languages too). We will seldom have to 
deal with stacks explicitly, as the implementation of NLTK parsers, treebank 
corpus readers, (and even Python functions), all use stacks behind the scenes. 
However, it is important to understand what stacks are and how they work.</p> 
<p></p> &nbsp; 
<pre> def check_parens(tokens): stack = [] for token in tokens: if token == '('
:# push stack.append(token) elif token == ')': # pop stack.pop() return stack
</pre> <br>
<br>
<br>
&nbsp; 
<pre> &gt;&gt;&gt; phrase = &quot;( the cat ) ( sat ( on ( the mat )&quot; 
&gt;&gt;&gt;print check_parens(phrase.split()) ['(', '(']</pre> <br>
<br>
<br>

<p>Example 4.2 (code_check_parens.py): Figure 4.2: Check whether parentheses 
are balanced</p> <br>
<br>

<p>In Python, we can treat a list as a stack by limiting ourselves to the 
three operations defined on stacks:append(item) (to push item onto the stack), 
pop() to pop the item off the top of the stack, and [-1] to access the item on 
the top of the stack. The program inExample 4.2 processes a sentence with 
phrase markers, and checks that the parentheses are balanced. The loop pushes 
material onto the stack when it gets an open parenthesis, and pops the stack 
when it gets a close parenthesis. We see that two are left on the stack at the 
end; i.e. the parentheses are not balanced.</p> 
<p>Although the program in Example 4.2 is a useful illustration of stacks, it 
is overkill because we could have done a direct count:phrase.count('(') == 
phrase.count(')'). However, we can use stacks for more sophisticated processing 
of strings containing nested structure, as shown inExample 4.3. Here we build a 
(potentially deeply-nested) list of lists. Whenever a token other than a 
parenthesis is encountered, we add it to a list at the appropriate level of 
nesting. The stack keeps track of this level of nesting, exploiting the fact 
that the item at the top of the stack is actually shared with a more deeply 
nested item. (Hint: add diagnostic print statements to the function to help you 
see what it is doing.)</p> 
<p></p> &nbsp; 
<pre> def convert_parens(tokens): stack = [[]] for token in tokens: if token ==
'(': # push sublist = [] stack[-1].append(sublist) stack.append(sublist) elif 
token ==')': # pop stack.pop() else: # update top of stack 
stack[-1].append(token) return stack[0]</pre> <br>
<br>
<br>
&nbsp; 
<pre> &gt;&gt;&gt; phrase = &quot;( the cat ) ( sat ( on ( the mat ) ) )&quot; 
&gt;&gt;&gt;print convert_parens(phrase.split()) [['the', 'cat'], ['sat', 
['on', ['the', 'branch']]]]</pre> <br>
<br>
<br>

<p>Example 4.3 (code_convert_parens.py): Figure 4.3: Convert a nested phrase 
into a nested list using a stack</p> <br>
<br>

<p>Lists can be used to represent another important data structure. A queue is 
a container that has a first-in-first-out (or FIFO) policy for adding and 
removing items (seeFigure 4.1). We could use a queue of length n to create all 
the n-grams of a text. As with stacks, we will seldom have to deal with queues 
explicitly, as the implementation of NLTK n-gram taggers (Section 5.5) and 
chart parsers use queues behind the scenes. Here's how queues can be 
implemented using lists.</p> &nbsp; 
<pre> &gt;&gt;&gt; queue = ['the', 'cat', 'sat'] &gt;&gt;&gt; queue.append('on'
)&gt;&gt;&gt; queue.append('the') &gt;&gt;&gt; queue.append('branch') 
&gt;&gt;&gt;queue.pop(0) 'the' &gt;&gt;&gt; queue.pop(0) 'cat' &gt;&gt;&gt; 
queue['sat', 'on', 'the', 'branch']</pre> <br>
<br>
<br>
<br>

<p>Note</p> 
<p>The list-based implementation of queues is inefficient for large queues. In 
such cases, it is better to use Python's built-in support for 
&quot;double-ended queues&quot;,collections.deque.</p> 
<h2>4.3&nbsp;&nbsp;&nbsp;Chinese and XML</h2> 
<p>Codecs for processing Chinese text have been incorporated into Python 
(since version 2.4).</p> &nbsp; 
<pre> &gt;&gt;&gt; path = nltk.data.find('samples/sinorama-gb.xml') 
&gt;&gt;&gt;f = codecs.open(path, encoding='gb2312') &gt;&gt;&gt; lines = 
f.readlines()&gt;&gt;&gt; for l in lines: ...  l = l[:-1] ...  utf_enc = 
l.encode('utf8') ... print repr(utf_enc) '&lt;?xml version=&quot;1.0&quot; 
encoding=&quot;gb2312&quot; ?&gt;' '' '&lt;sent&gt;' 
'\xe7\x94\x9a\xe8\x87\xb3\xe7\x8c\xab\xe4\xbb\xa5\xe4\xba\xba\xe8\xb4\xb5' '' 
'In some cases, cats were valued above humans.' '&lt;/sent&gt;'</pre> <br>
<br>

<br> <br>

<p>With appropriate support on your terminal, the escaped text string inside 
the&lt;SENT&gt; element above will be rendered as the following string of 
ideographs: &#29978;&#33267;&#29483;&#20197;&#20154;&#36149;.</p> 
<p>We can also read in the contents of an XML file using the etree package (at 
least, if the file is encoded as UTF-8 &mdash; as of writing, there seems to be 
a problem reading GB2312-encoded files inetree).</p> &nbsp; 
<pre> &gt;&gt;&gt; path = nltk.data.find('samples/sinorama-utf8.xml') 
&gt;&gt;&gt;from nltk.etree import ElementTree as ET &gt;&gt;&gt; tree = 
ET.parse(path)&gt;&gt;&gt; text = tree.findtext('sent') &gt;&gt;&gt; uni_text = 
text.encode('utf8') &gt;&gt;&gt; print repr(uni_text.splitlines()[1]) 
'\xe7\x94\x9a\xe8\x87\xb3\xe7\x8c\xab\xe4\xbb\xa5\xe4\xba\xba\xe8\xb4\xb5'</pre>
<br> <br>
<br>
<br>

<h2>4.4&nbsp;&nbsp;&nbsp;More on Defensive Programming</h2> 
<h3>The Return Statement</h3> 
<p>Another aspect of defensive programming concerns the return statement of a 
function. In order to be confident that all execution paths through a function 
lead to a return statement, it is best to have a single return statement at the 
end of the function definition. This approach has a further benefit: it makes 
it more likely that the function will only return a single type. Thus, the 
following version of ourtag() function is safer. First we assign a default value
, then in certain cases we replace it with a different value . All paths 
through the function body end at the single return statement.</p> &nbsp; 
<pre> &gt;&gt;&gt; def tag(word): ...  result = 'noun' ... if word in ['a', 
'the', 'all']: ...  result = 'det' ...  return result </pre> <br>
<br>
<br>
<br>
<p>A return statement can be used to pass multiple values back to the calling 
program, by packing them into a tuple. Here we define a function that returns a 
tuple consisting of the average word length of a sentence, and the inventory of 
letters used in the sentence. It would have been clearer to write two separate 
functions.</p> &nbsp; 
<pre> &gt;&gt;&gt; def proc_words(words): ...  avg_wordlen = sum(len(word) for 
wordin words)/len(words) ...  chars_used = ''.join(sorted(set(''.join(words)))) 
... return avg_wordlen, chars_used &gt;&gt;&gt; proc_words(['Not', 'a', 'good', 
'way', 'to', 'write', 'functions']) (3, 'Nacdefginorstuwy')</pre> <br>
<br>
<br>
<br> 
<p>[write version with two separate functions]</p> 
<h2>4.5&nbsp;&nbsp;&nbsp;Algorithm Design</h2> 
<p>An <em>algorithm</em> is a &quot;recipe&quot; for solving a problem. For 
example, to multiply 16 by 12 we might use any of the following methods:</p> 
<ol> 
<li>Add 16 to itself 12 times over</li> 
<li>Perform &quot;long multiplication&quot;, starting with the 
least-significant digits of both numbers</li> 
<li>Look up a multiplication table</li> 
<li>Repeatedly halve the first number and double the second, 16*12 = 8*24 = 
4*48 = 2*96 = 192</li> 
<li>Do 10*12 to get 120, then add 6*12</li> 
<li>Rewrite 16*12 as (x+2)(x-2), remember that 14*14=196, and add (+2)(-2) = -4
</li> </ol> 
<p>Each of these methods is a different algorithm, and requires different 
amounts of computation time and different amounts of intermediate information 
to store. A similar situation holds for many other superficially simple tasks, 
such as sorting a list of words.</p> 
<h3>Sorting Algorithms</h3> 
<p>Now, as we saw above, Python provides a built-in function sort() that 
performs this task efficiently. However, NLTK also provides several algorithms 
for sorting lists, to illustrate the variety of possible methods. To illustrate 
the difference in efficiency, we will create a list of 1000 numbers, randomize 
the list, then sort it, counting the number of list manipulations required.</p> 
&nbsp; 
<pre> &gt;&gt;&gt; from random import shuffle &gt;&gt;&gt; a = range(1000) # 
[0,1,2,...999] &gt;&gt;&gt; shuffle(a) # randomize</pre> <br>
<br>
<br>
<br>

<p>Now we can try a simple sort method called <em>bubble sort</em>, that scans 
through the list many times, exchanging adjacent items if they are out of 
order. It sorts the lista in-place, and returns the number of times it modified 
the list:</p> &nbsp; 
<pre> &gt;&gt;&gt; from nltk.misc import sort &gt;&gt;&gt; sort.bubble(a) 
250918</pre> <br>
<br>
<br>
<br>

<p>We can try the same task using various sorting algorithms. Evidently <em>
merge sort</em> is much better than bubble sort, and <em>quicksort</em> is 
better still.</p> &nbsp; 
<pre> &gt;&gt;&gt; shuffle(a); sort.merge(a) 6175 &gt;&gt;&gt; shuffle(a); 
sort.quick(a)2378</pre> <br>
<br>
<br>
<br>

<p>Readers are encouraged to look at nltk.misc.sort to see how these different 
methods work. The collection of NLTK modules exemplify a variety of algorithm 
design techniques, including brute-force, divide-and-conquer, dynamic 
programming, and greedy search. Readers who would like a systematic 
introduction to algorithm design should consult the resources mentioned at the 
end of this tutorial.</p> 
<h3>Decorate-Sort-Undecorate</h3> 
<p>In Chapter 4 we saw how to sort a list of items according to some property 
of the list.</p> &nbsp; 
<pre> &gt;&gt;&gt; words = 'I turned off the spectroroute'.split() &gt;&gt;&gt;
words.sort(cmp)&gt;&gt;&gt; words ['I', 'off', 'spectroroute', 'the', 'turned'] 
&gt;&gt;&gt;words.sort(lambda x, y: cmp(len(y), len(x))) &gt;&gt;&gt; words 
['spectroroute', 'turned', 'off', 'the', 'I']</pre> <br>
<br>
<br>
<br>

<p>This is inefficient when the list of items gets long, as we compute len() 
twice for every comparison (about 2nlog(n) times). The following is more 
efficient:</p> &nbsp; 
<pre> &gt;&gt;&gt; [pair[1] for pair in sorted((len(w), w) for w in 
words)[::-1]]['spectroroute', 'turned', 'the', 'off', 'I']</pre> <br>
<br>
<br>

<br> 
<p>This technique is called decorate-sort-undecorate. We can compare its 
performance by timing how long it takes to execute it a million times.</p> 
&nbsp; 
<pre> &gt;&gt;&gt; from timeit import Timer &gt;&gt;&gt; Timer(
&quot;sorted(words, lambda x, y: cmp(len(y), len(x)))&quot;, ... &quot;words='I 
turned off the spectroroute'.split()&quot;).timeit() 8.3548779487609863 
&gt;&gt;&gt;Timer(&quot;[pair[1] for pair in sorted((len(w), w) for w in 
words)]&quot;, ... &quot;words='I turned off the spectroroute'.split()&quot;
).timeit()9.9698889255523682</pre> <br>
<br>
<br>
<br>

<p>MORE: consider what happens as the lists get longer...</p> 
<p>Another example: sorting dates of the form &quot;1 Jan 1970&quot;</p> &nbsp;
<pre> &gt;&gt;&gt; month_index = { ... &quot;Jan&quot; : 1, &quot;Feb&quot; : 
2,&quot;Mar&quot; : 3, &quot;Apr&quot; : 4, ... &quot;May&quot; : 5, 
&quot;Jun&quot; : 6, &quot;Jul&quot; : 7, &quot;Aug&quot; : 8, ... 
&quot;Sep&quot; : 9, &quot;Oct&quot; : 10, &quot;Nov&quot; : 11, &quot;Dec&quot;
 : 12... } &gt;&gt;&gt; def date_cmp(date_string1, date_string2): ...  
(d1,m1,y1) = date_string1.split()...  (d2,m2,y2) = date_string2.split() ...  
conv1 = y1, month_index[m1], d1...  conv2 = y2, month_index[m2], d2 ...  return 
cmp(a2, b2)&gt;&gt;&gt; sort(date_list, date_cmp)</pre> <br>
<br>
<br>
<br>

<p>The comparison function says that we compare two times of the form ('Mar', 
'2004') by reversing the order of the month and year, and converting the month 
into a number to get('2004', '3'), then using Python's built-in cmp function to 
compare them.</p> 
<p>Now do this using decorate-sort-undecorate, for large data size</p> 
<p>Time comparison</p> 
<h3>Brute Force</h3> 
<p>Wordfinder Puzzle</p> 
<p>Here we will generate a grid of letters, containing words found in the 
dictionary. First we remove any duplicates and disregard the order in which the 
lexemes appeared in the dictionary. We do this by converting it to a set, then 
back to a list. Then we select the first 200 words, and then only keep those 
words having a reasonable length.</p> &nbsp; 
<pre> &gt;&gt;&gt; words = list(set(lexemes)) &gt;&gt;&gt; words = words[:200] 
&gt;&gt;&gt;words = [w for w in words if 3 &lt;= len(w) &lt;= 12]</pre> <br>

<br> <br>
<br>

<p>Now we generate the wordfinder grid, and print it out.</p> &nbsp; 
<pre> &gt;&gt;&gt; from nltk.misc.wordfinder import wordfinder &gt;&gt;&gt; 
grid, used = wordfinder(words)&gt;&gt;&gt; for i in range(len(grid)): ... for j 
in range(len(grid[i])): ... print grid[i][j], ... print O G H K U U V U V K U O 
R O V A K U N C K Z O T O I S E K S N A I E R E P A K C I A R A A K I O Y O V R 
S K A W J K U Y L R N H N K R G V U K G I A U D J K V N I I Y E A U N O K O O U 
K T R K Z A E L A V U K O X V K E R V T I A A E R K R K A U I U G O K U T X U I 
K N V V L I E O R R K O K N U A J Z T K A K O O S U T R I A U A U A S P V F O R 
O O K I C A O U V K R R T U I V A O A U K V V S L P E K A I O A I A K R S V K U 
S A A I X I K O P S V I K R O E O A R E R S E T R O J X O I I S U A G K R O R E 
R I T A I Y O A R R R A T O O K O I K I W A K E A A R O O E A K I K V O P I K H 
V O K K G I K T K K L A K A A R M U G E P A U A V Q A I O O O U K N X O G K G A 
R E A A P O O R K V V P U J E T Z P K B E I E T K U R A N E O A V A E O R U K B 
V K S Q A V U E C E K K U K I K I R A E K O J I Q K K K</pre> <br>
<br>
<br>

<br> 
<p>Finally we generate the words which need to be found.</p> &nbsp; 
<pre> &gt;&gt;&gt; for i in range(len(used)): ... print &quot;%-12s&quot; % 
used[i],... if float(i+1)%5 == 0: print KOKOROPAVIRA KOROROVIVIRA KAEREASIVIRA 
KOTOKOTOARA KOPUASIVIRA KATAITOAREI KAITUTUVIRA KERIKERISI KOKARAPATO KOKOVURITO
KAUKAUVIRA KOKOPUVIRA KAEKAESOTO KAVOVOVIRA KOVAKOVARA KAAREKOPIE KAEPIEVIRA 
KAPUUPIEPA KOKORUUTO KIKIRAEKO KATAAVIRA KOVOKOVOA KARIVAITO KARUVIRA KAPOKARI 
KUROVIRA KITUKITU KAKUPUTE KAEREASI KUKURIKO KUPEROO KAKAPUA KIKISI KAVORA 
KIKIPI KAPUA KAARE KOETO KATAI KUVA KUSI KOVO KOAI</pre> <br>
<br>
<br>
<br>

<h3>Problem Transformation (aka Transform-and-Conquer)</h3> 
<p>Find words which, when reversed, make legal words. Extremely wasteful brute 
force solution:</p> &nbsp; 
<pre> &gt;&gt;&gt; words = nltk.corpus.words.words('en') &gt;&gt;&gt; for word1
in words: ... for word2 in words: ... if word1 == word2[::-1]: ... print word1
</pre> <br>
<br>
<br>
<br>

<p>More efficient:</p> &nbsp; 
<pre> &gt;&gt;&gt; wordlist = set(words) &gt;&gt;&gt; rev_wordlist = 
set(word[::-1]for word in words) &gt;&gt;&gt; 
sorted(wordlist.intersection(rev_wordlist))['ah', 'are', 'bag', 'ban', 'bard', 
'bat', 'bats', 'bib', 'bob', 'boob', 'brag', 'bud', 'buns', 'bus', 'but', 
'civic', 'dad', 'dam', 'decal', 'deed', 'deeps', 'deer', 'deliver', 'denier', 
'desserts', 'deus', 'devil', 'dial', 'diaper', 'did', 'dim', 'dog', 'don', 
'doom', 'drab', 'draw', 'drawer', 'dub', 'dud', 'edit', 'eel', 'eke', 'em', 
'emit', 'era', 'ere', 'evil', 'ewe', 'eye', 'fires', 'flog', 'flow', 'gab', 
'gag', 'garb', 'gas', 'gel', 'gig', 'gnat', 'god', 'golf', 'gulp', 'gum', 
'gums', 'guns', 'gut', 'ha', 'huh', 'keel', 'keels', 'keep', 'knits', 'laced', 
'lager', 'laid', 'lap', 'lee', 'leek', 'leer', 'leg', 'leper', 'level', 
'lever', 'liar', 'live', 'lived', 'loop', 'loops', 'loot', 'loots', 'mad', 
'madam', 'me', 'meet', 'mets', 'mid', 'mood', 'mug', 'nab', 'nap', 'naps', 
'net', 'nip', 'nips', 'no', 'nod', 'non', 'noon', 'not', 'now', 'nun', 'nuts', 
'on', 'pal', 'pals', 'pan', 'pans', 'par', 'part', 'parts', 'pat', 'paws', 
'peek', 'peels', 'peep', 'pep', 'pets', 'pin', 'pins', 'pip', 'pit', 'plug', 
'pool', 'pools', 'pop', 'pot', 'pots', 'pup', 'radar', 'rail', 'rap', 'rat', 
'rats', 'raw', 'redder', 'redraw', 'reed', 'reel', 'refer', 'regal', 'reined', 
'remit', 'repaid', 'repel', 'revel', 'reviled', 'reviver', 'reward', 'rotator', 
'rotor', 'sag', 'saw', 'sees', 'serif', 'sexes', 'slap', 'sleek', 'sleep', 
'sloop', 'smug', 'snap', 'snaps', 'snip', 'snoops', 'snub', 'snug', 'solos', 
'span', 'spans', 'spat', 'speed', 'spin', 'spit', 'spool', 'spoons', 'spot', 
'spots', 'stab', 'star', 'stem', 'step', 'stew', 'stink', 'stool', 'stop', 
'stops', 'strap', 'straw', 'stressed', 'stun', 'sub', 'sued', 'swap', 'tab', 
'tang', 'tap', 'taps', 'tar', 'teem', 'ten', 'tide', 'time', 'timer', 'tip', 
'tips', 'tit', 'ton', 'tool', 'top', 'tops', 'trap', 'tub', 'tug', 'war', 
'ward', 'warder', 'warts', 'was', 'wets', 'wolf', 'won']</pre> <br>
<br>
<br>

<br> 
<p>Observe that this output contains redundant information; each word and its 
reverse is included. How could we remove this redundancy?</p> 
<p>Presorting, sets:</p> 
<p>Find words which have at least (or exactly) one instance of all vowels. 
Instead of writing extremely complex regular expressions, some simple 
preprocessing does the trick:</p> &nbsp; 
<pre> &gt;&gt;&gt; words = [&quot;sequoia&quot;, &quot;abacadabra&quot;, 
&quot;yiieeaouuu!&quot;] &gt;&gt;&gt; vowels = &quot;aeiou&quot; &gt;&gt;&gt; [w
for w in words if set(w).issuperset(vowels)] ['sequoia', 'yiieeaouuu!'] 
&gt;&gt;&gt;[w for w in words if sorted(c for c in w if c in vowels) == 
list(vowels)]['sequoia']</pre> <br>
<br>
<br>
<br>

<h3>Space-Time Tradeoffs</h3> 
<p>Indexing</p> 
<p>Fuzzy Spelling</p> 
<h2>4.6&nbsp;&nbsp;&nbsp;Exercises</h2> 
<ol> 
<li>&#9681; Consider again the problem of hyphenation across line-breaks. 
Suppose that you have successfully written a tokenizer that returns a list of 
strings, where some strings may contain a hyphen followed by a newline 
character, e.g.long-\nterm. Write a function that iterates over the tokens in a 
list, removing the newline character from each, in each of the following ways:
<ol> 
<li>Use doubly-nested for loops. The outer loop will iterate over each token 
in the list, while the inner loop will iterate over each character of a string.
</li> 
<li>Replace the inner loop with a call to re.sub()</li> 
<li>Finally, replace the outer loop with call to the map() function, to apply 
this substitution to each token.</li> 
<li>Discuss the clarity (or otherwise) of each of these approaches.</li> </ol> 
</li> </ol> 
<h2>4.7&nbsp;&nbsp;&nbsp;Search</h2> 
<p>Many NLP tasks can be construed as search problems. For example, the task 
of a parser is to identify one or more parse trees for a given sentence. As we 
saw in Part II, there are several algorithms for parsing. Arecursive descent 
parser performs backtracking search, applying grammar productions in turn until 
a match with the next input word is found, and backtracking when there is no 
match. As we will see inSection 8.6, the space of possible parse trees is very 
large; a parser can be thought of as providing a relatively efficient way to 
find the right solution(s) within a very large space of candidates.</p> 
<p>As another example of search, suppose we want to find the most complex 
sentence in a text corpus. Before we can begin we have to be explicit about how 
the complexity of a sentence is to be measured: word count, verb count, 
character count, parse-tree depth, etc. In the context of learning this is 
known as theobjective function, the property of candidate solutions we want to 
optimize.</p> 
<h3>Exhaustive Search</h3> 
<ul> 
<li>brute-force approach</li> 
<li>enumerate search space, evaluate at each point</li> 
<li>this example: search space size is 255 = 36,028,797,018,963,968</li> </ul> 
<p>For a computer that can do 100,000 evaluations per second, this would take 
over 10,000 years!</p> 
<p>Backtracking search -- saw this in the recursive descent parser.</p> 
<h3>Hill-Climbing Search</h3> 
<p>Starting from a given location in the search space, evaluate nearby 
locations and move to a new location only if it is an improvement on the 
current location.</p> 
<p></p> &nbsp; 
<pre> def flip(segs, pos): return segs[:pos] + `1-int(segs[pos])` + 
segs[pos+1:]def hill_climb(text, segs, iterations): for i in range(iterations): 
pos, best = 0, evaluate(text, segs)for i in range(len(segs)): score = 
evaluate(text, flip(segs, i))if score &lt; best: pos, best = i, score if pos != 
0: segs = flip(segs, pos)print evaluate(text, segs), segment(text, segs) return 
segs</pre> <br>
<br>
<br>
&nbsp; 
<pre> &gt;&gt;&gt; print evaluate(text, seg1), segment(text, seg1) 63 
['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy'] 
&gt;&gt;&gt;hill_climb(text, seg1, 20) 61 ['doyouseethekittyseethedoggy', 
'doyoulikethekitty', 'likethedoggy'] 59 
['doyouseethekittyseethedoggydoyoulikethekitty', 'likethedoggy'] 57 
['doyouseethekittyseethedoggydoyoulikethekittylikethedoggy']</pre> <br>
<br>

<br> 
<p>Example 4.4 (code_hill_climb.py): Figure 4.4: Hill-Climbing Search</p> <br>

<br> 
<h2>4.8&nbsp;&nbsp;&nbsp;Object-Oriented Programming in Python (DRAFT)</h2> 
<p>Object-Oriented Programming is a programming paradigm in which complex 
structures and processes are decomposed intoclasses, each encapsulating a 
single data type and the legal operations on that type. In this section we show 
you how to create simple data classes and processing classes by example. For a 
systematic introduction to Object-Oriented design, please consult the large 
literature of books on this topic.</p> 
<h3>Data Classes: Trees in NLTK</h3> 
<p>An important data type in language processing is the syntactic tree. Here 
we will review the parts of the NLTK code that defines theTree class.</p> 
<p>The first line of a class definition is the class keyword followed by the 
class name, in this caseTree. This class is derived from Python's built-in list 
class, permitting us to use standard list operations to access the children of 
a tree node.</p> &nbsp; 
<pre> &gt;&gt;&gt; class Tree(list):</pre> <br>
<br>
<br>
<br>

<p>Next we define the initializer __init__(); Python knows to call this 
function when you ask for a new tree object by writingt = Tree(node, children). 
The constructor's first argument is special, and is standardly calledself, 
giving us a way to refer to the current object from within its definition. This 
particular constructor calls the list initializer (similar to callingself = 
list(children)), then defines the node property of a tree.</p> &nbsp; 
<pre> ... def __init__(self, node, children): ... list.__init__(self, 
children) ... self.node = node</pre> <br>
<br>
<br>
<br>

<p>Next we define another special function that Python knows to call when we 
index a Tree. The first case is the simplest, when the index is an integer, e.g.
t[2], we just ask for the list item in the obvious way. The other cases are for 
handling slices, liket[1:2], or t[:].</p> &nbsp; 
<pre> ... def __getitem__(self, index): ... if isinstance(index, int): ... 
return list.__getitem__(self, index) ... else: ... if len(index) == 0: ... 
return self ... elif len(index) == 1: ... return self[int(index[0])] ... else: 
... return self[int(index[0])][index[1:]] ...</pre> <br>
<br>
<br>
<br>

<p>This method was for accessing a child node. Similar methods are provided 
for setting and deleting a child (using__setitem__) and __delitem__).</p> 
<p>Two other special member functions are __repr__() and __str__(). The 
__repr__() function produces a string representation of the object, one that 
can be executed to re-create the object, and is accessed from the interpreter 
simply by typing the name of the object and pressing 'enter'. The__str__() 
function produces a human-readable version of the object; here we call a 
pretty-printing function we have defined calledpp().</p> &nbsp; 
<pre> ... def __repr__(self): ... childstr = ' '.join([repr(c) for c in self]) 
... return '(%s: %s)' % (self.node, childstr) ... def __str__(self): ... return 
self.pp()</pre> <br>
<br>
<br>
<br>

<p>Next we define some member functions that do other standard operations on 
trees. First, for accessing the leaves:</p> &nbsp; 
<pre> ... def leaves(self): ... leaves = [] ... for child in self: ... if 
isinstance(child, Tree): ... leaves.extend(child.leaves()) ... else: ... 
leaves.append(child) ... return leaves</pre> <br>
<br>
<br>
<br>

<p>Next, for computing the height:</p> &nbsp; 
<pre> ... def height(self): ... max_child_height = 0 ... for child in self: 
... if isinstance(child, Tree): ... max_child_height = max(max_child_height, 
child.height()) ... else: ... max_child_height = max(max_child_height, 1) ... 
return 1 + max_child_height</pre> <br>
<br>
<br>
<br>

<p>And finally, for enumerating all the subtrees (optionally filtered):</p> 
&nbsp; 
<pre> ... def subtrees(self, filter=None): ... if not filter or filter(self): 
... yield self ... for child in self: ... if isinstance(child, Tree): ... for 
subtree in child.subtrees(filter): ... yield subtree</pre> <br>
<br>
<br>
<br>

<h3>Processing Classes: N-gram Taggers in NLTK</h3> 
<p>This section will discuss the tag.ngram module.</p> 
<h3>Duck Typing</h3> 
<p>[to be written]</p> 
<p>(Hunt &amp; Thomas, 2000)</p> 
<p>About this document...</p> 
<p>This is a chapter from <em>Natural Language Processing with Python</em>, by 
Steven Bird, Ewan Klein and Edward Loper, Copyright &copy; 2009 the authors. It 
is distributed with the<em>Natural Language Toolkit</em> [http://www.nltk.org/
], Version 2.0b7, under the terms of the<em>Creative Commons 
Attribution-Noncommercial-No Derivative Works 3.0 United States License</em> [
http://creativecommons.org/licenses/by-nc-nd/3.0/us/].</p> 
<p>This document is Revision: 8464 Mon 14 Dec 2009 10:58:42 EST</p> 
</body>
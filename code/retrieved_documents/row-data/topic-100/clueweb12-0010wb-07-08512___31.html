<!doctype html>
<meta charset="utf-8">
<title>What is data science? - O'Reilly Radar</title>
<body>
<br>
<br>

<p>Your Account</p> <br>
Shopping Cart <br>
<br>
<br>
<br>
<br>

<ul> 
<li>Home</li> 
<li>Shop</li> 
<li>Radar: News &amp; Commentary</li> 
<li>Answers</li> 
<li>Safari Books Online</li> 
<li>Conferences</li> 
<li>Training</li> 
<li>School of Technology</li> 
<li>Community</li> </ul> 
<ul> 
<li> Data <br>
</li> 
<li> Gov 2.0 <br>
</li> 
<li> Mobile <br>
</li> 
<li> Programming <br>
</li> 
<li> Publishing <br>
</li> 
<li> Web 2.0 <br>
</li> 
<li> Web Ops &amp; Performance <br>
</li> 
<li> About Radar <br>
</li> </ul> <br>
<br>
<br>

<ul> 
<li>Print </li> 
<li> Listen <br>
</li> </ul> 
<h2> What is data science?</h2> 
<h3>Analysis: The future belongs to the companies and people that turn data 
into products.</h3> 
<p>by Mike Loukides  |&nbsp;@mikeloukides &nbsp;|&nbsp;+Mike Loukides 
&nbsp;|&nbsp;Comments: 52 |&nbsp; 2 June 2010</p> 
<p> Tweet </p> 
<p></p> 
<h3>Report sections</h3> 
<p>What is data science?</p> 
<p>Where data comes from</p> 
<p>Working with data at scale</p> 
<p>Making data tell its story</p> 
<p>Data scientists</p> 
<p></p> 
<p>Download free PDF version</p> 
<p></p> We've all heard it: according to Hal Varian, statistics is the next 
sexy job. Five years ago, in What is Web 2.0, Tim O'Reilly said that &quot;data 
is the next Intel Inside.&quot; But what does that statement mean? Why do we 
suddenly care about statistics and about data?
<p></p> 
<p>In this post, I examine the many sides of data science -- the technologies, 
the companies and the unique skill sets.</p> 
<p></p> 
<h2>What is data science?</h2> 
<p></p> 
<p>The web is full of &quot;data-driven apps.&quot; Almost any e-commerce 
application is a data-driven application. There's a database behind a web front 
end, and middleware that talks to a number of other databases and data services 
(credit card processing companies, banks, and so on). But merely using data 
isn't really what we mean by &quot;data science.&quot; A data application 
acquires its value from the data itself, and creates more data as a result. 
It's not just an application with data; it's a data product. Data science 
enables the creation of data products.</p> 
<p>One of the earlier data products on the Web was the CDDB database. The 
developers of CDDB realized that any CD had a unique signature, based on the 
exact length (in samples) of each track on the CD. Gracenote built a database 
of track lengths, and coupled it to a database of album metadata (track titles, 
artists, album titles). If you've ever used iTunes to rip a CD, you've taken 
advantage of this database. Before it does anything else, iTunes reads the 
length of every track, sends it to CDDB, and gets back the track titles. If you 
have a CD that's not in the database (including a CD you've made yourself), you 
can create an entry for an unknown album. While this sounds simple enough, it's 
revolutionary: CDDB views music as data, not as audio, and creates new value in 
doing so. Their business is fundamentally different from selling music, sharing 
music, or analyzing musical tastes (though these can also be &quot;data 
products&quot;). CDDB arises entirely from viewing a musical problem as a data 
problem.</p> <strong>Strata Conference New York 2011</strong>, being held Sept. 
22-23, covers the latest and best tools and technologies for data science -- 
from gathering, cleaning, analyzing, and storing data to communicating data 
intelligence effectively.<br>
<br>
<strong>Save 30% on registration with the 
code STN11RAD</strong> <br>

<p>Google is a master at creating data products. Here's a few examples:</p> 
<ul> 
<li> 
<p>Google's breakthrough was realizing that a search engine could use input 
other than the text on the page. Google'sPageRank algorithm was among the first 
to use data outside of the page itself, in particular, the number of links 
pointing to a page. Tracking links made Google searches much more useful, and 
PageRank has been a key ingredient to the company's success.</p></li> 
<li>
<p>Spell checking isn't a terribly difficult problem, but by suggesting 
corrections to misspelled searches, and observing what the user clicks in 
response, Google made it much more accurate. They've built a dictionary of 
common misspellings, their corrections, and the contexts in which they occur.
</p></li> 
<li> 
<p> Speech recognition has always been a hard problem, and it remains 
difficult. But Google has made huge strides by using the voice data they've 
collected, and has been able tointegrate voice search into their core search 
engine.</p></li> 
<li> 
<p>During the Swine Flu epidemic of 2009, Google was able to track the 
progress of the epidemicby following searches for flu-related topics.</p></li> 
</ul> 
<p>Flu trends</p> 
<p></p> 
<p>Google was able to spot trends in the Swine Flu epidemic roughly two weeks 
before the Center for Disease Control by analyzing searches that people were 
making in different regions of the country.</p> 
<p>Google isn't the only company that knows how to use data. Facebook and 
LinkedIn use patterns of friendship relationships to suggest other people you 
may know, or should know, with sometimes frightening accuracy.Amazon saves your 
searches, correlates what you search for with what other users search for, and 
uses it to create surprisingly appropriate recommendations. These 
recommendations are &quot;data products&quot; that help to drive Amazon's more 
traditional retail business. They come about because Amazon understands that a 
book isn't just a book, a camera isn't just a camera, and a customer isn't just 
a customer; customers generate a trail of &quot;data exhaust&quot; that can be 
mined and put to use, and a camera is a cloud of data that can be correlated 
with the customers' behavior, the data they leave every time they visit the 
site.</p> 
<p>The thread that ties most of these applications together is that data 
collected from users provides added value. Whether that data is search terms, 
voice samples, or product reviews, the users are in a feedback loop in which 
they contribute to the products they use. That's the beginning of data science.
</p> 
<p>In the last few years, there has been an explosion in the amount of data 
that's available. Whether we're talking about web server logs, tweet streams, 
online transaction records, &quot;citizen science,&quot; data from sensors, 
government data, or some other source, the problem isn't finding data, it's 
figuring out what to do with it. And it's not just companies using their own 
data, or the data contributed by their users. It's increasingly common to 
mashup data from a number of sources. &quot;Data Mashups in R&quot; analyzes 
mortgage foreclosures in Philadelphia County by taking a public report from the 
county sheriff's office, extracting addresses and using Yahoo to convert the 
addresses to latitude and longitude, then using the geographical data to place 
the foreclosures on a map (another data source), and group them by 
neighborhood, valuation, neighborhood per-capita income, and other 
socio-economic factors.</p> 
<p>The question facing every company today, every startup, every non-profit, 
every project site that wants to attract a community, is how to use data 
effectively -- not just their own data, but all the data that's available and 
relevant. Using data effectively requires something different from traditional 
statistics, where actuaries in business suits perform arcane but fairly 
well-defined kinds of analysis. What differentiates data science from 
statistics is that data science is a holistic approach. We're increasingly 
finding data in the wild, and data scientists are involved with gathering data, 
massaging it into a tractable form, making it tell its story, and presenting 
that story to others.</p> 
<p>To get a sense for what skills are required, let's look at the data 
lifecycle: where it comes from, how you use it, and where it goes.</p> 
<p></p> 
<h2>Where data comes from</h2> 
<p></p> 
<p>Data is everywhere: your government, your web server, your business 
partners,even your body. While we aren't drowning in a sea of data, we're 
finding that almost everything can (or has) been instrumented. At O'Reilly, we 
frequently combine publishing industry data fromNielsen BookScan with our own 
sales data, publicly available Amazon data, and even job data to see what's 
happening in the publishing industry. Sites likeInfochimps and Factual provide 
access to many large datasets, including climate data, MySpace activity 
streams, and game logs from sporting events. Factual enlists users to update 
and improve its datasets, which cover topics as diverse as endocrinologists to 
hiking trails.</p> 
<p></p> 
<p>1956 disk drive</p> 
<p>One of the first commercial disk drives from IBM. It has a 5 MB capacity 
and it's stored in a cabinet roughly the size of a luxury refrigerator. In 
contrast, a 32 GB microSD card measures around 5/8 x 3/8 inch and weighs about 
0.5 gram.</p> 
<p>Photo: Mike Loukides. Disk drive on display at IBM Almaden Research</p> 
Much of the data we currently work with is the direct consequence of Web 2.0, 
and of Moore's Law applied to data. The web has people spending more time 
online, and leaving a trail of data wherever they go. Mobile applications leave 
an even richer data trail, since many of them are annotated with geolocation, 
or involve video or audio, all of which can be mined. Point-of-sale devices and 
frequent-shopper's cards make it possible to capture all of your retail 
transactions, not just the ones you make online. All of this data would be 
useless if we couldn't store it, and that's where Moore's Law comes in. Since 
the early '80s, processor speed has increased from10 MHz to 3.6 GHz -- an 
increase of 360 (not counting increases in word length and number of cores). 
But we've seen much bigger increases in storage capacity, on every level. RAM 
has moved from $1,000/MB to roughly $25/GB -- a price reduction of about 40000, 
to say nothing of the reduction in size and increase in speed. Hitachi made the
first gigabyte disk drives in 1982, weighing in at roughly 250 pounds; now 
terabyte drives are consumer equipment, and a 32 GB microSD card weighs about 
half a gram. Whether you look at bits per gram, bits per dollar, or raw 
capacity, storage has more than kept pace with the increase of CPU speed.
<p></p> 
<p>The importance of Moore's law as applied to data isn't just geek 
pyrotechnics. Data expands to fill the space you have to store it. The more 
storage is available, the more data you will find to put into it. The data 
exhaust you leave behind whenever you surf the web, friend someone on Facebook, 
or make a purchase in your local supermarket, is all carefully collected and 
analyzed. Increased storage capacity demands increased sophistication in the 
analysis and use of that data. That's the foundation of data science.</p> 
<p>So, how do we make that data useful? The first step of any data analysis 
project is &quot;data conditioning,&quot; or getting data into a state where 
it's usable. We are seeing more data in formats that are easier to consume: 
Atom data feeds, web services, microformats, and other newer technologies 
provide data in formats that's directly machine-consumable. But old-stylescreen 
scraping hasn't died, and isn't going to die. Many sources of &quot;wild 
data&quot; are extremely messy. They aren't well-behaved XML files with all the 
metadata nicely in place. The foreclosure data used in &quot;Data Mashups in R
&quot; was posted on a public website by the Philadelphia county sheriff's 
office. This data was presented as an HTML file that was probably generated 
automatically from a spreadsheet. If you've ever seen the HTML that's generated 
by Excel, you know that's going to be fun to process.</p> 
<p>Data conditioning can involve cleaning up messy HTML with tools like 
Beautiful Soup, natural language processing to parse plain text in English and 
other languages, or even getting humans to do the dirty work. You're likely to 
be dealing with an array of data sources, all in different forms. It would be 
nice if there was a standard set of tools to do the job, but there isn't. To do 
data conditioning, you have to be ready for whatever comes, and be willing to 
use anything from ancient Unix utilities such asawk to XML parsers and machine 
learning libraries. Scripting languages, such asPerl and Python, are essential. 
</p> 
<p>Once you've parsed the data, you can start thinking about the quality of 
your data. Data is frequently missing or incongruous. If data is missing, do 
you simply ignore the missing points? That isn't always possible. If data is 
incongruous, do you decide that something is wrong with badly behaved data 
(after all, equipment fails), or that the incongruous data is telling its own 
story, which may be more interesting? It's reported that the discovery of ozone 
layer depletion was delayed becauseautomated data collection tools discarded 
readings that were too low 1. In data science, what you have is frequently all 
you're going to get. It's usually impossible to get &quot;better&quot; data, 
and you have no alternative but to work with the data at hand.</p> 
<p>If the problem involves human language, understanding the data adds another 
dimension to the problem. Roger Magoulas, who runs the data analysis group at 
O'Reilly, was recently searching a database for Apple job listings requiring 
geolocation skills. While that sounds like a simple task, the trick was 
disambiguating &quot;Apple&quot; from many job postings in the growing Apple 
industry. To do it well you need to understand the grammatical structure of a 
job posting; you need to be able to parse the English. And that problem is 
showing up more and more frequently. Try usingGoogle Trends to figure out 
what's happening with theCassandra database or the Python language, and you'll 
get a sense of the problem. Google has indexed many, many websites about large 
snakes. Disambiguation is never an easy task, but tools like theNatural 
Language Toolkit library can make it simpler. </p> 
<p>When natural language processing fails, you can replace artificial 
intelligence with human intelligence. That's where services like Amazon's
Mechanical Turk come in. If you can split your task up into a large number of 
subtasks that are easily described, you can use Mechanical Turk's marketplace 
for cheap labor. For example, if you're looking at job listings, and want to 
know which originated with Apple, you can have real people do the 
classification for roughly $0.01 each. If you have already reduced the set to 
10,000 postings with the word &quot;Apple,&quot; paying humans $0.01 to 
classify them only costs $100.</p> 
<p></p> 
<h2>Working with data at scale</h2> 
<p></p> 
<p>We've all heard a lot about &quot;big data,&quot; but &quot;big&quot; is 
really a red herring. Oil companies, telecommunications companies, and other 
data-centric industries have had huge datasets for a long time. And as storage 
capacity continues to expand, today's &quot;big&quot; is certainly tomorrow's 
&quot;medium&quot; and next week's &quot;small.&quot; The most meaningful 
definition I've heard:<em>&quot;big data&quot; is when the size of the data 
itself becomes part of the problem</em>. We're discussing data problems ranging 
from gigabytes to petabytes of data. At some point, traditional techniques for 
working with data run out of steam.</p> 
<p>What are we trying to do with data that's different? According to Jeff 
Hammerbacher2 (@hackingdata), we're trying to build information platforms or 
dataspaces. Information platforms are similar to traditional data warehouses, 
but different. They expose rich APIs, and are designed for exploring and 
understanding the data rather than for traditional analysis and reporting. They 
accept all data formats, including the most messy, and their schemas evolve as 
the understanding of the data changes.</p> 
<p>Most of the organizations that have built data platforms have found it 
necessary to go beyond the relational database model. Traditional relational 
database systems stop being effective at this scale. Managing sharding and 
replication across a horde of database servers is difficult and slow. The need 
to define a schema in advance conflicts with reality of multiple, unstructured 
data sources, in which you may not know what's important until after you've 
analyzed the data. Relational databases are designed for consistency, to 
support complex transactions that can easily be rolled back if any one of a 
complex set of operations fails. While rock-solid consistency is crucial to 
many applications, it's not really necessary for the kind of analysis we're 
discussing here. Do you really care if you have 1,010 or 1,012 Twitter 
followers? Precision has an allure, but in most data-driven applications 
outside of finance, that allure is deceptive. Most data analysis is 
comparative: if you're asking whether sales to Northern Europe are increasing 
faster than sales to Southern Europe, you aren't concerned about the difference 
between 5.92 percent annual growth and 5.93 percent.</p> 
<p>To store huge datasets effectively, we've seen a new breed of databases 
appear. These are frequently called NoSQL databases, or Non-Relational 
databases, though neither term is very useful. They group together 
fundamentally dissimilar products by telling you what they aren't. Many of 
these databases are the logical descendants of Google'sBigTable and Amazon's 
Dynamo, and are designed to be distributed across many nodes, to provide 
&quot;eventual consistency&quot; but not absolute consistency, and to have very 
flexible schema. While there are two dozen or so products available (almost all 
of them open source), a few leaders have established themselves:</p> 
<ul> 
<li> 
<p> Cassandra: Developed at Facebook, in production use at Twitter, Rackspace, 
Reddit, and other large sites. Cassandra is designed for high performance, 
reliability, and automatic replication. It has a very flexible data model. A 
new startup,Riptano, provides commercial support. </p> </li> 
<li> 
<p> HBase: Part of the Apache Hadoop project, and modelled on Google's 
BigTable. Suitable for extremely large databases (billions of rows, millions of 
columns), distributed across thousands of nodes. Along with Hadoop, commercial 
support is provided byCloudera. </p> </li> </ul> 
<p>Storing data is only part of building a data platform, though. Data is only 
useful if you can do something with it, and enormous datasets present 
computational problems. Google popularized theMapReduce approach, which is 
basically a divide-and-conquer strategy for distributing an extremely large 
problem across an extremely large computing cluster. In the &quot;map&quot; 
stage, a programming task is divided into a number of identical subtasks, which 
are then distributed across many processors; the intermediate results are then 
combined by a single reduce task. In hindsight, MapReduce seems like an obvious 
solution to Google's biggest problem, creating large searches. It's easy to 
distribute a search across thousands of processors, and then combine the 
results into a single set of answers. What's less obvious is that MapReduce has 
proven to be widely applicable to many large data problems, ranging from search 
to machine learning.</p> 
<p>The most popular open source implementation of MapReduce is the Hadoop 
project. Yahoo's claim that they had built the world's largest production 
Hadoop application, with 10,000 cores running Linux, brought it onto center 
stage. Many of the key Hadoop developers have found a home atCloudera, which 
provides commercial support. Amazon'sElastic MapReduce makes it much easier to 
put Hadoop to work without investing in racks of Linux machines, by providing 
preconfigured Hadoop images for its EC2 clusters. You can allocate and 
de-allocate processors as needed, paying only for the time you use them.</p> 
<p>Hadoop goes far beyond a simple MapReduce implementation (of which there 
are several); it's the key component of a data platform. It incorporatesHDFS, a 
distributed filesystem designed for the performance and reliability 
requirements of huge datasets; the HBase database;Hive, which lets developers 
explore Hadoop datasets using SQL-like queries; a high-level dataflow language 
calledPig; and other components. If anything can be called a one-stop 
information platform, Hadoop is it.</p> 
<p>Hadoop has been instrumental in enabling &quot;agile&quot; data analysis. 
In software development, &quot;agile practices&quot; are associated with faster 
product cycles, closer interaction between developers and consumers, and 
testing. Traditional data analysis has been hampered by extremely long 
turn-around times. If you start a calculation, it might not finish for hours, 
or even days. But Hadoop (and particularly Elastic MapReduce) make it easy to 
build clusters that can perform computations on long datasets quickly. Faster 
computations make it easier to test different assumptions, different datasets, 
and different algorithms. It's easer to consult with clients to figure out 
whether you're asking the right questions, and it's possible to pursue 
intriguing possibilities that you'd otherwise have to drop for lack of time.</p>
<p>Hadoop is essentially a batch system, but Hadoop Online Prototype (HOP) is 
an experimental project that enables stream processing. Hadoop processes data 
as it arrives, and delivers intermediate results in (near) real-time. Near 
real-time data analysis enables features liketrending topics on sites like 
Twitter. These features only require soft real-time; reports on trending topics 
don't require millisecond accuracy. As with the number of followers on Twitter, 
a &quot;trending topics&quot; report only needs to be current to within five 
minutes -- or even an hour. According to Hilary Mason (@hmason), data scientist 
atbit.ly, it's possible to precompute much of the calculation, then use one of 
the experiments in real-time MapReduce to get presentable results.</p> 
<p>Machine learning is another essential tool for the data scientist. We now 
expect web and mobile applications to incorporate recommendation engines, and 
building a recommendation engine is a quintessential artificial intelligence 
problem. You don't have to look at many modern web applications to see 
classification, error detection, image matching (behindGoogle Goggles and 
SnapTell) and even face detection -- an ill-advised mobile application lets you 
take someone's picture with a cell phone, and look up that person's identity 
using photos available online.Andrew Ng's Machine Learning course is one of the 
most popular courses in computer science at Stanford, with hundreds of students 
(this video is highly recommended). </p> 
<p>There are many libraries available for machine learning: PyBrain in Python, 
Elefant, Weka in Java, and Mahout (coupled to Hadoop). Google has just 
announced theirPrediction API, which exposes their machine learning algorithms 
for public use via a RESTful interface. For computer vision, theOpenCV library 
is a de-facto standard.</p> 
<p>Mechanical Turk is also an important part of the toolbox. Machine learning 
almost always requires a &quot;training set,&quot; or a significant body of 
known data with which to develop and tune the application. The Turk is an 
excellent way to develop training sets. Once you've collected your training 
data (perhaps a large collection of public photos from Twitter), you can have 
humans classify them inexpensively -- possibly sorting them into categories, 
possibly drawing circles around faces, cars, or whatever interests you. It's an 
excellent way to classify a few thousand data points at a cost of a few cents 
each. Even a relatively large job only costs a few hundred dollars.</p> 
<p>While I haven't stressed traditional statistics, building statistical 
models plays an important role in any data analysis. According toMike Driscoll (
@dataspora), statistics is the &quot;grammar of data science.&quot; It is 
crucial to &quot;making data speak coherently.&quot; We've all heard the joke 
that eating pickles causes death, because everyone who dies has eaten pickles. 
That joke doesn't work if you understand what correlation means. More to the 
point, it's easy to notice that one advertisement for<em>R in a Nutshell</em> 
generated 2 percent more conversions than another. But it takes statistics to 
know whether this difference is significant, or just a random fluctuation. Data 
science isn't just about the existence of data, or making guesses about what 
that data might mean; it's about testing hypotheses and making sure that the 
conclusions you're drawing from the data are valid. Statistics plays a role in 
everything from traditional business intelligence (BI) to understanding how 
Google's ad auctions work. Statistics has become a basic skill. It isn't 
superseded by newer techniques from machine learning and other disciplines; it 
complements them.</p> 
<p>While there are many commercial statistical packages, the open source R 
language -- and its comprehensive package library, CRAN -- is an essential 
tool. Although R is an odd and quirky language, particularly to someone with a 
background in computer science, it comes close to providing &quot;one stop 
shopping&quot; for most statistical work. It has excellent graphics facilities; 
CRAN includes parsers for many kinds of data; and newer extensions extend R 
into distributed computing. If there's a single tool that provides an 
end-to-end solution for statistics work, R is it.</p> 
<p></p> 
<h2>Making data tell its story</h2> 
<p></p> 
<p>A picture may or may not be worth a thousand words, but a picture is 
certainly worth a thousand numbers. The problem with most data analysis 
algorithms is that they generate a set of numbers. To understand what the 
numbers mean, the stories they are really telling, you need to generate a 
graph. Edward Tufte'sVisual Display of Quantitative Information is the classic 
for data visualization, and a foundational text for anyone practicing data 
science. But that's not really what concerns us here. Visualization is crucial 
to each stage of the data scientist. According to Martin Wattenberg (@wattenberg
, founder ofFlowing Media), visualization is key to data conditioning: if you 
want to find out just how bad your data is, try plotting it. Visualization is 
also frequently the first step in analysis. Hilary Mason says that when she 
gets a new data set, she starts by making a dozen or more scatter plots, trying 
to get a sense of what might be interesting. Once you've gotten some hints at 
what the data might be saying, you can follow it up with more detailed analysis.
</p> 
<p>There are many packages for plotting and presenting data. GnuPlot is very 
effective; R incorporates a fairly comprehensive graphics package; Casey Reas' 
and Ben Fry'sProcessing is the state of the art, particularly if you need to 
create animations that show how things change over time. At IBM'sMany Eyes, 
many of the visualizations are full-fledged interactive applications.</p> 
<p>Nathan Yau's FlowingData blog is a great place to look for creative 
visualizations. One of my favorites is this animation of thegrowth of Walmart 
over time. And this is one place where &quot;art&quot; comes in: not just the 
aesthetics of the visualization itself, but how you understand it. Does it look 
like the spread of cancer throughout a body? Or the spread of a flu virus 
through a population? Making data tell its story isn't just a matter of 
presenting results; it involves making connections, then going back to other 
data sources to verify them. Does a successful retail chain spread like an 
epidemic, and if so, does that give us new insights into how economies work? 
That's not a question we could even have asked a few years ago. There was 
insufficient computing power, the data was all locked up in proprietary 
sources, and the tools for working with the data were insufficient. It's the 
kind of question we now ask routinely.</p> 
<p></p> 
<h2>Data scientists</h2> 
<p></p> 
<p>Data science requires skills ranging from traditional computer science to 
mathematics to art. Describing the data science group he put together at 
Facebook (possibly the first data science group at a consumer-oriented web 
property), Jeff Hammerbacher said:</p> 
<blockquote> 
<p> ... on any given day, a team member could author a multistage processing 
pipeline in Python, design a hypothesis test, perform a regression analysis 
over data samples with R, design and implement an algorithm for some 
data-intensive product or service in Hadoop, or communicate the results of our 
analyses to other members of the organization3 </p> </blockquote> 
<p>Where do you find the people this versatile? According to DJ Patil, chief 
scientist atLinkedIn (@dpatil), the best data scientists tend to be &quot;hard 
scientists,&quot; particularly physicists, rather than computer science majors. 
Physicists have a strong mathematical background, computing skills, and come 
from a discipline in which survival depends on getting the most from the data. 
They have to think about the big picture, the big problem. When you've just 
spent a lot of grant money generating data, you can't just throw the data out 
if it isn't as clean as you'd like. You have to make it tell its story. You 
need some creativity for when the story the data is telling isn't what you 
think it's telling.</p> 
<p>Scientists also know how to break large problems up into smaller problems. 
Patil described the process of creating the group recommendation feature at 
LinkedIn. It would have been easy to turn this into a high-ceremony development 
project that would take thousands of hours of developer time, plus thousands of 
hours of computing time to do massive correlations across LinkedIn's 
membership. But the process worked quite differently: it started out with a 
relatively small, simple program that looked at members' profiles and made 
recommendations accordingly. Asking things like, did you go to Cornell? Then 
you might like to join the Cornell Alumni group. It then branched out 
incrementally. In addition to looking at profiles, LinkedIn's data scientists 
started looking at events that members attended. Then at books members had in 
their libraries. The result was a valuable data product that analyzed a huge 
database -- but it was never conceived as such. It started small, and added 
value iteratively. It was an agile, flexible process that built toward its goal 
incrementally, rather than tackling a huge mountain of data all at once.</p> 
<p>This is the heart of what Patil calls &quot;data jiujitsu&quot; -- using 
smaller auxiliary problems to solve a large, difficult problem that appears 
intractable. CDDB is a great example of data jiujitsu: identifying music by 
analyzing an audio stream directly is a very difficult problem (though not 
unsolvable -- seemidomi, for example). But the CDDB staff used data creatively 
to solve a much more tractable problem that gave them the same result. 
Computing a signature based on track lengths, and then looking up that 
signature in a database, is trivially simple.</p> 
<p>Hiring trends for data science</p> 
<p></p> 
<p>It's not easy to get a handle on jobs in data science. However, data from 
O'Reilly Research shows a steady year-over-year increase in Hadoop and 
Cassandra job listings, which are good proxies for the &quot;data science&quot; 
market as a whole. This graph shows the increase in Cassandra jobs, and the 
companies listing Cassandra positions, over time.</p> 
<p>Entrepreneurship is another piece of the puzzle. Patil's first flippant 
answer to &quot;what kind of person are you looking for when you hire a data 
scientist?&quot; was &quot;someone you would start a company with.&quot; That's 
an important insight: we're entering the era of products that are built on 
data. We don't yet know what those products are, but we do know that the 
winners will be the people, and the companies, that find those products. Hilary 
Mason came to the same conclusion. Her job as scientist at bit.ly is really to 
investigate the data that bit.ly is generating, and find out how to build 
interesting products from it. No one in the nascent data industry is trying to 
build the 2012 Nissan Stanza or Office 2015; they're all trying to find new 
products. In addition to being physicists, mathematicians, programmers, and 
artists, they're entrepreneurs.</p> 
<p>Data scientists combine entrepreneurship with patience, the willingness to 
build data products incrementally, the ability to explore, and the ability to 
iterate over a solution. They are inherently interdiscplinary. They can tackle 
all aspects of a problem, from initial data collection and data conditioning to 
drawing conclusions. They can think outside the box to come up with new ways to 
view the problem, or to work with very broadly defined problems: &quot;here's a 
lot of data, what can you make from it?&quot;</p> 
<p>The future belongs to the companies who figure out how to collect and use 
data successfully. Google, Amazon, Facebook, and LinkedIn have all tapped into 
their datastreams and made that the core of their success. They were the 
vanguard, but newer companies like bit.ly are following their path. Whether 
it's mining your personal biology, building maps from the shared experience of 
millions of travellers, or studying the URLs that people pass to others, the 
next generation of successful businesses will be built around data.The part of 
Hal Varian's quote that nobody remembers says it all: </p> 
<blockquote> 
<p><strong>The ability to take data -- to be able to understand it, to process 
it, to extract value from it, to visualize it, to communicate it -- that's 
going to be a hugely important skill in the next decades.</strong></p> 
</blockquote> 
<p>Data is indeed the new Intel Inside. </p> <br>
<br>

<p><strong>O'Reilly publications related to data science</strong></p> 
<p>R in a Nutshell<br>
 A quick and practical reference to learn what is 
becoming the standard for developing statistical software.</p> 
<p>Statistics in a Nutshell<br>
 An introduction and reference for anyone with 
no previous background in statistics.</p> 
<p>Data Analysis with Open Source Tools<br>
 This book shows you how to think 
about data and the results you want to achieve with it.</p> 
<p>Programming Collective Intelligence<br>
 Learn how to build web 
applications that mine the data created by people on the Internet.</p> 
<p>Beautiful Data<br>
 Learn from the best data practitioners in the field 
about how wide-ranging -- and beautiful -- working with data can be.</p> 
<p>Beautiful Visualization<br>
 This book demonstrates why visualizations are 
beautiful not only for their aesthetic design, but also for elegant layers of 
detail.</p> 
<p>Head First Statistics<br>
 This book teaches statistics through puzzles, 
stories, visual aids, and real-world examples.</p> 
<p>Head First Data Analysis<br>
 Learn how to collect your data, sort the 
distractions from the truth, and find meaningful patterns.</p> <br>
<br>

<p>1 The NASA article denies this, but also says that in 1984, they decided 
that the low values (whch went back to the 70s) were &quot;real.&quot; Whether 
humans or software decided to ignore anomalous data, it appears that data was 
ignored.</p> 
<p>2  &quot;Information Platforms as Dataspaces,&quot; by Jeff Hammerbacher (in
<em>Beautiful Data</em>)</p> 
<p>3  &quot;Information Platforms as Dataspaces,&quot; by Jeff Hammerbacher (in
<em>Beautiful Data</em>)</p> <br>

<p>tags:&nbsp;data, data science, data scientist</p> 
<p> Tweet </p> <br>
<br>

<p>Comments: 52</p> 
<p> Tim O'Reilly [ 2 June 2010 12:25 PM] </p> 
<p> </p> 
<p>This is a really important and articulate post, Mike. I wish I'd written it.
</p> 
<p>I'm surprised that there are no comments, but 123 retweets already. I guess 
retweets are the new comments.<br>
</p> 
<p></p> 
<p> joe [ 2 June 2010 12:33 PM] </p> 
<p> </p> 
<p>Good post, definitely where things are going. I just wrote about this and 
related topics over the long weekend.
http://www.joeandmotorboat.com/2010/05/31/beyond-bigdata/</p> 
<p></p> 
<p> Robert Richards [ 2 June 2010 01:10 PM] </p> 
<p> </p> 
<p>Thanks for this post. It might be useful to note that data science has been 
recognized as a discipline since at least 2001; that one of the first scholars 
to use &quot;data science&quot; in the sense used in this post was Dr. William 
S. Cleveland in his 2001 article, &quot;Data Science: An Action Plan for 
Expanding the Technical Areas of the Field of Statistics,&quot;
http://j.mp/a8mQeP ; that people working in this discipline have organized 
professional associations, including CODATA, the International Council for 
Science: Committee on Data for Science and Technologyhttp://www.codata.org/ ; 
that there are conferences devoted to this discipline, including the 
International CODATA Conferencehttp://www.codata2010.com/ ; and that there are 
journals that publish data science research, including Journal of Data Science
http://www.jds-online.com/ (founded in 2003), and Data Science Journal 
http://j.mp/9YYh5r (founded in 2002).</p> 
<p></p> 
<p> Michael F. Martin [ 2 June 2010 01:33 PM] </p> 
<p> </p> 
<p>Accountants are a kind of data scientist too.</p> 
<p></p> 
<p> Andrew Walkingshaw [ 2 June 2010 02:42 PM] </p> 
<p> </p> 
<p>As one of the three founders of Timetric (http://timetric.com), all of whom 
have a background in the physical sciences, I'm biased: but we couldn't agree 
more.</p> 
<p></p> 
<p> Alex Tolley [ 2 June 2010 07:21 PM] </p> 
<p> </p> 
<p>Good article. I do question the term data 'science', as in &quot;Data 
science enables the creation of data products.&quot;. Science is a process for 
discovering truth (or at least discarding falsity). Extracting information from 
data is more like cooking, an art.</p> 
<p>I also am a bit skeptical about the fetish-izing of 'big data'. Certainly 
sometimes data sets are necessarily large, but arguably better data, acquired 
through better questions, is preferable. A second best can be data sampling to 
reduce the analyzable data set to more tractable proportions. Mining large data 
sets to death is very similar to &quot;spreadsheet-itis&quot; that spread 
quickly in te 1980's once spreadsheets became widely available. Nothing wrong 
with using spreadsheets, except that they tended to focus attention on the 
questions where data that was available, rather than the questions that really 
needed answers.</p> 
<p></p> 
<p> Brandyn [ 2 June 2010 08:06 PM] </p> 
<p> </p> 
<p>&quot;In hindsight, MapReduce seems like an obvious solution to Google's 
biggest problem, creating large searches. It's easy to distribute a search 
across thousands of processors, and then combine the results into a single set 
of answers.&quot;</p> 
<p>You wouldn't use MapReduce like this in a low-latency query situation. It 
is made for high throughput batch processing, not quick turn around.</p> 
<p></p> 
<p> Joel [ 2 June 2010 09:25 PM] </p> 
<p> </p> 
<p>...and still, to date, the best tool to mess around with large data sets is 
SAS - been around for ages. Seems that as the web matures, so too does the tool 
sets and methodologies it uses to extract value.</p> 
<p></p> 
<p> Rukmal Fernando [ 2 June 2010 10:25 PM] </p> 
<p> </p> 
<p>Great read! On the subject of visualization, Stanford's Protovis is also 
probably worthy of considerationhttp://vis.stanford.edu/protovis/</p> 
<p>I've only started trying it out, but Protovis is just Javascript and SVG, 
and that's a big plus for me.</p> 
<p></p> 
<p> Kirk [ 2 June 2010 11:02 PM] </p> 
<p> </p> 
<p>Thanks Mike, &quot;terabyte drives are consumer equipment&quot; are the 
cord wood to my campfire. I want to know more about how video data is being 
analyzed. Does Google's new machine transcription service change the game?</p> 
<p></p> 
<p> Brian Ahier [ 2 June 2010 11:08 PM] </p> 
<p> </p> 
<p>This is the best post on data I have ever read. I am stunned...</p> 
<p></p> 
<p> Ken [ 2 June 2010 11:16 PM] </p> 
<p> </p> 
<p>Aw heck, I hate to say this Mike, but &quot;data are&quot;, not &quot;data 
is&quot;. But despite my nitpicking, this a good article.</p> 
<p></p> 
<p> Igor [ 3 June 2010 01:14 AM] </p> 
<p> </p> 
<p>Great article!</p> 
<p>Please, consider to include http://MyTaskHelper.com in your review.</p> 
<p></p> 
<p> Brand Niemann [ 3 June 2010 03:38 AM] </p> 
<p> </p> 
<p>I think I have always been a &quot;Data Scientist&quot; as coined by Jeff 
Hammerbacher in Facebook 2007, combining Data Analyst &amp; Research Scientist:
<br> http://twitter.com/bniemannsr/status/15207340783<br>
 Please see my Data 
Science Library in the Cloud:<br>

http://ondemand.spotfire.com/public/library.aspx?folder=Users/USEEVL-4372/public
</p> 
<p></p> 
<p> Juan Moya [ 3 June 2010 07:54 AM] </p> 
<p> </p> 
<p>Thanks Mike.</p> 
<p>One of the best articles I've ever read.</p> 
<p>Data are our future.</p> 
<p>jf</p> 
<p></p> 
<p> Robert Young [ 3 June 2010 08:03 AM] </p> 
<p> </p> 
<p>First (or most recently, at least) yall made up Web 2.0. Now you make up 
Data Science.</p> 
<p>If you want to create new information from existing, read Codd. That's what 
relations do.</p> 
<p>As to R, yes, it is useful, but SAS still rules the job listings.</p> 
<p>The explosion of text data (distinct from audio, video, etc.) is wholly due 
to Kiddies such as Google who didn't want to build BCNF datastores because 
they're (the Kiddies, not the data) just too dense. So they build, using xml to 
compound the insult, massively redundant flat files, which in turn demand vast 
amounts of HDD and CPU to process. And they (and you) pat them on the back as 
being oh so clever. Yikes.</p> 
<p>The errors in the R and Stat books are such to make it obvious your authors 
(you, too) have an understanding of inferential statistics (the only variety of 
any value) about as deep as the ISO-9000 and Six Sigma folks. No, that isn't an 
AttaBoy.</p> 
<p></p> 
<p> Michael R. Bernstein [ 3 June 2010 08:11 AM] </p> 
<p> </p> 
<p>&quot;The future belongs to the companies who figure out how to collect and 
use data successfully. Google, Amazon, Facebook, and LinkedIn have all tapped 
into their datastreams and made that the core of their success.&quot;</p> 
<p>Also important is the ability to figure out how to tap into existing 
sources of data, rather than collecting your own.</p> 
<p></p> 
<p> Eric Christensen [ 3 June 2010 07:23 PM] </p> 
<p> </p> 
<p>Data is cool. Data Science is cool. Put out the feelers and collect more 
data (figure out what to do with it later).</p> 
<p></p> 
<p> Muhammad Mudssar [ 3 June 2010 10:45 PM] </p> 
<p> </p> 
<p>Good article about data and its its importance specially importance of 
massive unstructured data. The point is how one can get benefit from 
unstructured massive amount of data. I think by using MapReduce one can find 
out the hidden patterns in unstructured data.<br>
 I think data along with 
better processing techniques is the future.</p> 
<p></p> 
<p> marc [ 4 June 2010 03:54 AM] </p> 
<p> </p> 
<p>Your graph of job listings vs time displays no indication of its vertical 
scale. Does the maximum vertical point on the graph represent 3 listings or 
300,000? In an article on the importance of data, that leaves me wondering how 
much of the content is hype and how much is well thought out.</p> 
<p></p> 
<p> Alan Howlett aka @Technontologist [ 4 June 2010 07:19 AM] </p> 
<p> </p> 
<p>I see lots of commonality with Richard Saul Wurman's definition of 
Information Architect[ure] (not the co-opted Web developer version), 
&quot;&quot;information architect. 1) the individual who organizes the patterns 
inherent in data, making the complex clear. 2) a person who creates the 
structure or map of information which allows others to find their personal 
paths to knowledge. 3) the emerging 21st century professional occupation 
addressing the needs of the age focused upon clarity, human understanding, and 
the science of the organization of information.&quot;</p> 
<p>@Technontologist</p> 
<p></p> 
<p> Jorn Bettin [ 4 June 2010 03:16 PM] </p> 
<p> </p> 
<p>Very nice article. However there is still a long way to go for data 
products. The value of data analysis and representation critically depends on 
access to trustworthy source data. Quantification of trustworthiness is one of 
the harder problems.</p> 
<p>For example, the entire financial system hinges on trust, and the level of 
trust in financial instruments is not known to be particularly stable. 
Manufacturing and publicising statements that are designed to influence the 
level of trust in financial instruments is an entire industry in its own right.
</p> 
<p>The article would be even better if it didn't contain simple calculation 
errors like &quot;RAM has moved from $1,000/MB to roughly $25/GB -- a price 
reduction of about 4000&quot;. Errors like this one have a direct impact on 
trustworthiness.</p> 
<p></p> 
<p> Brian Shaler [ 4 June 2010 04:40 PM] </p> 
<p> </p> 
<p>I'm going to cite the hell out of this article. While a little 
scatterbrained, it covers an amazingly broad array of subjects within the realm 
of data science. The number of references &amp; links to languages, 
platforms/frameworks, tools, and projects was very impressive! Thanks!</p> 
<p>While all these hip new projects have been cropping up during the last five 
years, we're still at the beginning of this conversation. I'm looking forward 
to seeing where the industry goes in the next five years and hope to play a 
part!</p> 
<p></p> 
<p> Brian Shaler [ 4 June 2010 04:44 PM] </p> 
<p> </p> 
<p>I want to clarify on my last comment, when I said &quot;all these&quot; I 
was not referring to all data-related projects in general. I'm aware that tools 
have been available to manipulate data for decades. I probably should have 
phrased it more like, &quot;With all these hip new projects cropping up 
[...]&quot;</p> 
<p></p> 
<p> Jessie Wells [ 4 June 2010 08:04 PM] </p> 
<p> </p> 
<p>Hi Mike, <br>
 thanks for a really interesting article.</p> 
<p>I think you mean that NASA's data screening delayed discovery of Ozone 
Depletion (as your links discuss), not global warming.<br>
<br>
 The two 
physical phenomena are often confused, but please don't further this problem!
</p> 
<p>They share the fact that their principal drivers are forms of pollution 
from human activities, but they are not the same in their impacts or the 
actions needed to mitigate them.</p> 
<p>Recently a meteorological link was discovered whereby a the increased UVB 
radiation due to the depleted ozone layer does affect air circulation patterns 
in the Antarctic, but this is basically a reflection of the fact that almost 
any aspects of the earth system if you look hard enough.</p> 
<p></p> 
<p> Mike Loukides [ 5 June 2010 05:37 AM] </p> 
<p> </p> 
<p>I'm glad you liked the article. I've just fixed the reference to the Ozone 
Layer. Thanks for pointing that out.</p> 
<p></p> 
<p> Mike Loukides [ 5 June 2010 06:17 AM] </p> 
<p> </p> 
<p>Ouch. That arithmetic error really hurts. Thanks for pointing it out--it's 
been corrected.</p> 
<p>The problem of trust is interesting. It depends to a large extent on the 
nature of the product: we demand a different level of trust for financial 
products (and that trust has obviously been shaken badly over the past two 
years) than we do for Facebook or LinkedIn recommendations. And you're right, 
in finance there's a marketing industry that's designed to manufacture trust 
entirely aside from the quality of the data or the analysis.</p> 
<p></p> 
<p> Megan [ 5 June 2010 01:33 PM] </p> 
<p> </p> 
<p>Kudos for encouraging thoughtfulness about data, and interdisciplinary 
approaches to data analysis problems. However, I must make two points:</p> 
<p>1) actuarial science != Statistics</p> 
<p>2) For those commenting that this is the best article they've read about 
data, please allow me to introduce you to W. Edwards Deming 
(http://en.wikipedia.org/wiki/W._Edwards_Deming). Statisticians have been 
thinking hard about data for many, many years. Let's not discard them with a 
flippant comment about actuaries and the false belief that anyone can run a few 
programs in R and correctly analyze data.</p> 
<p></p> 
<p> G. Boyd [ 6 June 2010 09:20 PM] </p> 
<p> </p> 
<p>Data Science exists in order to usurp an individual's OODA loop without 
their knowledge, as described by former Air Force Pilot John Boyd. Of course 
data science is described as a sexy gig when you're bosses are control freaks.
</p> 
<p>It doesn't matter whether we get the image of the golden ring, or the 
network centric system, as both images represent identical ends.<br>
</p> 
<p></p> 
<p> Ben Hoyle [ 7 June 2010 03:32 AM] </p> 
<p> </p> 
<p>Great article.</p> 
<p>In many ways &quot;data science&quot; and &quot;information 
engineering&quot; are synonymous; both deal with a large amount of unstructured 
data and form conclusions, inferences and predictions. Both can also be thought 
of as overlapping with AI and brain science; what does the brain (human, fly 
etc.) do if not process lots of (sensory) data extremely cleverly.</p> 
<p>We are reaching a stage where the great limiting factor to significant 
progress is our intelligent processing systems, we are beginning to have the 
data but we don't yet have suitably developed systems to intelligently process 
that data (both in a human and non-human manner).</p> 
<p></p> 
<p> G. Boyd [ 7 June 2010 10:42 AM] </p> 
<p> </p> 
<p>@Ben Hoyle,</p> 
<p>First, information engineering is a means to an end, this end which can be 
referred to as &quot;social engineering&quot;, or the designing of society and 
its participants. Usurp ones &quot;Observe&quot; and &quot;Orient&quot; 
functions in their closed-loop decision processes, and you can then predict an 
individual's &quot;Decide&quot; and &quot;Act&quot; part of the loop (as 
described by John Boyd).</p> 
<p>Second, of course we have, or will soon have, the algorithms available to 
process this data. What do you think &quot;intent harvesting&quot; and 
&quot;intent generation&quot; is about? This is Facebook's play in the display 
advertising game, with their launch of the &quot;I Like&quot; feature. We're 
talking about generating individual profiles on the 'feedback' side of the loop 
such that 'controls' can be implemented on other half of the loop.</p> 
<p>Loss of privacy and the simultaneous rise of data engineering is no 
accident. All these moves are driven by the goal of usurping ones OODA loop. Do 
that across enough people in the public domain, and guess what you have.</p> 
<p>Now, why is O'Reilly et all not discussing this critical aspect of the 
technologies and strategies that are being pushed on to the tech community for 
deployment?</p> 
<p>Tell us it's not relevant doesn't cut it.</p> 
<p></p> 
<p> M S Prasad [ 9 June 2010 01:40 AM] </p> 
<p> </p> 
<p>Great knowledge filled post and good coverage of tehnology.</p> 
<p>Data science , as I understand has been a forte of statistics &amp; 
mathematicians for long . we converted it as a formidable tool and gave a face 
to understand it better by user's.( something like putting relativity theory in 
simple words e.g. LINUX for Dummies .</p> 
<p>Information Engineering is a process to extract the useful knowledge or 
result from a set of information which can be data and sometime intangibles 
also or perceived ones.<br>
 As far image data sets are concerned , being 
multidimensional has a total mathematical basis for its processing and result 
retrievals.</p> 
<p>just my thoughts.<br>
 i have posted this article on Cloud Secuirty 
alliance group in Linked in.<br>
 thanks<br>
</p> 
<p></p> 
<p> Torrance S [16 June 2010 08:59 AM] </p> 
<p> </p> 
<p>An incredibly well written article...</p> 
<p></p> 
<p> martin king [20 June 2010 10:46 AM] </p> 
<p> </p> 
<p>There are many aspects to science much of it consists of the daily grind 
with data and the tools of science.</p> 
<p>I think we need to be careful not to lose sight of the drivers of science 
and where much of science comes from - intuition and great ideas .. after which 
we create experiments to test hypotheses which inevitably involve data of some 
kind.</p> 
<p></p> 
<p> Jewel Ward [21 June 2010 12:53 PM] </p> 
<p> </p> 
<p>I would add to this list of aspects of Data Science the ability to 
determine the best way to migrate, store, &amp; archive data. As well as 
whether or not to keep it. Storage, is indeed, cheap, and getting cheaper. But 
given petabytes or yottabytes of data, if you aren't using 90% of what you are 
storing, does it make sense to keep paying to store, migrate, etc., the data 
(barring legal requirements to do so)? Think in terms of the larger scale of it 
-- the electricity required (both in generating the electricity itself &amp; 
paying for use), plus the machines that must be purchased, and the humans who 
must be paid to manage the data through its lifecycle. Cost-benefit analysis is 
an important component.</p> 
<p>Having said that, this is a great article.</p> 
<p></p> 
<p> Fred Beringer [28 June 2010 03:04 AM] </p> 
<p> </p> 
<p>Excellent article and a good summary of where data is headed. On the where 
data comes from, you don't mentions the OpenData movement I'm mentioning in my 
article about the Future of Data and Predictive Analytics 
(http://www.fredberinger.com/musings-on-the-future-of-data-and-predictive-analytics-3/). 
I hope this movement will play a big role in the future as there is so much 
benefits to leverage data from the public and private sectors.<br>
 Another 
source of data you briefly mentions are the millions of sensors we're going to 
see in our life in the next 5-10 years, with the so called Internet of Things. 
This is really getting real ! 
(http://www.fredberinger.com/is-the-internet-of-things-ready-for-prime-time/)
</p> 
<p>We've haven't seen anything yet !</p> 
<p>Fred </p> 
<p></p> 
<p> Leo Irakliotis [ 5 July 2010 08:22 PM] </p> 
<p> </p> 
<p>As I keel reading and reading the article, the academic inside me keeps 
repeating the same question: do we (academics) do a good, or even adequate job, 
at educating future data scientists? Should we consider creating data science 
programs? What can we do to offer better more relevant education to individuals 
who may be pursuing or using data science? And who can possibly advise us about 
the practical realities of data science?</p> 
<p></p> 
<p> Vassilis Nikolopoulos [21 July 2010 07:47 AM] </p> 
<p> </p> 
<p>Excellent article on new data management trends... traditional approaches 
to DB and storage / analytics have to be changed, in order to cope with this 
huge evolution on data and stream decision analysis...</p> 
<p>OLAP, BI and traditional Data mining have to be &quot;updated&quot; with 
new R&amp;D tips...</p> 
<p></p> 
<p> David Alan Christopher [22 August 2010 05:17 PM] </p> 
<p> </p> 
<p>Fascinating article Mike. I'm a librarian and our lot faces a handful of 
data as well. We've been writing some Java and Ruby code lately for in-house 
data analysis and even some basic segmentation of our huge inventory.</p> 
<p>We often need to pull content from external public websites (usually 
government and NPO's), which don't have any form of exportable data formats. 
We've started using a tool called Feedity -http://feedity.com for that purpose. 
So far it's worked out pretty well as we've been able to &quot;scrape&quot; raw 
data as structured information from hundreds of public sources.</p> 
<p>I bet this space will only grow as data becomes more and more important for 
organizations world-wide.</p> 
<p></p> 
<p> Ellen O'Neal [23 August 2010 08:41 AM] </p> 
<p> </p> 
<p>This stuff fascinates me! I love the examples of Facebook, LinkedIn and 
Amazon using patterns to suggest other relationships and cross sell. Another 
example that comes to mind is Pandora. A friend of mine wrote an article that I 
think may be interesting to fellow readers:</p> 
<p>
http://info.livelogic.net/customer-intelligence-project-success/bid/47060/Pandora-s-Jar-and-the-Art-of-the-Cross-Sell
</p> 
<p>Thanks for the post. I'm excited to see the future of Predictive Analytics, 
as my company calls it.</p> 
<p></p> 
<p> Casey Cheshire [ 2 September 2010 06:40 AM] </p> 
<p> </p> 
<p>Amazing! I'm looking at the future, thank you for teaching and sharing!</p> 
<p></p> 
<p> Tim H. [23 October 2010 03:08 PM] </p> 
<p> </p> 
<p>Great article. How does a young guy with strong communications skills 
experience in statistics, computing, math, and data get into this field? 
Suggestions?</p> 
<p></p> 
<p> Mike Loukides [28 October 2010 11:14 AM] </p> 
<p> </p> 
<p>There's been a good discussion on Quora on how to become a data scientist: 
http://www.quora.com/How-do-I-become-a-data-scientist With the skills you list, 
it sounds like you're a good part of the way there already...</p> 
<p>Mike</p> 
<p></p> 
<p> Steve Ardire [15 November 2010 06:53 PM] </p> 
<p> </p> 
<p>Great post !</p> 
<p>Yes the web is full of data-driven apps but semantic data-driven apps is 
the future.</p> 
<p></p> 
<p> vector [ 4 December 2010 10:46 AM] </p> 
<p> </p> 
<p>Thanks for the good and quite useful article. This is really an issue </p> 
<p></p> 
<p> Booten Gurg [ 5 December 2010 11:27 AM] </p> 
<p> </p> 
<p>Does Wikileaks apply here?</p> 
<p>Anyway, this is Government 2.0 at work !</p> 
<p>http://www.nytimes.com/2010/12/05/world/05restrict.html?hp</p> 
<p></p> 
<p> Larry (IEOR Tools) [23 December 2010 05:50 AM] </p> 
<p> </p> 
<p>This is a great article about how getting information from data is going to 
be so important in the future. I've been writing about this for years. I love 
how &quot;data science&quot; is really taking on a life of its own now.</p> 
<p></p> 
<p> Max J. Pucher - Chief Architect ISIS Papyrus Software [20 February 2011 
12:05 AM] </p> 
<p> </p> 
<p>Mike, great research on a relevant subject. </p> 
<p>let me however add that much of the data that is collected is irrelevant. 
CDDB is a great success because their data model is simple and the data are 
really very limited, a few million rows. Filtering the relevant stuff is not 
easy and mostly an intuitive human skill as long as the model can be grasped.
</p> 
<p>Statistical processing does however not replicate reality, but is always 
built on a model illusion. Don't forget that all models are wrong but some are 
useful (Einstein). Further it depends how the data is collected: how it is 
timed, how accurate it is, how it is filtered, which questions were asked or 
points that were measured, and a few more elements that rely on model 
assumption. Statistics further only produce correlation but not causation.</p> 
<p>Humans tend to misinterpret many effects as causes because the effect can 
be seen while THE CAUSE doesn't exist as a singular event but is a complex web 
if feeding stimuli that need a complex web of receptive context to actually 
make something happen. I.e. bringing a pot of water to boil is not caused by 
switching on the stove. Think of all the other causal elements that are needed 
including air pressure and saline content.</p> 
<p>All human activity cannot be causally interpreted because social activity 
must be seen as complex adaptive systems that undergo continuous change that is 
hardly ever reflected in the data model and processing. Which is why managing a 
business with BPM workflows and predictive analytics is for idiots who don't 
understand nature and science. They look for predictability where there is none.
</p> 
<p>The models of global warming and climate change are complete rubbish and so 
will be many of the data models that try to make sense of all the random data 
collected. Yes, some of it will be useful as an AHA moment, but it certainly 
won't predict the future or allow us to control things better. And if that 
isn't possible, why bother.</p> 
<p>I won't even go into the already mentioned issues of privacy and misuse of 
the collected data by governments and big business.</p> 
<p></p> 
<p> Attorney [22 March 2011 03:38 AM] </p> 
<p> </p> 
<p>While data science is incredibly beneficial and helping to improve the 
standard of living of humanity, I'm not so sure that I go so far as to agree 
with Hal Varian that statistics is the next sexy job. That's a bit of a 
stretch! Important? Absolutely. Sexy? Well....</p> 
<p>Great post by the way.</p> 
<p>Jim</p> 
<p></p> 
<p> Revitol [31 May 2011 03:05 PM] </p> 
<p> </p> 
<p>A very interesting and thorough article! I do feel a little uneasy, though, 
reading about the ability to extract all the types of information from raw 
data. It looks like the big companies such as Google may know a little bit too 
much about us...</p> 
<p></p> 
<p> Rissy [20 June 2011 12:45 AM] </p> 
<p> </p> 
<p>Very interesting post..I get a lot of information and insights here..I just 
stumbled here in your blog and thanks a lot for this great post.:D</p> 
<p></p> 
<p> Ahmet Ardal [29 June 2011 03:57 AM] </p> 
<p> </p> 
<p>Thanks for this excellent article. It really encouraged and motivated me a 
lot as a newbie data scientist.</p> 
<p></p> <br>
<br>

<p>Most Recently Discussed</p> 
<ul> 
<li>There's a map for that</li> 
<li>Dating with data</li> 
<li>Got an iPhone or 3G iPad? Apple is recording your moves</li> 
<li>Why the fuss about iBooks Author?</li> 
<li>Survey results: How businesses are adopting and dealing with data</li> 
</ul> 
<p>Related Topics</p> 
<ul> 
<li>Data</li> </ul> 
<p>Related Events</p> 
<ul> 
<li> 
<h3>Strata Conference 2012</h3>
<p>Strata 2012, February 28-March 1, Santa Clara, Calif. Save 20% with code 
RADAR20</p> </li> </ul> <br>

<p>Archives</p> 
<ul> 
<li> Archives by Month... January 2012 December 2011 November 2011 October 2011
September 2011 August 2011 July 2011 June 2011 May 2011 April 2011 March 2011 
February 2011 January 2011 December 2010 November 2010 October 2010 September 
2010 August 2010 July 2010 June 2010 May 2010 April 2010 March 2010 February 
2010 January 2010 December 2009 November 2009 October 2009 September 2009 
August 2009 July 2009 June 2009 May 2009 April 2009 March 2009 February 2009 
January 2009 December 2008 November 2008 October 2008 September 2008 August 2008
July 2008 June 2008 May 2008 April 2008 March 2008 February 2008 January 2008 
December 2007 November 2007 October 2007 September 2007 August 2007 July 2007 
June 2007 May 2007 April 2007 March 2007 February 2007 January 2007 December 
2006 November 2006 October 2006 September 2006 August 2006 July 2006 June 2006 
May 2006 April 2006 March 2006 February 2006 January 2006 December 2005 
November 2005 October 2005 September 2005 August 2005 July 2005 June 2005 May 
2005 April 2005 March 2005 </li> 
<li> Archives by Topic... Data Gov 2.0 Mobile Programming Publishing Web 2.0 
Web Ops &amp; Performance </li> 
<li> Archives by Author... Alasdair Allan Adam Messinger Adam DuVander Adam 
Witwer Alexander Macgillivray Alex Bowyer Alex Howard Alex Iskold Alistair Croll
Allen Noren Allison Randal Anant Jhingran Andrew Savikas Andrew Odewahn Andrew 
Shafer Andy Kirk Andy Oram Ari Gesher Artur Bergman Audrey Watters Brian Ahier 
Barry Devlin Ben Lorica Bill McCoy Brady Forrest Brett McLaughlin Brett Sandusky
Brett Sheppard Brian Boyer Brian O'Leary Bruce Stewart Carl Hewitt Carl Malamud 
Chris Meade Ciara Byrne Cliff Miller Christine Perey Dale Dougherty Darren 
Barefoot Dave McClure David Sims David Leinweber David Recordon DC Denison Doug 
Hill DJ Patil Dylan Field E.A. Vander Veer Edd Dumbill Elizabeth Corcoran Eoin 
Purcell Eric Ries Francis Pedraza Fred Trotter Gavin Starks Gretchen Giles Greg 
Whisenant Gabe Zichermann Heather McCormack Howard Wen Hugh McGuire Imran Ali 
James Bridle James Turner Jason Grigsby Jeevan Padiyar Jeffrey Carr Jenn Webb 
Jennifer Pahlka Jesper Andersen Jesse Robbins John Graham-Cumming John Geraci 
Jimmy Guterman Jim Stogdill Jodee&Acirc;&nbsp;Rich Joseph Hellerstein Joe Wikert
John Allspaw John Battelle John Labovitz John Warren Jonathan Alexander Jono 
Bacon Jon Udell Joseph J. Esposito Joshua-Mich&eacute;le Ross Jon Spinney Jud 
Valeski Julie Steele Justin Hall Justo Hidalgo Kassia Krozser Kate Eltham Kate 
Pullinger Kat Meyer Keith Fahlgren Ken Yarmosh Kevin Shockey Kevin Smokler Karl 
Fogel Kurt Cagle Laura Dawson Leigh Dodds Linda Stone Liza Daly Lucy Gray Lukas 
Biewald Madhusudhan Konda Marc Hedlund Marc Goodman Marie Bjerede Mark Drapeau 
Mark Nelson Marko Gargenta Matthew Russell Matt Wood Melissa Morgan Matt Garrish
Mike Honda Michael Driscoll Michael Ferrari Michael Jon Jensen Mike Hendrickson 
Mike Loukides Mike Shatzkin Mark Sigal Mac Slocum Nat Torkington Nick Bilton 
Nick Farina Nick Ruffilo Nikolaj Nyholm O'Reilly Radar Osman Rashid Pablo 
Francisco Arrieta Gomez Pamela Samuelson Paul Spinrad Peter Meyers Peter 
Brantley Peter Bennett Pete Warden Quinn Norton Rael Dornfest Ramez Naam Raven 
Zachary Jonathan Reichental, Ph.D. Robbie Allen Roberta Cairney Robert 
Passarella Robert Kaye Rob Tucker Roger Magoulas Ryan Stewart Sanders Kleinfeld 
Sara Winge Sarah Milstein Sarah Novotny Sara Peyton Scott Ruthfield 
S&eacute;bastien Pierre Silona Bonewald Simon Wardley Simon St. Laurent Steve 
Souders Suzanne Axtell Tim Anderson Tara Hunt Terry Jones Tim O'Reilly Timothy 
M. O'Brien Tish Shute Todd Sattersten Tom Steinberg Tony Quartarolo Troy Topnik 
Tyler Bell Vanessa Fox </li> </ul> 
<p> </p> <br>
<br>
 Sign up today to receive special discounts, <br>
 product 
alerts, and news from O'Reilly. <br>
<br>
Privacy Policy &gt; <br>
View Sample 
Newsletter &gt; 
<ul> 
<li>YouTube</li> 
<li>Slideshare</li> 
<li>Facebook</li> 
<li>Twitter</li> 
<li>Google+</li> 
<li>RSS</li> 
<li>View All RSS Feeds &gt;</li> </ul> <br>
<br>

<p>&copy; 2012, O'Reilly Media, Inc.</p> 
<p>(707) 827-7019(800) 889-8969</p> 
<p>All trademarks and registered trademarks appearing on oreilly.com are the 
property of their respective owners.</p> 
<h3>About O'Reilly</h3> 
<ul> 
<li>Academic Solutions</li> 
<li>Jobs</li> 
<li>Contacts</li> 
<li>Corporate Information</li> 
<li>Press Room</li> 
<li>Privacy Policy</li> 
<li>Terms of Service</li> 
<li>Writing for O'Reilly</li> </ul> <br>

<h3>Community</h3> 
<ul> 
<li>Authors</li> 
<li>Community &amp; Featured Users</li> 
<li>Forums</li> 
<li>Membership</li> 
<li>Newsletters</li> 
<li>O'Reilly Answers</li> 
<li>RSS Feeds</li> 
<li>User Groups</li> </ul> <br>

<h3>More O'Reilly Sites</h3> 
<ul> 
<li>igniteshow.com</li> 
<li>makerfaire.com</li> 
<li>makezine.com</li> 
<li>craftzine.com</li> 
<li>labs.oreilly.com</li> 
<li>
<h2>Partner Sites</h2></li> 
<li>PayPal Developer Zone</li> 
<li>O'Reilly Insights on Forbes.com</li> </ul> <br>

<h3>Shop O'Reilly</h3> 
<ul> 
<li>Customer Service</li> 
<li>Contact Us</li> 
<li>Shipping Information</li> 
<li>Ordering &amp; Payment</li> 
<li>The O'Reilly Guarantee</li> </ul> <br>
<br>
<br>

</body>
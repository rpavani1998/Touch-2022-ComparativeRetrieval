<!doctype html>
<meta charset="utf-8">
<title></title>
<body>
Get the Flash Player to see this player. <br>

<h1>CSE 373/548 - Analysis of Algorithms</h1> 
<h2>Spring 1996</h2> <br>
 Steven Skiena <br>
 Department of Computer Science 
<br> SUNY Stony Brook 
<p> </p> 
<p> In Spring 1996, I taught my Analysis of Algorithms course via EngiNet, the 
SUNY Stony Brook distance learning program. Each of my lectures that semester 
was videotaped, and the tapes made available to off-site students. I found it 
an enjoyable experience.</p> 
<p> As an experiment in using the Internet for distance learning, we have 
digitized the complete audio of all 23 lectures, and have made this available 
on the WWW. We partitioned the full audio track into sound clips, each 
corresponding to one page of lecture notes, and linked them to the associated 
text and images.</p> 
<p> In a real sense, listening to all the audio is analogous to sitting 
through a one-semester college course on algorithms! Properly compressed, the 
full semester's audio requires less than 300 megabytes of storage, which is 
much less than I would have imagined. The<i>entire</i> semesters lectures, over 
thirty hours of audio files, fit comfortably onThe Algorithm Design Manual 
CD-ROM, which also includes a hypertext version of the book and a substantial 
amount of software.</p> <br>
Menu 
<ul> 
<li> The Stony Brook Algorithm Repository </li> 
<li> Audio files of this lectures </li> </ul> 
<ul> 
<li> Lecture 1 - analyzing algorithms </li> 
<li> Lecture 2 - asymptotic notation </li> 
<li> Lecture 3 - recurrence relations </li> 
<li> Lecture 4 - heapsort </li> 
<li> Lecture 5 - quicksort </li> 
<li> Lecture 6 - linear sorting </li> 
<li> Lecture 7 - elementary data structures </li> 
<li> Lecture 8 - binary trees </li> 
<li> Lecture 9 - catch up </li> 
<li> Lecture 10 - tree restructuring </li> 
<li> Lecture 11 - backtracking </li> 
<li> Lecture 12 - introduction to dynamic programming </li> 
<li> Lecture 13 - dynamic programming applications </li> 
<li> Lecture 14 - data structures for graphs </li> 
<li> Lecture 15 - DFS and BFS </li> 
<li> Lecture 16 - applications of DFS and BFS </li> 
<li> Lecture 17 - minimum spanning trees </li> 
<li> Lecture 18 - shortest path algorthms </li> 
<li> Lecture 19 - satisfiability </li> 
<li> Lecture 20 - integer programming </li> 
<li> Lecture 21 - vertex cover </li> 
<li> Lecture 22 - techniques for proving hardness </li> 
<li> Lecture 23 - approximation algorithms and Cook's theorem </li> </ul> <br>

<h1>Lecture 1 - analyzing algorithms</h1> 
<p> Listening To Part 1-7 </p> 
<p> </p> 
<p>Lecture Schedule</p> 
<p> </p> 
<p></p>  subject  topics  reading <br>
Preliminaries  Analyzing algorithms  
1-32 <br>
 &quot;  Asymptotic notation  32-37 <br>
&quot;  Recurrence relations 
 53-64 <br>
 Sorting  Heapsort  140-150 <br>
&quot;  Quicksort  153-167 <br>
 
&quot;  Linear Sorting  172-182 <br>
Searching  Data structures  200-215 <br>
 
&quot;  Binary search trees  244-245 <br>
&quot;  Red-Black trees:insertion  
262-272 <br>
 ``  Red-Black trees:deletion  272-277 <br>
MIDTERM 1 <br>
 Comb. 
Search  Backtracking <br>
&quot;  Elements of dynamic programming  301-314 <br>

 &quot;  Examples of dynamic programming  314-323 <br>
Graph Algorithms  Data 
structures  465-477 <br>
 for graphs <br>
 &quot;  Breadth/depth-first search  
477-483 <br>
&quot;  Topological Sort/Connectivity  485-493 <br>
 &quot;  
Minimum Spanning Trees  498-510 <br>
&quot;  Single-source shortest paths  
514-532 <br>
 &quot;  All-pairs shortest paths  550-563 <br>
MIDTERM 2 <br>
 
Intractability  P and NP  916-928 <br>
&quot;  NP-completeness  929-939 <br>
 
&quot;  NP-completeness proofs  939-951 <br>
 &quot;  Further reductions  
951-960 <br>
 &quot;  Approximation algorithms  964-974 <br>
&quot;  Set cover 
/ knapsack heuristics  974-983 <br>
FINAL EXAM <br>
<br>

<p></p> Listening To Part 1-8 
<p> </p> 
<p>What Is An Algorithm?</p> 
<p> Algorithms are the ideas behind computer programs. &nbsp; </p> 
<p> An algorithm is the thing which stays the same whether the program is in 
Pascal running on a Cray in New York or is in BASIC running on a Macintosh in 
Kathmandu!</p> 
<p> To be interesting, an algorithm has to solve a general, specified problem. 
An algorithmic problem is specified by describing the set of instances it must 
work on and what desired properties the output must have. &nbsp;</p> 
<p> </p> 
<p>Example: Sorting</p> Input: A sequence of N numbers 
<p> Output: the permutation (reordering) of the input sequence such as  . </p> 
<p> We seek algorithms which are <em>correct</em> and <em>efficient</em>. </p> 
<p> </p> 
<p>Correctness</p> For any algorithm, we must prove that it <em>always</em> 
returns the desired output for all legal instances of the problem. &nbsp;
<p> For sorting, this means even if (1) the input is already sorted, or (2) it 
contains repeated elements.</p> 
<p> Listening To Part 1-9 </p> 
<p> </p> 
<p>Correctness is Not Obvious!</p> 
<p> The following problem arises often in manufacturing and transportation 
testing applications.</p> 
<p> Suppose you have a robot arm equipped with a tool, say a soldering iron. 
To enable the robot arm to do a soldering job, we must construct an ordering of 
the contact points, so the robot visits (and solders) the first contact point, 
then visits the second point, third, and so forth until the job is done. 
&nbsp;&nbsp;</p> 
<p> Since robots are expensive, we need to find the order which minimizes the 
time (ie. travel distance) it takes to assemble the circuit board.</p> 
<p> </p> <br>
 You are given the job to program the robot arm. Give me an 
algorithm to find the best tour!
<p> Listening To Part 1-10 </p> 
<p> </p> 
<p>Nearest Neighbor Tour</p> 
<p> A very popular solution starts at some point  and then walks to its 
nearest neighbor first, then repeats from  , etc. until done. &nbsp; </p> 
<p> </p> 
<pre> 
<p> Pick and visit an initial point </p>
<p> </p>
<p> <i>i</i> = 0 </p>
<p> While there are still unvisited points </p>
<p> <i>i</i> = <i>i</i>+1 </p>
<p> Let  be the closest unvisited point to </p>
<p> Visit </p>
<p> Return to  from </p>
<p> </p></pre> 
<p> This algorithm is simple to understand and implement and very efficient. 
However, it is<b>not correct!</b> </p> 
<p> </p> <br>
<br>
 Always starting from the leftmost point or any other point 
will not fix the problem.
<p> Listening To Part 1-11 </p> 
<p> </p> 
<p>Closest Pair Tour</p> 
<p> Always walking to the closest point is too restrictive, since that point 
might trap us into making moves we don't want. &nbsp;</p> 
<p> Another idea would be to repeatedly connect the closest pair of points 
whose connection will not cause a cycle or a three-way branch to be formed, 
until we have a single chain with all the points in it.</p> 
<p> </p> 
<pre> 
<p> Let <em>n</em> be the number of points in the set </p>
<p> </p>
<p> For <i>i</i>=1 to <em>n-1</em> do </p>
<p> For each pair of endpoints (<i>x</i>,<i>y</i>) of partial paths </p>
<p> If  then </p>
<p>  ,  , <i>d</i> = <i>dist</i>(<i>x</i>,<i>y</i>) </p>
<p> Connect  by an edge </p>
<p> Connect the two endpoints by an edge. </p>
<p> </p></pre> 
<p> Although it works correctly on the previous example, other data causes 
trouble:</p> 
<p> </p> <br>
 This algorithm is <b>not correct</b>! 
<p> Listening To Part 1-12 </p> 
<p> </p> 
<p>A Correct Algorithm</p> 
<p> We could try all possible orderings of the points, then select the 
ordering which minimizes the total length: &nbsp;</p> 
<p> </p> 
<pre> 
<p> </p>
<p> For each of the <i>n</i>! permutations  of the <em>n</em> points </p>
<p> If  then </p>
<p>  and </p>
<p> Return </p>
<p> </p></pre> 
<p> Since all possible orderings are considered, we are guaranteed to end up 
with the shortest possible tour.</p> 
<p> Because it trys all <i>n</i>! permutations, it is extremely slow, much too 
slow to use when there are more than 10-20 points. &nbsp;</p> 
<p> No efficient, correct algorithm exists for the <em>traveling salesman 
problem</em>, as we will see later. </p> 
<p> Listening To Part 1-13 </p> 
<p> </p> 
<p>Efficiency</p> 
<p> <i>&quot;Why not just use a supercomputer?&quot;</i> </p> 
<p> Supercomputers are for people too rich and too stupid to design efficient 
algorithms! &nbsp;</p> 
<p> A faster algorithm running on a slower computer will <em>always</em> win 
for sufficiently large instances, as we shall see.</p> 
<p> Usually, problems don't have to get that large before the faster algorithm 
wins.</p> 
<p> </p> 
<p>Expressing Algorithms</p> We need some way to express the sequence of steps 
comprising an algorithm.
<p> In order of increasing precision, we have English, pseudocode, and real 
programming languages. Unfortunately, ease of expression moves in the reverse 
order.</p> 
<p> I prefer to describe the <em>ideas</em> of an algorithm in English, moving 
to pseudocode to clarify sufficiently tricky details of the algorithm. &nbsp;
</p> 
<p> Listening To Part 1-14 </p> 
<p> </p> 
<p>The RAM Model</p> 
<p> Algorithms are the <em>only</em> important, durable, and original part of 
computer science<em>because</em> they can be studied in a machine and language 
independent way.</p> 
<p> The reason is that we will do all our design and analysis for the RAM 
model of computation: &nbsp;&nbsp;</p> 
<p> </p> 
<ul> 
<li> Each &quot;simple&quot; operation (+, -, =, if, call) takes exactly 1 
step.</li> 
<li> Loops and subroutine calls are <em>not</em> simple operations, but depend 
upon the size of the data and the contents of a subroutine. We do not want 
``sort'' to be a single step operation.</li> 
<li> Each memory access takes exactly 1 step. </li> </ul> 
<p> We measure the run time of an algorithm by counting the number of steps. 
</p> 
<p> This model is useful and accurate in the same sense as the flat-earth 
model (which<em>is</em> useful)! &nbsp; </p> 
<p> Listening To Part 1-15 </p> 
<p> </p> 
<p>Best, Worst, and Average-Case</p> 
<p> The <em>worst case complexity</em> of the algorithm is the function 
defined by the maximum number of steps taken on any instance of size<em>n</em>. 
&nbsp;</p> 
<p> </p> <br>
 The <em>best case complexity</em> of the algorithm is the 
function defined by the minimum number of steps taken on any instance of size
<em>n</em>. &nbsp; 
<p> The <em>average-case complexity</em> of the algorithm is the function 
defined by an average number of steps taken on any instance of size<em>n</em>. 
&nbsp;</p> 
<p> Each of these complexities defines a numerical function - time vs. size! 
</p> 
<p> </p> 
<p>Insertion Sort</p> 
<p> One way to sort an array of n elements is to start with  empty list, then 
successively insert new elements in the proper position: &nbsp;</p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> At each stage, the inserted element leaves a sorted list, and after <em>n
</em> insertions contains exactly the right elements. Thus the algorithm must 
be correct.</p> 
<p> But how <i>efficient</i> is it? </p> 
<p> Note that the run time changes with the permutation instance! (even for a 
fixed size problem)</p> 
<p> How does insertion sort do on sorted permutations? </p> 
<p> How about unsorted permutations? </p> 
<p> </p> 
<p>Exact Analysis of Insertion Sort</p> 
<p> Count the number of times each line of pseudocode will be executed. </p> 
<p> </p> 
<p></p>  Line  InsertionSort(A)  #Inst. #Exec. <br>
1  for j:=2 to len. of A do
 c1  n <br>
 2  key:=A[j]  c2  n-1 <br>
 3  /* put A[j] into A[1..j-1] */  c3=0 
 / <br>
 4  i:=j-1  c4  n-1 <br>
 5  while  do  c5  tj <br>
 6  A[i+1]:= A[i]  
c6 <br>
 7  i := i-1  c7 <br>
 8  A[i+1]:=key  c8  n-1 <br>
<br>

<p></p>  The <b>for</b> statement is executed ( <i>n</i>-1)+1 times (why?) 
<p> Within the <b>for</b> statement, &quot;key:=A[j]&quot; is executed n-1 
times.</p> 
<p> Steps 5, 6, 7 are harder to count. </p> 
<p> Let  the number of elements that have to be slide right to insert the <em>j
</em>th item. </p> 
<p> Step 5 is executed  times. </p> 
<p> Step 6 is  . </p> 
<p> Add up the executed instructions for all pseudocode lines to get the 
run-time of the algorithm:</p> 
<p> </p> 
<p> What are the  ? They depend on the particular input. </p> 
<p> </p> 
<p>Best Case</p> If it's already sorted, all  's are 1. 
<p> Hence, the best case time is </p> 
<p> </p> 
<p> where <em>C</em> and <em>D</em> are constants. </p> 
<p> </p> 
<p>Worst Case</p> If the input is sorted in <em>descending</em> order, we will 
have to slide<em>all</em> of the already-sorted elements, so  , and step 5 is 
executed
<p> </p> 
<p></p> <b>Next:</b> Lecture 2 - asymptotic notation <b>Up:</b> Table of 
contents <b>Previous:</b> None 
<h1>Lecture 2 - asymptotic notation</h1> 
<p> Listening To Part 2-1 </p> 
<p> <em> Problem 1.2-6: &nbsp; How can we modify almost any algorithm to have 
a good best-case running time?</em> </p> To improve the best case, all we have 
to do it to be able to solve one instance of each size efficiently. We could 
modify our algorithm to first test whether the input is the special instance we 
know how to solve, and then output the canned answer.
<p> For sorting, we can check if the values are already ordered, and if so 
output them. For the traveling salesman, we can check if the points lie on a 
line, and if so output the points in that order.</p> 
<p> The supercomputer people pull this trick on the linpack benchmarks! </p> 
<p> </p> Because it is so easy to cheat with the best case running time, we 
usually don't rely too much about it.
<p> Because it is usually very hard to compute the average running time, since 
we must somehow average over all the instances, we usually strive to analyze 
the worst case running time.</p> 
<p> The worst case is usually fairly easy to analyze and often close to the 
average or real running time.</p> 
<p> Listening To Part 2-2 </p> 
<p> </p> 
<p>Exact Analysis is Hard!</p> 
<p> We have agreed that the best, worst, and average case complexity of an 
algorithm is a numerical function of the size of the instances.</p> 
<p> </p> <br>
 However, it is difficult to work with exactly because it is 
typically very complicated!
<p> Thus it is usually cleaner and easier to talk about <em>upper and lower 
bounds</em> of the function. &nbsp;&nbsp; </p> 
<p> This is where the dreaded big O notation comes in! &nbsp; </p> 
<p> Since running our algorithm on a machine which is twice as fast will 
effect the running times by a multiplicative constant of 2 - we are going to 
have to ignore constant factors anyway.</p> 
<p> Listening To Part 2-3 </p> 
<p> </p> 
<p>Names of Bounding Functions</p> 
<p> Now that we have clearly defined the complexity functions we are talking 
about, we can talk about upper and lower bounds on it: &nbsp;&nbsp;</p> 
<p> </p> 
<ul> 
<li> <i>g</i>(<i>n</i>) = <i>O</i>(<i>f</i>(<i>n</i>)) means  is an <em>upper 
bound</em> on <i>g</i>(<i>n</i>).</li> 
<li>  means  is a <em>lower bound</em> on <i>g</i>(<i>n</i>).</li> 
<li>  means  is an upper bound on <i>g</i>(<i>n</i>) and  is a lower bound on 
<i>g</i>(<i>n</i>). 
<p> </p></li> </ul> 
<p> Got it? <em>C</em>,  , and  are all constants independent of <em>n</em>. 
</p> 
<p> All of these definitions imply a constant <em>beyond which</em> they are 
satisfied. We do not care about small values of<em>n</em>. </p> 
<p> Listening To Part 2-4 </p> 
<p> </p> 
<p><em>O</em>,  , and </p> 
<p> </p> <br>
 The value of  shown is the minimum possible value; any greater 
value would also work.
<p> (a)  if there exist positive constants  ,  , and  such that to the right of
 , the value of<i>f</i>(<i>n</i>) always lies between  and  inclusive. </p> 
<p> (b) <i>f</i>(<i>n</i>) = <i>O</i>(<i>g</i>(<i>n</i>)) if there are 
positive constants and <em>c</em> such that to the right of  , the value of <i>f
</i>(<i>n</i>) always lies on or below  . </p> 
<p> (c)  if there are positive constants  and <em>c</em> such that to the 
right of , the value of <i>f</i>(<i>n</i>) always lies on or above  . </p> 
<p> Asymptotic notation  are as well as we can practically deal with 
complexity functions.</p> 
<p> Listening To Part 2-5 </p> 
<p> </p> 
<p>What does all this mean?</p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> Think of the equality as meaning <em>in the set of functions</em>. </p> 
<p> Note that time complexity is every bit as well defined a function as  or 
you bank account as a function of time.</p> 
<p> Listening To Part 2-6 </p> 
<p> </p> 
<p>Testing Dominance</p> 
<p> <i>f</i>(<i>n</i>) dominates <i>g</i>(<i>n</i>) if  , which is the same as 
saying<i>g</i>(<i>n</i>)=<i>o</i>(<i>f</i>(<i>n</i>)). &nbsp; </p> 
<p> Note the little-oh - it means ``grows strictly slower than''. </p> 
<p> Knowing the dominance relation between common functions is important 
because we want algorithms whose time complexity is as low as possible in the 
hierarchy. If<i>f</i>(<i>n</i>) dominates <i>g</i>(<i>n</i>), <em>f</em> is 
much larger (ie. slower) than<em>g</em>. </p> 
<p> </p> 
<ul> 
<li> 
<p>  dominates  if <i>a</i> &gt; <i>b</i> since </p>
<p> </p>
<p></p></li> 
<li>  doesn't dominate  since 
<p> </p>
<p></p></li> </ul> 
<p> </p> 
<p></p>  Complexity  10  20  30  40  50  60 <br>
<em>n</em>  0.00001 sec  
0.00002 sec  0.00003 sec  0.00004 sec  0.00005 sec  0.00006 sec <br>
 0.0001 sec
 0.0004 sec  0.0009 sec  0.016 sec  0.025 sec  0.036 sec <br>
 0.001 sec  0.008 
sec  0.027 sec  0.064 sec  0.125 sec  0.216 sec <br>
 0.1 sec  3.2 sec  24.3 sec
 1.7 min  5.2 min  13.0 min <br>
 0.001 sec  1.0 sec  17.9 min  12.7 days  35.7 
years  366 cent <br>
 0.59 sec  58 min  6.5 years  3855 cent  cent  cent <br>

<br> 
<p></p> <br>

<p> Listening To Part 2-7 </p> 
<p> </p> 
<p>Logarithms</p> 
<p> It is important to understand deep in your bones what logarithms are and 
where they come from. &nbsp;&nbsp;</p> 
<p> A logarithm is simply an inverse exponential function. Saying  is 
equivalent to saying that . </p> 
<p> Exponential functions, like the amount owed on a <em>n</em> year mortgage 
at an interest rate of per year, are functions which grow distressingly fast, 
as anyone who has tried to pay off a mortgage knows.</p> 
<p> Thus inverse exponential functions, ie. logarithms, grow refreshingly 
slowly. &nbsp;</p> 
<p> Binary search is an example of an  algorithm. After each comparison, we 
can throw away half the possible number of keys. Thus twenty comparisons 
suffice to find any name in the million-name Manhattan phone book!</p> 
<p> If you have an algorithm which runs in  time, take it, because this is 
blindingly fast even on very large instances.</p> 
<p> Listening To Part 2-8 </p> 
<p> </p> 
<p>Properties of Logarithms</p> 
<p> Recall the definition,  . </p> 
<p> </p> 
<p>Asymptotically, the base of the log does not matter:</p> &nbsp; 
<p> </p> 
<p> </p> 
<p> </p> 
<p> Thus,  , and note that  is just a constant. </p> 
<p> </p> 
<p>Asymptotically, any polynomial function of <em>n</em> does not matter:</p> 
Note that
<p> </p> 
<p> since  , and  . </p> 
<p> <em>Any</em> exponential dominates <em>every</em> polynomial. This is why 
we will seek to avoid exponential time algorithms.</p> 
<p> Listening To Part 2-9 </p> 
<p> </p> 
<p>Federal Sentencing Guidelines</p> 
<p> 2F1.1. Fraud and Deceit; Forgery; Offenses Involving Altered or 
Counterfeit Instruments other than Counterfeit Bearer Obligations of the United 
States. &nbsp;</p> 
<p> (a) Base offense Level: 6 </p> 
<p> (b) Specific offense Characteristics </p> 
<p> (1) If the loss exceeded $2,000, increase the offense level as follows: 
</p> 
<p> </p> 
<p></p>  Loss(Apply the Greatest)  Increase in Level <br>
(A) $2,000 or less  
no increase <br>
(B) More than $2,000  add 1 <br>
(C) More than $5,000  add 2 
<br> (D) More than $10,000  add 3 <br>
(E) More than $20,000  add 4 <br>
(F) 
More than $40,000  add 5 <br>
(G) More than $70,000  add 6 <br>
(H) More than 
$120,000  add 7 <br>
(I) More than $200,000  add 8 <br>
(J) More than $350,000  
add 9 <br>
(K) More than $500,000  add 10 <br>
(L) More than $800,000  add 11 
<br> (M) More than $1,500,000  add 12 <br>
(N) More than $2,500,000  add 13 <br>
(O) More than $5,000,000  add 14 <br>
(P) More than $10,000,000  add 15 <br>

(Q) More than $20,000,000  add 16 <br>
(R) More than $40,000,000  add 17 <br>

(Q) More than $80,000,000  add 18 <br>
<br>

<p></p> Listening To Part 2-10 
<p> The federal sentencing guidelines are designed to help judges be 
consistent in assigning punishment. The time-to-serve is a roughly linear 
function of the total<em>level</em>. </p> 
<p> However, notice that the increase in level as a function of the amount of 
money you steal grows<em>logarithmically</em> in the amount of money stolen. 
&nbsp;</p> 
<p> This very slow growth means it pays to commit one crime stealing a lot of 
money, rather than many small crimes adding up to the same amount of money, 
because the time to serve if you get caught is much less.</p> 
<p> The Moral: <em>``if you are gonna do the crime, make it worth the time!''
</em> </p> 
<p> Listening To Part 2-11 </p> 
<p> </p> 
<p>Working with the Asymptotic Notation</p> 
<p> Suppose  and  . &nbsp; </p> 
<p> What do we know about <i>g</i>'(<i>n</i>) = <i>f</i>(<i>n</i>)+<i>g</i>(<i>
n</i>)? Adding the bounding constants shows  . </p> 
<p> What do we know about <i>g</i>''(<i>n</i>) = <i>f</i>(<i>n</i>)-<i>g</i>(
<i>n</i>)? Since the bounding constants don't necessary cancel, </p> 
<p> We know nothing about the lower bounds on <i>g</i>'+<i>g</i>'' because we 
know nothing about lower bounds on<em>f</em>, <em>g</em>. </p> 
<p> </p> Suppose  and  . 
<p> What do we know about <i>g</i>'(<i>n</i>) = <i>f</i>(<i>n</i>)+<i>g</i>(<i>
n</i>)? Adding the lower bounding constants shows  . </p> 
<p> What do we know about <i>g</i>''(<i>n</i>) = <i>f</i>(<i>n</i>)-<i>g</i>(
<i>n</i>)? We know nothing about the lower bound of this! </p> 
<p> Listening To Part 2-12 </p> 
<p> </p> 
<p>The Complexity of Songs</p> 
<p> Suppose we want to sing a song which lasts for <em>n</em> units of time. 
Since<em>n</em> can be large, we want to memorize songs which require only a 
small amount of brain space, i.e. memory. &nbsp; &nbsp;</p> 
<p> Let <i>S</i>(<i>n</i>) be the <em>space complexity</em> of a song which 
lasts for<em>n</em> units of time. </p> 
<p> The amount of space we need to store a song can be measured in either the 
words or characters needed to memorize it. Note that the number of characters is
 since every word in a song is at most 34 letters long - 
Supercalifragilisticexpialidocious!</p> 
<p> What bounds can we establish on <i>S</i>(<i>n</i>)? </p> 
<p> </p> 
<ul> 
<li> <i>S</i>(<i>n</i>) = <i>O</i>(<i>n</i>), since in the worst case we must 
explicitly memorize every word we sing - ``The Star-Spangled Banner''</li> 
<li>  , since we must know something about our song to sing it. 
<p> </p></li> </ul> 
<p> Listening To Part 2-13 </p> 
<p> </p> 
<p>The Refrain</p> 
<p> Most popular songs have a refrain, which is a block of text which gets 
repeated after each stanza in the song: &nbsp;</p> 
<p> </p> 
<p> Bye, bye Miss American pie <br>
 Drove my chevy to the levy but the levy 
was dry<br>
 Them good old boys were drinking whiskey and rye <br>
 Singing 
this will be the day that I die.<br>
</p> 
<p> </p> 
<p> Refrains made a song easier to remember, since you memorize it once yet 
sing it<i>O</i>(<i>n</i>) times. But do they reduce the space complexity? </p> 
<p> Not according to the big oh. If </p> 
<p> </p> 
<p> Then the space complexity is still <i>O</i>(<i>n</i>) since it is only 
halved (if the verse-size = refrain-size):</p> 
<p> </p> 
<p> </p> 
<p> Listening To Part 2-14 </p> 
<p> </p> 
<p>The <em>k</em> Days of Christmas</p> 
<p> To reduce <i>S</i>(<i>n</i>), we must structure the song differently. </p> 
<p> Consider ``The <em>k</em> Days of Christmas''. All one must memorize is: 
</p> 
<p> </p> 
<p> On the <em>k</em>th Day of Christmas, my true love gave to me, <br>
<br>
 
On the First Day of Christmas, my true love gave to me, a partridge in a pear 
tree<br>
</p> 
<p> </p> 
<p> But the time it takes to sing it is </p> 
<p> </p> 
<p> </p> 
<p> If  , then  , so  . </p> 
<p> Listening To Part 2-15 </p> 
<p> </p> 
<p>100 Bottles of Beer</p> 
<p> What do kids sing on really long car trips? </p> 
<p> </p> 
<p> <em>n</em> bottles of beer on the wall, <br>
<em>n</em> bottles of beer. 
<br> You take one down and pass it around <br>
<em>n-1</em> bottles of beer on 
the ball.</p> 
<p> </p> 
<p> All you must remember in this song is this template of size  , and the 
current value of<em>n</em>. The storage size for <em>n</em> depends on its 
value, but bits suffice. </p> 
<p> This for this song,  . </p> 
<p> </p> Is there a song which eliminates even the need to count? 
<p> </p> 
<p> That's the way, uh-huh, uh-huh <br>
 I like it, uh-huh, huh <br>
</p> 
<p> </p> 
<p> Reference: D. Knuth, `The Complexity of Songs', <em>Comm. ACM</em>, April 
1984, pp.18-24</p> 
<p> </p> <b>Next:</b> Lecture 3 - recurrence <b>Up:</b> Table of contents <b>
Previous:</b> Lecture 1 - analyzing 
<h1>Lecture 3 - recurrence relations</h1> 
<p> Listening To Part 3-1 </p> 
<p> <em>Problem 2.1-2: Show that for any real constants <em>a</em> and <em>b
</em>, <i>b</i> &gt; 0, &nbsp; </em></p> 
<p><em> </em></p> 
<p><em> </em> </p> To show  , we must show <em>O</em> and  . <em>Go back to 
the definition!</em> 
<p> </p> 
<ul> 
<li> <em>Big <em>O</em></em> - Must show that  for all  . When is this true? If
 , this is true for all<i>n</i> &gt; |<i>a</i>| since <i>n</i>+<i>a</i> &lt; 2
<i>n</i>, and raise both sides to the <em>b</em>.</li> 
<li> <em>Big </em> - Must show that  for all  . When is this true? If  , this 
is true for all<i>n</i> &gt; 3|<i>a</i>|/2 since <i>n</i>+<i>a</i> &gt; <i>n</i>
/2, and raise both sides to the<em>b</em>. 
<p> </p></li> </ul> 
<p> Note the need for absolute values. </p> 
<p> Listening To Part 3-2 </p> 
<p> <em>Problem 2.1-4: </em></p> 
<p><em> (a) Is  ? </em></p> 
<p><em> (b) Is  ? </em> </p> (a) Is  ? 
<p> Is  ? </p> 
<p> Yes, if  for all <em>n</em> </p> 
<p> (b) Is </p> 
<p> Is  ? </p> 
<p> note </p> 
<p> Is  ? </p> 
<p> Is  ? </p> 
<p> No! Certainly for any constant <em>c</em> we can find an <em>n</em> such 
that this is not true.</p> 
<p> Listening To Part 3-3 </p> 
<p> </p> 
<p>Recurrence Relations</p> 
<p> Many algorithms, particularly divide and conquer algorithms, have time 
complexities which are naturally modeled by recurrence relations. &nbsp;</p> 
<p> A recurrence relation is an equation which is defined in terms of itself. 
</p> 
<p> Why are recurrences good things? </p> 
<p> </p> 
<ol> 
<li> Many natural functions are easily expressed as recurrences: 
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p></p></li> 
<li> It is often easy to find a recurrence as the solution of a counting 
problem.<em>Solving</em> the recurrence can be done for many special cases as 
we will see, although it is somewhat of an art.
<p> </p></li> </ol> 
<p> Listening To Part 3-4 </p> 
<p> </p> 
<p>Recursion <em>is</em> Mathematical Induction!</p> 
<p> In both, we have general and boundary conditions, with the general 
condition breaking the problem into smaller and smaller pieces. &nbsp;&nbsp;</p>
<p> The <em>initial</em> or boundary condition terminate the recursion. &nbsp; 
</p> 
<p> As we will see, induction provides a useful tool to solve recurrences - 
guess a solution and prove it by induction.</p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p></p> <em>n</em>  0  1  2  3  4  5  6  7 <br>
 0  1  3  7  15  31  63  127 
<br> <br>

<p></p>  Guess what the solution is? 
<p> Prove  by induction: </p> 
<p> </p> 
<ol> 
<li> Show that the basis is true:  .</li> 
<li> Now assume true for  .</li> 
<li> Using this assumption show: 
<p> </p>
<p></p></li> </ol> 
<p> height6pt width4pt</p> 
<p> </p> 
<p> Listening To Part 3-5 </p> 
<p> </p> 
<p>Solving Recurrences</p> 
<p> No general procedure for solving recurrence relations is known, which is 
why it is an art. My approach is: &nbsp;</p> 
<p> </p> 
<p>Realize that linear, finite history, constant coefficient recurrences 
always can be solved</p> Check out any combinatorics or differential equations 
book for a procedure.
<p> Consider  ,  , </p> 
<p> It has history = 2, degree = 1, and coefficients of 2 and 1. Thus it can 
be solved mechanically! Proceed:</p> 
<p> </p> 
<ul> 
<li> Find the characteristic equation, eg. 
<p> </p>
<p></p></li> 
<li> Solve to get roots, which appear in the exponents.</li> 
<li> Take care of repeated roots and inhomogeneous parts.</li> 
<li> Find the constants to finish the job. 
<p> </p></li> </ul> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> Systems like Mathematica and Maple have packages for doing this. 
&nbsp;&nbsp;</p> 
<p> Listening To Part 3-6 </p> 
<p> </p> 
<p>Guess a solution and prove by induction</p> 
<p> To guess the solution, play around with small values for insight. </p> 
<p> Note that you can do inductive proofs with the big-O's notations - just be 
sure you use it right. &nbsp;</p> 
<p> <em>Example</em>:  . </p> 
<p> Show that  for large enough <em>c</em> and <em>n</em>. Assume that it is 
true for<i>n</i>/2, then </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> Starting with basis cases <i>T</i>(2)=4, <i>T</i>(3)=5, lets us complete 
the proof for . </p> 
<p> Listening To Part 3-7 </p> 
<p> </p> 
<p>Try backsubstituting until you know what is going on</p> 
<p> Also known as the iteration method. Plug the recurrence back into itself 
until you see a pattern. &nbsp;</p> 
<p> <em>Example:</em>  . </p> 
<p> Try backsubstituting: </p> 
<p> </p> 
<p> </p> 
<p> The  term should now be obvious. </p> 
<p> Although there are only  terms before we get to <i>T</i>(1), it doesn't 
hurt to sum them all since this is a fast growing geometric series:</p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> Listening To Part 3-8 </p> 
<p> </p> 
<p>Recursion Trees</p> 
<p> Drawing a picture of the backsubstitution process gives you a idea of what 
is going on. &nbsp;</p> 
<p> We must keep track of two things - (1) the size of the remaining argument 
to the recurrence, and (2) the additive stuff to be accumulated during this 
call.</p> 
<p> <em>Example:</em> </p> 
<p> </p> <br>
 The remaining arguments are on the left, the additive terms on 
the right.
<p> Although this tree has height  , the total sum at each level decreases 
geometrically, so:</p> 
<p> </p> 
<p> </p> 
<p> The recursion tree framework made this much easier to see than with 
algebraic backsubstitution.</p> 
<p> Listening To Part 3-9 </p> 
<p> </p> 
<p>See if you can use the Master theorem to provide an instant asymptotic 
solution</p> 
<p> <em>The Master Theorem:</em> &nbsp; Let  and <i>b</i>&gt;1 be constants, 
let<i>f</i>(<i>n</i>) be a function, and let <i>T</i>(<i>n</i>) be defined on 
the nonnegative integers by the recurrence</p> 
<p> </p> 
<p> where we interpret <i>n</i>/<i>b</i> as  or  . Then <i>T</i>(<i>n</i>) can 
be bounded asymptotically as follows:</p> 
<p> </p> 
<ol> 
<li> If  for some constant  , then  .</li> 
<li> If  , then  .</li> 
<li> If  for some constant  , and if  for some constant <i>c</i>&lt;1, and all 
sufficiently large<em>n</em>, then  . 
<p> </p></li> </ol> 
<p> Listening To Part 3-10 </p> 
<p> </p> 
<p>Examples of the Master Theorem</p> 
<p> Which case of the Master Theorem applies? </p> 
<p> </p> 
<ul> 
<li> <i>T</i>(<i>n</i>) = 4 <i>T</i>(<i>n</i>/2) + <i>n</i> 
<p> Reading from the equation, <i>a</i>=4, <i>b</i>=2, and <i>f</i>(<i>n</i>) =
<i>n</i>. </p>
<p> Is  ? </p>
<p> Yes, so case 1 applies and  .</p></li> 
<li> 
<p> Reading from the equation, <i>a</i>=4, <i>b</i>=2, and  . </p>
<p> Is  ? </p>
<p> No, if  , but it is true if  , so case 2 applies and  .</p></li> 
<li> 
<p> Reading from the equation, <i>a</i>=4, <i>b</i>=2, and  . </p>
<p> Is  ? </p>
<p> Yes, for  , so case 3 <em>might</em> apply. </p>
<p> Is  ? </p>
<p> Yes, for  , so there exists a <i>c</i> &lt; 1 to satisfy the regularity 
condition, so case 3 applies and . </p>
<p> </p></li> </ul> 
<p> Listening To Part 3-11 </p> 
<p> </p> 
<p>Why should the Master Theorem be true?</p> 
<p> Consider <i>T</i>(<i>n</i>) = <i>a T</i>(<i>n</i>/<i>b</i>) + <i>f</i>(<i>n
</i>). </p> 
<p> </p> 
<p>Suppose <i>f</i>(<i>n</i>) is small enough</p> Say <i>f</i>( <i>n</i>)=0, 
ie.<i>T</i>( <i>n</i>) = <i>a T</i>( <i>n</i>/ <i>b</i>). 
<p> Then we have a recursion tree where the only contribution is at the 
leaves. &nbsp;</p> 
<p> There will be  levels, with  leaves at level <em>l</em>. </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> <br>
 so long as <i>f</i>( <i>n</i>) is small enough that it is 
dwarfed by this, we have case 1 of the Master Theorem!
<p> Listening To Part 3-12 </p> 
<p> </p> 
<p>Suppose f(n) is large enough</p> If we draw the recursion tree for <i>T</i>(
<i>n</i>) = <i>a T</i>( <i>n</i>/ <i>b</i>) + <i>f</i>( <i>n</i>). 
<p> </p> <br>
 If <i>f</i>( <i>n</i>) is a big enough function, the one top 
call can be bigger than the sum of all the little calls.
<p> <em>Example:</em>  . In fact this holds unless  ! </p> 
<p> In case 3 of the Master Theorem, the additive term dominates. </p> 
<p> In case 2, both parts contribute equally, which is why the log pops up. It 
is (usually) what we want to have happen in a divide and conquer algorithm.</p> 
<p> Listening To Part 3-13 </p> 
<p> </p> 
<p>Famous Algorithms and their Recurrence</p> 
<p> </p> 
<p>Matrix Multiplication</p> The standard matrix multiplication algorithm for 
two matrices is  . &nbsp; &nbsp; 
<p> </p> <br>
 Strassen discovered a divide-and-conquer algorithm which takes  
time.
<p> Since  dwarfs  , case 1 of the master theorem applies and  . </p> 
<p> This has been ``improved'' by more and more complicated recurrences until 
the current best in . </p> 
<p> Listening To Part 3-14 </p> 
<p> </p> 
<p>Polygon Triangulation</p> Given a polygon in the plane, add diagonals so 
that each face is a triangle None of the diagonals are allowed to cross. 
&nbsp;&nbsp;
<p> </p> <br>
 Triangulation is an important first step in many geometric 
algorithms.
<p> The simplest algorithm might be to try each pair of points and check if 
they see each other. If so, add the diagonal and recur on both halves, for a 
total of . </p> 
<p> However, Chazelle gave an algorithm which runs in  time. Since  , by case 
1 of the Master Theorem, Chazelle's algorithm is linear, ie.<i>T</i>(<i>n</i>) =
<i>O</i>(<i>n</i>). </p> 
<p> </p> 
<p>Sorting</p> The classic divide and conquer recurrence is Mergesort's <i>T
</i>( <i>n</i>) = 2 <i>T</i>( <i>n</i>/2) + <i>O</i>( <i>n</i>), which divides 
the data into equal-sized halves and spends linear time merging the halves 
after they are sorted. &nbsp;
<p> Since  but not  , Case 2 of the Master Theorem applies and  . </p> 
<p> In case 2, the divide and merge steps balance out perfectly, as we usually 
hope for from a divide-and-conquer algorithm.</p> 
<p> </p> 
<p>Mergesort Animations</p> 
<p> </p> 
<p>Approaches to Algorithms Design</p> 
<p> </p> 
<p>Incremental</p> Job is partly done - do a little more, repeat until done. 
&nbsp;
<p> A good example of this approach is insertion sort </p> 
<p> </p> 
<p>Divide-and-Conquer</p> A recursive technique &nbsp; 
<p> </p> 
<ul> 
<li> Divide problem into sub-problems of the same kind.</li> 
<li> For subproblems that are really small (trivial), solve them directly. 
Else solve them recursively. (conquer)</li> 
<li> Combine subproblem solutions to solve the whole thing (combine) 
<p> </p></li> </ul> 
<p> A good example of this approach is Mergesort. </p> 
<p> </p> <b>Next:</b> Lecture 4 - heapsort <b>Up:</b> Table of contents <b>
Previous:</b> Lecture 2 - asymptotic 
<h1>Lecture 4 - heapsort</h1> 
<p> Listening To Part 4-1 </p> 
<p> <em> 4.2-2 Argue the solution to </em></p> 
<p><em> </em></p> 
<p><em> is  by appealing to the recursion tree. &nbsp; </em> </p> Draw the 
recursion tree.
<p> </p> <br>
 How many levels does the tree have? This is equal to the 
longest path from the root to a leaf.
<p> The shortest path to a leaf occurs when we take the heavy branch each 
time. The height<em>k</em> is given by  , meaning  or  . </p> 
<p> The longest path to a leaf occurs when we take the light branch each time. 
The height<em>k</em> is given by  , meaning  or  . </p> 
<p> The problem asks to show that  , meaning we are looking for a lower bound 
</p> 
<p> On any <em>full</em> level, the additive terms sums to <em>n</em>. There 
are full levels. Thus </p> 
<p> Listening To Part 4-2 </p> 
<p> <em> 4.2-4 Use iteration to solve <i>T</i>(<i>n</i>) = <i>T</i>(<i>n</i>-
<i>a</i>) + <i>T</i>(<i>a</i>) + <i>n</i>, where  is a constant. </em> </p> 
Note iteration is backsubstitution. &nbsp;
<p> </p> 
<p> </p> 
<p> </p> 
<p> Listening To Part 4-3 </p> 
<p> </p> 
<p>Why don't CS profs ever stop talking about sorting?!</p> 
<p> </p> 
<ol> 
<li> Computers spend more time sorting than anything else, historically 25% on 
mainframes. &nbsp; &nbsp;</li> 
<li> Sorting is the best studied problem in computer science, with a variety 
of different algorithms known.</li> 
<li> Most of the interesting ideas we will encounter in the course can be 
taught in the context of sorting, such as divide-and-conquer, randomized 
algorithms, and lower bounds.
<p> </p></li> </ol> 
<p> You should have seen most of the algorithms - we will concentrate on the 
analysis.</p> 
<p> Listening To Part 4-4 </p> 
<p> </p> 
<p>Applications of Sorting</p> 
<p> One reason why sorting is so important is that once a set of items is 
sorted, many other problems become easy. &nbsp;</p> 
<p> </p> 
<p>Searching</p> Binary search lets you test whether an item is in a 
dictionary in time. &nbsp; 
<p> Speeding up searching is perhaps the most important application of sorting.
</p> 
<p> </p> 
<p>Closest pair</p> Given <em>n</em> numbers, find the pair which are closest 
to each other. &nbsp;
<p> Once the numbers are sorted, the closest pair will be next to each other 
in sorted order, so an<i>O</i>(<i>n</i>) linear scan completes the job. </p> 
<p> Listening To Part 4-5 </p> 
<p> </p> 
<p>Element uniqueness</p> Given a set of <em>n</em> items, are they all unique 
or are there any duplicates? &nbsp; &nbsp;
<p> Sort them and do a linear scan to check all adjacent pairs. </p> 
<p> This is a special case of closest pair above. </p> 
<p> </p> 
<p>Frequency distribution - Mode</p> Given a set of <em>n</em> items, which 
element occurs the largest number of times? &nbsp;&nbsp;
<p> Sort them and do a linear scan to measure the length of all adjacent runs. 
</p> 
<p> </p> 
<p>Median and Selection</p> What is the <em>k</em>th largest item in the set? 
&nbsp;&nbsp;
<p> Once the keys are placed in sorted order in an array, the <em>k</em>th 
largest can be found in constant time by simply looking in the<em>k</em>th 
position of the array.</p> 
<p> Listening To Part 4-6 </p> 
<p> </p> 
<p>Convex hulls</p> Given <em>n</em> points in two dimensions, find the 
smallest area polygon which contains them all. &nbsp;
<p> </p> <br>
 The convex hull is like a rubber band stretched over the points.
<p> Convex hulls are the most important building block for more sophisticated 
geometric algorithms. &nbsp;</p> 
<p> Once you have the points sorted by x-coordinate, they can be inserted from 
left to right into the hull, since the rightmost point is always on the 
boundary.</p> 
<p> Without sorting the points, we would have to check whether the point is 
inside or outside the current hull.</p> 
<p> Adding a new rightmost point might cause others to be deleted. </p> 
<p> </p> 
<p> </p> 
<p>Huffman codes</p> 
<p> If you are trying to minimize the amount of space a text file is taking 
up, it is silly to assign each letter the same length (ie. one byte) code. 
&nbsp;&nbsp;</p> 
<p> Example: <em>e</em> is more common than <em>q</em>, <em>a</em> is more 
common than<em>z</em>. </p> 
<p> If we were storing English text, we would want <em>a</em> and <em>e</em> 
to have shorter codes than<em>q</em> and <em>z</em>. </p> 
<p> To design the best possible code, the first and most important step is to 
sort the characters in order of frequency of use.</p> 
<p> </p> 
<p></p>  Character  Frequency  Code <br>
f  5  1100 <br>
 e  9  1101 <br>
 c  
12  100 <br>
 b  13  101 <br>
 d  16  111 <br>
 a  45 &gt;Listening t0  0 <br>

<br> 
<p></p> Listening to Part 4-8 
<p> </p> 
<p>Selection Sort</p> 
<p> A simple  sorting algorithm is selection sort. &nbsp; </p> 
<p> Sweep through all the elements to find the smallest item, then the 
smallest remaining item, etc. until the array is sorted.</p> 
<p> </p> 
<pre> 
<p> Selection-sort(A) </p>
<p> for <i>i</i> = 1 to <em>n</em> </p>
<p> for <i>j</i> = <i>i</i>+1 to <em>n</em> </p>
<p> if (<i>A</i>[<i>j</i>] &lt; <i>A</i>[<i>i</i>]) then swap(A[i],A[j]) </p>
<p> </p></pre> 
<p> It is clear this algorithm must be correct from an inductive argument, 
since the<em>i</em>th element is in its correct position. </p> 
<p> It is clear that this algorithm takes  time. </p> 
<p> It is clear that the analysis of this algorithm cannot be improved because 
there will be<i>n</i>/2 iterations which will require at least <i>n</i>/2 
comparisons each, so at least comparisons will be made. More careful analysis 
doubles this.</p> 
<p> Thus selection sort runs in  time. </p> 
<p> Listening to Part 4-9 </p> 
<p> </p> 
<p>Binary Heaps</p> 
<p> A <em>binary heap</em> is defined to be a binary tree with a key in each 
node such that: &nbsp;</p> 
<p> </p> 
<ol> 
<li> All leaves are on, at most, two adjacent levels.</li> 
<li> All leaves on the lowest level occur to the left, and all levels except 
the lowest one are completely filled.</li> 
<li> The key in root is  all its children, and the left and right subtrees are 
again binary heaps.
<p> </p></li> </ol> 
<p> Conditions 1 and 2 specify shape of the tree, and condition 3 the labeling 
of the tree.</p> 
<p> </p> <br>
Listening to Part 4-10 
<p> The ancestor relation in a heap defines a <em>partial order</em> on its 
elements, which means it is reflexive, anti-symmetric, and transitive. &nbsp;
</p> 
<p> </p> 
<ol> 
<li> <em>Reflexive:</em> <em>x</em> is an ancestor of itself.</li> 
<li> <em>Anti-symmetric:</em> if <em>x</em> is an ancestor of <em>y</em> and 
<em>y</em> is an ancestor of <em>x</em>, then <i>x</i>=<i>y</i>.</li> 
<li> <em>Transitive:</em> if <em>x</em> is an ancestor of <em>y</em> and <em>y
</em> is an ancestor of <em>z</em>, <em>x</em> is an ancestor of <em>z</em>. 
<p> </p></li> </ol> 
<p> Partial orders can be used to model heirarchies with incomplete 
information or equal-valued elements. One of my favorite games with my parents 
is fleshing out the partial order of ``big'' old-time movie stars. &nbsp;</p> 
<p> The partial order defined by the heap structure is weaker than that of the 
total order, which explains</p> 
<p> </p> 
<ol> 
<li> Why it is easier to build.</li> 
<li> Why it is less useful than sorting (but still very important). 
<p> </p></li> </ol> 
<p> Listening to Part 4-11 </p> 
<p> </p> 
<p>Constructing Heaps</p> 
<p> Heaps can be constructed incrementally, by inserting new elements into the 
left-most open spot in the array. &nbsp;</p> 
<p> If the new element is greater than its parent, swap their positions and 
recur.</p> 
<p> Since at each step, we replace the root of a subtree by a larger one, we 
preserve the heap order.</p> 
<p> Since all but the last level is always filled, the height <em>h</em> of an 
<em>n</em> element heap is bounded because: </p> 
<p> </p> 
<p> so  . </p> 
<p> Doing <em>n</em> such insertions takes  , since the last <i>n</i>/2 
insertions require time each. </p> 
<p> Listening to Part 4-12 </p> 
<p> </p> 
<p>Heapify</p> 
<p> The bottom up insertion algorithm gives a good way to build a heap, but 
Robert Floyd found a better way, using a<em>merge</em> procedure called <em>
heapify</em>. &nbsp; </p> 
<p> Given two heaps and a fresh element, they can be merged into one by making 
the new one the root and trickling down.</p> 
<p> </p> 
<pre> 
<p> Build-heap(A) </p>
<p> <i>n</i> = |<i>A</i>| </p>
<p> For  do </p>
<p> Heapify(A,i) </p>
<p> </p></pre> 
<p> </p> 
<pre> 
<p> Heapify(A,i) </p>
<p> left = <em>2i</em> </p>
<p> right = <em>2i+1</em> </p>
<p> if  then </p>
<p> max = left </p>
<p> else max = i </p>
<p> if  and (<i>A</i>(<i>right</i>] &gt; <i>A</i>[<i>max</i>]) then </p>
<p> max = right </p>
<p> if  then </p>
<p> swap(A[i],A[max]) </p>
<p> Heapify(A,max) </p>
<p> </p></pre> 
<p> </p> 
<p> </p> 
<p>Rough Analysis of Heapify</p> 
<p> Heapify on a subtree containing <em>n</em> nodes takes </p> 
<p> </p> 
<p> </p> 
<p> The 2/3 comes from merging heaps whose levels differ by one. The last row 
could be exactly half filled. Besides, the asymptotic answer won't change so 
long the fraction is less than one. &nbsp;</p> 
<p> Solve the recurrence using the Master Theorem. </p> 
<p> Let <i>a</i> = 1, <i>b</i>= 3/2 and <i>f</i>(<i>n</i>) = 1. </p> 
<p> Note that  , since  . </p> 
<p> Thus Case 2 of the Master theorem applies. </p> 
<p> </p> <em>The Master Theorem:</em> Let  and <i>b</i>&gt;1 be constants, let 
<i>f</i>(<i>n</i>) be a function, and let <i>T</i>(<i>n</i>) be defined on the 
nonnegative integers by the recurrence
<p> </p>
<p> where we interpret <i>n</i>/<i>b</i> to mean either  or  . Then <i>T</i>(
<i>n</i>) can be bounded asymptotically as follows: </p>
<p> </p> 
<ol> 
<li> If  for some constant  , then  .</li> 
<li> If  , then  .</li> 
<li> If  for some constant  , and if  for some constant <i>c</i>&lt;1, and all 
sufficiently large<em>n</em>, then  . 
<p> </p></li> </ol> Listening to Part 4-14 
<p> </p> 
<p>Exact Analysis of Heapify</p> 
<p> In fact, Heapify performs better than  , because most of the heaps we 
merge are extremely small.</p> 
<p> </p> <br>
 In a full binary tree on <em>n</em> nodes, there are <i>n</i>/2 
nodes which are leaves (i.e. height 0),<i>n</i>/4 nodes which are height 1, <i>n
</i>/8 nodes which are height 2, ... 
<p> In general, there are at most  nodes of height <em>h</em>, so the cost of 
building a heap is:</p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> Since this sum is not quite a geometric series, we can't apply the usual 
identity to get the sum. But it should be clear that the series converges.</p> 
<p> Listening to Part 4-15 </p> 
<p> </p> 
<p>Proof of Convergence</p> 
<p> Series convergence is the ``free lunch'' of algorithm analysis. &nbsp; 
&nbsp;</p> 
<p> The identify for the sum of a geometric series is </p> 
<p> </p> 
<p> </p> 
<p> If we take the derivative of both sides, ... </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> Multiplying both sides of the equation by <em>x</em> gives the identity we 
need:</p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> Substituting <i>x</i> = 1/2 gives a sum of 2, so Build-heap uses at most 
<em>2n</em> comparisons and thus linear time. </p> 
<p> Listening to Part 4-16 </p> 
<p> </p> 
<p>The Lessons of Heapsort, I</p> 
<p> &quot;Are we doing a careful analysis? Might our algorithm be faster than 
it seems?&quot;</p> 
<p> Typically in our analysis, we will say that since we are doing at most <em>
x</em> operations of at most <em>y</em> time each, the total time is <i>O</i>(
<i>x y</i>). </p> 
<p> However, if we overestimate too much, our bound may not be as tight as it 
should be!</p> 
<p> Listening to Part 4-17 </p> 
<p> </p> 
<p>Heapsort</p> 
<p> Heapify can be used to construct a heap, using the observation that an 
isolated element forms a heap of size 1. &nbsp;</p> 
<p> </p> 
<pre> 
<p> Heapsort(A) </p>
<p> Build-heap(A) </p>
<p> for <i>i</i> = <i>n</i> to <em>1</em> do </p>
<p> swap(A[1],A[i]) </p>
<p> <i>n</i> = <i>n</i> - 1 </p>
<p> Heapify(A,1) </p>
<p> </p></pre> 
<p> If we construct our heap from bottom to top using Heapify, we do not have 
to do anything with the last<i>n</i>/2 elements. </p> 
<p> With the implicit tree defined by array positions, (i.e. the <em>i</em>th 
position is the parent of the<em>2i</em>th and (2<i>i</i>+1)st positions) the 
leaves start out as heaps.</p> 
<p> Exchanging the maximum element with the last element and calling heapify 
repeatedly gives an sorting algorithm, named <em>Heapsort</em>. </p> 
<p> Lecture Sound../sounds/lec4-17a.au </p> 
<p> </p> 
<p>Heapsort Animations</p> 
<p> Listening to Part 4-18 </p> 
<p> </p> 
<p>The Lessons of Heapsort, II</p> 
<p> Always ask yourself, ``Can we use a different data structure?'' </p> 
<p> Selection sort scans throught the entire array, repeatedly finding the 
smallest remaining element. &nbsp;</p> 
<p> </p> 
<pre> 
<p> For <i>i</i> = 1 to <em>n</em> </p>
<p> A: Find the smallest of the first <em>n-i+1</em> items. </p>
<p> B: Pull it out of the array and put it first. </p>
<p> </p></pre> 
<p> Using arrays or unsorted linked lists as the data structure, operation <em>
A</em> takes <i>O</i>(<i>n</i>) time and operation <em>B</em> takes <i>O</i>(1).
</p> 
<p> Using heaps, both of these operations can be done within  time, balancing 
the work and achieving a better tradeoff.</p> 
<p> Listening to Part 4-19 </p> 
<p> </p> 
<p>Priority Queues</p> 
<p> A <em>priority queue</em> is a data structure on sets of keys supporting 
the following operations: &nbsp;</p> 
<p> </p> 
<ul> 
<li> <em>Insert(S, x)</em> - insert <em>x</em> into set <em>S</em></li> 
<li> <em>Maximum(S)</em> - return the largest key in <em>S</em></li> 
<li> <em>ExtractMax(S)</em> - return and remove the largest key in <em>S</em> 
<p> </p></li> </ul> 
<p> These operations can be easily supported using a heap. </p> 
<p> </p> 
<ul> 
<li> <em>Insert</em> - use the trickle up insertion in  .</li> 
<li> <em>Maximum</em> - read the first element in the array in <i>O</i>(1).
</li> 
<li> <em>Extract-Max</em> - delete first element, replace it with the last, 
decrement the element counter, then heapify in . 
<p> </p></li> </ul> 
<p> Listening to Part 4-20 </p> 
<p> </p> 
<p>Applications of Priority Queues</p> 
<p> </p> 
<p>Heaps as stacks or queues</p> &nbsp;&nbsp; 
<p> </p> 
<ul> 
<li> In a stack, <em>push</em> inserts a new item and <em>pop</em> removes the 
most recently pushed item.</li> 
<li> In a queue, <em>enqueue</em> inserts a new item and <em>dequeue</em> 
removes the least recently enqueued item.
<p> </p></li> </ul> 
<p> Both stacks and queues can be simulated by using a heap, when we add a new 
<em>time</em> field to each item and order the heap according it this time 
field.</p> 
<p> </p> 
<ul> 
<li> To simulate the stack, increment the time with each insertion and put the 
maximum on top of the heap.</li> 
<li> To simulate the queue, decrement the time with each insertion and put the 
maximum on top of the heap (or increment times and keep the minimum on top)
<p> </p></li> </ul> 
<p> This simulation is not as efficient as a normal stack/queue 
implementation, but it is a cute demonstration of the flexibility of a priority 
queue.</p> 
<p> Listening to Part 4-21 </p> 
<p> </p> 
<p>Discrete Event Simulations</p> In simulations of airports, parking lots, 
and jai-alai - priority queues can be used to maintain who goes next. 
&nbsp;&nbsp;
<p> The stack and queue orders are just special cases of orderings. In real 
life, certain people cut in line.</p> 
<p> </p> 
<p>Sweepline Algorithms in Computational Geometry</p> &nbsp; &nbsp; 
<p> </p> <br>
 In the priority queue, we will store the points we have not yet 
encountered, ordered by<em>x</em> coordinate. and push the line forward one 
stop at a time.
<p> Listening to Part 4-22 </p> 
<p> </p> 
<p>Greedy Algorithms</p> In greedy algorithms, we always pick the next thing 
which locally maximizes our score. By placing all the things in a priority 
queue and pulling them off in order, we can improve performance over linear 
search or sorting, particularly if the weights change. &nbsp;
<p> Example: Sequential strips in triangulations. </p> 
<p> </p> 
<p>Danny Heep</p>  &nbsp; 
<p> </p> <b>Next:</b> Lecture 5 - quicksort <b>Up:</b> Table of contents <b>
Previous:</b> Lecture 3 - recurrence 
<h1>Lecture 5 - quicksort</h1> 
<p> Listening to Part 5-1 </p> 
<p> <em> 4-2 Find the missing integer from 0 to <em>n</em> using <i>O</i>(<i>n
</i>) ``is bit[j] in A[i]'' queries. </em> </p> Note - there are a total of  
bits, so we are not allowed to<em>read</em> the entire input! &nbsp; 
<p> Also note, the problem is asking us to minimize the number of bits we 
read. We can spend as much time as we want doing other things provided we don't 
look at extra bits.</p> 
<p> How can we find the last bit of the missing integer? </p> 
<p> Ask all the <em>n</em> integers what their last bit is and see whether 0 
or 1 is the bit which occurs less often than it is supposed to. That is the 
last bit of the missing integer!</p> 
<p> How can we determine the second-to-last bit? </p> 
<p> Ask the  numbers which ended with the correct last bit! By analyzing the 
bit patterns of the numbers from<em>0</em> to <em>n</em> which end with this 
bit. &nbsp;</p> 
<p> By recurring on the remaining candidate numbers, we get the answer in <i>T
</i>(<i>n</i>) = <i>T</i>(<i>n</i>/2) + <i>n</i> =<i>O</i>(<i>n</i>), by the 
Master Theorem.</p> 
<p> Listening to Part 5-2 </p> 
<p> </p> 
<p>Quicksort</p> 
<p> Although mergesort is  , it is quite inconvenient for implementation with 
arrays, since we need space to merge. &nbsp;</p> 
<p> In practice, the fastest sorting algorithm is Quicksort, which uses <em>
partitioning</em> as its main idea. &nbsp; </p> 
<p> Example: Pivot about 10. </p> 
<p> 17 12 6 19 23 8 5 10 - before </p> 
<p> 6 8 5 10 23 19 12 17 - after </p> 
<p> Partitioning places all the elements less than the pivot in the <em>left
</em> part of the array, and all elements greater than the pivot in the <em>
right</em> part of the array. The pivot fits in the slot between them. &nbsp; 
</p> 
<p> Note that the pivot element ends up in the correct place in the total 
order!</p> 
<p> Listening to Part 5-3 </p> 
<p> </p> 
<p>Partitioning the elements</p> 
<p> Once we have selected a pivot element, we can partition the array in one 
linear scan, by maintaining three sections of the array: &lt; pivot, &gt; 
pivot, and unexplored.</p> 
<p> Example: pivot about 10 </p> 
<p> | 17 12 6 19 23 8 5 | 10 </p> 
<p> | 5 12 6 19 23 8 | 17 </p> 
<p> 5 | 12 6 19 23 8 | 17 </p> 
<p> 5 | 8 6 19 23 | 12 17 </p> 
<p> 5 8 | 6 19 23 | 12 17 </p> 
<p> 5 8 6 | 19 23 | 12 17 </p> 
<p> 5 8 6 | 23 | 19 12 17 </p> 
<p> 5 8 6 ||23 19 12 17 </p> 
<p> 5 8 6 10 19 12 17 23 </p> 
<p> As we scan from left to right, we move the left bound to the right when 
the element is less than the pivot, otherwise we swap it with the<em>rightmost 
unexplored</em> element and move the right bound one step closer to the left. 
</p> 
<p> Listening to Part 5-4 </p> 
<p> Since the partitioning step consists of at most <em>n</em> swaps, takes 
time linear in the number of keys. But what does it buy us?</p> 
<p> </p> 
<ol> 
<li> The pivot element ends up in the position it retains in the final sorted 
order.</li> 
<li> After a partitioning, no element flops to the other side of the pivot in 
the final sorted order.
<p> </p></li> </ol> 
<p> <em> Thus we can sort the elements to the left of the pivot and the right 
of the pivot independently!</em> </p> 
<p> This gives us a recursive sorting algorithm, since we can use the 
partitioning approach to sort each subproblem.</p> 
<p> Listening to Part 5-5 </p> 
<p> </p> 
<p>Quicksort Animations</p> 
<p> Listening to Part 5-6 </p> 
<p> </p> 
<p>Pseudocode</p> 
<p> </p> 
<pre> 
<p> Sort(A) </p>
<p> Quicksort(A,1,n) </p>
<p> </p></pre> 
<p> </p> 
<pre> 
<p> Quicksort(A, low, high) </p>
<p> if (low &lt; high) </p>
<p> pivot-location = Partition(A,low,high) </p>
<p> Quicksort(A,low, pivot-location - 1) </p>
<p> Quicksort(A, pivot-location+1, high) </p>
<p> </p></pre> 
<p> </p> 
<pre> 
<p> Partition(A,low,high) </p>
<p> pivot = A[low] </p>
<p> leftwall = low </p>
<p> for <i>i</i> = low+1 to high </p>
<p> if (A[i] &lt; pivot) then </p>
<p> leftwall = leftwall+1 </p>
<p> swap(A[i],A[leftwall]) </p>
<p> swap(A[low],A[leftwall]) </p>
<p> </p></pre> 
<p> Listening to Part 5-7 </p> 
<p> </p> 
<p>Best Case for Quicksort</p> 
<p> Since each element ultimately ends up in the correct position, the 
algorithm correctly sorts. But how long does it take? &nbsp;</p> 
<p> The best case for <em>divide-and-conquer</em> algorithms comes when we 
split the input as evenly as possible. Thus in the best case, each subproblem 
is of size<i>n</i>/2. </p> 
<p> The partition step on each subproblem is linear in its size. Thus the 
total effort in partitioning the problems of size  is <i>O</i>(<i>n</i>). </p> 
<p> The recursion tree for the best case looks like this: </p> 
<p> </p> <br>
 The total partitioning on each level is <i>O</i>( <i>n</i>), 
and it take levels of perfect partitions to get to single element subproblems. 
When we are down to single elements, the problems are sorted. Thus the total 
time in the best case is . 
<p> Listening to Part 5-8 </p> 
<p> </p> 
<p>Worst Case for Quicksort</p> 
<p> Suppose instead our pivot element splits the array as unequally as 
possible. Thus instead of<i>n</i>/2 elements in the smaller half, we get zero, 
meaning that the pivot element is the biggest or smallest element in the array.
</p> 
<p> </p> <br>
 Now we have <em>n-1</em> levels, instead of  , for a worst case 
time of , since the first <i>n</i>/2 levels each have  elements to partition. 
<p> Thus the worst case time for Quicksort is worse than Heapsort or Mergesort.
</p> 
<p> To justify its name, Quicksort had better be good in the average case. 
Showing this requires some fairly intricate analysis.</p> 
<p> The divide and conquer principle applies to real life. If you will break a 
job into pieces, it is best to make the pieces of equal size!</p> 
<p> Listening to Part 5-9 </p> 
<p> </p> 
<p>Intuition: The Average Case for Quicksort</p> 
<p> Suppose we pick the pivot element at random in an array of <em>n</em> keys.
</p> 
<p> </p> <br>
 Half the time, the pivot element will be from the center half 
of the sorted array.
<p> Whenever the pivot element is from positions <i>n</i>/4 to 3<i>n</i>/4, 
the larger remaining subarray contains at most 3<i>n</i>/4 elements. </p> 
<p> If we assume that the pivot element is always in this range, what is the 
maximum number of partitions we need to get from<em>n</em> elements down to 1 
element?</p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> Listening to Part 5-10 </p> 
<p> </p> 
<p>What have we shown?</p> 
<p> At most  levels of <em>decent partitions</em> suffices to sort an array of 
<em>n</em> elements. &nbsp; </p> 
<p> But how often when we pick an arbitrary element as pivot will it generate 
a decent partition?</p> 
<p> Since any number ranked between <i>n</i>/4 and 3<i>n</i>/4 would make a 
decent pivot, we get one half the time on average.</p> 
<p> If we need  levels of decent partitions to finish the job, and half of 
random partitions are decent, then on average the recursion tree to quicksort 
the array has levels. </p> 
<p> </p> <br>
 Since <i>O</i>( <i>n</i>) work is done partitioning on each 
level, the average time is . 
<p> More careful analysis shows that the expected number of comparisons is  . 
</p> 
<p> Listening to Part 5-11 </p> 
<p> </p> 
<p>Average-Case Analysis of Quicksort</p> 
<p> To do a precise average-case analysis of quicksort, we formulate a 
recurrence given the exact expected time<i>T</i>(<i>n</i>): </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> Each possible pivot <em>p</em> is selected with equal probability. The 
number of comparisons needed to do the partition is<em>n-1</em>. &nbsp; </p> 
<p> We will need one useful fact about the Harmonic numbers  , namely </p> 
<p> </p> 
<p> </p> 
<p> It is important to understand (1) where the recurrence relation comes from 
and (2) how the log comes out from the summation. The rest is just messy 
algebra.</p> 
<p> Listening to Part 5-12 </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> rearranging the terms give us: </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> substituting  gives </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> We are really interested in <i>A</i>(<i>n</i>), so </p> 
<p> </p> 
<p> </p> 
<p> Listening to Part 5-13 </p> 
<p> </p> 
<p>What <em>is</em> the Worst Case?</p> 
<p> The worst case for Quicksort depends upon how we select our partition or 
pivot element. If we always select either the first or last element of the 
subarray, the worst-case occurs when the input is already sorted!</p> 
<p> A B D F H J K </p> 
<p> B D F H J K </p> 
<p> D F H J K </p> 
<p> F H J K </p> 
<p> H J K </p> 
<p> J K </p> 
<p> K </p> 
<p> Having the worst case occur when they are sorted or almost sorted is <em>
very bad</em>, since that is likely to be the case in certain applications. </p>
<p> To eliminate this problem, pick a better pivot: </p> 
<p> </p> 
<ol> 
<li> Use the middle element of the subarray as pivot.</li> 
<li> Use a <em>random</em> element of the array as the pivot.</li> 
<li> Perhaps best of all, take the median of three elements (first, last, 
middle) as the pivot. Why should we use median instead of the mean?
<p> </p></li> </ol> 
<p> Whichever of these three rules we use, the worst case remains  . However, 
because the worst case is no longer a natural order it is much more difficult 
to occur.</p> 
<p> Listening to Part 5-14 </p> 
<p> </p> 
<p>Is Quicksort really faster than Heapsort?</p> 
<p> Since Heapsort is  and selection sort is  , there is no debate about which 
will be better for decent-sized files. &nbsp;</p> 
<p> But how can we compare two  algorithms to see which is faster? Using the 
RAM model and the big Oh notation, we can't!</p> 
<p> When Quicksort is implemented well, it is typically 2-3 times faster than 
mergesort or heapsort. The primary reason is that the operations in the 
innermost loop are simpler. The best way to see this is to implement both and 
experiment with different inputs.</p> 
<p> Since the difference between the two programs will be limited to a 
multiplicative constant factor, the details of how you program each algorithm 
will make a big difference.</p> 
<p> If you don't want to believe me when I say Quicksort is faster, I won't 
argue with you. It is a question whose solution lies outside the tools we are 
using.</p> 
<p> Listening to Part 5-15 </p> 
<p> </p> 
<p>Randomization</p> 
<p> Suppose you are writing a sorting program, to run on data given to you by 
your worst enemy. Quicksort is good on average, but bad on certain worst-case 
instances. &nbsp;</p> 
<p> If you used Quicksort, what kind of data would your enemy give you to run 
it on? Exactly the worst-case instance, to make you look bad.</p> 
<p> But instead of picking the median of three or the first element as pivot, 
suppose you picked the pivot element at<em>random</em>. </p> 
<p> Now your enemy cannot design a worst-case instance to give to you, because 
no matter which data they give you, you would have the same probability of 
picking a good pivot!</p> 
<p> Randomization is a very important and useful idea. By either picking a 
random pivot or scrambling the permutation before sorting it, we can say:</p> 
<p> </p> 
<blockquote> ``With high probability, randomized quicksort runs in  time.'' 
</blockquote> 
<p> Where before, all we could say is: </p> 
<p> </p> 
<blockquote> ``If you give me random input data, quicksort runs in expected  
time.''</blockquote> 
<p> Since the time bound how does not depend upon your input distribution, 
this means that unless we are<em>extremely</em> unlucky (as opposed to ill 
prepared or unpopular) we will certainly get good performance.</p> 
<p> Randomization is a general tool to improve algorithms with bad worst-case 
but good average-case complexity.</p> 
<p> The worst-case is still there, but we almost certainly won't see it. </p> 
<p> </p> 
<p> </p> <b>Next:</b> Lecture 6 - linear <b>Up:</b> Table of contents <b>
Previous:</b> Lecture 4 - heapsort 
<h1>Lecture 6 - linear sorting</h1> 
<p> Listening to Part 6-1 </p> 
<p> <em> 7.1-2: Show that an <em>n</em>-element heap has height  . </em> </p> 
Since it is balanced binary tree, the height of a heap is clearly , but the 
problem asks for an exact answer. &nbsp;
<p> The height is defined as the number of edges in the longest simple path 
from the root.</p> 
<p> </p> <br>
 The number of nodes in a complete balanced binary tree of height
<em>h</em> is  . 
<p> Thus the height increases only when  , or in other words when  is an 
integer.</p> 
<p> Listening to Part 6-2 </p> 
<p> <em>7.1-5 Is a reverse sorted array a heap?</em> </p> In a heap, each 
element is greater than or equal to each of its descendants.
<p> In the array representation of a heap, the descendants of the <em>i</em>th 
element are the<em>2i</em>th and (2<i>i</i>+1)th elements. </p> 
<p> If <em>A</em> is sorted in reverse order, then  implies that  . </p> 
<p> Since 2<i>i</i> &gt; <i>i</i> and 2<i>i</i>+1 &gt; <i>i</i> then  and  . 
</p> 
<p> Thus by definition <em>A</em> is a heap! </p> 
<p> Listening to Part 6-3 </p> 
<p> </p> 
<p>Can we sort in better than  ?</p> 
<p> Any comparison-based sorting program can be thought of as defining a 
decision tree of possible executions. &nbsp;</p> 
<p> Running the same program twice on the same permutation causes it to do 
exactly the same thing, but running it on different permutations of the same 
data causes a different sequence of comparisons to be made on each.</p> 
<p> </p> <br>
 Claim: the height of this decision tree is the worst-case 
complexity of sorting. &nbsp;
<p> Listening to Part 6-4 </p> 
<p> Once you believe this, a lower bound on the time complexity of sorting 
follows easily. &nbsp;</p> 
<p> Since any two different permutations of <em>n</em> elements requires a 
different sequence of steps to sort, there must be at least<i>n</i>! different 
paths from the root to leaves in the decision tree, ie. at least<i>n</i>! 
different leaves in the tree.</p> 
<p> Since only binary comparisons (less than or greater than) are used, the 
decision tree is a binary tree.</p> 
<p> Since a binary tree of height <em>h</em> has at most  leaves, we know  , or
 .</p> 
<p> By inspection  , since the last <i>n</i>/2 terms of the product are each 
greater than<i>n</i>/2. By Sterling's approximation, a better bound is  where 
<i>e</i>=2.718. </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> Listening to Part 6-5 </p> 
<p> </p> 
<p>Non-Comparison-Based Sorting</p> 
<p> All the sorting algorithms we have seen assume binary comparisons as the 
basic primative, questions of the form ``is<em>x</em> before <em>y</em>?''. 
&nbsp;</p> 
<p> Suppose you were given a deck of playing cards to sort. Most likely you 
would set up 13 piles and put all cards with the same number in one pile.</p> 
<p> A 2 3 4 5 6 7 8 9 10 J Q K </p> 
<p> A 2 3 4 5 6 7 8 9 10 J Q K </p> 
<p> A 2 3 4 5 6 7 8 9 10 J Q K </p> 
<p> A 2 3 4 5 6 7 8 9 10 J Q K </p> 
<p> With only a constant number of cards left in each pile, you can use 
insertion sort to order by suite and concatenate everything together.</p> 
<p> If we could find the correct pile for each card in constant time, and each 
pile gets<i>O</i>(1) cards, this algorithm takes <i>O</i>(<i>n</i>) time. </p> 
<p> Listening to Part 6-6 </p> 
<p> </p> 
<p>Bucketsort</p> 
<p> Suppose we are sorting <em>n</em> numbers from <em>1</em> to <em>m</em>, 
where we know the numbers are approximately uniformly distributed. &nbsp;</p> 
<p> We can set up <em>n</em> buckets, each responsible for an interval of <i>m
</i>/<i>n</i> numbers from <em>1</em> to <em>m</em> </p> 
<p> </p> <br>
 Given an input number <em>x</em>, it belongs in bucket number  .
<p> If we use an array of buckets, each item gets mapped to the right bucket in
<i>O</i>(1) time. </p> 
<p> With uniformly distributed keys, the expected number of items per bucket 
is 1. Thus sorting each bucket takes<i>O</i>(1) time! </p> 
<p> The total effort of bucketing, sorting buckets, and concatenating the 
sorted buckets together is<i>O</i>(<i>n</i>). </p> 
<p> What happened to our  lower bound! </p> 
<p> Listening to Part 6-7 </p> 
<p> We can use bucketsort effectively whenever we understand the distribution 
of the data.</p> 
<p> However, bad things happen when we assume the wrong distribution. </p> 
<p> Suppose in the previous example all the keys happened to be 1. After the 
bucketing phase, we have:</p> 
<p> </p> <br>
 We spent linear time distributing our items into buckets and 
learned<em>nothing</em>. Perhaps we could split the big bucket recursively, but 
it is not certain that we will ever win unless we understand the distribution.
<p> Problems like this are why we worry about the worst-case performance of 
algorithms!</p> 
<p> Such distribution techniques can be used on strings instead of just 
numbers. The buckets will correspond to letter ranges instead of just number 
ranges.</p> 
<p> The worst case ``shouldn't'' happen if we understand the distribution of 
our data.</p> 
<p> Listening to Part 6-8 </p> 
<p> </p> 
<p>Real World Distributions</p> 
<p> Consider the distribution of names in a telephone book. &nbsp; </p> 
<p> </p> 
<ul> 
<li> Will there be a lot of Skiena's?</li> 
<li> Will there be a lot of Smith's?</li> 
<li> Will there be a lot of Shifflett's? 
<p> </p></li> </ul> 
<p> Either make <em>sure</em> you understand your data, or use a good 
worst-case or randomized algorithm!</p> 
<p> </p> 
<p>The Shifflett's of Charlottesville</p> 
<p> For comparison, note that there are seven Shifflett's (of various 
spellings) in the 1000 page Manhattan telephone directory. &nbsp;</p> 
<p> </p> <br>
Listening to Part 6-10 
<p> </p> 
<p>Rules for Algorithm Design</p> 
<p> The secret to successful algorithm design, and problem solving in general, 
is to make sure you ask the right questions. Below, I give a possible series of 
questions for you to ask yourself as you try to solve difficult algorithm 
design problems: &nbsp; &nbsp;</p> 
<p> </p> 
<ol> 
<li> Do I really understand the problem? 
<p> </p> 
<ol> 
<li> What exactly does the input consist of?</li> 
<li> What exactly are the desired results or output?</li> 
<li> Can I construct some examples small enough to solve by hand? What happens 
when I solve them?</li> 
<li> Are you trying to solve a numerical problem? A graph algorithm problem? A 
geometric problem? A string problem? A set problem? Might your problem be 
formulated in more than one way? Which formulation seems easiest?
<p> </p></li> </ol></li> 
<li> Can I find a simple algorithm for the problem? 
<p> </p> 
<ol> 
<li> Can I find the solve my problem exactly by searching all subsets or 
arrangements and picking the best one?
<p> </p> 
<ol> 
<li> If so, why am I sure that this algorithm always gives the correct answer?
</li> 
<li> How do I measure the quality of a solution once I construct it? 
<p> Listening to Part 6-11</p></li> 
<li> Does this simple, slow solution run in polynomial or exponential time?
</li> 
<li> If I can't find a slow, <em>guaranteed</em> correct algorithm, am I sure 
that my problem is well defined enough to permit a solution?</li> </ol></li> 
<li> Can I solve my problem by repeatedly trying some heuristic rule, like 
picking the biggest item first? The smallest item first? A random item first?
<ol> 
<li> If so, on what types of inputs does this heuristic rule work well? Do 
these correspond to the types of inputs that might arise in the application?
</li> 
<li> On what types of inputs does this heuristic rule work badly? If no such 
examples can be found, can I show that in fact it always works well?</li> 
<li> How fast does my heuristic rule come up with an answer? 
<p> </p></li> </ol></li> </ol></li> 
<li> Are there special cases of this problem I know how to solve exactly? 
<p> </p> 
<ol> 
<li> Can I solve it efficiently when I ignore some of the input parameters?
</li> 
<li> What happens when I set some of the input parameters to trivial values, 
such as 0 or 1?
<p> Listening to Part 6-12</p></li> 
<li> Can I simplify the problem to create a problem I can solve efficiently? 
How simple do I have to make it?</li> 
<li> If I can solve a certain special case, why can't this be generalized to a 
wider class of inputs?
<p> </p></li> </ol></li> 
<li> Which of the standard algorithm design paradigms seem most relevant to 
the problem?
<p> </p> 
<ol> 
<li> Is there a set of items which can be sorted by size or some key? Does 
this sorted order make it easier to find what might be the answer?</li> 
<li> Is there a way to split the problem in two smaller problems, perhaps by 
doing a binary search, or a partition of the elements into big and small, or 
left and right? If so, does this suggest a divide-and-conquer algorithm?</li> 
<li> Are there certain operations being repeatedly done on the same data, such 
as searching it for some element, or finding the largest/smallest remaining 
element? If so, can I use a data structure of speed up these queries, like hash 
tables or a heap/priority queue?
<p> </p></li> </ol></li> 
<li> Am I still stumped? 
<p> </p> 
<ol> 
<li> Why don't I go back to the beginning of the list and work through the 
questions again? Do any of my answers from the first trip change on the second?
<p> </p></li> </ol></li> </ol> <b>Next:</b> Lecture 7 - elementary <b>Up:</b> 
Table of contents <b>Previous:</b> Lecture 5 - quicksort 
<h1>Lecture 7 - elementary data structures</h1> 
<p> Listening to Part 7-1 </p> 
<p> <em>8.2-3 Argue that insertion sort is better than Quicksort for sorting 
checks</em> </p> In the best case, Quicksort takes  . Although using 
median-of-three turns the sorted permutation into the best case, we lose if 
insertion sort is better on the given data. &nbsp;&nbsp;
<p> -- 5 </p> 
<p> In insertion sort, the cost of each insertion is the number of items which 
we have to jump over. In the check example, the expected number of moves per 
items is small, say<em>c</em>. We win if  . </p> 
<p> Listening to Part 7-2 </p> 
<p> <em>8.3-1 Why do we analyze the average-case performance of a randomized 
algorithm, instead of the worst-case?</em> </p> In a randomized algorithm, the 
worst case is not a matter of the input but only of luck. Thus we want to know 
what kind of luck to expect. Every input we see is drawn from the uniform 
distribution. &nbsp;
<p> Listening to Part 7-3 </p> 
<p> <em>8.3-2 How many calls are made to Random in randomized quicksort in the 
best and worst cases?</em> </p> Each call to random occurs once in each call to 
partition.
<p> The number of partitions is  in any run of quicksort!! </p> 
<p> </p> <br>
 There is some potential variation depending upon what you do 
with intervals of size<em>1</em> - do you call partition on intervals of size 
one? However, there is no asymptotic difference between best and worst case.
<p> The reason - any binary tree with <em>n</em> leaves has <em>n-1</em> 
internal nodes, each of which corresponds to a call to partition in the 
quicksort recursion tree.</p> 
<p> Listening to Part 7-4 </p> 
<p> </p> 
<p>Elementary Data Structures</p> 
<p> ``Mankind's progress is measured by the number of things we can do without 
thinking.''</p> 
<p> Elementary data structures such as stacks, queues, lists, and heaps will 
be the ``of-the-shelf'' components we build our algorithm from. There are two 
aspects to any data structure: &nbsp;</p> 
<p> </p> 
<ul> 
<li> The abstract operations which it supports.</li> 
<li> The implementation of these operations. 
<p> </p></li> </ul> 
<p> The fact that we can describe the behavior of our data structures in terms 
of abstract operations explains why we can use them without thinking, while the 
fact that we have different implementation of the same abstract operations 
enables us to optimize performance. &nbsp;</p> 
<p> Listening to Part 7-5 </p> 
<p> </p> 
<p>Stacks and Queues</p> 
<p> Sometimes, the order in which we retrieve data is independent of its 
content, being only a function of when it arrived. &nbsp;&nbsp;&nbsp;</p> 
<p> A <i>stack</i> supports last-in, first-out operations: push and pop. </p> 
<p> A <i>queue</i> supports first-in, first-out operations: enqueue and 
dequeue.</p> 
<p> A <i>deque</i> is a double ended queue and supports all four operations: 
push, pop, enqueue, dequeue.</p> 
<p> Lines in banks are based on queues, while food in my refrigerator is 
treated as a stack. &nbsp;</p> 
<p> Both can be used to traverse a tree, but the order is completely different.
</p> 
<p> </p> <br>
 Which order is better for WWW crawler robots? 
<p> Listening to Part 7-6 </p> 
<p> </p> 
<p>Stack Implementation</p> 
<p> Although this implementation uses an array, a linked list would eliminate 
the need to declare the array size in advance.</p> 
<p> </p> 
<pre> 
<p> STACK-EMPTY(S) </p>
<p> if top[S] = 0 </p>
<p> then return TRUE </p>
<p> else return FALSE </p>
<p> </p></pre> 
<p> </p> 
<pre> 
<p> PUSH(S, x) </p>
<p> </p>
<p> </p>
<p> </p></pre> 
<p> </p> 
<pre> 
<p> POP(S) </p>
<p> if STACK-EMPTY(S) </p>
<p> then error ``underflow'' </p>
<p> else </p>
<p> return <i>S</i>[<i>top</i>[<i>S</i>] + 1] </p>
<p> </p></pre> 
<p> </p> <br>
 All are <i>O</i>(1) time operations. 
<p> Listening to Part 7-7 </p> 
<p> </p> 
<p>Queue Implementation</p> 
<p> A circular queue implementation requires pointers to the head and tail 
elements, and wraps around to reuse array elements.</p> 
<p> </p> 
<pre> 
<p> ENQUEUE(Q, x) </p>
<p> Q[tail[Q]]  x </p>
<p> if tail[Q] = length[Q] </p>
<p> then tail[Q]  1 </p>
<p> else tail[Q]  tail[Q] + 1 </p>
<p> </p></pre> 
<p> </p> <br>

<p> </p> 
<pre> 
<p> DEQUEUE(Q) </p>
<p> x = Q[head[Q]] </p>
<p> if head[Q] = length[Q] </p>
<p> then head[Q] = 1 </p>
<p> else head[Q] = head[Q] + 1 </p>
<p> return x </p>
<p> </p></pre> 
<p> A list-based implementation would eliminate the possibility of overflow. 
</p> 
<p> All are <i>O</i>(1) time operations. </p> 
<p> Listening to Part 7-8 </p> 
<p> </p> 
<p>Dynamic Set Operations</p> 
<p> Perhaps the most important class of data structures maintain a set of 
items, indexed by keys. &nbsp;&nbsp;</p> 
<p> There are a variety of implementations of these <em>dictionary</em> 
operations, each of which yield different time bounds for various operations.
</p> 
<p> </p> 
<ul> 
<li> <em>Search(S,k)</em> - A query that, given a set S and a key value <em>k
</em>, returns a pointer <em>x</em> to an element in <em>S</em> such that <i>key
</i>[<i>x</i>] = <em>k</em>, or nil if no such element belongs to <em>S</em>.
</li> 
<li> <em>Insert(S,x)</em> - A modifying operation that augments the set <em>S
</em> with the element <em>x</em>.</li> 
<li> <em>Delete(S,x)</em> - Given a pointer <em>x</em> to an element in the set
<em>S</em>, remove <em>x</em> from <em>S</em>. Observe we are given a pointer 
to an element<em>x</em>, not a key value.</li> 
<li> <em>Min(S), Max(S)</em> - Returns the element of the totally ordered set 
<em>S</em> which has the smallest (largest) key.</li> 
<li> <em>Next(S,x), Previous(S,x)</em> - Given an element <em>x</em> whose key 
is from a totally ordered set<em>S</em>, returns the next largest (smallest) 
element in<em>S</em>, or NIL if <em>x</em> is the maximum (minimum) element. 
<p> </p></li> </ul> 
<p> Listening to Part 7-9 </p> 
<p> </p> 
<p>Pointer Based Implementation</p> 
<p> We can maintain a dictionary in either a singly or doubly linked list. 
&nbsp;&nbsp;</p> 
<p> </p> <br>
 We gain extra flexibility on predecessor queries at a cost of 
doubling the number of pointers by using doubly-linked lists.
<p> Since the extra big-Oh costs of doubly-linkly lists is zero, we will 
usually assume they are, although it might not be necessary.</p> 
<p> Singly linked to doubly-linked list is as a Conga line is to a Can-Can 
line.</p> 
<p> Lecture Sound../sounds/lec7-8a.au </p> 
<p> </p> 
<p>Array Based Sets</p> 
<p> </p> 
<p>Unsorted Arrays </p> 
<p> </p> 
<ul> 
<li> Search(S,k) - sequential search, <i>O</i>(<i>n</i>)</li> 
<li> Insert(S,x) - place in first empty spot, <i>O</i>(1)</li> 
<li> Delete(S,x) - copy <em>n</em>th item to the <em>x</em>th spot, <i>O</i>(1)
</li> 
<li> Min(S,x), Max(S,x) - sequential search, <i>O</i>(<i>n</i>)</li> 
<li> Successor(S,x), Predecessor(S,x) - sequential search, <i>O</i>(<i>n</i>)
</li> </ul> 
<p> Listening to Part 7-10 </p> 
<p>Sorted Arrays </p> 
<p> </p> 
<ul> 
<li> Search(S,k) - binary search, </li> 
<li> Insert(S,x) - search, then move to make space, <i>O</i>(<i>n</i>)</li> 
<li> Delete(S,x) - move to fill up the hole, <i>O</i>(<i>n</i>)</li> 
<li> Min(S,x), Max(S,x) - first or last element, <i>O</i>(1)</li> 
<li> Successor(S,x), Predecessor(S,x) - Add or subtract 1 from pointer, <i>O
</i>(1)</li> </ul> 
<p> What are the costs for a heap? </p> 
<p> Listening to Part 7-11 </p> 
<p> </p> 
<p>Unsorted List Implementation</p> 
<p> </p> 
<pre> 
<p> LIST-SEARCH(L, k) </p>
<p> <em>x</em> = head[L] </p>
<p> while <i>x</i> &lt;&gt; <i>NIL</i> and <i>key</i>[<i>x</i>] &lt;&gt; <i>k
</i> </p>
<p> do <em>x</em> = next[x] </p>
<p> return <em>x</em> </p>
<p> </p></pre> 
<p> Note: the while loop might require two lines in some programming languages.
</p> 
<p> </p> <br>

<p> </p> 
<pre> 
<p> LIST-INSERT(L, x) </p>
<p> next[x] = head[L] </p>
<p> if head[L] &lt;&gt; NIL </p>
<p> then prev[head[L]] = x </p>
<p> head[L] = x </p>
<p> prev[x] = NIL </p>
<p> </p></pre> 
<p> </p> 
<pre> 
<p> LIST-DELETE(L, x) </p>
<p> if <i>prev</i>[<i>x</i>] &lt;&gt; <i>NIL</i> </p>
<p> then next[prev[x]] = next[x] </p>
<p> else head[L] = next[x] </p>
<p> if <i>next</i>[<i>x</i>] &lt;&gt; <i>NIL</i> </p>
<p> then prev[next[x]] = prev[x] </p>
<p> </p></pre> 
<p> </p> 
<p> </p> 
<p>Sentinels</p> 
<p> Boundary conditions can be eliminated using a sentinel element which 
doesn't go away. &nbsp;&nbsp;</p> 
<p> </p> <br>

<p> </p> 
<pre> 
<p> LIST-SEARCH'(L, k) </p>
<p> <em>x</em> = next[nil[L]] </p>
<p> while <i>x</i> &lt;&gt; <i>NIL</i>[<i>L</i>] and <i>key</i>[<i>x</i>] 
&lt;&gt;<i>k</i> </p>
<p> do <em>x</em> = next[x] </p>
<p> return <em>x</em> </p>
<p> </p></pre> 
<p> </p> 
<pre> 
<p> LIST-INSERT'(L, x) </p>
<p> next[x] = next[nil[L]] </p>
<p> prev[next[nil[L]]] = x </p>
<p> next[nil[L]] = x </p>
<p> prev[x] = NIL[L] </p>
<p> </p></pre> 
<p> </p> 
<pre> 
<p> LIST-DELETE'(L, x) </p>
<p> next[prev[x]] &lt;&gt; next[x] </p>
<p> next[prev[x]] = prev[x] </p>
<p> </p></pre> 
<p> Listening to Part 7-13 </p> 
<p> </p> 
<p>Hash Tables</p> 
<p> Hash tables are a <em>very practical</em> way to maintain a dictionary. As 
with bucket sort, it assumes we know that the distribution of keys is fairly 
well-behaved. &nbsp;</p> 
<p> The idea is simply that looking an item up in an array is  once you have 
its index. A hash function is a mathematical function which maps keys to 
integers.</p> 
<p> In bucket sort, our hash function mapped the key to a bucket based on the 
first letters of the key. ``Collisions'' were the set of keys mapped to the 
same bucket.</p> 
<p> If the keys were uniformly distributed, then each bucket contains very few 
keys!</p> 
<p> The resulting short lists were easily sorted, and could just as easily be 
searched!</p> 
<p> </p> <br>
Listening to Part 7-14 
<p> </p> 
<p>Hash Functions</p> 
<p> It is the job of the hash function to map keys to integers. A good hash 
function: &nbsp;</p> 
<ol> 
<li> Is cheap to evaluate</li> 
<li> Tends to use all positions from  with uniform frequency.</li> 
<li> Tends to put similar keys in different parts of the tables (Remember the 
Shifletts!!)</li> </ol> 
<p> The first step is usually to map the key to a big integer, for example </p>
<p> </p> 
<p> </p> 
<p> </p> 
<p> This large number must be reduced to an integer whose size is between 1 
and the size of our hash table.</p> 
<p> One way is by  , where <em>M</em> is best a large prime not too close to  
, which would just mask off the high bits.</p> 
<p> This works on the same principle as a roulette wheel! </p> 
<p> Listening to Part 7-15 </p> 
<p> </p> 
<p>Good and Bad Hash functions</p> 
<p> The first three digits of the Social Security Number &nbsp; </p> 
<p> </p> <br>
 The last three digits of the Social Security Number 
<p> </p> <br>
Listening to Part 7-16 
<p> </p> 
<p>The Birthday Paradox</p> 
<p> No matter how good our hash function is, we had better be prepared for 
collisions, because of the birthday paradox. &nbsp;</p> 
<p> </p> <br>
 The probability of there being <em>no</em> collisions after <em>
n</em> insertions into an <em>m</em>-element table is 
<p> </p> 
<p> </p> 
<p> When <i>m</i> = 366, this probability sinks below 1/2 when <i>N</i> = 23 
and to almost 0 when . </p> 
<p> </p> <br>
Listening to Part 7-17 
<p> </p> 
<p>Collision Resolution by Chaining</p> 
<p> The easiest approach is to let each element in the hash table be a pointer 
to a list of keys. &nbsp;</p> 
<p> </p> <br>
 Insertion, deletion, and query reduce to the problem in linked 
lists. If the<em>n</em> keys are distributed uniformly in a table of size <i>m
</i>/ <i>n</i>, each operation takes <i>O</i>( <i>m</i>/ <i>n</i>) time. 
<p> Chaining is easy, but devotes a considerable amount of memory to pointers, 
which could be used to make the table larger. Still, it is my preferred method.
</p> 
<p> Listening to Part 7-18 </p> 
<p> </p> 
<p>Open Addressing</p> 
<p> We can dispense with all these pointers by using an implicit reference 
derived from a simple function: &nbsp;</p> 
<p> </p> <br>
 If the space we want to use is filled, we can examine the 
remaining locations:
<ol> 
<li> Sequentially </li> 
<li> Quadratically </li> 
<li> Linearly </li> </ol> 
<p> The reason for using a more complicated science is to avoid long runs from 
similarly hashed keys.</p> 
<p> Deletion in an open addressing scheme is ugly, since removing one element 
can break a chain of insertions, making some elements inaccessible.</p> 
<p> Listening to Part 7-19 </p> 
<p> </p> 
<p>Performance on Set Operations</p> 
<p> With either chaining or open addressing: </p> 
<p> </p> 
<ul> 
<li> Search - <i>O</i>(1) expected, <i>O</i>(<i>n</i>) worst case</li> 
<li> Insert - <i>O</i>(1) expected, <i>O</i>(<i>n</i>) worst case</li> 
<li> Delete - <i>O</i>(1) expected, <i>O</i>(<i>n</i>) worst case</li> 
<li> Min, Max and Predecessor, Successor  expected and worst case </li> </ul> 
<p> Pragmatically, a hash table is often the best data structure to maintain a 
dictionary. However, we will not use it much in proving the efficiency of our 
algorithms, since the worst-case time is unpredictable.</p> 
<p> The best worst-case bounds come from balanced binary trees, such as 
red-black trees.</p> 
<p> </p> <b>Next:</b> Lecture 8 - binary <b>Up:</b> Table of contents <b>
Previous:</b> Lecture 6 - linear 
<h1>Lecture 8 - binary trees</h1> 
<p> Listening to Part 8-1 </p> 
<p> <em> 9.1-3 Show that there is no sorting algorithm which sorts at least  
instances in<i>O</i>(<i>n</i>) time.</em> </p> Think of the decision tree which 
can do this. &nbsp;&nbsp; What is the shortest tree with leaves? 
<p> </p> <br>

<p> </p> 
<p> </p> 
<p> Moral: there cannot be too many good cases for any sorting algorithm! </p> 
<p> Listening to Part 8-2 </p> 
<p> <em> 9.1-4 Show that the  lower bound for sorting still holds with ternary 
comparisons.</em> </p> <br>
 The maximum number of leaves in a tree of height h 
is , 
<p> </p> 
<p> </p> 
<p> </p> 
<p> So it goes for any constant base. </p> 
<p> Listening to Part 8-3 </p> 
<p> </p> 
<p>Binary Search Trees</p> 
<p> </p> 
<p> ``I think that I shall never see<br>
 a poem as lovely as a tree Poem's<br>
 are wrote by fools like me but only<br>
 G-d can make a tree ``<br>
 - Joyce 
Kilmer<br>
</p> 
<p> </p> 
<p> Binary search trees provide a data structure which efficiently supports 
all six dictionary operations. &nbsp;&nbsp;</p> 
<p> A binary tree is a rooted tree where each node contains at most two 
children.</p> 
<p> Each child can be identified as either a left or right child. </p> 
<p> </p> <br>
 A binary tree can be implemented where each node has <em>left
</em> and <em>right</em> pointer fields, an (optional) <em>parent</em> pointer, 
and a data field.
<p> Listening to Part 8-4 </p> 
<p> </p> 
<p>Binary Search Trees</p> 
<p> A binary <em>search</em> tree labels each node in a binary tree with a 
single key such that for any node<em>x</em>, and nodes in the left subtree of 
<em>x</em> have keys  and all nodes in the right subtree of <em>x</em> have 
key's . </p> 
<p> </p> <br>
 Left: A binary search tree. Right: A heap but not a binary 
search tree.
<p> The search tree labeling enables us to find where any key is. Start at the 
root - if that is not the one we want, search either left or right depending 
upon whether what we want is or  then the root. </p> 
<p> Listening to Part 8-5 </p> 
<p> </p> 
<p>Searching in a Binary Tree</p> 
<p> Dictionary search operations are easy in binary trees ... </p> 
<p> </p> 
<pre> 
<p> TREE-SEARCH(x, k) </p>
<p> if (<i>x</i> = <i>NIL</i>) and (<i>k</i> = <i>key</i>[<i>x</i>]) </p>
<p> then return x </p>
<p> if (<i>k</i> &lt; <i>key</i>[<i>x</i>]) </p>
<p> then return TREE-SEARCH(left[x],k) </p>
<p> else return TREE-SEARCH(right[x],k) </p>
<p> </p></pre> 
<p> The algorithm works because both the left and right subtrees of a binary 
search tree<em>are</em> binary search trees - recursive structure, recursive 
algorithm.</p> 
<p> This takes time proportional to the height of the tree, <i>O</i>(<i>h</i>).
</p> 
<p> Listening to Part 8-6 </p> 
<p> </p> 
<p>Maximum and Minimum</p> 
<p> Where are the maximum and minimum elements in a binary tree? &nbsp; </p> 
<p> </p> <br>

<p> </p> 
<pre> 
<p> TREE-MAXIMUM(X) </p>
<p> while </p>
<p> do x = right[x] </p>
<p> return x </p>
<p> </p></pre> 
<p> </p> 
<pre> 
<p> TREE-MINIMUM(x) </p>
<p> while </p>
<p> do x = left[x] </p>
<p> return x </p>
<p> </p></pre> 
<p> Both take time proportional to the height of the tree, <i>O</i>(<i>h</i>). 
</p> 
<p> Listening to Part 8-7 </p> 
<p> </p> 
<p>Where is the predecessor?</p> 
<p> Where is the predecessor of a node in a tree, assuming all keys are 
distinct? &nbsp;&nbsp;</p> 
<p> </p> <br>
 If <em>X</em> has two children, its predecessor is the maximum 
value in its left subtree and its successor the minimum value in its right 
subtree.
<p> Listening to Part 8-8 </p> 
<p> </p> 
<p>What if a node doesn't have children?</p> 
<p> </p> <br>
 If it does not have a left child, a node's predecessor is its 
first left ancestor.
<p> The proof of correctness comes from looking at the in-order traversal of 
the tree.</p> 
<p> </p> 
<pre> 
<p> Tree-Successor(<em>x</em>) </p>
<p> if </p>
<p> then return Tree-Minimum(<i>right</i>[<i>x</i>]) </p>
<p> </p>
<p> while  and (<i>x</i> = <i>right</i>[<i>y</i>]) </p>
<p> do </p>
<p> </p>
<p> return <em>y</em> </p>
<p> </p></pre> 
<p> Tree predecessor/successor both run in time proportional to the height of 
the tree.</p> 
<p> Listening to Part 8-9 </p> 
<p> </p> 
<p>In-Order Traversal</p> 
<p> </p> <br>
 &nbsp; 
<p> </p> 
<pre> 
<p> Inorder-Tree-walk(<em>x</em>) </p>
<p> if (<i>x</i> &lt;&gt; <i>NIL</i>) </p>
<p> then Inorder-Tree-Walk(<i>left</i>[<i>x</i>]) </p>
<p> print <i>key</i>[<i>x</i>] </p>
<p> Inorder-Tree-walk(<i>right</i>[<i>x</i>]) </p>
<p> </p></pre> 
<p> A-B-C-D-E-F-G-H </p> 
<p> Listening to Part 8-10 </p> 
<p> </p> 
<p>Tree Insertion</p> 
<p> Do a binary search to find where it should be, then replace the 
termination NIL pointer with the new item. &nbsp;</p> 
<p> </p> <br>

<p> </p> 
<pre> 
<p> Tree-insert(<em>T,z</em>) </p>
<p> <em>y</em> = NIL </p>
<p> <i>x</i> = <i>root</i>[<i>T</i>] </p>
<p> while </p>
<p> do <i>y</i> = <i>x</i> </p>
<p> if <i>key</i>[<i>z</i>] &lt; <i>key</i>[<i>x</i>] </p>
<p> then <i>x</i> = <i>left</i>[<i>x</i>] </p>
<p> else <i>x</i> = <i>right</i>[<i>x</i>] </p>
<p> </p>
<p> if <em>y</em> = NIL </p>
<p> then </p>
<p> else if <i>key</i>[<i>z</i>] &lt; <i>key</i>[<i>y</i>] </p>
<p> then </p>
<p> else </p>
<p> </p></pre> 
<p> <em>y</em> is maintained as the parent of <em>x</em>, since <em>x</em> 
eventually becomes NIL.</p> 
<p> The final test establishes whether the NIL was a left or right turn from 
<em>y</em>. </p> 
<p> Insertion takes time proportional to the height of the tree, <i>O</i>(<i>h
</i>). </p> 
<p> Listening to Part 8-12 </p> 
<p> </p> 
<p>Tree Deletion</p> 
<p> Deletion is somewhat more tricky than insertion, because the node to die 
may not be a leaf, and thus effect other nodes. &nbsp;</p> 
<p> Case (a), where the node is a leaf, is simple - just NIL out the parents 
child pointer.</p> 
<p> Case (b), where a node has one chld, the doomed node can just be cut out. 
</p> 
<p> Case (c), relabel the node as its successor (which has at most one child 
when z has two children!) and delete the successor!</p> 
<p> This implementation of deletion assumes parent pointers to make the code 
nicer, but if you had to save space they could be dispensed with by keeping the 
pointers on the search path stored in a stack.</p> 
<p> </p> 
<pre> 
<p> Tree-Delete(<em>T,z</em>) </p>
<p> if (<i>left</i>[<i>z</i>] = <i>NIL</i>) or (<i>right</i>[<i>z</i>] = <i>NIL
</i>) </p>
<p> then </p>
<p> else  Tree-Successor(z) </p>
<p> if </p>
<p> then </p>
<p> else </p>
<p> if </p>
<p> then </p>
<p> if <i>p</i>[<i>y</i>] = <i>NIL</i> </p>
<p> then </p>
<p> else if (<i>y</i> = <i>left</i>[<i>p</i>[<i>y</i>]]) </p>
<p> then </p>
<p> else </p>
<p> if (<i>y</i> &lt;&gt; <i>z</i>) </p>
<p> then </p>
<p> /* If <em>y</em> has other fields, copy them, too. */ </p>
<p> return <em>y</em> </p>
<p> </p></pre> 
<p> Lines 1-3 determine which node <em>y</em> is physically removed. </p> 
<p> Lines 4-6 identify <em>x</em> as the non-nil decendant, if any. </p> 
<p> Lines 7-8 give <em>x</em> a new parent. </p> 
<p> Lines 9-10 modify the root node, if necessary </p> 
<p> Lines 11-13 reattach the subtree, if necessary. </p> 
<p> Lines 14-16 if the removed node is deleted, copy. </p> 
<p> Conclusion: deletion takes time proportional to the height of the tree. 
Listening to Part 8-13 </p> 
<p> </p> 
<p>Balanced Search Trees</p> 
<p> All six of our dictionary operations, when implemented with binary search 
trees, take<i>O</i>(<i>h</i>), where <em>h</em> is the height of the tree. 
&nbsp;</p> 
<p> The best height we could hope to get is  , if the tree was perfectly 
balanced, since</p> 
<p> </p> 
<p> But if we get unlucky with our order of insertion or deletion, we could 
get linear height!</p> 
<p> </p> 
<pre> 
<p> insert(<em>a</em>) </p>
<p> insert(<em>b</em>) </p>
<p> insert(<em>c</em>) </p>
<p> insert(<em>d</em>) </p>
<p> </p></pre> 
<p> </p> <br>
 In fact, random search trees on average have  height, but we 
are worried about worst case height.
<p> We can't easily use randomization - Why? </p> 
<p> Listening to Part 8-14 </p> 
<p> </p> 
<p>Perfectly Balanced Trees</p> 
<p> Perfectly balanced trees require a lot of work to maintain: </p> 
<p> </p> <br>
 If we insert the key 1, we must move every single node in the 
tree to rebalance it, taking time. 
<p> Therefore, when we talk about &quot;balanced&quot; trees, we mean trees 
whose height is , so all dictionary operations (insert, delete, search, 
min/max, successor/predecessor) take time. </p> 
<p> <em>Red-Black trees</em> are binary search trees where each node is 
assigned a color, where the coloring scheme helps us maintain the height as . 
</p> 
<p> Listening to Part 8-15 </p> 
<p> </p> 
<p>Red-Black Tree Definition</p> 
<p> Red-black trees have the following properties: &nbsp; </p> 
<p> </p> 
<ol> 
<li> Every node is colored either red or black.</li> 
<li> Every leaf (NIL pointer) is black.</li> 
<li> If a node is red then both its children are black.</li> 
<li> Every single path from a node to a decendant leaf contains the same 
number of black nodes.</li> </ol> 
<p> Listening to Part 8-16 </p> 
<p> </p> 
<p>What does this mean?</p> 
<p> If the root of a red-black tree is black can we just color it red? </p> 
<p> No! For one of its children might be red. </p> 
<p> </p> If an arbitrary node is red can we color it black? 
<p> No! Because now all nodes may not have the same black height. </p> 
<p> </p> <br>
What tree maximizes the number of nodes in a tree of black height
<em>h</em>? 
<p> </p> <br>
Listening to Part 8-17 
<p> </p> What does a red-black tree with two real nodes look like? 
<p> </p> <br>
 Not (1) - consecutive reds Not (2), (4) - Non-Uniform black 
height
<p> Listening to Part 8-18 </p> 
<p> </p> 
<p>Red-Black Tree Height</p> 
<p> <em>Lemma:</em> A red-black tree with <em>n</em> internal nodes has height 
at most . </p> 
<p> <em>Proof:</em> Our strategy; first we bound the number of nodes in any 
subtree, then we bound the height of any subtree.</p> 
<p> We claim that any subtree rooted at <em>x</em> has at least  - 1 <em>
internal</em> nodes, where <i>bh</i>(<i>x</i>) is the black height of node <em>x
</em>. </p> 
<p> Proof, by induction: </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> Now assume it is true for all tree with black height &lt; <i>bh</i>(<i>x
</i>). </p> 
<p> If <em>x</em> is black, both subtrees have black height <i>bh</i>(<i>x</i>
)-1. If<em>x</em> is red, the subtrees have black height <i>bh</i>(<i>x</i>). 
</p> 
<p> Therefore, the number of internal nodes in any subtree is </p> 
<p> </p> 
<p> </p> 
<p> Listening to Part 8-19 </p> 
<p> Now, let <em>h</em> be the height of our red-black tree. At least half the 
nodes on any single path from root to leaf must be black if we ignore the root.
</p> 
<p> Thus  and  , so  . </p> 
<p> This implies that  ,so  . height6pt width4pt</p> 
<p> </p> 
<p> Therefore red-black trees have height at most twice optimal. We have a 
balanced search tree if we can maintain the red-black tree structure under 
insertion and deletion.</p> 
<p> </p> <b>Next:</b> Lecture 9 - catch <b>Up:</b> Table of contents <b>
Previous:</b> Lecture 7 - elementary 
<h1>Lecture 9 - catch up</h1> 
<p> Listening to Part 9-1 </p> 
<p> <em>11-1 For each of the four types of linked lists in the following 
table, what is the asymptotic worst-case running time for each dynamic-set 
operation listed? &nbsp;</em> </p> singly  singly  doubly  doubly <br>
unsorted 
 sorted  unsorted  sorted <br>
Search(<em>L</em>, <em>k</em>)  O(N)  O(N)  O(N) 
 O(N)- <br>
 Insert(<em>L</em>, <em>x</em>)  O(1)  O(N)  O(1)  O(N)- <br>
 
Delete(<em>L</em>, <em>x</em>)  O(N)*  O(N)*  O(1)  O(1) <br>
 Successor(<em>L
</em>, <em>x</em>)  O(N)  O(1)  O(N)  O(1) <br>
 Predecessor(<em>L</em>, <em>x
</em>)  O(N)  O(N)  O(N)  O(1) <br>
 Minimum(<em>L</em>)  O(N)  O(1)  O(N)  O(1)
<br>  Maximum(<em>L</em>)  O(N)  O(1)+  O(N)  O(1)+ <br>
<br>

<p> </p> 
<ul> 
<li> I need a pointer to the predecessor! (*)</li> 
<li> I need a pointer to the tail! (+)</li> 
<li> Only bottlenecks in otherwise perfect dictionary! (-) </li> </ul> 
<p> </p> <b>Next:</b> Lecture 10 - tree <b>Up:</b> Table of contents <b>
Previous:</b> Lecture 8 - binary 
<h1>Lecture 10 - tree restructuring</h1> 
<p> Listening to Part 10-1 </p> 
<p> <em> 14.1-5 Describe a Red-Black tree with the largest and smallest ratio 
of red nodes.</em> </p> To minimize the ratio of red-black nodes, make all 
black &nbsp; (possible for ) 
<p> </p> <br>
 To maximize the ratio of red nodes, interleave with red nodes as
<em>real</em> leaves 
<p> </p> <br>

<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> Listening to Part 10-2 </p> 
<p> </p> 
<p>Rotations</p> 
<p> The basic restructuring step for binary search trees are left and right 
rotation: &nbsp;</p> 
<p> </p> <br>

<ol> 
<li> Rotation is a local operation changing <i>O</i>(1) pointers.</li> 
<li> An in-order search tree before a rotation <em>stays</em> an in-order 
search tree.</li> 
<li> In a rotation, one subtree gets one level closer to the root and one 
subtree one level further from the root.
<p> </p></li> </ol> 
<p> Lecture Sound../sounds/lec10-3.1.au </p> 
<p> </p> 
<pre> 
<p> LEFT-ROTATE(T,x) </p>
<p>  (* Set <em>y</em>*) </p>
<p>  (* Turn <em>y</em>'s left into <em>x</em>'s right*) </p>
<p> if <i>left</i>[<i>y</i>]= NIL </p>
<p> then </p>
<p>  (* Link x's parent to y *) </p>
<p> if <i>p</i>[<i>x</i>] = NIL </p>
<p> then </p>
<p> else if <i>x</i>= <i>left</i>[<i>p</i>[<i>x</i>]] </p>
<p> then </p>
<p> else </p>
<p> </p>
<p> </p>
<p> </p></pre> 
<p> Note the in-order property is preserved. </p> 
<p> Listening to Part 10-3 </p> 
<p> </p> <br>
Listening to Part 10-4 
<p> <em> 14.2-5 Show that any <em>n</em>-node tree can be transformed to any 
other using<i>O</i>(<i>n</i>) rotations (hint: convert to a right going chain). 
</em> </p> I will start by showing weaker bounds - that  and  rotations suffice 
- because that is how I proceeded when I first saw the problem.
<p> First, observe that creating a right-going, for  path from  &lt; and 
reversing the same construction gives a path from to  . </p> 
<p> Note that it will take at most <em>n</em> rotations to make the lowest 
valued key the root. Once it is root, all keys are to the right of it, so no 
more rotations need go through it to create a right-going chain. Repeating with 
the second lowest key, third, etc. gives that rotations suffice. </p> 
<p> Now that if we try to create a completely balanced tree instead. To get the
<i>n</i>/2 key to the root takes at most <em>n</em> rotations. Now each subtree 
has half the nodes and we can recur...</p> 
<p> </p> <br>

<p> Listening to Part 10-5 </p> 
<p> To get a linear algorithm, we must beware of trees like: </p> 
<p> </p> <br>
 The correct answer is that <em>n-1</em> rotations suffice to 
get to a rightmost chain.
<p> By picking the lowest node on the rightmost chain which has a left 
ancestor, we can add one node<em>per</em> rotation to the right most chain! </p>
<p> </p> <br>
 Initially, the rightmost chain contained at least 1 node, so 
after<em>1</em> rotations it contains all <em>n</em>. Slick! 
<p> Listening to Part 10-6 </p> 
<p> </p> 
<p>Red-Black Insertion</p> 
<p> Since red-black trees have  height, if we can preserve all properties of 
such trees under insertion/deletion, we have a balanced tree! &nbsp;</p> 
<p> Suppose we just did a regular insertion. Under what conditions does it 
stay a red-black tree?</p> 
<p> Since every insertion take places at a leaf, we will change a black NIL 
pointer to a node with two black NIL pointers.</p> 
<p> </p> <br>
 To preserve the black height of the tree, the new node must be 
<em>red</em>. If its <em>new</em> parent is black, we can stop, otherwise we 
must restructure!
<p> Listening to Part 10-7 </p> 
<p> </p> 
<p>How can we fix two reds in a row?</p> 
<p> It depends upon our uncle's color: </p> 
<p> </p> <br>
 If our uncle is red, reversing our relatives' color either 
solves the problem or pushes it higher!
<p> </p> <br>

<p> Note that after the recoloring: </p> 
<ol> 
<li> The black height is unchanged.</li> 
<li> The shape of the tree is unchanged.</li> 
<li> We are done if our great-grandparent is black. </li> </ol> 
<p> If we get all the way to the root, recall we can always color a red-black 
tree's root black. We always will, so initially it<em>was</em> black, and so 
this process terminates.</p> 
<p> Listening to Part 10-8 </p> 
<p> </p> 
<p>The Case of the Black Uncle</p> 
<p> If our uncle was black, observe that all the nodes around us have to be 
black:</p> 
<p> </p> <br>
 Solution - rotate right about B: 
<p> </p> <br>
 Since the root of the subtree is now black with the same 
black-height as before, we have restored the colors and can stop!
<p> Listening to Part 10-9 </p> 
<p> A double rotation can be required to set things up depending upon the 
left-right turn sequence, but the principle is the same.</p> 
<p> DOUBLE ROTATION ILLUSTRATION </p> 
<p> Listening to Part 10-10 </p> 
<p> </p> 
<p>Pseudocode and Figures</p> 
<p> Listening to Part 10-11 </p> 
<p> </p> 
<p>Deletion from Red-Black Trees</p> 
<p> Recall the three cases for deletion from a binary tree: &nbsp; </p> 
<p> Case (a) The node to be deleted was a leaf; </p> 
<p> </p> <br>
 Case (b) The node to be deleted had one child; 
<p> </p> <br>
 Case (c) relabel to node as its successor and delete the 
successor.
<p> </p> <br>
Listening to Part 10-12 
<p> </p> 
<p>Deletion Color Cases</p> 
<p> Suppose the node we remove was <em>red</em>, do we still have a red-black 
tree?</p> 
<p> <em>Yes!</em> No two reds will be together, and the black height for each 
leaf stays the same.</p> 
<p> However, if the dead node <em>y</em> was black, we must give each of its 
decendants another black ancestor. If an appropriate node is red, we can simply 
color it black otherwise we must restructure.</p> 
<p> Case (a) black NIL becomes ``double black''; </p> 
<p> Case (b) red  becomes black and black  becomes ``double black''; </p> 
<p> Case (c) red  becomes black and black  becomes ``double black''. </p> 
<p> Our goal will be to recolor and restructure the tree so as to get rid of 
the ``double black'' node.</p> 
<p> Listening to Part 10-13 </p> 
<p> In setting up any case analysis, we must be sure that: </p> 
<p> </p> 
<ol> 
<li> All possible cases are covered.</li> 
<li> No case is covered twice. </li> </ol> 
<p> In the case analysis for red-black trees, the breakdown is: </p> 
<p> Case 1: The double black node <em>x</em> has a red brother. </p> 
<p> Case 2: <em>x</em> has a black brother and two black nephews. </p> 
<p> Case 3: <em>x</em> has a black brother, and its left nephew is red and its 
right nephew is black.</p> 
<p> Case 4: <em>x</em> has a black brother, and its right nephew is red (left 
nephew can be any color).</p> 
<p> Listening to Part 10-14 </p> 
<p> </p> 
<p>Conclusion</p> 
<p> Red-Black trees let us implement all dictionary operations in  . Further, 
in no case are more than 3 rotations done to rebalance. Certain very advanced 
data structures have data stored at nodes which requires a lot of work to 
adjust after a rotation -- red-black trees ensure it won't happen often.</p> 
<p> <em>Example:</em> Each node represents the endpoint of a line, and is 
augmented with a list of segments in its subtree which it intersects.</p> 
<p> We will not study such complicated structures, however. </p> 
<p> </p> <b>Next:</b> Lecture 11 - backtracking <b>Up:</b> Table of contents 
<b>Previous:</b> Lecture 9 - catch 
<h1>Lecture 11 - backtracking</h1> 
<p> Listening to Part 11-1 </p> 
<p> </p> 
<p>Parallel Bubblesort</p> 
<p> In order for me to give back your midterms, please form a line and sort 
yourselves in alphabetical order, from A to Z. &nbsp;</p> 
<p> There is traditionally a strong correlation between the midterm grades and 
the number of daily problems attempted:</p> 
<p> daily: 0, sum: 134, count: 3, avg: 44.67 </p> 
<p> daily: 1, sum: 0, count: 2, avg: 0.00 </p> 
<p> daily: 2, sum: 63, count: 1, avg: 63.00 </p> 
<p> daily: 3, sum: 194, count: 3, avg: 64.67 </p> 
<p> daily: 4, sum: 335, count: 5, avg: 67.00 </p> 
<p> daily: 5, sum: 489, count: 8, avg: 61.12 </p> 
<p> daily: 6, sum: 381, count: 6, avg: 63.50 </p> 
<p> daily: 7, sum: 432, count: 6, avg: 72.00 </p> 
<p> daily: 8, sum: 217, count: 3, avg: 72.33 </p> 
<p> daily: 9, sum: 293, count: 4, avg: 73.25 </p> 
<p> Listening to Part 11-2 </p> 
<p> </p> 
<p>Combinatorial Search</p> 
<p> We have seen how clever algorithms can reduce sorting from  to  . However, 
the stakes are even higher for combinatorially explosive problems: &nbsp;</p> 
<p> </p> 
<p>The Traveling Salesman Problem</p> Given a weighted graph, find the 
shortest cycle which visits each vertex once. &nbsp;
<p> </p> <br>
 Applications include minimizing plotter movement, 
printed-circuit board wiring, transportation problems, etc.
<p> There is no known polynomial time algorithm (ie.  for some fixed <em>k</em>
) for this problem, so search-based algorithms are the only way to go if you 
need an optional solution.</p> 
<p> Listening to Part 11-3 </p> 
<p> </p> 
<p>But I want to use a Supercomputer</p> 
<p> Moving to a faster computer can only buy you a relatively small 
improvement: &nbsp;</p> 
<p> </p> 
<ul> 
<li> Hardware clock rates on the fastest computers only improved by a factor 
of 6 from 1976 to 1989, from 12ns to 2ns.</li> 
<li> Moving to a machine with 100 processors can only give you a factor of 100 
speedup, even if your job can be perfectly parallelized (but of course it 
can't).</li> 
<li> The fast Fourier algorithm (FFT) reduced computation from  to  . This is 
a speedup of 340 times on<i>n</i>=4096 and revolutionized the field of image 
processing. &nbsp;</li> 
<li> The fast multipole method for <em>n</em>-particle interaction reduced the 
computation from to <i>O</i>(<i>n</i>). This is a speedup of 4000 times on <i>n
</i>=4096. 
<p> </p></li> </ul> 
<p> Listening to Part 11-4 </p> 
<p> </p> 
<p>Can Eight Pieces Cover a Chess Board?</p> 
<p> Consider the 8 main pieces in chess (king, queen, two rooks, two bishops, 
two knights). Can they be positioned on a chessboard so every square is 
threatened? &nbsp;</p> 
<p> </p> <br>
 Only 63 square are threatened in this configuration. Since 
1849, no one had been able to find an arrangement with bishops on different 
colors to cover all squares.
<p> Of course, this is not an important problem, but we will use it as an 
example of how to attack a combinatorial search problem.</p> 
<p> Listening to Part 11-5 </p> 
<p> </p> 
<p>How many positions to test?</p> 
<p> Picking a square for each piece gives us the bound: </p> 
<p> </p> 
<p> Anything much larger than  is unreasonable to search on a modest computer 
in a modest amount of time. &nbsp;</p> 
<p> However, we can exploit symmetry to save work. With reflections along 
horizontal, vertical, and diagonal axis, the queen can go in only 10 
non-equivallent positions.</p> 
<p> Even better, we can restrict the white bishop to <em>16</em> spots and the 
queen to<em>16</em>, while being certain that we get all distinct 
configurations.</p> 
<p> </p> <br>

<p> </p> 
<p> </p> 
<p> Listening to Part 11-6 </p> 
<p> </p> 
<p>Backtracking</p> 
<p> Backtracking is a systematic way to go through all the possible 
configurations of a search space. &nbsp;</p> 
<p> In the general case, we assume our solution is a vector  where each element
 is selected from a finite ordered set , </p> 
<p> We build from a partial solution of length <em>k</em>  and try to extend 
it by adding another element. After extending it, we will test whether what we 
have so far is still possible as a partial solution.</p> 
<p> If it is still a candidate solution, great. If not, we delete  and try the 
next element from : </p> 
<p> </p> 
<pre> 
<p> Compute  , the set of candidate first elements of <em>v</em>. </p>
<p> <i>k</i> = 1 </p>
<p> While <i>k</i> &gt; 0 do </p>
<p> While  do (*advance*) </p>
<p>  = an element in </p>
<p> </p>
<p> if (  ) is solution, print! </p>
<p> <i>k</i> = <i>k</i> + 1 </p>
<p> compute  , the candidate <em>k</em>th elements given <em>v</em>. </p>
<p> <i>k</i> = <i>k</i> - 1 (*backtrack*) </p>
<p> </p></pre> 
<p> Listening to Part 11-7 </p> 
<p> </p> 
<p>Recursive Backtracking</p> 
<p> Recursion can be used for elegant and easy implementation of backtracking. 
&nbsp;</p> 
<p> </p> 
<pre> 
<p> Backtrack(a, k) </p>
<p> if a is a solution, print(a) </p>
<p> else { </p>
<p> <i>k</i> = <i>k</i> +1 </p>
<p> compute </p>
<p> while  do </p>
<p>  = an element in </p>
<p>  = </p>
<p> Backtrack(a, k) </p>
<p> } </p>
<p> </p></pre> 
<p> Backtracking can easily be used to iterate through all subsets or 
permutations of a set.</p> 
<p> Backtracking ensures correctness by enumerating all possibilities. </p> 
<p> For backtracking to be efficient, we must prune the search space. </p> 
<p> Listening to Part 11-8 </p> 
<p> </p> 
<p>Constructing all Subsets</p> 
<p> How many subsets are there of an <em>n</em>-element set? &nbsp; </p> 
<p> To construct all  subsets, set up an array/vector of <em>n</em> cells, 
where the value of is either true or false, signifying whether the <em>i</em>th 
item is or is not in the subset.</p> 
<p> To use the notation of the general backtrack algorithm,  , and <em>v</em> 
is a solution whenever . </p> 
<p> What order will this generate the subsets of  ? </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> Listening to Part 11-9 </p> 
<p> </p> 
<p>Constructing all Permutations</p> 
<p> How many permutations are there of an <em>n</em>-element set? &nbsp; </p> 
<p> To construct all <i>n</i>! permutations, set up an array/vector of <em>n
</em> cells, where the value of  is an integer from <em>1</em> to <em>n</em> 
which has not appeared thus far in the vector, corresponding to the<em>i</em>th 
element of the permutation.</p> 
<p> To use the notation of the general backtrack algorithm,  , and <em>v</em> 
is a solution whenever . </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p>The <em>n</em>-Queens Problem</p> 
<p> The first use of pruning to deal with the combinatorial explosion was by 
the king who rewarded the fellow who discovered chess! &nbsp;</p> 
<p> In the eight Queens, we prune whenever one queen threatens another. 
Listening to Part 11-11 </p> 
<p> </p> 
<p>Covering the Chess Board</p> 
<p> In covering the chess board, we prune whenever we find there is a square 
which we<em>cannot</em> cover given the initial configuration! </p> 
<p> Specifically, each piece can threaten a certain maximum number of squares 
(queen 27, king 8, rook 14, etc.) Whenever the number of unthreated squares 
exceeds the sum of the maximum number of coverage remaining in unplaced 
squares, we can<em>prune</em>. </p> 
<p> As implemented by a graduate student project, this backtrack search 
eliminates of the search space, when the pieces are ordered by decreasing 
mobility.</p> 
<p> With precomputing the list of possible moves, this program could search 
1,000 positions per second. But this is too slow!</p> 
<p> </p> 
<p> Although we might further speed the program by an order of magnitude, we 
need to prune more nodes!</p> 
<p> By using a more clever algorithm, we eventually were able to prove no 
solution existed, in less than one day's worth of computing.</p> 
<p> You too can fight the combinatorial explosion! </p> 
<p> Listening to Part 11-12 </p> 
<p> </p> 
<p>The Backtracking Contest: Bandwidth</p> 
<p> The <em>bandwidth problem</em> takes as input a graph <em>G</em>, with <em>
n</em> vertices and <em>m</em> edges (ie. pairs of vertices). The goal is to 
find a permutation of the vertices on the line which minimizes the maximum 
length of any edge. &nbsp; &nbsp;</p> 
<p> </p> <br>
 The bandwidth problem has a variety of applications, including 
circuit layout, linear algebra, and optimizing memory usage in hypertext 
documents.
<p> The problem is NP-complete, meaning that it is <em>exceedingly</em> 
unlikely that you will be able to find an algorithm with polynomial worst-case 
running time. It remains NP-complete even for restricted classes of trees.</p> 
<p> Since the goal of the problem is to find a permutation, a backtracking 
program which iterates through all the<i>n</i>! possible permutations and 
computes the length of the longest edge for each gives an easy algorithm. But 
the goal of this assignment is to find as practically good an algorithm as 
possible.</p> 
<p> Listening to Part 12-4 </p> 
<p> </p> 
<p>Rules of the Game</p> 
<p> </p> 
<ol> 
<li> Everyone must do this assignment separately. Just this once, you are not 
allowed to work with your partner. The idea is to think about the problem from 
scratch.</li> 
<li> If you do not completely understand what the bandwidth of a graph is, you 
don't have the<em>slightest</em> chance of producing a working program. <em>
Don't be afraid to ask for a clarification or explanation!!!!!</em></li> 
<li> There will be a variety of different data files of different sizes. Test 
on the smaller files first. Do not be afraid to create your own test files to 
help debug your program.</li> 
<li> The data files are available via the course WWW page.</li> 
<li> You will be graded on how fast and clever your program is, not on style. 
No credit will be given for incorrect programs.</li> 
<li> The programs are to run on the whatever computer you have access to, 
although it must be vanilla enough that I can run the program on something I 
have access to.</li> 
<li> You are to turn in a listing of your program, along with a brief 
description of your algorithm and any interesting optimizations, sample runs, 
and the time it takes on sample data files. Report the largest test file your 
program could handle in one minute or less of wall clock time.</li> 
<li> The top five self-reported times / largest sizes will be collected and 
tested by me to determine the winner.
<p> </p></li> </ol> 
<p> Listening to Part 12-5 </p> 
<p> </p> 
<p>Producing Efficient Programs</p> 
<p> </p> 
<ol> 
<li> <b>Don't optimize prematurely:</b> Worrying about recursion vs. iteration 
is counter-productive until you have worked out the best way to prune the tree. 
That is where the money is. &nbsp;</li> 
<li> <b>Choose your data structures for a reason:</b> What operations will you 
be doing? Is case of insertion/deletion more crucial than fast retrieval?
<p> When in doubt, keep it simple, stupid (KISS).</p></li> 
<li> <b>Let the profiler determine where to do final tuning:</b> Your program 
is probably spending time where you don't expect.
<p> </p></li> </ol> <b>Next:</b> Lecture 12 - introduction <b>Up:</b> Table of 
contents <b>Previous:</b> Lecture 10 - tree 
<h1>Lecture 12 - introduction to dynamic programming</h1> 
<p> Listening to Part 12-1 </p> 
<p> <em>15.1-5 Given an element <em>x</em> in an <em>n</em>-node 
order-statistic binary tree and a natural number<em>i</em>, how can the <em>i
</em>th successor of <em>x</em> be determined in  time. &nbsp; </em> </p> This 
problem can be solved if our data structure supports two operations:
<p> </p> 
<ul> 
<li> Rank(x) - what is the position of <em>x</em> in the total order of keys?
</li> 
<li> Get(i) - what is the key in the <em>i</em>th position of the total order 
of keys?
<p> </p></li> </ul> 
<p> What we are interested in is <i>Get</i>(<i>Rank</i>(<i>x</i>)+<i>i</i>). 
</p> 
<p> In an order statistic tree, each node <em>x</em> is labeled with the 
number of nodes contained in the subtree rooted in<em>x</em>. </p> 
<p> </p> <br>
 Implementing both operations involves keeping track of how many 
nodes lie to the left of our path.
<p> Listening to Part 12-6 </p> 
<p> </p> 
<p>Optimization Problems</p> 
<p> In the algorithms we have studied so far, correctness tended to be easier 
than efficiency. In optimization problems, we are interested in finding a<em>
thing</em> which maximizes or minimizes some function. &nbsp; </p> 
<p> In designing algorithms for optimization problem - we must prove that the 
algorithm in fact gives the best possible solution.</p> 
<p> <em>Greedy</em> algorithms, which makes the best local decision at each 
step, occasionally produce a global optimum - but you need a proof! &nbsp;</p> 
<p> </p> 
<p>Dynamic Programming</p> Dynamic Programming is a technique for computing 
recurrence relations efficiently by sorting partial results. &nbsp;
<p> Listening to Part 12-9 </p> 
<p> </p> 
<p>Computing Fibonacci Numbers</p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> Implementing it as a recursive procedure is easy but slow! &nbsp; </p> 
<p> We keep calculating the same value over and over! </p> 
<p> </p> <br>

<p>How slow is slow?</p> 
<p> Thus  , and since our recursion tree has <em>0</em> and <em>1</em> as 
leaves, means we have calls! </p> 
<p> Listening to Part 12-10 </p> 
<p> </p> 
<p>What about Dynamic Programming?</p> 
<p> We can calculate  in linear time by storing small values: </p> 
<p> </p> 
<pre> 
<p> </p>
<p> </p>
<p> For <i>i</i>=1 to <em>n</em> </p>
<p> </p></pre> 
<p> <em>Moral: we traded space for time.</em> </p> 
<p> Dynamic programming is a technique for efficiently computing recurrences 
by storing partial results.</p> 
<p> Once you understand dynamic programming, it is usually easier to reinvent 
certain algorithms than try to look them up!</p> 
<p> Dynamic programming is best understood by looking at a bunch of different 
examples.</p> 
<p> I have found dynamic programming to be one of the most useful algorithmic 
techniques in practice:</p> 
<p> </p> 
<ul> 
<li> Morphing in Computer Graphics</li> 
<li> Data Compression for High Density Bar Codes</li> 
<li> Utilizing Grammatical Constraints for Telephone Keypads 
<p> </p></li> </ul> 
<p> Listening to Part 12-11 </p> 
<p> </p> 
<p>Multiplying a Sequence of Matrices</p> 
<p> Suppose we want to multiply a long sequence of matrices  . &nbsp; </p> 
<p> Multiplying an  matrix by a  matrix (using the common algorithm) takes  
multiplications.</p> 
<p> </p> <br>
 We would like to avoid big intermediate matrices, and since 
matrix multiplication is<em>associative</em>, we can parenthesise however we 
want.
<p> Matrix multiplication is <em>not communitive</em>, so we cannot permute 
the order of the matrices without changing the result.</p> 
<p> Listening to Part 12-12 </p> 
<p> </p> 
<p>Example</p> 
<p> Consider  , where <em>A</em> is  , <em>B</em> is  , <em>C</em> is  , and 
<em>D</em> is  . </p> 
<p> There are three possible parenthesizations: </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> The order makes a big difference in real computation. How do we find the 
best order?</p> 
<p> Let <i>M</i>(<i>i</i>,<i>j</i>) be the <em>minimum</em> number of 
multiplications necessary to compute . </p> 
<p> The key observations are </p> 
<ul> 
<li> The outermost parentheses partition the chain of matricies (<i>i</i>,<i>j
</i>) at some <em>k</em>.</li> 
<li> The optimal parenthesization order has optimal ordering on either side of 
<em>k</em>. &nbsp; 
<p> </p></li> </ul> 
<p> Listening to Part 12-13 </p> 
<p> A recurrence for this is: </p> 
<p> </p> 
<p> </p> 
<p> If there are <em>n</em> matrices, there are <em>n+1</em> dimensions. </p> 
<p> A direct recursive implementation of this will be exponential, since there 
is a lot of duplicated work as in the Fibonacci recurrence.</p> 
<p> Divide-and-conquer is seems efficient because there is no overlap, but ... 
</p> 
<p> There are only  substrings between <em>1</em> and <em>n</em>. Thus it 
requires only space to store the optimal cost for each of them. </p> 
<p> We can represent all the possibilities in a triangle matrix. We can also 
store the value of<em>k</em> in another triangle matrix to reconstruct to order 
of the optimal parenthesisation.</p> 
<p> The diagonal moves up to the right as the computation progresses. On each 
element of the<em>k</em>th diagonal |<i>j</i>-<i>i</i>| = <i>k</i>. </p> 
<p> For the previous example: </p> 
<p> Listening to Part 13-3 </p> 
<p> </p> 
<pre> 
<p> Procedure MatrixOrder </p>
<p> for <i>i</i>=1 to <em>n</em> do <i>M</i>[<i>i</i>, <i>j</i>]=0 </p>
<p> for <i>diagonal</i>=1 to <em>n-1</em> </p>
<p> for <i>i</i>=1 to <em>n-diagonal</em> do </p>
<p> <i>j</i>=<i>i</i>+<i>diagonal</i> </p>
<p> </p>
<p> faster(<i>i</i>,<i>j</i>)=<i>k</i> </p>
<p> return [<i>m</i>(1, <i>n</i>)] </p>
<p> </p></pre> 
<p> </p> 
<pre> 
<p> Procedure ShowOrder(<i>i</i>, <i>j</i>) </p>
<p> if (<i>i</i>=<i>j</i>) write (  ) </p>
<p> else </p>
<p> <i>k</i>=factor(<i>i</i>, <i>j</i>) </p>
<p> write ``('' </p>
<p> ShowOrder(<i>i</i>, <i>k</i>) </p>
<p> write ``*'' </p>
<p> ShowOrder (<i>k</i>+1, <i>j</i>) </p>
<p> write ``)'' </p>
<p> </p></pre> 
<p> Listening to Part 13-4 </p> 
<p> </p> 
<p>A dynamic programming solution has three components:</p>  &nbsp; 
<p> </p> 
<ol> 
<li> Formulate the answer as a recurrence relation or recursive algorithm.</li>
<li> Show that the number of different instances of your recurrence is bounded 
by a polynomial.</li> 
<li> Specify an order of evaluation for the recurrence so you always have what 
you need.</li> </ol> 
<p> Listening to Part 13-5 </p> 
<p> </p> 
<p>Approximate String Matching</p> 
<p> A common task in text editing is string matching - finding all occurrences 
of a word in a text. &nbsp;&nbsp; &nbsp;</p> 
<p> Unfortunately, many words are mispelled. How can we search for the string 
closest to the pattern?</p> 
<p> Let <em>p</em> be a pattern string and <em>T</em> a text string over the 
same alphabet.</p> 
<p> A <em>k</em>-approximate match between <em>P</em> and <em>T</em> is a 
substring of<em>T</em> with at most <em>k</em> differences. </p> 
<p> Differences may be: </p> 
<ol> 
<li> the corresponding characters may differ: KAT  CAT</li> 
<li> <em>P</em> is missing a character from <em>T</em>: CAAT  CAT</li> 
<li> <em>T</em> is missing a character from <em>P</em>: CT  CAT </li> </ol> 
<p> Approximate Matching is important in genetics as well as spell checking. 
</p> 
<p> Listening to Part 13-6 </p> 
<p> </p> 
<p>A 3-Approximate Match</p> 
<p> A match with one of each of three edit operations is: </p> 
<p> </p> 
<p><em>P</em> = unescessaraly </p> 
<p> <em>T</em> = unnecessarily </p> 
<p> Finding such a matching seems like a hard problem because we must figure 
out where you add<em>blanks</em>, but we can solve it with dynamic programming. 
</p> 
<p> <i>D</i>[<i>i</i>, <i>j</i>] = the minimum number of differences between  
and the segment of<em>T</em> ending at <em>j</em>. </p> 
<p> <i>D</i>[<i>i</i>, <i>j</i>] is the <em>minimum</em> of the three possible 
ways to extend smaller strings:</p> 
<p> </p> 
<ol> 
<li> If  then <i>D</i>[<i>i</i>-1, <i>j</i>-1] else <i>D</i>[<i>i</i>-1, <i>j
</i>-1]+1 (corresponding characters do or do not match)</li> 
<li> <i>D</i>[<i>i</i>-1, <i>j</i>]+1 (extra character in text - we do not 
advance the pattern pointer).</li> 
<li> <i>D</i>[<i>i</i>, <i>j</i>-1]+1 (character in pattern which is not in 
text).</li> </ol> 
<p> Once you accept the recurrence it is easy. </p> 
<p> To fill each cell, we need only consider three other cells, not <i>O</i>(
<i>n</i>) as in other examples. This means we need only store two rows of the 
table. The total time is<i>O</i>(<i>mn</i>). </p> 
<p> Listening to Part 13-10 </p> 
<p> </p> 
<p>Boundary conditions for string matching</p> 
<p> What should the value of <i>D</i>[0,<i>i</i>] be, corresponding to the 
cost of matching the first<em>i</em> characters of the text with none of the 
pattern? &nbsp;</p> 
<p> It depends. Are we doing string matching in the text or substring matching?
</p> 
<p> </p> 
<ul> 
<li> If you want to match all of the pattern against all of the text, this 
meant that would have to delete the first<em>i</em> characters of the pattern, 
so<i>D</i>[0,<i>i</i>] = <i>i</i> to pay the cost of the deletions.</li> 
<li> if we want to find the place in the text where the pattern occurs? We do 
not want to pay more of a cost if the pattern occurs far into the text than 
near the front, so it is important that starting cost be equal for all 
positions. In this case,<i>D</i>[0,<i>i</i>] = 0, since we pay no cost for 
deleting the first<em>i</em> characters of the text. </li> </ul> 
<p> In both cases, <i>D</i>[<i>i</i>,0] = <i>i</i>, since we cannot excuse 
deleting the first<em>i</em> characters of the pattern without cost. </p> 
<p> Listening to Part 13-9 </p> 
<p> </p> 
<p>What do we return?</p> 
<p> If we want the <em>cost</em> of comparing all of the pattern against all 
of the text, such as comparing the spelling of two words, all we are interested 
in is<i>D</i>[<i>n</i>,<i>m</i>]. </p> 
<p> But what if we want the cheapest match between the pattern anywhere in the 
text? Assuming the initialization for substring matching, we seek the cheapest 
matching of the full pattern ending anywhere in the text. This means the cost 
equals . </p> 
<p> This only gives the cost of the optimal matching. The actual alignment - 
what got matched, substituted, and deleted - can be reconstructed from the 
pattern/text and table without an auxiliary storage, once we have identified 
the cell with the lowest cost.</p> 
<p> Listening to Part 13-11 </p> 
<p> </p> 
<p>How much space do we need?</p>  &nbsp; 
<p> Do we need to keep all <i>O</i>(<i>mn</i>) cells, since if we evaluate the 
recurrence filling in the columns of the matrix from left to right, we will 
never need more than two columns of cells to do what we need. Thus<i>O</i>(<i>m
</i>) space is sufficient to evaluate the recurrence without changing the time 
complexity at all.</p> 
<p> Unfortunately, because we won't have the full matrix we cannot reconstruct 
the alignment, as above.</p> 
<p> Saving space in dynamic programming is very important. Since memory on any 
computer is limited,<i>O</i>(<i>nm</i>) space is more of a bottleneck than <i>O
</i>(<i>nm</i>) time. </p> 
<p> Fortunately, there is a clever divide-and-conquer algorithm which computes 
the actual alignment in<i>O</i>(<i>nm</i>) time and <i>O</i>(<i>m</i>) space. 
</p> 
<p> </p> <b>Next:</b> Lecture 13 - dynamic <b>Up:</b> Table of contents <b>
Previous:</b> Lecture 11 - backtracking 
<h1>Lecture 13 - dynamic programming applications</h1> 
<p> Listening to Part 13-1 </p> 
<p> <em> 16.3-5 Give an  algorithm to find the longest montonically increasing 
sequence in a sequence of<em>n</em> numbers. &nbsp;&nbsp; </em> </p> Build an 
example first: (5, 2, 8, 7, 1, 6, 4)
<p> Ask yourself what would you like to know about the first <em>n-1</em> 
elements to tell you the answer for the entire sequence?</p> 
<p> </p> 
<ol> 
<li> The length of the longest sequence in  . (seems obvious)</li> 
<li> The length of the longest sequence  will extend! (not as obvious - this 
is the idea!)</li> </ol> 
<p> Let  be the length of the longest sequence ending with the <em>i</em>th 
character:</p> 
<p> </p> 
<p></p>  sequence  5  2  8  7  3  1  6  4 <br>
 1  1  2  2  2  1  3  3 <br>

<br> 
<p></p>  How do we compute <i>s</i> <i>i</i>? 
<p> </p> 
<p> </p> 
<p> To find the longest sequence - we know it ends somewhere, so Length = </p> 
<p> Listening to Part 14-5 </p> 
<p> </p> 
<p>The Principle of Optimality</p> 
<p> To use dynamic programming, the problem must observe the <em>principle of 
optimality</em>, that whatever the initial state is, remaining decisions must 
be optimal with regard the state following from the first decision. &nbsp;</p> 
<p> Combinatorial problems may have this property but may use too much 
memory/time to be efficient.</p> 
<p> </p> 
<p>Example: The Traveling Salesman Problem</p> Let  be the cost of the optimal 
tour for<em>i</em> to 1 that goes thru each of the other cities once &nbsp; 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> Here there can be any subset of  instead of any subinterval - hence 
exponential.</p> 
<p> Still, with other ideas (some type of pruning or best-first search) it can 
be effective for combinatorial search.</p> 
<p> Listening to Part 14-6 </p> 
<p> </p> 
<p>When can you use Dynamic Programming?</p> 
<p> Dynamic programming computes recurrences efficiently by storing partial 
results. Thus dynamic programming can only be efficient when there are not too 
many partial results to compute! &nbsp;</p> 
<p> There are <i>n</i>! permutations of an <em>n</em>-element set - we cannot 
use dynamic programming to store the best solution for each subpermutation. 
There are subsets of an <em>n</em>-element set - we cannot use dynamic 
programming to store the best solution for each.</p> 
<p> However, there are only <i>n</i>(<i>n</i>-1)/2 continguous substrings of a 
string, each described by a starting and ending point, so we can use it for 
string problems.</p> 
<p> There are only <i>n</i>(<i>n</i>-1)/2 possible subtrees of a binary search 
tree, each described by a maximum and minimum key, so we can use it for 
optimizing binary search trees.</p> 
<p> <em> Dynamic programming works best on objects which are linearly ordered 
and cannot be rearranged - characters in a string, matrices in a chain, points 
around the boundary of a polygon, the left-to-right order of leaves in a search 
tree.</em> </p> 
<p> Whenever your objects are ordered in a left-to-right way, you should smell 
dynamic programming!</p> 
<p> Listening to Part 14-7 </p> 
<p> </p> 
<p>Minimum Length Triangulation</p> 
<p> A triangulation of a polygon is a set of non-intersecting diagonals which 
partitions the polygon into diagonals. &nbsp;</p> 
<p> </p> <br>
 The length of a triangulation is the sum of the diagonal 
lengths.
<p> We seek to find the minimum length triangulation. For a convex polygon, or 
part thereof:</p> 
<p> </p> <br>
 Once we identify the correct connecting vertex, the polygon is 
partitioned into two smaller pieces, both of which must be triangulated 
optimally!
<p> </p> 
<p> </p> 
<p> </p> 
<p> Evaluation proceeds as in the matrix multiplication example -  values of 
<em>t</em>, each of which takes <i>O</i>(<i>j</i>-<i>i</i>) time if we evaluate 
the sections in order of increasing size.</p> 
<p> </p> <br>
 What if there are points in the interior of the polygon? 
<p> Listening to Part 14-8 </p> 
<p> </p> 
<p>Dynamic Programming and High Density Bar Codes</p> 
<p> Symbol Technology has developed a new design for bar codes, PDF-417 that 
has a capacity of several hundred bytes. What is the best way to encode text 
for this design? &nbsp;</p> 
<p> </p> <br>
 They developed a complicated mode-switching data compression 
scheme.
<p> </p> <br>
 Latch commands permanently put you in a different mode. Shift 
commands temporarily put you in a different mode.
<p> Listening to Part 14-9 </p> 
<p> Originally, Symbol used a greedy algorithm to encode a string, making 
local decisions only. We realized that for any prefix, you want an optimal 
encoding which might leave you in every possible mode.</p> 
<p> </p> <br>
 the cost of encoding the <em>i</em>th character and ending up 
in node<em>j</em>). 
<p> Our simple dynamic programming algorithm improved to capacity of PDF-417 
by an average of ! </p> 
<p> Listening to Part 14-10 </p> 
<p> </p> 
<p>Dynamic Programming and Morphing</p> 
<p> Morphing is the problem of creating a smooth series of intermediate images 
given a starting and ending image. &nbsp;&nbsp;</p> 
<p> The key problem is establishing a correspondence between features in the 
two images. You want to morph an eye to an eye, not an ear to an ear.</p> 
<p> We can do this matching on a line-by-line basis: </p> 
<p> </p> <br>
 This should sound like string matching, but with a different 
set of operations:
<p> </p> 
<ul> 
<li> <em>Full run match:</em> We may match run <em>i</em> on top to run <em>j
</em> on bottom for a cost which is a function of the difference in the lengths 
of the two runs and their positions.</li> 
<li> <em>Merging runs:</em> We may match a string of consecutive runs on top 
to a run on bottom. The cost will be a function of the number of runs, their 
relative positions, and lengths.
<p> Listening to Part 14-11</p></li> 
<li> <em>Splitting runs:</em> We may match a big run on top to a string of 
consecutive runs on the bottom. This is just the converse of the merge. Again, 
the cost will be a function of the number of runs, their relative positions, 
and lengths.</li> </ul> 
<p> This algorithm was incorported into a morphing system, with the following 
results:</p> 
<p> </p> <br>
<b>Next:</b> Lecture 14 - data <b>Up:</b> Table of contents <b>
Previous:</b> Lecture 12 - introduction 
<h1>Lecture 14 - data structures for graphs</h1> 
<p> Listening to Part 14-1 </p> 
<p> </p> 
<p>Problem Solving Techniques</p> 
<p> Most important: make sure you understand exactly what the question is 
asking - if not, you have no hope of answer it!! &nbsp;</p> 
<p> Never be afraid to ask for another explanation of a problem until it is 
clear.</p> 
<p> Play around with the problem by constructing examples to get insight into 
it.</p> 
<p> Ask yourself questions. Does the first idea which comes into my head work? 
If not, why not?</p> 
<p> Am I using all information that I am given about the problem? </p> 
<p> Read Polya's book <em>How to Solve it</em>. </p> 
<p> Listening to Part 14-2 </p> 
<p> <em> 16-1: The Euclidean traveling-salesman problem is the problem of 
determining the shortest closed tour that connects a given set of<em>n</em> 
points in the plane. &nbsp; &nbsp;</em></p> 
<p><em> Bentley suggested simplifying the problem by restricting attention to 
<em>bitonic tours</em>, that is tours which start at the leftmost point, go 
strictly left to right to the rightmost point, and then go strictly right back 
to the starting point.</em></p> 
<p><em> </em></p> <em> <br>
</em> <em> Describe an  algorithm for finding the 
optimal bitonic tour. You may assume that no two points have the same<em>x</em>
-coordinate. (Hint: scan left to right, maintaining optimal possibilities for 
the two parts of the tour.)</em> Make sure you understand what a bitonic tour 
is, or else it is hopeless.
<p> First of all, play with the problem. Why isn't it trivial? </p> 
<p> Listening to Part 14-3 </p> 
<p> </p> <br>
 Am I using all the information? 
<p> Why will they let us assume that no two <em>x</em>-coordinates are the 
same? What does the hint mean? What happens if I scan from left to right?</p> 
<p> If we scan from left to right, we get an open tour which uses all points 
to the left of our scan line. &nbsp;</p> 
<p> </p> <br>
 In the optimal tour, the <em>k</em>th point is connected to 
exactly one point to the left of<em>k</em>.  Once I decide which point that is, 
say<em>x</em>. I need the optimal partial tour where the two endpoints are <em>x
</em> and <em>k-1</em>, because if it isn't optimal I could come up with a 
better one.
<p> Listening to Part 14-4 </p> 
<p> Hey, I have got a recurrence! And look, the two parameters which describe 
my optimal tour are the two endpoints.</p> 
<p> Let <i>c</i>[<i>k</i>,<i>n</i>] be the optimal cost partial tour where the 
two endpoints are<i>k</i>&lt;<i>n</i>. </p> 
<p>  (when <i>k</i> &lt; <i>n</i>-1) </p> 
<p> </p> 
<p> <i>c</i>[0, 1]=<i>d</i>[0, 1] </p> 
<p> </p> <br>
<i>c</i>[ <i>n</i>-1, <i>n</i>] takes <i>O</i>( <i>n</i>) to 
update,<i>c</i>[ <i>k</i>, <i>n</i>] <i>k</i>&lt; <i>n</i>-1 takes <i>O</i>(1) 
to update. Total time is . 
<p> But this doesn't quite give the tour, but just an open tour. We simply 
must figure where the last edge to<em>n</em> must go. </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> Listening to Part 15-1 </p> 
<p> </p> 
<p>Graphs</p> 
<p> A graph <em>G</em> consists of a set of <em>vertices</em> <em>V</em> 
together with a set<em>E</em> of vertex pairs or <em>edges</em>. 
&nbsp;&nbsp;&nbsp;</p> 
<p> Graphs are important because any binary relation is a graph, so graphs can 
be used to represent essentially<em>any</em> relationship. </p> 
<p> Example: A network of roads, with cities as vertices and roads between 
cities as edges.</p> 
<p> </p> <br>
 Example: An electronic circuit, with junctions as vertices as 
components as edges.
<p> </p> <br>
 To understand many problems, we must think of them in terms of 
graphs!
<p> Listening to Part 15-2 </p> 
<p> </p> 
<p>The Friendship Graph</p> 
<p> Consider a graph where the vertices are people, and there is an edge 
between two people if and only if they are friends. &nbsp;</p> 
<p> </p> <br>
 This graph is well-defined on any set of people: SUNY SB, New 
York, or the world.
<p> What questions might we ask about the friendship graph? </p> 
<p> </p> 
<ul> 
<li> <b>If I am your friend, does that mean you are my friend?</b> 
<p> A graph is <em>undirected</em> if (<i>x</i>,<i>y</i>) implies (<i>y</i>,<i>
x</i>). Otherwise the graph is directed. The ``heard-of'' graph is directed 
since countless famous people have never heard of me! The ``had-sex-with'' 
graph is presumably undirected, since it requires a partner. &nbsp;&nbsp;</p>
</li> 
<li> <b>Am I my own friend?</b> &nbsp;&nbsp; 
<p> An edge of the form (<i>x</i>,<i>x</i>) is said to be a <em>loop</em>. If 
<em>x</em> is <em>y</em>'s friend several times over, that could be modeled 
using<em>multiedges</em>, multiple edges between the same pair of vertices. A 
graph is said to be<em>simple</em> if it contains no loops and multiple edges. 
</p>
<p> Listening to Part 15-3</p></li> 
<li> <b>Am I linked by some chain of friends to the President?</b> 
<p> A <em>path</em> is a sequence of edges connecting two vertices. Since <em>
Mel Brooks</em> is my father's-sister's-husband's cousin, there is a path 
between me and him! &nbsp;&nbsp;</p>
<p> </p> <br>
</li> 
<li> <b>How close is my link to the President?</b> 
<p> If I were trying to impress you with how tight I am with Mel Brooks, I 
would be much better off saying that Uncle Lenny knows him than to go into the 
details of how connected I am to Uncle Lenny. Thus we are often interested in 
the<em>shortest path</em> between two nodes. &nbsp;</p></li> 
<li> <b>Is there a path of friends between any two people?</b> 
<p> A graph is <em>connected</em> if there is a path between any two vertices. 
A directed graph is<em>strongly connected</em> if there is a directed path 
between any two vertices. &nbsp;&nbsp;</p></li> 
<li> <b>Who has the most friends?</b> 
<p> The <em>degree</em> of a vertex is the number of edges adjacent to it. 
&nbsp;</p>
<p> Listening to Part 15-4 </p>
<p> </p></li> 
<li> <b>What is the largest clique?</b> &nbsp; 
<p> A social clique is a group of mutual friends who all hang around together. 
A graph theoretic<em>clique</em> is a complete subgraph, where each vertex pair 
has an edge between them. Cliques are the densest possible subgraphs. Within 
the friendship graph, we would expect that large cliques correspond to 
workplaces, neighborhoods, religious organizations, schools, and the like.</p>
</li> 
<li> <b>How long will it take for my gossip to get back to me?</b> 
<p> A <em>cycle</em> is a path where the last vertex is adjacent to the first. 
A cycle in which no vertex repeats (such as 1-2-3-1 verus 1-2-3-2-1) is said to 
be<em>simple</em>. The shortest cycle in the graph defines its <em>girth</em>, 
while a simple cycle which passes through each vertex is said to be a<em>
Hamiltonian cycle</em>. &nbsp;&nbsp;&nbsp;&nbsp; </p>
<p> </p></li> </ul> 
<p> Listening to Part 15-5 </p> 
<p> </p> 
<p>Data Structures for Graphs</p> 
<p> There are two main data structures used to represent graphs. </p> 
<p> </p> 
<p>Adjacency Matrices</p> An <em>adjacency matrix</em> is an  matrix, where <i>
M</i>[ <i>i</i>, <i>j</i>] = 0 iff there is no edge from vertex <em>i</em> to 
vertex<em>j</em> &nbsp; 
<p> </p> <br>
 It takes  time to test if ( <i>i</i>, <i>j</i>) is in a graph 
represented by an adjacency matrix.
<p> Can we save space if (1) the graph is undirected? (2) if the graph is 
sparse?</p> 
<p> Listening to Part 15-6 </p> 
<p> </p> 
<p>Adjacency Lists</p> An <em>adjacency list</em> consists of a  array of 
pointers, where the<em>i</em>th element points to a linked list of the edges 
incident on vertex<em>i</em>. &nbsp; 
<p> </p> <br>
 To test if edge ( <i>i</i>, <i>j</i>) is in the graph, we 
search the<em>i</em>th list for <em>j</em>, which takes  , where  is the degree 
of the<em>i</em>th vertex. 
<p> Note that  can be much less than <em>n</em> when the graph is sparse. If 
necessary, the two<em>copies</em> of each edge can be linked by a pointer to 
facilitate deletions.</p> 
<p> Listening to Part 15-7 </p> 
<p> </p> 
<p>Tradeoffs Between Adjacency Lists and Adjacency Matrices</p> 
<p> </p> 
<p></p>  Comparison  Winner <br>
Faster to test if (<i>x</i>, <i>y</i>) exists?
 matrices <br>
Faster to find vertex degree?  lists <br>
Less memory on small 
graphs? lists (<i>m</i>+<i>n</i>) vs. <br>
Less memory on big graphs?  matrices 
(small win) <br>
Edge insertion or deletion?  matrices <i>O</i>(1) <br>
Faster 
to traverse the graph?  lists <em>m+n</em> vs. <br>
Better for most problems?  
lists <br>
<br>

<p></p> <br>

<p> Both representations are very useful and have different properties, 
although adjacency lists are probably better for most problems.</p> 
<p> Listening to Part 16-2 </p> 
<p> </p> 
<p>Traversing a Graph</p> 
<p> One of the most fundamental graph problems is to traverse every edge and 
vertex in a graph. Applications include: &nbsp;</p> 
<p> </p> 
<ul> 
<li> Printing out the contents of each edge and vertex.</li> 
<li> Counting the number of edges.</li> 
<li> Identifying connected components of a graph. </li> </ul> 
<p> For <em>efficiency</em>, we must make sure we visit each edge at most 
twice.</p> 
<p> For <em>correctness</em>, we must do the traversal in a systematic way so 
that we don't miss anything.</p> 
<p> Since a maze is just a graph, such an algorithm must be powerful enough to 
enable us to get out of an arbitrary maze. &nbsp;</p> 
<p> Listening to Part 16-3 </p> 
<p> </p> 
<p>Marking Vertices</p> 
<p> <em> The idea in graph traversal is that we must mark each vertex when we 
first visit it, and keep track of what have not yet completely explored.</em> 
</p> 
<p> For each vertex, we can maintain two flags: </p> 
<p> </p> 
<ul> 
<li> <em>discovered</em> - have we ever encountered this vertex before?</li> 
<li> <em>completely-explored</em> - have we finished exploring this vertex yet?
<p> </p></li> </ul> 
<p> We must also maintain a structure containing all the vertices we have 
discovered but not yet completely explored.</p> 
<p> Initially, only a single start vertex is considered to be discovered. </p> 
<p> To completely explore a vertex, we look at each edge going out of it. For 
each edge which goes to an undiscovered vertex, we mark it<em>discovered</em> 
and add it to the list of work to do.</p> 
<p> Note that regardless of what order we fetch the next vertex to explore, 
each edge is considered exactly twice, when each of its endpoints are explored.
</p> 
<p> Listening to Part 16-4 </p> 
<p> </p> 
<p>Correctness of Graph Traversal</p> 
<p> Every edge and vertex in the connected component is eventually visited. 
</p> 
<p> Suppose not, ie. there exists a vertex which was unvisited whose neighbor 
<em>was</em> visited. This neighbor will eventually be explored so we <em>would
</em> visit it: </p> 
<p> </p> <br>
Listening to Part 16-5 
<p> </p> 
<p>Traversal Orders</p> 
<p> The order we explore the vertices depends upon what kind of data structure 
is used:</p> 
<p> </p> 
<ul> 
<li> <em>Queue</em> - by storing the vertices in a first-in, first out (FIFO) 
queue, we explore the oldest unexplored vertices first. Thus our explorations 
radiate out slowly from the starting vertex, defining a so-called<em>
breadth-first search</em>. &nbsp;&nbsp;</li> 
<li> <em>Stack</em> - by storing the vertices in a last-in, first-out (LIFO) 
stack, we explore the vertices by lurching along a path, constantly visiting a 
new neighbor if one is available, and backing up only if we are surrounded by 
previously discovered vertices. Thus our explorations quickly wander away from 
our starting point, defining a so-called<em>depth-first search</em>. 
&nbsp;&nbsp;</li> </ul> 
<p> The three possible colors of each node reflect if it is unvisited (white), 
visited but unexplored (grey) or completely explored (black).</p> 
<p> Listening to Part 16-6 </p> 
<p> </p> 
<p>Breadth-First Search</p>  &nbsp; 
<p> </p> 
<pre> 
<p> BFS(G,s) </p>
<p> for each vertex  do </p>
<p> color[u] = white </p>
<p>  , ie. the distance from <em>s</em> </p>
<p> <i>p</i>[<i>u</i>] = <i>NIL</i>, ie. the parent in the BFS tree </p>
<p> color[u] = grey </p>
<p> <i>d</i>[<i>s</i>] = 0 </p>
<p> <i>p</i>[<i>s</i>] = <i>NIL</i> </p>
<p> </p>
<p> while  do </p>
<p> <i>u</i> = <i>head</i>[<i>Q</i>] </p>
<p> for each  do </p>
<p> if <i>color</i>[<i>v</i>] = <i>white</i> then </p>
<p> <i>color</i>[<i>v</i>] = <i>gray</i> </p>
<p> <i>d</i>[<i>v</i>] = <i>d</i>[<i>u</i>] + 1 </p>
<p> <i>p</i>[<i>v</i>] = <i>u</i> </p>
<p> enqueue[Q,v] </p>
<p> dequeue[Q] </p>
<p> <i>color</i>[<i>u</i>] = <i>black</i> </p></pre> 
<p> Listening to Part 16-8 </p> 
<p> </p> 
<p>Depth-First Search</p> 
<p> DFS has a neat recursive implementation which eliminates the need to 
explicitly use a stack. &nbsp;</p> 
<p> Discovery and final times are sometimes a convenience to maintain. </p> 
<p> </p> 
<pre> 
<p> DFS(G) </p>
<p> for each vertex  do </p>
<p> <i>color</i>[<i>u</i>] = <i>white</i> </p>
<p> <i>parent</i>[<i>u</i>] = <i>nil</i> </p>
<p> <i>time</i> = 0 </p>
<p> for each vertex  do </p>
<p> if <i>color</i>[<i>u</i>] = <i>white</i> then DFS-VISIT[u] </p>
<p> </p></pre> 
<p> Initialize each vertex in the main routine, then do a search from each 
connected component. BFS must also start from a vertex in each component to 
completely visit the graph. &nbsp;</p> 
<p> </p> 
<pre> 
<p> DFS-VISIT[u] </p>
<p> <i>color</i>[<i>u</i>] = <i>grey</i> (*u had been white/undiscovered*) </p>
<p> <i>discover</i>[<i>u</i>] = <i>time</i> </p>
<p> <i>time</i> = <i>time</i>+1 </p>
<p> for each  do </p>
<p> if <i>color</i>[<i>v</i>] = <i>white</i> then </p>
<p> <i>parent</i>[<i>v</i>] = <i>u</i> </p>
<p> DFS-VISIT(v) </p>
<p> <i>color</i>[<i>u</i>] = <i>black</i> (*now finished with <em>u</em>*) </p>
<p> <i>finish</i>[<i>u</i>] = <i>time</i> </p>
<p> <i>time</i> = <i>time</i>+1 </p>
<p> </p></pre> 
<p> </p> <b>Next:</b> Lecture 15 - DFS <b>Up:</b> Table of contents <b>
Previous:</b> Lecture 13 - dynamic 
<h1>Lecture 15 - DFS and BFS</h1> 
<p> Listening to Part 15-8 </p> 
<p> <em>23.1-5 - The <em>square</em> of a directed graph <i>G</i>=(<i>V</i>,<i>
E</i>) is the graph  such that  iff for some  , both  and  ; ie. there is a 
path of exactly two edges. &nbsp;</em></p> 
<p><em> Give efficient algorithms for both adjacency lists and matricies. </em>
</p> Given an adjacency matrix, we can check in constant time whether a given 
edge exists. To discover whether there is an edge , for each possible 
intermediate vertex<em>v</em> we can check whether ( <i>u</i>, <i>v</i>) and ( 
<i>v</i>, <i>w</i>) exist in <i>O</i>(1). 
<p> Since there are at most <em>n</em> intermediate vertices to check, and  
pairs of vertices to ask about, this takes time. </p> 
<p> With adjacency lists, we have a list of all the edges in the graph. For a 
given edge (<i>u</i>,<i>v</i>), we can run through all the edges from <em>v</em>
 in<i>O</i>(<i>n</i>) time, and fill the results into an adjacency matrix of  , 
which is initially empty.</p> 
<p> It takes <i>O</i>(<i>mn</i>) to construct the edges, and  to initialize 
and read the adjacency matrix, a total of<i>O</i>((<i>n</i>+<i>m</i>)<i>n</i>). 
Since unless the graph is disconnected, this is usually simplified to <i>O</i>(
<i>mn</i>), and is faster than the previous algorithm on sparse graphs. &nbsp; 
</p> 
<p> Why is it called the square of a graph? Because the square of the 
adjacency matrix is the adjacency matrix of the square! This provides a 
theoretically faster algorithm.</p> 
<p> Listening to Part 16-10 </p> 
<p> </p> 
<p>BFS Trees</p> 
<p> If BFS is performed on a connected, undirected graph, a tree is defined by 
the edges involved with the discovery of new nodes: &nbsp;</p> 
<p> </p> <br>
<em> This tree defines a shortest path from the root to every 
other node in the tree.</em> 
<p> The proof is by induction on the length of the shortest path from the root:
</p> 
<p> </p> 
<ul> 
<li> <em>Length = 1</em> First step of BFS explores all neighbors of the root. 
In an unweighted graph one edge must be the shortest path to any node.</li> 
<li> <em>Length = s</em> Assume the BFS tree has the shortest paths up to 
length<em>s-1</em>. Any node at a distance of <em>s</em> will first be 
discovered by expanding a distance<em>s-1</em> node. 
<p> </p></li> </ul> 
<p> Listening to Part 16-11 </p> 
<p> </p> 
<p>The <em>key</em> idea about DFS</p> 
<p> A depth-first search of a graph organizes the edges of the graph in a 
precise way. &nbsp;</p> 
<p> In a DFS of an undirected graph, we assign a direction to each edge, from 
the vertex which discover it:</p> 
<p> </p> <br>
 In a DFS of a directed graph, every edge is either a tree edge 
or a black edge.
<p> In a DFS of a directed graph, no cross edge goes to a higher numbered or 
rightward vertex. Thus, no edge from 4 to 5 is possible:</p> 
<p> </p> <br>
Listening to Part 16-12 
<p> </p> 
<p>Edge Classification for DFS</p> 
<p> What about the other edges in the graph? Where can they go on a search? 
</p> 
<p> Every edge is either: </p> 
<p> </p> <br>
 On any particular DFS or BFS of a directed or undirected graph, 
each edge gets classified as one of the above.
<p> Listening to Part 17-3 </p> 
<p> </p> 
<p>DFS Trees</p> 
<p> The reason DFS is so important is that it defines a very nice ordering to 
the edges of the graph.</p> 
<p> <em> In a DFS of an undirected graph, every edge is either a tree edge or 
a back edge. &nbsp;</em> </p> 
<p> Why? Suppose we have a forward edge. We would have encountered (4,1) when 
expanding 4, so this is a back edge. &nbsp;</p> 
<p> </p> <br>
 Suppose we have a cross-edge &nbsp; 
<p> </p> <br>

<p>Paths in search trees</p> 
<p> Where is the shortest path in a DFS? </p> 
<p> </p> <br>
 It could use multiple back and tree edges, where BFS only uses 
tree edges.
<p> DFS gives a better approximation of the longest path than BFS. </p> 
<p> </p> <br>
Listening to Part 17-4 
<p> </p> 
<p>Topological Sorting</p> 
<p> A directed, acyclic graph is a directed graph with no directed cycles. 
&nbsp;&nbsp;&nbsp;</p> 
<p> </p> <br>
 A topological sort of a graph is an ordering on the vertices so 
that all edges go from left to right.
<p> Only a DAG can have a topological sort. </p> 
<p> </p> <br>
 Any DAG has (at least one) topological sort. 
<p> Listening to Part 17-5 </p> 
<p> </p> 
<p>Applications of Topological Sorting</p> 
<p> Topological sorting is often useful in scheduling jobs in their proper 
sequence. In general, we can use it to order things given constraints, such as 
a set of left-right constraints on the positions of objects.</p> 
<p> Example: Dressing schedule from CLR. </p> 
<p> Example: Identifying errors in DNA fragment assembly. &nbsp; </p> 
<p> Certain fragments are constrained to be to the left or right of other 
fragments, unless there are errors.</p> 
<p> </p> <br>
 Solution - build a DAG representing all the left-right 
constraints. Any topological sort of this DAG is a consistant ordering. If 
there are cycles, there must be errors.
<p> A DFS can test if a graph is a DAG (it is iff there are no back edges - 
forward edges are allowed for DFS on directed graph).</p> 
<p> Listening to Part 17-6 </p> 
<p> </p> 
<p>Algorithm</p> 
<p> <b>Theorem</b>: Arranging vertices in decreasing order of DFS finishing 
time gives a topological sort of a DAG.</p> 
<p> <b>Proof</b>: Consider any directed edge <i>u</i>,<i>v</i>, when we 
encounter it during the exploration of vertex<em>u</em>: </p> 
<p> </p> 
<ul> 
<li> If <em>v</em> is white - we then start a DFS of <em>v</em> before we 
continue with<em>u</em>.</li> 
<li> If <em>v</em> is grey - then <i>u</i>, <i>v</i> is a back edge, which 
cannot happen in a DAG.</li> 
<li> If <em>v</em> is black - we have already finished with <em>v</em>, so <i>f
</i>[<i>v</i>]&lt;<i>f</i>[<i>u</i>]. 
<p> </p></li> </ul> 
<p> Thus we can do topological sorting in <i>O</i>(<i>n</i>+<i>m</i>) time. 
</p> 
<p> Listening to Part 17-8 </p> 
<p> </p> 
<p>Articulation Vertices</p> 
<p> Suppose you are a terrorist, seeking to disrupt the telephone network. 
Which station do you blow up? &nbsp;&nbsp;&nbsp;</p> 
<p> </p> <br>
 An <em>articulation vertex</em> is a vertex of a connected 
graph whose deletion disconnects the graph.
<p> Clearly connectivity is an important concern in the design of any network. 
&nbsp;</p> 
<p> Articulation vertices can be found in <i>O</i>(<i>n</i>(<i>m</i>+<i>n</i>
)) - just delete each vertex to do a DFS on the remaining graph to see if it is 
connected.</p> 
<p> Listening to Part 17-9 </p> 
<p> </p> 
<p>A Faster <i>O</i>(<i>n</i>+<i>m</i>) DFS Algorithm</p> 
<p> <b>Theorem:</b> In a DFS tree, a vertex <em>v</em> (other than the root) 
is an articulation vertex iff<em>v</em> is not a leaf and some subtree of <em>v
</em> has no back edge incident until a proper ancestor of <em>v</em>. </p> 
<p> </p> <br>
<b>Proof:</b> (1) <em>v</em> is an articulation vertex <em>v</em>
 cannot be a leaf.
<p> Why? Deleting <em>v</em> must seperate a pair of vertices <em>x</em> and 
<em>y</em>. Because of the other tree edges, this cannot happen unless <em>y
</em> is a decendant of <em>v</em>. </p> 
<p> Listening to Part 17-10 </p> 
<p> </p> <br>
<em>v</em> separating <em>x,y</em> implies there is no back edge 
in the subtree of<em>y</em> to a proper ancestor of <em>v</em>. 
<p> (2) Conditions <em>v</em> is a non-root articulation vertex. <em>v</em> 
separates any ancestor of<em>v</em> from any decendant in the appropriate 
subtree.</p> 
<p> Actually implementing this test in <i>O</i>(<i>n</i>+<i>m</i>) is tricky - 
but believable once you accept this theorem.</p> 
<p> </p> <b>Next:</b> Lecture 16 - applications <b>Up:</b> Table of contents 
<b>Previous:</b> Lecture 14 - data 
<h1>Lecture 16 - applications of DFS and BFS</h1> 
<p> Listening to Part 16-1 </p> 
<p> <em> 23.2-6 Give an efficient algorithm to test if a graph is bipartite. 
</em> </p> Bipartite means the vertices can be colored red or black such that 
no edge links vertices of the same color. &nbsp;&nbsp;
<p> </p> <br>
 Suppose we color a vertex red - what color must its neighbors 
be?<em>black!</em> 
<p> We can augment either BFS or DFS when we first discover a new vertex, 
color it opposited its parents, and for each other edge, check it doesn't link 
two vertices of the same color. The first vertex in any connected component can 
be red or black!</p> 
<p> Bipartite graphs arise in many situations, and special algorithms are 
often available for them. What is the interpretation of a bipartite 
``had-sex-with'' graph?</p> 
<p> How would you break people into two groups such that no group contains a 
pair of people who hate each other?</p> 
<p> Listening to Part 17-1 </p> 
<p> <em> 23.4-3 Give an <i>O</i>(<i>n</i>) algorithm to test whether an 
undirected graph contains a cycle. &nbsp;</em> </p> If you do a DFS, you have a 
cycle iff you have a back edge. This gives an<i>O</i>( <i>n</i>+ <i>m</i>) 
algorithm. But where does the<em>m</em> go? If the graph contains more than <em>
n-1</em> edges, it must contain a cycle! Thus we never need look at more than 
<em>n</em> edges if we are given an adjacency list representation! 
<p> Listening to Part 17-7 </p> 
<p> <em> 23.4-5 Show that you can topologically sort in <i>O</i>(<i>n</i>+<i>m
</i>) by repeatedly deleting vertices of degree <em>0</em>. &nbsp; </em> </p> 
The correctness of this algorithm follows since in a DAG there must always be a 
vertex of indegree 0, and such a vertex can be first in topological sort. 
Suppose each vertex is initialized with its indegree (do DFS on G to get this). 
Deleting a vertex takes<em>O</em>(degree v). Reduce the indegree of each 
efficient vertex - and keep a list of degree-0 vertices to delete next.
<p> Time: </p> 
<p> Listening to Part 17-12 </p> 
<p> </p> 
<p>Strongly Connected Components</p> 
<p> A directed graph is strongly connected iff there is a directed path 
between any two vertices. &nbsp;</p> 
<p> The strongly connected components of a graph is a partition of the 
vertices into subsets (maximal) such that each subset is strongly connected.</p>
<p> </p> <br>
 Observe that no vertex can be in two maximal components, so it 
is a partition.
<p> </p> <br>
 There is an amazingly elegant, linear time algorithm to find 
the strongly connected components of a directed graph, using DFS.
<p> Listening to Part 17-13 </p> 
<p> </p> 
<ul> 
<li> Call DFS(  ) to compute finishing times for each vertex.</li> 
<li> Compute the transpose graph  (reverse all edges in G)</li> 
<li> Call DFS(  ), but order the vertices in decreasing order of finish time.
</li> 
<li> The vertices of each DFS tree in the forest of DFS(  ) is a strongly 
connected component.</li> </ul> 
<p> This algorithm takes <i>O</i>(<i>n</i>+<i>m</i>), but why does it compute 
strongly connected components?</p> 
<p> <b>Lemma</b>: If two vertices are in the same strong component, no path 
between them ever leaves the component.</p> 
<p> </p> <br>
<b>Lemma</b>: In any DFS forest, all vertices in the same 
strongly connected component are in the same tree.
<p> Proof: Consider the first vertex <em>v</em> in the component to be 
discovered. Everything in the component is reachable from it, so we will 
traverse it before finishing with<em>v</em>. </p> 
<p> Listening to Part 17-14 </p> 
<p> </p> 
<p>What does DFS(  , v) Do?</p> 
<p> It tells you what vertices have directed paths to <em>v</em>, while DFS(  ,
<em>v</em>) tells what vertices have directed paths from <em>v</em>. But why 
must any vertex in the search tree of DFS( , <em>v</em>) also have a path from 
<em>u</em>? </p> 
<p> </p> <br>
 Because there is no edge from any previous DFS tree into the 
last tree!! Because we ordered the vertices by decreasing order of finish time, 
we can peel off the strongly connected components from right to left just be 
doing a DFS( ). 
<p> Listening to Part 17-16 </p> 
<p> </p> 
<p>Example of Strong Components Algorithm</p> 
<p> </p> <br>
 9, 10, 11, 12 can reach <em>9</em>, oldest remaining finished is
<em>5</em>. 
<p> 5, 6, 8 can reach <em>5</em>, oldest remaining is <em>7</em>. </p> 
<p> 7 can reach <em>7</em>, oldest remaining is <em>1</em>. </p> 
<p> 1, 2, 3 can reach <em>1</em>, oldest remaining is <em>4</em>. </p> 
<p> 4 can reach <em>4</em>. </p> 
<p> </p> <br>
<b>Next:</b> Lecture 17 - minimum <b>Up:</b> Table of contents 
<b>Previous:</b> Lecture 15 - DFS 
<h1>Lecture 17 - minimum spanning trees</h1> 
<p> Listening to Part 19-4 </p> 
<p> <em> 24.2-6 Describe an efficent algorithm that, given an undirected graph 
<em>G</em>, determines a spanning tree <em>G</em> whose largest edge weight is 
minimum over all spanning trees of<em>G</em>. &nbsp; </em> </p> First, make 
sure you understand the question
<p> </p> <br>
 ``Hey, doesn't Kruskal's algorithm do something like this.'' 
&nbsp;
<p> Certainly! Since Krushal's algorithm considers the edges in order of 
increasing weight, and stops the moment these edges form a connected graph, the 
tree it gives must minimize the edge weight.</p> 
<p> ``Hey, but then why doesn't Prim's algorithm also work?'' </p> 
<p> It gives the same thing as Kruskal's algorithm, so it must be true that 
any minimum spanning tree minimizes the maximum edge weight!</p> 
<p> Proof: Give me a MST and consider the largest edge weight, </p> 
<p> Listening to Part 19-5 </p> 
<p> </p> <br>
 Deleting it disconnects the MST. If there was a lower edge 
connects the two subtrees, I didn't have a MST!
<p> Listening to Part 18-2 </p> 
<p> </p> 
<p>Minimum Spanning Trees</p> 
<p> A tree is a connected graph with no cycles. A spanning tree is a subgraph 
of<em>G</em> which has the same set of vertices of <em>G</em> and is a tree. 
&nbsp;</p> 
<p> A minimum spanning tree of a weighted graph <em>G</em> is the spanning 
tree of<em>G</em> whose edges sum to minimum weight. &nbsp; </p> 
<p> There can be more than one minimum spanning tree in a graph  consider a 
graph with identical weight edges.</p> 
<p> The minimum spanning tree problem has a long history - the first algorithm 
dates back at least to 1926!.</p> 
<p> Minimum spanning tree is always taught in algorithm courses since (1) it 
arises in many applications, (2) it is an important example where<em>greedy</em>
 algorithms always give the optimal answer, and (3) Clever data structures are 
necessary to make it work.</p> 
<p> In greedy algorithms, we make the decision of what next to do by selecting 
the best local option from all available choices - without regard to the global 
structure.</p> 
<p> Listening to Part 18-3 </p> 
<p> </p> 
<p>Applications of Minimum Spanning Trees</p> 
<p> Minimum spanning trees are useful in constructing networks, by describing 
the way to connect a set of sites using the smallest total amount of wire. Much 
of the work on minimum spanning (and related Steiner) trees has been conducted 
by the phone company. &nbsp;</p> 
<p> Minimum spanning trees provide a reasonable way for <em>clustering</em> 
points in space into natural groups. &nbsp;</p> 
<p> When the cities are points in the Euclidean plane, the minimum spanning 
tree provides a good heuristic for traveling salesman problems. The optimum 
traveling salesman tour is at most twice the length of the minimum spanning 
tree. &nbsp;</p> 
<p> </p> <br>
Listening to Part 18-4 
<p> </p> 
<p>Prim's Algorithm</p> 
<p> If <em>G</em> is connected, every vertex will appear in the minimum 
spanning tree. If not, we can talk about a minimum spanning forest. &nbsp;</p> 
<p> Prim's algorithm starts from one vertex and grows the rest of the tree an 
edge at a time.</p> 
<p> As a greedy algorithm, which edge should we pick? The cheapest edge with 
which can grow the tree by one vertex without creating a cycle.</p> 
<p> During execution we will label each vertex as either in the tree, <em>
fringe</em> - meaning there exists an edge from a tree vertex, or <em>unseen
</em> - meaning the vertex is more than one edge away. </p> 
<p> </p> 
<pre> 
<p> Select an arbitrary vertex to start. </p>
<p> While (there are fringe vertices) </p>
<p> select minimum weight edge between tree and fringe </p>
<p> add the selected edge and vertex to the tree </p>
<p> </p></pre> 
<p> Clearly this creates a spanning tree, since no cycle can be introduced via 
edges between tree and fringe vertices, but is it minimum?</p> 
<p> Listening to Part 18-5 </p> 
<p> </p> 
<p>Why is Prim's algorithm correct?</p> 
<p> Don't be scared by the proof - the reason is really quite basic: </p> 
<p> Theorem: Let <em>G</em> be a connected, weighted graph and let  be a 
subset of the edges in a MST . Let <i>V</i>' be the vertices incident with 
edges in<i>E</i>'. If (<i>x</i>,<i>y</i>) is an edge of minimum weight such that
 and<em>y</em> is not in <i>V</i>', then  is a subset of a minimum spanning 
tree.</p> 
<p> Proof: If the edge is in <em>T</em>, this is trivial. </p> 
<p> Suppose (<i>x</i>,<i>y</i>) is not in <em>T</em> Then there must be a path 
in<em>T</em> from <em>x</em> to <em>y</em> since <em>T</em> is connected. If (
<i>v</i>,<i>w</i>) is the first edge on this path with one edge in <i>V</i>', 
if we delete it and replace it with (<i>x</i>, <i>y</i>) we get a spanning tree.
</p> 
<p> This tree must have smaller weight than <em>T</em>, since <i>W</i>(<i>v</i>
,<i>w</i>)&gt;<i>W</i>(<i>x</i>,<i>y</i>). Thus <em>T</em> could not have been 
the MST.</p> 
<p> </p> <br>
 Prim's Algorithm is correct! 
<p> Thus we cannot go wrong with the greedy strategy the way we could with the 
traveling salesman problem.</p> 
<p> Listening to Part 18-6 </p> 
<p> </p> 
<p>But how fast is Prim's?</p> 
<p> That depends on what data structures are used. In the simplest 
implementation, we can simply mark each vertex as tree and non-tree and search 
always from scratch:</p> 
<p> </p> 
<pre> 
<p> Select an arbitrary vertex to start. </p>
<p> While (there are non-tree vertices) </p>
<p> select minimum weight edge between tree and fringe </p>
<p> add the selected edge and vertex to the tree </p>
<p> </p></pre> 
<p> This can be done in <i>O</i>(<i>n m</i>) time, by doing a DFS or BFS to 
loop through all edges, with a constant time test per edge, and a total of<em>n
</em> iterations. </p> 
<p> Can we do faster? If so, we need to be able to identify fringe vertices 
and the minimum cost edge associated with it, fast. We will augment an 
adjacency list with fields maintaining fringe information.</p> 
<p> Vertex: </p> 
<p> </p> 
<dl> 
<dt></dt> 
<dd> <em>fringelink</em> pointer to next vertex in fringe list. </dd> 
<dt></dt> 
<dd> <em>fringe weight</em> cheapest edge linking <em>v</em> to <em>l</em>. 
</dd> 
<dt></dt> 
<dd> <em>parent</em> other vertex with <em>v</em> having fringeweight. </dd> 
<dt></dt> 
<dd> <em>status</em> intree, fringe, unseen. </dd> 
<dt></dt> 
<dd> <em>adjacency list</em> the list of edges. 
<p> </p> </dd> </dl> 
<p> Listening to Part 18-8 </p> 
<p> Finding the minimum weight fringe-edge takes <i>O</i>(<i>n</i>) time - 
just bump through fringe list.</p> 
<p> After adding a vertex to the tree, running through its adjacency list to 
update the cost of adding fringe vertices (there may be a cheaper way through 
the new vertex) can be done in<i>O</i>(<i>n</i>) time. </p> 
<p> Total time is  . </p> 
<p> Listening to Part 18-9 </p> 
<p> </p> 
<p>Kruskal's Algorithm</p> 
<p> Since an easy lower bound argument shows that every edge must be looked at 
to find the minimum spanning tree, and the number of edges , Prim's algorithm 
is optimal in the worst case. Is that all she wrote? &nbsp;</p> 
<p> The complexity of Prim's algorithm is independent of the number of edges. 
Can we do better with sparse graphs? Yes! &nbsp;</p> 
<p> Kruskal's algorithm is also greedy. It repeatedly adds the smallest edge 
to the spanning tree that does not create a cycle. Obviously, this gives a 
spanning tree, but is it minimal?</p> 
<p> Listening to Part 18-10 </p> 
<p> </p> 
<p>Why is Kruskal's algorithm correct?</p> 
<p> Theorem: Let <em>G</em> be a weighted graph and let  . If <i>E</i>' is 
contained in a MST<em>T</em> and <em>e</em> is the smallest edge in <i>E</i>-<i>
E</i>' which does not create a cycle,  . </p> 
<p> Proof: As before, suppose <em>e</em> is not in <em>T</em>. Adding <em>e
</em> to <em>T</em> makes a cycle. Deleting another edge from this cycle leaves 
a connected graph, and if it is one from<i>E</i>-<i>E</i>' the cost of this 
tree goes down. Since such an edge exists,<em>T</em> could not be a MST. </p> 
<p> </p> <br>
Listening to Part 18-11 
<p> </p> 
<p>How fast is Kruskal's algorithm?</p> 
<p> What is the simplest implementation? </p> 
<p> </p> 
<ul> 
<li> Sort the <em>m</em> edges in  time.</li> 
<li> For each edge in order, test whether it creates a cycle the forest we 
have thus far built - if so discard, else add to forest. With a BFS/DFS, this 
can be done in<i>O</i>(<i>n</i>) time (since the tree has at most <em>n</em> 
edges).
<p> </p></li> </ul> 
<p> The total time is <i>O</i>(<i>mn</i>), but can we do better? </p> 
<p> Kruskal's algorithm builds up connected components. Any edge where both 
vertices are in the same connected component create a cycle. Thus if we can 
maintain which vertices are in which component fast, we do not have test for 
cycles!</p> 
<p> </p> 
<pre> 
<p> Put the edges in a heap </p>
<p> <i>count</i>=0 </p>
<p> while (<i>count</i> &lt; <i>n</i>-1) do </p>
<p> get next edge (<i>v</i>,<i>w</i>) </p>
<p> if (component (v)  component(w)) </p>
<p> add to T </p>
<p> component (v)=component(w) </p>
<p> </p></pre> 
<p> If we can test components in  , we can find the MST in  ! </p> 
<p> <em>Question:</em> Is  better than  ? </p> 
<p> Listening to Part 19-6 </p> 
<p> </p> 
<p>Union-Find Programs</p> 
<p> Our analysis that Kruskal's MST algorithm is  requires a fast way to test 
whether an edge links two vertices in the same connected component. &nbsp;</p> 
<p> Thus we need a data structure for maintaining sets which can test if two 
elements are in the same and merge two sets together. These can be implemented 
by<em>UNION</em> and <em>FIND</em> operations: </p> 
<p> </p> 
<pre> 
<p> Is </p>
<p> <i>t</i>= Find </p>
<p> <i>u</i>= Find </p>
<p> Return (Is <i>t</i>=<i>u</i>?) </p>
<p> </p></pre> 
<p> </p> 
<pre> 
<p> Make </p>
<p> </p>
<p> </p>
<p> Union(<i>t</i>, <i>u</i>) </p>
<p> </p></pre> 
<p> <em>Find</em> returns the name of the set and <em>Union</em> sets the 
members of<em>t</em> to have the same name as <em>u</em>. </p> 
<p> We are interested in minimizing the time it takes to execute <em>any</em> 
sequence of unions and finds.</p> 
<p> A simple implementation is to represent each set as a tree, with pointers 
from a node to its parent. Each element is contained in a node, and the<em>name
</em> of the set is the key at the root: </p> 
<p> Listening to Part 19-7 </p> 
<p> </p> <br>
 In the worst case, these structures can be very unbalanced: 
<p> </p> 
<pre> 
<p> For <i>i</i> = 1 to <i>n</i>/2 do </p>
<p> UNION(i,i+1) </p>
<p> For <i>i</i> = 1 to <i>n</i>/2 do </p>
<p> FIND(1) </p>
<p> </p></pre> 
<p> We want the limit the height of our trees which are effected by <em>UNIONs
</em>. When we union, we can make the tree with fewer nodes the child. </p> 
<p> Since the number of nodes is related to the height, the height of the 
final tree will increase only if both subtrees are of equal height!</p> 
<p> Lemma: If <i>Union</i>(<i>t</i>,<i>v</i>) attaches the root of <em>v</em> 
as a subtree of<em>t</em> iff the number of nodes in <em>t</em> is greater than 
or equal to the number in<em>v</em>, after any sequence of unions, any tree with
<i>h</i>/4 nodes has height at most  . </p> 
<p> Listening to Part 19-8 </p> 
<p> Proof: By induction on the number of nodes <em>k</em>, <i>k</i>=1 has 
height<em>0</em>. </p> 
<p> Assume true to <em>k-1</em> nodes. Let  be the height of the tree </p> 
<p> </p> <br>
 If  then 
<p> If  , then  . </p> 
<p> </p> 
<p> Listening to Part 19-9 </p> 
<p> </p> 
<p>Can we do better?</p> 
<p> We can do <em>unions</em> and <em>finds</em> in  , good enough for 
Kruskal's algorithm. But can we do better?</p> 
<p> The ideal <em>Union-Find</em> tree has depth 1: </p> 
<p> </p> <br>
 On a find, if we are going down a path anyway, why not change 
the pointers to point to the root?
<p> </p> <br>
 This path compression will let us do better than  for <em>n</em>
 union-finds. &nbsp;
<p> <i>O</i>(<i>n</i>)? Not quite ... Difficult analysis shows that it takes  
time, where is the inverse Ackerman function and  number of atoms in the 
universe)=5.</p> 
<p> </p> <b>Next:</b> Lecture 18 - shortest <b>Up:</b> Table of contents <b>
Previous:</b> Lecture 16 - applications 
<h1>Lecture 18 - shortest path algorthms</h1> 
<p> Listening to Part 20-7 </p> 
<p> <em> 25.1-1 Give two more shortest path trees for the following graph: 
</em></p> 
<p><em> </em></p> <em> <br>
</em> <em> </em> Run through Dijkstra's algorithm, 
and see where there are ties which can be arbitrarily selected. &nbsp;
<p> There are two choices for how to get to the third vertex <em>x</em>, both 
of which cost 5.</p> 
<p> There are two choices for how to get to vertex <em>v</em>, both of which 
cost 9.</p> 
<p> Listening to Part 19-1 </p> 
<p> </p> 
<p>Lessons from the Backtracking contest</p> 
<p> </p> 
<ul> 
<li> As predicted, the speed difference between the fastest programs and 
average program dwarfed the difference between a supercomputer and a 
microcomputer. Algorithms have a bigger impact on performance than hardware! 
&nbsp;</li> 
<li> Different algorithms perform differently on different data. Thus even 
hard problems may be tractable on the kind of data you might be interested in.
</li> 
<li> None of the programs could efficiently handle all instances for  . We 
will find out why after the midterm, when we discuss NP-completeness.</li> 
<li> Many of the fastest programs were very short and simple (KISS). My bet is 
that many of the enhancements students built into them actually showed them 
down! This is where profiling can come in handy.</li> 
<li> The fast programs were often recursive. 
<p> </p></li> </ul> 
<p> Listening to Part 19-3 </p> 
<p> </p> 
<p>Winning Optimizations</p> 
<p> </p> 
<ul> 
<li> Finding a good initial solution via randomization or heuristic 
improvement helped by establishing a good upper bound, to constrict search. 
&nbsp;</li> 
<li> Using half the largest vertex degree as a lower bound similarly 
constricted search.</li> 
<li> Pruning a partial permutation the instant an edge was  the target made 
the difference in going from (say) 8 to 18.</li> 
<li> Positioning the partial permutation vertices separated by <em>b</em> 
instead of 1 meant significantly earlier cutoffs, since any edge does the job.
</li> 
<li> Mirror symmetry can only save a factor of 2, but perhaps more could 
follow from partitioning the vertices into equivalence classes by the same 
neighborhood.
<p> </p></li> </ul> 
<p> Listening to Part 19-10 </p> 
<p> </p> 
<p>Shortest Paths</p> 
<p> Finding the shortest path between two nodes in a graph arises in many 
different applications: &nbsp;</p> 
<p> </p> 
<ul> 
<li> Transportation problems - finding the cheapest way to travel between two 
locations. &nbsp;</li> 
<li> Motion planning - what is the most natural way for a cartoon character to 
move about a simulated environment. &nbsp;</li> 
<li> Communications problems - how look will it take for a message to get 
between two places? Which two locations are furthest apart, ie. what is the<em>
diameter</em> of the network. &nbsp;&nbsp; 
<p> </p></li> </ul> 
<p> Listening to Part 20-1 </p> 
<p> </p> 
<p>Shortest Paths and Sentence Disambiguation</p> 
<p> In our work on reconstructing text typed on an (overloaded) telephone 
keypad, we had to select which of many possible interpretations was most 
likely. &nbsp;&nbsp;</p> 
<p> </p> <br>
 We constructed a graph where the vertices were the possible 
words/positions in the sentence, with an edge between possible neighboring 
words.
<p> Listening to Part 20-2 </p> 
<p> </p> <br>
 The weight of each edge is a function of the probability that 
these two words will be next to each other in a sentence. `hive me' would be 
less than `give me', for example.
<p> The final system worked extremely well - identifying over 99% of 
characters correctly based on grammatical and statistical constraints.</p> 
<p> Dynamic programming (the Viterbi algorithm) can be used on the sentences 
to obtain the same results, by finding the shortest paths in the underlying 
DAG. &nbsp;</p> 
<p> Listening to Part 20-3 </p> 
<p> </p> 
<p>Finding Shortest Paths</p> 
<p> In an unweighted graph, the cost of a path is just the number of edges on 
the shortest path, which can be found in<i>O</i>(<i>n</i>+<i>m</i>) time via 
breadth-first search. &nbsp;</p> 
<p> In a weighted graph, the weight of a path between two vertices is the sum 
of the weights of the edges on a path.</p> 
<p> BFS will not work on weighted graphs because sometimes visiting more edges 
can lead to shorter distance, ie. 1+1+1+1+1+1+1 &lt; 10.</p> 
<p> Note that there can be an exponential number of shortest paths between two 
nodes - so we cannot report all shortest paths efficiently.</p> 
<p> Note that negative cost cycles render the problem of finding the shortest 
path meaningless, since you can always loop around the negative cost cycle more 
to reduce the cost of the path.</p> 
<p> Thus in our discussions, we will assume that all edge weights are 
positive. Other algorithms deal correctly with negative cost edges.</p> 
<p> Minimum spanning trees are uneffected by negative cost edges. </p> 
<p> Listening to Part 20-4 </p> 
<p> </p> 
<p>Dijkstra's Algorithm</p> 
<p> We can use <em>Dijkstra's algorithm</em> to find the shortest path between 
any two vertices<em>s</em> and <em>t</em> in <em>G</em>. &nbsp; </p> 
<p> The principle behind Dijkstra's algorithm is that if  is the shortest path 
from<em>s</em> to <em>t</em>, then  had better be the shortest path from <em>s
</em> to <em>x</em>. </p> 
<p> This suggests a dynamic programming-like strategy, where we store the 
distance from<em>s</em> to all nearby nodes, and use them to find the shortest 
path to more distant nodes.</p> 
<p> The shortest path from <em>s</em> to <em>s</em>, <i>d</i>(<i>s</i>,<i>s</i>
)=0. If all edge weights are positive, the<em>smallest</em> edge incident to 
<em>s</em>, say (<i>s</i>,<i>x</i>), defines <i>d</i>(<i>s</i>,<i>x</i>). </p> 
<p> We can use an array to store the length of the shortest path to each node. 
Initialize each to to start. </p> 
<p> Soon as we establish the shortest path from <em>s</em> to a new node <em>x
</em>, we go through each of its incident edges to see if there is a better way 
from<em>s</em> to other nodes thru <em>x</em>. </p> 
<p> Listening to Part 20-5 </p> 
<p> </p> 
<pre> 
<p> </p>
<p> for <i>i</i>=1 to <em>n</em>, </p>
<p> for each edge (<i>s</i>,<i>v</i>), <i>dist</i>[<i>v</i>]=<i>d</i>(<i>s</i>,
<i>v</i>) </p>
<p> last=<em>s</em> </p>
<p> while (  ) </p>
<p> select <em>v</em> such that </p>
<p> for each (<i>v</i>,<i>x</i>), </p>
<p> last=<em>v</em> </p>
<p> </p>
<p> </p></pre> 
<p> Complexity  if we use adjacency lists and a Boolean array to mark what is 
known.</p> 
<p> <em>This is essentially the same as Prim's algorithm</em>. </p> 
<p> An  implementation of Dijkstra's algorithm would be faster for sparse 
graphs, and comes from using a heap of the vertices (ordered by distance), and 
updating the distance to each vertex (if necessary) in time for each edge out 
from freshly known vertices.</p> 
<p> Even better,  follows from using Fibonacci heaps, since they permit one to 
do a decrease-key operation in<i>O</i>(1) amortized time. </p> 
<p> Listening to Part 20-8 </p> 
<p> </p> 
<p>All-Pairs Shortest Path</p> 
<p> Notice that finding the shortest path between a pair of vertices (<i>s</i>,
<i>t</i>) in worst case requires first finding the shortest path from <em>s</em>
 to all other vertices in the graph. &nbsp;</p> 
<p> Many applications, such as finding the center or diameter of a graph, 
require finding the shortest path between all pairs of vertices.</p> 
<p> We can run Dijkstra's algorithm <em>n</em> times (once from each possible 
start vertex) to solve all-pairs shortest path problem in . Can we do better? 
</p> 
<p> Improving the complexity is an open question but there is a <em>super-slick
</em> dynamic programming algorithm which also runs in  . </p> 
<p> Listening to Part 20-9 </p> 
<p> </p> 
<p>Dynamic Programming and Shortest Paths</p> 
<p> The four-step approach to dynamic programming is: </p> 
<ol> 
<li> Characterize the structure of an optimal solution.</li> 
<li> Recursively define the value of an optimal solution.</li> 
<li> Compute this recurrence in a bottom-up fashion.</li> 
<li> Extract the optimal solution from computed information. </li> </ol> 
<p> From the adjacency matrix, we can construct the following matrix: </p> 
<p> </p> 
<pre> 
<p>  , if  and  is not in <em>E</em> </p>
<p> <i>D</i>[<i>i</i>,<i>j</i>] = <i>w</i>(<i>i</i>,<i>j</i>), if </p>
<p> <i>D</i>[<i>i</i>,<i>j</i>] = 0, if <i>i</i>=<i>j</i> </p></pre> 
<p> This tells us the shortest path going through no intermediate nodes. </p> 
<p> There are several ways to characterize the shortest path between two nodes 
in a graph. Note that the shortest path from<em>i</em> to <em>j</em>,  , using 
at most<em>M</em> edges consists of the shortest path from <em>i</em> to <em>k
</em> using at most <em>M-1</em> edges + <i>W</i>(<i>k</i>, <i>j</i>) for some 
<em>k</em>. </p> 
<p> Listening to Part 20-10 </p> 
<p> This suggests that we can compute all-pair shortest path with an induction 
based on the number of edges in the optimal path.</p> 
<p> Let  be the length of the shortest path from <em>i</em> to <em>j</em> 
using at most<em>m</em> edges. </p> 
<p> What is  ? </p> 
<p> </p> 
<p> </p> 
<p> What if we know  for all <em>i,j</em>? </p> 
<p> </p> 
<p> since <i>w</i>[<i>k</i>, <i>k</i>]=0 </p> 
<p> This gives us a recurrence, which we can evaluate in a bottom up fashion: 
</p> 
<p> </p> 
<pre> 
<p> for <i>i</i>=1 to <em>n</em> </p>
<p> for <i>j</i>=1 to <em>n</em> </p>
<p> </p>
<p> for <i>k</i>=1 to <em>n</em> </p>
<p>  =Min(  ,  ) </p>
<p> </p></pre> 
<p> This is an  algorithm just like matrix multiplication, but it only goes 
from<em>m</em> to <em>m+1</em> edges. </p> 
<p> Listening to Part 20-11 </p> 
<p> Since the shortest path between any two nodes must use at most <em>n</em> 
edges (unless we have negative cost cycles), we must repeat that procedure<em>n
</em> times (<i>m</i>=1 to <em>n</em>) for an  algorithm. </p> 
<p> We can improve this to  with the observation that any path using at most 
<em>2m</em> edges is the function of paths using at most <em>m</em> edges each. 
This is just like computing . So a logarithmic number of multiplications 
suffice for exponentiation.</p> 
<p> Although this is slick, observe that even  is slower than running 
Dijkstra's algorithm starting from each vertex!</p> 
<p> Listening to Part 20-12 </p> 
<p> </p> 
<p>The Floyd-Warshall Algorithm</p>  &nbsp; 
<p> An alternate recurrence yields a more efficient dynamic programming 
formulation. Number the vertices from<em>1</em> to <em>n</em>. </p> 
<p> <em> Let  be the shortest path from <em>i</em> to <em>j</em> using only 
vertices from 1, 2,...,<i>k</i> as possible intermediate vertices. </em> </p> 
<p> What is  ? With no intermediate vertices, any path consists of at most one 
edge, so . </p> 
<p> In general, adding a new vertex <em>k+1</em> helps iff a path goes through 
it, so</p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> Although this looks similar to the previous recurrence, it isn't. The 
following algorithm implements it:</p> 
<p> </p> 
<pre> 
<p> </p>
<p> for <i>k</i>=1 to <em>n</em> </p>
<p> for <i>i</i>=1 to <em>n</em> </p>
<p> for <i>j</i>=1 to <em>n</em> </p>
<p> </p>
<p> </p></pre> 
<p> This obviously runs in  time, which asymptotically is no better than a 
calls to Dijkstra's algorithm. However, the loops are so tight and it is so 
short and simple that it runs better in practice by a constant factor.</p> 
<p> </p> <b>Next:</b> Lecture 19 - satisfiability <b>Up:</b> Table of contents 
<b>Previous:</b> Lecture 17 - minimum 
<h1>Lecture 19 - satisfiability</h1> 
<p> Listening to Part 21-7 </p> 
<p> </p> 
<p>The Theory of NP-Completeness</p> 
<p> Several times this semester we have encountered problems for which we 
couldn't find efficient algorithms, such as the traveling salesman problem. We 
also couldn't prove an exponential time lower bound for the problem. &nbsp;</p> 
<p> By the early 1970s, literally hundreds of problems were stuck in this 
limbo. The theory of NP-Compleness, developed by Stephen Cook and Richard Karp, 
provided the tools to show that all of these problems were really the same 
problem.</p> 
<p> Listening to Part 21-8 </p> 
<p> </p> 
<p>Polynomial vs. Exponential Time</p>  &nbsp;&nbsp; 
<p> </p> <em>n</em> <i>f</i>(<i>n</i>) = <i>n</i> <i>f</i>(<i>n</i>) = <i>n</i>
! <br>
10  0.01  s  0.1  s  1  s  3.63 ms <br>
 20  0.02  s  0.4  s  1 ms  77.1 
years <br>
 30  0.03  s  0.9  s  1 sec  years <br>
 40  0.04  s  1.6  s  18.3 
min <br>
 50  0.05  s  2.5  s  13 days <br>
100  0.1  s  10  s  years <br>
 
1,000  1.00  s  1 ms <br>
<br>

<p> </p> 
<p> Listening to Part 21-9 </p> 
<p> </p> 
<p>The Main Idea</p> 
<p> Suppose I gave you the following algorithm to solve the <em>bandersnatch
</em> problem: &nbsp;&nbsp; </p> 
<p> </p> 
<pre> 
<p> Bandersnatch(<em>G</em>) </p>
<p> Convert <em>G</em> to an instance of the Bo-billy problem <em>Y</em>. </p>
<p> Call the subroutine Bo-billy on <em>Y</em> to solve this instance. </p>
<p> Return the answer of Bo-billy(<em>Y</em>) as the answer to <em>G</em>. </p>
<p> </p></pre> 
<p> Such a translation from instances of one type of problem to instances of 
another type such that answers are preserved is called a<em>reduction</em>. 
&nbsp;</p> 
<p> Now suppose my reduction translates <em>G</em> to <em>Y</em> in <i>O</i>(
<i>P</i>(<i>n</i>)): </p> 
<ol> 
<li> If my Bo-billy subroutine ran in <i>O</i>(<i>P</i>'(<i>n</i>)) I can 
solve the Bandersnatch problem in<i>O</i>(<i>P</i>(<i>n</i>)+<i>P</i>'(<i>n</i>
))</li> 
<li> If I know that  is a lower-bound to compute Bandersnatch, then  must be a 
lower-bound to compute Bo-billy.
<p> </p></li> </ol> 
<p> The second argument is the idea we use to prove problems hard! </p> 
<p> Listening to Part 21-10 </p> 
<p> </p> 
<p>Convex Hull and Sorting</p> 
<p> A nice example of a reduction goes from sorting numbers to the convex hull 
problem: &nbsp;&nbsp;</p> 
<p> </p> <br>
 We must translate each number to a point. We can map <em>x</em> 
to . 
<p> </p> <br>
 Why? That means each integer is mapped to a point on the 
parabola . 
<p> Listening to Part 21-11 </p> 
<p> Since this parabola is convex, every point is on the convex hull. Further 
since neighboring points on the convex hull have neighboring<em>x</em> values, 
the convex hull returns the points sorted by<em>x</em>-coordinate, ie. the 
original numbers.</p> 
<p> </p> 
<pre> 
<p> Sort(<em>S</em>) </p>
<p> For each  , create point  . </p>
<p> Call subroutine convex-hull on this point set. </p>
<p> From the leftmost point in the hull, </p>
<p> read off the points from left to right. </p>
<p> </p></pre> 
<p> Creating and reading off the points takes <i>O</i>(<i>n</i>) time. </p> 
<p> What does this mean? Recall the sorting lower bound of  . If we could do 
convex hull in better than , we could sort faster than  - which violates our 
lower bound.</p> 
<p> <em>Thus convex hull must take  as well!!!</em> </p> 
<p> Observe that any  convex hull algorithm also gives us a complicated but 
correct sorting algorithm as well. </p> 
<p> Listening to Part 22-2 </p> 
<p> </p> 
<p>What is a problem?</p> 
<p> A <em>problem</em> is a general question, with parameters for the input 
and conditions on what is a satisfactory answer or solution. &nbsp;&nbsp;</p> 
<p> An instance is a problem with the input parameters specified. </p> 
<p> Example: The Traveling Salesman </p> 
<p> Problem: Given a weighted graph <em>G</em>, what tour  minimizes  . </p> 
<p> Instance:  ,  ,  ,  ,  , </p> 
<p> </p> <br>
 Solution:  cost= <em>27</em> 
<p> A problem with answers restricted to <em>yes</em> and <em>no</em> is 
called a<em>decision problem</em>. Most interesting optimization problems can 
be phrased as decision problems which capture the essence of the computation. 
&nbsp;</p> 
<p> Listening to Part 22-3 </p> 
<p> Example: The Traveling Salesman Decision Problem. &nbsp; </p> 
<p> Given a weighted graph <em>G</em> and integer <em>k</em>, does there exist 
a traveling salesman tour with cost <em>k</em>? </p> 
<p> Using binary search and the decision version of the problem we can find 
the optimal TSP solution.</p> 
<p> For convenience, from now on we will talk <em>only</em> about decision 
problems.</p> 
<p> Note that there are many possible ways to encode the input graph: 
adjacency matrices, edge lists, etc. All reasonable encodings will be within 
polynomial size of each other.</p> 
<p> The fact that we can ignore minor differences in encoding is important. We 
are concerned with the difference between algorithms which are polynomial and 
exponential in the size of the input.</p> 
<p> Listening to Part 22-4 </p> 
<p> </p> 
<p>Satisfiability</p> 
<p> Consider the following logic problem: &nbsp;&nbsp; </p> 
<p> Instance: A set <em>V</em> of variables and a set of clauses <em>C</em> 
over<em>V</em>. </p> 
<p> Question: Does there exist a satisfying truth assignment for <em>C</em>? 
</p> 
<p> Example 1:  and </p> 
<p> A clause is satisfied when at least one literal in it is TRUE. <em>C</em> 
is satisfied when TRUE. </p> 
<p> Example 2:  , </p> 
<p> </p> 
<p> </p> 
<p> Although you try, and you try, and you try and you try, you can get no 
satisfaction. &nbsp;</p> 
<p> There is no satisfying assigment since  must be FALSE (third clause), so  
must be FALSE (second clause), but then the first clause is unsatisfiable!</p> 
<p> For various reasons, it is known that satisfiability is a hard problem. 
Every top-notch algorithm expert in the world (and countless other, lesser 
lights) have tried to come up with a fast algorithm to test whether a given set 
of clauses is satisfiable, but all have failed.</p> 
<p> Listening to Part 22-5 </p> 
<p> Further, many strange and impossible-to-believe things have been shown to 
be true if someone in fact did find a fast satisfiability algorithm.</p> 
<p> Clearly, Satisfiability is in <em>NP</em>, since we can guess an 
assignment of TRUE, FALSE to the literals and check it in polynomial time.</p> 
<p> Listening to Part 22-10 </p> 
<p> </p> 
<p>P versus NP</p> 
<p> The precise distinction between whether a problem is in <em>P</em> or <em>
NP</em> is somewhat technical, requiring formal language theory and Turing 
machines to state correctly. &nbsp;</p> 
<p> However, intuitively a problem is in <em>P</em>, (ie. polynomial) if it 
can be solved in time polynomial in the size of the input.</p> 
<p> A problem is in <em>NP</em> if, given the answer, it is possible to verify 
that the answer is correct within time polynomial in the size of the input. 
&nbsp;</p> 
<p> Example <em>P</em> - Is there a path from <em>s</em> to <em>t</em> in <em>G
</em> of length less than <em>k</em>. </p> 
<p> Example <em>NP</em> - Is there a TSP tour in <em>G</em> of length less than
<em>k</em>. Given the tour, it is easy to add up the costs and convince me it 
is correct.</p> 
<p> Example <em>not NP</em> - How many TSP tours are there in <em>G</em> of 
length less than<em>k</em>. Since there can be an exponential number of them, 
we cannot count them all in polynomial time.</p> 
<p> Don't let this issue confuse you - the important idea here is of 
reductions as a way of proving hardness.</p> 
<p> Listening to Part 22-7 </p> 
<p> </p> 
<p>3-Satisfiability</p> 
<p> Instance: A collection of clause <em>C</em> where each clause contains 
exactly<em>3</em> literals, boolean variable <em>v</em>. &nbsp; </p> 
<p> Question: Is there a truth assignment to <em>v</em> so that each clause is 
satisfied?</p> 
<p> Note that this is a more restricted problem than SAT. If <em>3</em>-SAT is 
NP-complete, it implies SAT is NP-complete but not visa-versa, perhaps long 
clauses are what makes SAT difficult?!</p> 
<p> After all, <em>1</em>-Sat is trivial! </p> 
<p> <b>Theorem:</b> <em>3</em>-SAT is NP-Complete </p> 
<p> <b>Proof:</b> <em>3</em>-SAT is NP - given an assignment, just check that 
each clause is covered. To prove it is complete, a reduction from must be 
provided. We will transform each clause independantly based on its<em>length
</em>. </p> 
<p> Suppose the clause  contains <em>k</em> literals. </p> 
<p> </p> 
<ul> 
<li> If <i>k</i>=1, meaning  , create two new variables  and four new <em>3
</em>-literal clauses: 
<p>  ,  ,  ,  . </p>
<p> Note that the only way all four of these can be satisfied is if <em>z</em> 
is TRUE.</p>
<p> Listening to Part 22-8</p></li> 
<li> If <i>k</i>=2, meaning  , create one new variable  and two new clauses:  ,
</li> 
<li> If <i>k</i>=3, meaning  , copy into the <em>3</em>-SAT instance as it is.
</li> 
<li> If <i>k</i>&gt;3, meaning  , create <em>n-3</em> new variables and <em>n-2
</em> new clauses in a chain:  , ... 
<p> </p></li> </ul> 
<p> If none of the original variables in a clause are TRUE, there is no way to 
satisfy all of them using the additional variable:</p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> But if any literal is TRUE, we have <em>n-3</em> free variables and <em>n-3
</em> remaining <em>3</em>-clauses, so we can satisfy each of them. </p> 
<p> Since any SAT solution will also satisfy the <em>3</em>-SAT instance and 
any<em>3</em>-SAT solution sets variables giving a SAT solution - the problems 
are equivallent. If there were<em>n</em> clauses and <em>m</em> total literals 
in the SAT instance, this transform takes<i>O</i>(<i>m</i>) time, so SAT and 
<em>3</em>-SAT. </p> 
<p> Note that a slight modification to this construction would prove <em>4</em>
-SAT, or<em>5</em>-SAT,... also NP-complete. However, it breaks down when we 
try to use it for<em>2</em>-SAT, since there is no way to stuff anything into 
the chain of clauses. It turns out that resolution gives a polynomial time 
algorithm for<em>2</em>-SAT. </p> 
<p> Listening to Part 22-9 </p> 
<p> Having at least <em>3</em>-literals per clause is what makes the problem 
difficult. Now that we have shown<em>3</em>-SAT is NP-complete, we may use it 
for further reductions. Since the set of<em>3</em>-SAT instances is smaller and 
more regular than the<em>SAT</em> instances, it will be easier to use <em>3</em>
-SAT for future reductions. Remember the direction to reduction!</p> 
<p> </p> 
<p> </p> 
<p></p> <b>Next:</b> Lecture 20 - integer <b>Up:</b> Table of contents <b>
Previous:</b> Lecture 18 - shortest 
<h1>Lecture 20 - integer programming</h1> 
<p> Listening to Part 22-6 </p> 
<p> <em> 36.4-5 Give a polynomial-time algorithm to satisfy Boolean formulas 
in disjunctive normal form. &nbsp;</em> </p> Satisfying one clause in DFS 
satisfied the whole formula. One clause can always be satisfied iff it does not 
contain both a variable and its complement.
<p> Why not use this reduction to give a polynomial-time algorithm for 3-SAT? 
The DNF formula can become exponentially large and hence the reduction cannot 
be done in polynomial time.</p> 
<p> Listening to Part 24-2 </p> 
<p> </p> 
<p>A Perpetual Point of Confusion</p> 
<p> Note carefully the direction of the reduction. &nbsp; </p> 
<p> We must transform <em>every</em> instance of a known NP-complete problem 
to an instance of the problem we are interested in. If we do the reduction the 
other way, all we get is a slow way to solve<em>x</em>, by using a subroutine 
which probably will take exponential time.</p> 
<p> This always is confusing at first - it seems bass-ackwards. Make sure you 
understand the direction of reduction now - and think back to this when you get 
confused.</p> 
<p> Listening to Part 24-3 </p> 
<p> </p> 
<p>Integer Programming</p> 
<p> Instance: A set <em>v</em> of integer variables, a set of inequalities 
over these variables, a function<i>f</i>(<i>v</i>) to maximize, and integer <em>
B</em>. &nbsp; </p> 
<p> Question: Does there exist an assignment of integers to <em>v</em> such 
that all inequalities are true and ? </p> 
<p> Example: </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> A solution to this is  ,  . </p> 
<p> Example: </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> Since the maximum value of <i>f</i>(<i>v</i>) given the constraints is  , 
there is no solution.</p> 
<p> <b>Theorem: Integer Programming is NP-Hard</b> </p> 
<p> <b>Proof:</b> By reduction from Satisfiability </p> 
<p> Any set instance has boolean variables and clauses. Our Integer 
programming problem will have twice as many variables as the SAT instance, one 
for each variable and its compliment, as well as the following inequalities:</p>
<p> Listening to Part 24-4 </p> 
<p> For each variable  in the set problem, we will add the following 
constraints:</p> 
<p> </p> 
<ul> 
<li>  and 
<p> Both IP variables are restricted to values of 0 or 1, which makes them 
equivalent to boolean variables restricted to true/false.</p></li> 
<li> 
<p> Exactly one of the IP variables associated with a given sat variable is 1. 
This means that exactly one of and  are true!</p></li> 
<li> for each clause  in the sat instance, construct a constraint: 
<p> </p>
<p> </p>
<p> Thus at least one IP variable must be one in each clause! Thus satisfying 
the constraint is equivalent to satisfying the clause!</p>
<p> </p></li> </ul> 
<p> Our maximization function and bound are relatively unimportant: <i>B</i>=0.
</p> 
<p> Clearly this reduction can be done in polynomial time. </p> 
<p> Listening to Part 24-5 </p> 
<p> We must show: </p> 
<p> </p> 
<ol> 
<li> Any SAT solution gives a solution to the IP problem. 
<p> In any SAT solution, a TRUE literal corresponds to a <em>1</em> in the IP, 
since if the expression is SATISFIED, at least one literal per clause in TRUE, 
so the sum in the inequality is 1.</p></li> 
<li> Any IP solution gives a SAT solution. 
<p> Given a solution to this IP instance, all variables will be <em>0</em> or 
<em>1</em>. Set the literals correspondly to <em>1</em> variable TRUE and the 
<em>0</em> to FALSE. No boolean variable and its complement will both be true, 
so it is a legal assignment with also must satisfy the clauses.</p></li> </ol> 
<p> Neat, sweet, and NP-complete! </p> 
<p> Listening to Part 24-6 </p> 
<p> </p> 
<p>Things to Notice</p> 
<p> </p> 
<ol> 
<li> The reduction preserved the structure of the problem. Note that the 
reduction did not<em>solve</em> the problem - it just put it in a different 
format.</li> 
<li> The possible IP instances which result are a small subset of the possible 
IP instances, but since some of them are hard, the problem in general must be 
hard.</li> 
<li> The transformation captures the essence of why IP is hard - it has 
nothing to do with big coefficients or big ranges on variables; for restricting 
to 0/1 is enough. A careful study of what properties we do need for our 
reduction tells us a lot about the problem.</li> 
<li> It is not obvious that IP  NP, since the numbers assigned to the 
variables may be too large to write in polynomial time - don't be too hasty!
</li> </ol> <b>Next:</b> Lecture 21 - vertex <b>Up:</b> Table of contents <b>
Previous:</b> Lecture 19 - satisfiability 
<h1>Lecture 21 - vertex cover</h1> 
<p> Listening to Part 24-7 </p> 
<p> <em>36.5-2 - Given an integer  matrix <em>A</em>, and in integer <em>m</em>
-vector<em>b</em>, the 0-1 integer programming problem asks whether there is an 
integer<em>n</em>-vector <em>x</em> with elements in the set (0,1) such that  . 
Prove that 0-1 integer programming is NP-hard (hint: reduce from 3-SAT). &nbsp;
</em> </p> This is really the exact same problem as the previous integer 
programming problem, slightly concealed by:
<p> </p> 
<ul> 
<li> The linear algebra notation - each row is one constraint.</li> 
<li> All inequalities are  - multiply both sides by -1 to reverse the 
constraint from to  if necessary. 
<p> </p></li> </ul> 
<p> Listening to Part 24-8 </p> 
<p> </p> 
<p>Vertex Cover</p> 
<p> Instance: A graph <i>G</i>=(<i>V</i>, <i>E</i>), and integer  &nbsp; </p> 
<p> Question: Is there a subset of at most <em>k</em> vertices such that every 
 has at least one vertex in the subset?</p> 
<p> </p> <br>
 Here, four of the eight vertices are enough to cover. It is 
trivial to find<em>a</em> vertex cover of a graph - just take all the vertices. 
The tricky part is to cover with as small a set as possible.
<p> <b>Theorem:</b> Vertex cover is NP-complete. &nbsp; </p> 
<p> <b>Proof:</b> VC in in <em>NP</em> - guess a subset of vertices, count 
them, and show that each edge is covered.</p> 
<p> To prove completeness, we show <em>3</em>-SAT and VC. From a <em>3</em>
-SAT instance with<em>n</em> variables and <em>C</em> clauses, we construct a 
graph with<em>2N+3C</em> vertices. </p> 
<p> Listening to Part 24-9 </p> 
<p> For each variable, we create two vertices connected by an edge: </p> 
<p> </p> <br>
 To cover each of these edges, at least <em>n</em> vertices must 
be in the cover, one for each pair. For each clause, we create three new 
vertices, one for each literal in each clause. Connect these in a triangle.
<p> At least two vertices per triangle must be in the cover to take care of 
edges in the triangle, for a total of at least<em>2C</em> vertices. </p> 
<p> Finally, we will connect each literal in the flat structure to the 
corresponding vertices in the triangles which share the same literal.</p> 
<p> </p> <br>
Listening to Part 24-10 
<p> <em>Claim:</em> This graph will have a vertex cover of size <em>N+2C</em> 
if and only if the expression is satisfiable.</p> 
<p> By the earlier analysis, any cover must have at least <em>N+2C</em> 
vertices. To show that our reduction is correct, we must show that:</p> 
<p> </p> 
<ol> 
<li> <em>Every satisfying truth assignment gives a cover.</em> 
<p> Select the <em>N</em> vertices cooresponding to the TRUE literals to be in 
the cover. Since it is a satisfying truth assignment, at least one of the three 
cross edges associated with each clause must already be covered - pick the 
other two vertices to complete the cover.</p></li> 
<li> <em>Every vertex cover gives a satisfying truth assignment</em>. 
<p> Every vertex cover must contain <em>n</em> first stage vertices and <em>2C
</em> second stage vertices. Let the first stage vertices define the truth 
assignment.</p>
<p> To give the cover, at least one cross-edge must be covered, so the truth 
assignment satisfies.</p>
<p> </p></li> </ol> 
<p> For a cover to have <em>N+2C</em> vertices, all the cross edges must be 
incident on a selected vertex.</p> 
<p> Let the <em>N</em> selected vertices from the first stage coorespond to 
TRUE literals. If there is a satisfying truth assignment, that means at least 
one of the three cross edges from each triangle is incident on a TRUE vertex.
</p> 
<p> By adding the other two vertices to the cover, we cover all edges 
associated with the clause.</p> 
<p> <em>Every SAT defines a cover and Every Cover Truth values for the SAT!
</em> </p> 
<p> Example:  ,  . </p> 
<p> </p> <br>
Listening to Part 25-1 
<p> </p> 
<p>Starting from the Right Problem</p> 
<p> As you can see, the reductions can be very clever and very complicated. 
While theoretically any<em>NP</em>-complete problem can be reduced to any other 
one, choosing the correct one makes finding a reduction much easier.</p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> As you can see, the reductions can be very clever and complicated. While 
theoretically any NP-complete problem will do, choosing the correct one can 
make it much easier.</p> 
<p> </p> 
<p>Maximum Clique</p> Instance: A graph <i>G</i>=( <i>V</i>, <i>E</i>) and 
integer . &nbsp; 
<p> Question: Does the graph contain a clique of <em>j</em> vertices, ie. is 
there a subset of<em>v</em> of size <em>j</em> such that every pair of vertices 
in the subset defines an edge of<em>G</em>? </p> 
<p> Example: this graph contains a clique of size 5. </p> 
<p> </p> <br>
Listening to Part 25-2 
<p> When talking about graph problems, it is most natural to work from a graph 
problem - the only<em>NP</em>-complete one we have is vertex cover! </p> 
<p> <b>Theorem: Clique is <em>NP</em>-complete</b> </p> 
<p> <b>Proof:</b> If you take a graph and find its vertex cover, the remaining 
vertices form an independent set, meaning there are no edges between any two 
vertices in the independent set, for if there were such an edge the rest of the 
vertices could not be a vertex cover. &nbsp;</p> 
<p> </p> <br>
 Clearly the smallest vertex cover gives the biggest independent 
set, and so the problems are equivallent - Delete the subset of vertices in one 
from the total set of vertices to get the order!
<p> Thus finding the maximum independent set must be NP-complete! </p> 
<p> Listening to Part 25-3 </p> 
<p> In an independent set, there are no edges between two vertices. In a 
clique, there are always between two vertices. Thus if we complement a graph 
(have an edge iff there was no edge in the original graph), a clique becomes an 
independent set and an independent set becomes a Clique!</p> 
<p> </p> <br>
 Thus finding the largest clique is NP-complete: 
<p> If <em>VC</em> is a vertex cover in <em>G</em>, then <em>V-VC</em> is a 
clique in<i>G</i>'. If <em>C</em> is a clique in <em>G</em>, <em>V-C</em> is a 
vertex cover in<i>G</i>'. </p> 
<p> Listening to Part 25-4 </p> 
<p> <em> 36.5-1 Prove that subgraph isomorphism is <em>NP</em>-complete. &nbsp;
</em> </p> 
<ol> 
<li> Guessing a subgraph of <em>G</em> and proving it is isomorphism to <em>h
</em> takes  time, so it is in <em>NP</em>.</li> 
<li> Clique and subgraph isomorphism. We must transform all instances of 
clique into some instances of subgraph isomorphism. Clique is a special case of 
subgraph isomorphism!
<p> Thus the following reduction suffices. Let <i>G</i>=<i>G</i>' and  , the 
complete subgraph on<em>k</em> nodes. </p>
<p> </p></li> </ol> 
<p> Listening to Part 25-5 </p> 
<p> </p> 
<p>Integer Partition (Subset Sum)</p> 
<p> Instance: A set of integers <em>S</em> and a target integer <em>t</em>. 
&nbsp;&nbsp;</p> 
<p> Problem: Is there a subset of <em>S</em> which adds up exactly to <em>t
</em>? </p> 
<p> Example:  and <i>T</i>=3754 </p> 
<p> Answer: 1+16+64+256+1040+1093+1284 = <i>T</i> </p> 
<p> Observe that integer partition is a number problem, as opposed to the 
graph and logic problems we have seen to date.</p> 
<p> <b>Theorem:</b> Integer Partition is <em>NP</em>-complete. </p> 
<p> <b>Proof:</b> First, we note that integer partition is in <em>NP</em>. 
Guess a subset of the input number and simply add them up.</p> 
<p> To prove completeness, we show that vertex cover  integer partition. We 
use a data structure called an incidence matrix to represent the graph<em>G</em>
.</p> 
<p> </p> <br>
Listening to Part 25-6 
<p> How many 1's are there in each column? Exactly two. </p> 
<p> How many 1's in a particular row? Depends on the vertex degree. </p> 
<p> The reduction from vertex cover will create <em>n+m</em> numbers from <em>G
</em>. </p> 
<p> The numbers from the vertices will be a base-4 realization of rows from 
the incidence matrix, plus a high order digit:</p> 
<p> </p> 
<p> ie.  becomes  . </p> 
<p> The numbers from the edges will be  . </p> 
<p> The target integer will be </p> 
<p> </p> 
<p> </p> 
<p> Why? Each column (digit) represents an edge. We want a subset of vertices 
which covers each edge. We can only use<em>k</em> x vertex/numbers, because of 
the high order digit of the target.</p> 
<p> </p> 
<p> Listening to Part 25-7 </p> 
<p> We might get only one instance of each edge in a cover - but we are free 
to take extra edge/numbers to grab an extra<em>1</em> per column. </p> 
<p> </p> 
<p><em>VC</em> in <em>G</em>  Integer Partition in <em>S</em></p> Given <em>k
</em> vertices covering <em>G</em>, pick the <em>k</em> cooresponding 
vertex/numbers. Each edge in<em>G</em> is incident on one or two cover 
vertices. If it is one, includes the cooresponding edge/number to give two per 
column.
<p> </p> 
<p>Integer Partition in <em>S</em> <em>VC</em> in <em>G</em></p> Any solution 
to<em>S</em> must contain <em>exactly</em> <em>k</em> vertex/numbers. Why? It 
cannot be more because the target in that digit is<em>k</em> and it cannot be 
less because, with at most<em>3</em> 1's per edge/digit-column, no sum of these 
can carry over into the next column. (This is why base-<em>4</em> number were 
chosen).
<p> This subset of <em>k</em> vertex/numbers must contain at least one 
edge-list per column, since if not there is no way to account for the two in 
each column of the target integer, given that we can pick up at most one 
edge-list using the edge number. (Again, the prevention of carrys across digits 
prevents any other possibilites).</p> 
<p> Neat, sweet, and <em>NP</em>-complete! </p> 
<p> Notice that this reduction could not be performed in polynomial time if 
the number were written in unary 5=11111. Big numbers is what makes integer 
partition hard!</p> 
<p> </p> <b>Next:</b> Lecture 22 - techniques <b>Up:</b> Table of contents <b>
Previous:</b> Lecture 20 - integer 
<h1>Lecture 22 - techniques for proving hardness</h1> 
<p> </p> 
<p>Hamiltonian Cycle</p> 
<p> Instance: A graph <em>G</em> &nbsp; </p> 
<p> Question: Does the graph contains a HC, i.e. an ordered of the vertices  ? 
</p> 
<p> This problem is intimately relates to the Traveling Salesman. </p> 
<p> Question: Is there an ordering of the vertices of a weighted graph such 
that ? </p> 
<p> Clearly,  . Assign each edge in <em>G</em> weight <em>1</em>, any edge not 
in<em>G</em> weight <em>2</em>. This new graph has a Traveling Salesman tour of 
cost<em>n</em> iff the graph is Hamiltonian. Thus TSP is <em>NP</em>-complete 
if we can show HC is<em>NP</em>-complete. </p> 
<p> <b>Theorem: Hamiltonian Circuit is <em>NP</em>-complete</b> </p> 
<p> <b>Proof:</b> Clearly HC is in <em>NP</em>-guess a permutation and check 
it out. To show it is complete, we use vertex cover. A vertex cover instance 
consists of a graph and a constant<em>k</em>, the minimum size of an acceptable 
cover. We must construct another graph. Each edge in the initial graph will be 
represented by the following component:</p> 
<p> </p> <br>
 All further connections to this gadget will be through vertices 
 , ,  and  . The key observation about this gadget is that there are only three 
ways to traverse all the vertices: &nbsp;
<p> </p> <br>
 Note that in each case, we exit out the same side we entered. 
Each side of each edge gadget is associated with a vertex. Assuming some 
arbitrary order to the edges incident on a particular vertex, we can link 
successive gadgets by edges forming a chain of gadgets. Doing this for all 
vertices in the original graph creates<em>n</em> intertwined chains with <em>n
</em> entry points and <em>n</em> exits. 
<p> </p> <br>
 Thus we have encoded the information about the initial graph. 
What about<em>k</em>? We set up <em>k</em> additional vertices and connect each 
of these to the<em>n</em> start points and <em>n</em> end points of each chain. 
<p> </p> <br>
 Total size of new graph: <em>GE+K</em> vertices and <em>
12E+2kN+2E</em> edges  construction is polynomial in size and time. 
<p> We claim this graph has a <em>HC</em> iff <em>G</em> has a <em>VC</em> of 
size<em>k</em>. </p> 
<p> </p> 
<ol> 
<li> Suppose  is a <em>HC</em>. 
<p> Assume it starts at one of the <em>k</em> selector vertices. It must then 
go through one of the chains of gadgets until it reaches a different selector 
vertex.</p>
<p> Since the tour is a <em>HC</em>, all gadgets are traversed. The <em>k</em> 
chains correspond to the vertices in the cover.</p>
<p> Note that if both vertices associated with an edge are in the cover, the 
gadget will be traversal in two pieces - otherwise one chain suffices.</p>
<p> To avoid visiting a vertex more than once, each chain is associated with a 
selector vertex.</p></li> 
<li> Now suppose we have a vertex cover of size  . 
<p> We can always add more vertices to the cover to bring it up to size <em>k
</em>. </p>
<p> For each vertex in the cover, start traversing the chain. At each entry 
point to a gadget, check if the other vertex is in the cover and traverse the 
gadget accordingly.</p>
<p> Select the selector edges to complete the circuit. </p></li> </ol> 
<p> Neat, sweet, and NP-complete. </p> 
<p> To show that Longest Path or Hamiltonian Path is <em>NP</em>-complete, add 
start and stop vertices and distinguish the first and last selector vertices. 
&nbsp;</p> 
<p> </p> <br>
 This has a Hamiltonian path from start to stop iff the original 
graph has a vertex cover of size<em>k</em>. 
<p> Listening to Part 26-2 </p> 
<p> </p> 
<p>Other <em>NP</em>-complete Problems</p> 
<p> </p> 
<ul> 
<li> Partition - can you partition <em>n</em> integers into two subsets so 
that the sums of the subset are equal?</li> 
<li> Bin Packing - how many bins of a given size do you need to hold <em>n</em>
 items of variable size? &nbsp;</li> 
<li> Chromatic Number - how many colors do you need to color a graph? &nbsp;
</li> 
<li>  checkers - does black have a forced win from a given position? &nbsp;
</li> 
<li> Scheduling, Code Optimization, Permanent Evaluation, Quadratic 
Programming, etc.
<p> </p></li> </ul> 
<p> Open: Graph Isomorphism, Composite Number, Minimum Length Triangulation. 
</p> 
<p> Listening to Part 26-3 </p> 
<p> </p> 
<p>Polynomial or Exponential?</p> 
<p> Just changing a problem a little can make the difference between it being 
in<em>P</em> or <em>NP</em>-complete: </p> 
<p> </p> 
<p></p> <em>P</em> <em>NP</em>-complete <br>
 Shortest Path  Longest Path <br>

 Eulerian Circuit  Hamiltonian Circuit <br>
 Edge Cover  Vertex Cover <br>
<br>

<p></p>  The first thing you should do when you suspect a problem might be 
NP-complete is look in Garey and Johnson,<em>Computers and Intractability</em>. 
It contains a list of several hundred problems known to be<em>NP</em>-complete. 
Either what you are looking for will be there or you might find a closely 
related problem to use in a reduction. &nbsp;
<p> Listening to Part 26-4 </p> 
<p> </p> 
<p>Techniques for Proving <em>NP</em>-completeness</p> 
<p> </p> 
<ol> 
<li> <em>Restriction</em> - Show that a special case of the problem you are 
interested in is<em>NP</em>-complete. For example, the problem of finding a 
path of length<em>k</em> is really Hamiltonian Path. &nbsp;</li> 
<li> <em>Local Replacement</em> - Make local changes to the structure. An 
example is the reduction . Another example is showing isomorphism is no easier 
for bipartite graphs: &nbsp;
<p> </p> <br>
 For any graph, replacing an edge with makes it bipartite.</li> 
<li> <em>Component Design</em> - These are the ugly, elaborate constructions 
&nbsp;</li> </ol> 
<p> Listening to Part 26-5 </p> 
<p> </p> 
<p>The Art of Proving Hardness</p> 
<p> Proving that problems are hard is an skill. Once you get the hang of it, 
it is surprisingly straightforward and pleasurable to do. Indeed, the dirty 
little secret of NP-completeness proofs is that they are usually easier to 
recreate than explain, in the same way that it is usually easier to rewrite old 
code than the try to understand it.</p> 
<p> I offer the following advice to those needing to prove the hardness of a 
given problem:</p> 
<p> </p> 
<ul> 
<li> <em>Make your source problem as simple (i.e. restricted) as possible.</em>
<p> Never use the general traveling salesman problem (TSP) as a target 
problem. Instead, use TSP on instances restricted to the triangle inequality. 
Better, use Hamiltonian cycle, i.e. where all the weights are 1 or . Even 
better, use Hamiltonian path instead of cycle. Best of all, use Hamiltonian 
path on directed, planar graphs where each vertex has total degree 3. All of 
these problems are equally hard, and the more you can restrict the problem you 
are reducing, the less work your reduction has to do.</p></li> 
<li> <em>Make your target problem as hard as possible.</em> 
<p> Don't be afraid to add extra constraints or freedoms in order to make your 
problem more general (at least temporarily).</p>
<p> Listening to Part 26-6</p></li> 
<li> <em>Select the right source problem for the right reason.</em> 
<p> Selecting the right source problem makes a big difference is how difficult 
it is to prove a problem hard. This is the first and easiest place to go wrong.
</p>
<p> I usually consider four and only four problems as candidates for my hard 
source problem. Limiting them to four means that I know a lot about these 
problems - which variants of these problems are hard and which are soft. My 
favorites are:</p>
<p> </p> 
<ul> 
<li> 3-Sat - that old reliable... When none of the three problems below seem 
appropriate, I go back to the source. &nbsp;</li> 
<li> Integer partition - the one and only choice for problems whose hardness 
seems to require using large numbers.</li> 
<li> Vertex cover - for any graph problems whose hardness depends upon <em>
selection</em>. Chromatic number, clique, and independent set all involve 
trying to select the correct subset of vertices or edges. &nbsp;</li> 
<li> Hamiltonian path - for any graph problems whose hardness depends upon <em>
ordering</em>. If you are trying to route or schedule something, this is likely 
your lever. &nbsp;
<p> </p></li> </ul> 
<p> Listening to Part 26-7 </p>
<p> </p></li> 
<li> <em>Amplify the penalties for making the undesired transition.</em> 
<p> You are trying to translate one problem into another, while making them 
stay the same as much as possible. The easiest way to do this is to be bold 
with your penalties, to punish anyone trying to deviate from your proposed 
solution. ``If you pick this, then you have to pick up this huge set which 
dooms you to lose.'' The sharper the consequences for doing what is undesired, 
the easier it is to prove if and only if.</p></li> 
<li> <em>Think strategically at a high level, then build gadgets to enforce 
tactics.</em> 
<p> You should be asking these kinds of questions. ``How can I force that 
either A or B but not both are chosen?'' ``How can I force that A is taken 
before B?'' ``How can I clean up the things I did not select?''</p></li> 
<li> <em>Alternate between looking for an algorithm or a reduction if you get 
stuck.</em> 
<p> Sometimes the reason you cannot prove hardness is that there is an 
efficient algorithm to solve your problem! When you can't prove hardness, it 
likely pays to change your thinking at least for a little while to keep you 
honest.</p>
<p> </p></li> </ul> 
<p> Listening to Part 26-8 </p> 
<p> </p> 
<p>Now watch me try it!</p> 
<p> To demonstrate how one goes about proving a problem hard, I accept the 
challenge of showing how a proof can be built on the fly.</p> 
<p> I need a volunteer to pick a random problem from the 400+ hard problems in 
the back of Garey and Johnson.</p> 
<p> Listening to Part 27-2 </p> 
<p> </p> 
<p>Dealing with <em>NP</em>-complete Problems</p>  &nbsp; 
<p> </p> 
<p>Option 1: Algorithm fast in the Average case</p> Examples are 
Branch-and-bound for the Traveling Salesman Problem, backtracking algorithms, 
etc.
<p> </p> 
<p>Option 2: Heuristics</p> Heuristics are rules of thumb; fast methods to 
find a solution with no requirement that it be the best one.
<p> Note that the theory of <em>NP</em>-completeness does not stipulate that 
it is hard to get close to the answer, only that it is hard to get the optimal 
answer.</p> 
<p> Often, we can prove performance bounds on heuristics, that the resulting 
answer is within<em>C</em> times that of the optimal one. </p> 
<p> </p> <b>Next:</b> Lecture 23 - approximation <b>Up:</b> Table of contents 
<b>Previous:</b> Lecture 21 - vertex 
<h1>Lecture 23 - approximation algorithms and Cook's theorem</h1> 
<p> Listening to Part 26-1 </p> 
<p> <em> 36.5-5 Prove that Hamiltonian Path is <em>NP</em>-complete. </em> </p>
This is not a special case of Hamiltonian cycle! (<em>G</em> may have a HP but 
not cycle) &nbsp;
<p> The easiest argument says that <em>G</em> contains a HP but no HC iff (<i>x
</i>,<i>y</i>) in <em>G</em> such that adding edge (<i>x</i>, <i>y</i>) to <em>G
</em> causes to have a HC, so  calls to a HC function solves HP. </p> 
<p> The cleanest proof modifies the <em>VC</em> and <em>HC</em> reduction from 
the book:</p> 
<p> </p> <br>

<p> Listening to Part 27-3 </p> 
<p> </p> 
<p>Approximating Vertex Cover</p> 
<p> As we have seen, finding the minimum vertex cover is <em>NP</em>-complete. 
However, a very simple strategy (heuristic) can get us a cover at most twice 
that of the optimal. &nbsp;&nbsp;</p> 
<p> </p> 
<pre> 
<p> While the graph has edges </p>
<p> pick an arbitrary edge <i>v</i>, <i>u</i> </p>
<p> add both <em>u</em> and <em>v</em> to the cover </p>
<p> delete all edges incident on either <em>u</em> and <em>v</em> </p>
<p> </p></pre> 
<p> If the graph is represented by an adjacency list this can be implemented in
<i>O</i>(<i>m</i>+<i>n</i>) time. </p> 
<p> This heuristic must always produce cover, since an edge is only deleted 
when it is adjacent to a cover vertex.</p> 
<p> Further, any cover uses at least half as many vertices as the greedy cover.
</p> 
<p> </p> <br>
 Why? Delete all edges from the graph except the edges we 
selected.
<p> No two of these edges share a vertex. Therefore, any cover of just these 
edges must include one vertex per edge, or half the greedy cover!</p> 
<p> Listening to Part 27-4 </p> 
<p> </p> 
<p>Things to Notice</p> 
<p> </p> 
<ul> 
<li> Although the heuristic is simple, it is not stupid. Many other seemingly 
smarter ones can give a far worse performance in the worst case. &nbsp;
<p> Example: Pick one of the two vertices instead of both (after all, the 
middle edge is already covered) The optimal cover is one vertex, the greedy 
heuristic is two vertices, while the new/bad heuristic can be as bad as<em>n-1
</em>. </p>
<p> </p> <br>
</li> 
<li> Proving a lower bound on the optimal solution is the key to getting an 
approximation result.</li> 
<li> Making a heuristic more complicated does not necessarily make it better. 
It just makes it more difficult to analyze.</li> 
<li> A post-processing clean-up step (delete any unecessessary vertex) can 
only improve things in practice, but might not help the bound.
<p> </p></li> </ul> 
<p> Listening to Part 27-5 </p> 
<p> </p> 
<p>The Euclidean Traveling Salesman</p> 
<p> In the traditional version of TSP - a salesman wants to plan a drive to 
visit all his customers exactly once and get back home. &nbsp;</p> 
<p> Euclidean geometry satisfies the triangle inequality,  . </p> 
<p> TSP remains hard even when the distances are Euclidean distances in the 
plane.</p> 
<p> </p> <br>
 Note that the cost of airfares is an example of a distance 
function which violates the triangle inequality.
<p> However, we can approximate the optimal Euclidean TSP tour using minimum 
spanning trees.</p> 
<p> <b>Claim:</b> the cost of a MST is a lower bound on the cost of a TSP tour.
</p> 
<p> Why? Deleting any edge from a TSP tour leaves a path, which is a tree of 
weight at least that of the MST!</p> 
<p> Listening to Part 27-6 </p> 
<p> If we were allowed to visit cities more than once, doing a depth-first 
traversal of a MST, and then walking out the tour specified is at most twice 
the cost of MST. Why? We will be using each edge exactly twice.</p> 
<p> </p> <br>
 Every edge is used exactly twice in the DFS tour: <em>1</em>. 
<p> However, how can we avoid revisiting cities? </p> 
<p> We can take a shortest path to the next unvisited vertex. The improved 
tour is<em>1-2-3-5-8-9-6-4-7-10-11-1</em>. Because we replaced a chain of edges 
by the edge, the triangle inequality ensures the tour only gets shorter. Thus 
this is still within twice optimal!</p> 
<p> Listening to Part 27-1 </p> 
<p> <em> 37.1-3 Give an efficient greedy algorithm that finds an optimal 
vertex cover of a tree in linear time. &nbsp;</em> </p> In a vertex cover we 
need to have at least one vertex for each edge.
<p> Every tree has at least two leaves, meaning that there is always an edge 
which is adjacent to a leaf. Which vertex can we never go wrong picking? The 
non-leaf, since it is the only one which can also cover other edges!</p> 
<p> After trimming off the covered edges, we have a smaller tree. We can 
repeat the process until the tree as 0 or 1 edges. When the tree consists only 
of an isolated edge, pick either vertex.</p> 
<p> All leaves can be identified and trimmed in <i>O</i>(<i>n</i>) time during 
a DFS.</p> 
<p> </p> 
<p>Formal Languages and the Theory of NP-completeness</p> 
<p> The theory of NP-completeness is based on formal languages and Turing 
machines, and so we will must work on a more abstract level than usual.</p> 
<p> For a given alphabet of symbols <em>0</em>, <em>1</em>, &amp;, we can form 
an infinite set of<em>strings</em> or words by arranging them in any order: 
`&amp;10', `111111',`&amp;&amp;&amp;', and `&amp;'.</p> 
<p> A subset of the set of strings over some alphabet is a <em>formal language
</em>. &nbsp; </p> 
<p> Formal language theory concerns the study of how powerful a machine you 
need to recognize whether a string is from a particular language.</p> 
<p> Example: Is the string a binary representation of a even number? A simple 
finite machine can check if the last symbol is zero:</p> 
<p> </p> <br>
 No memory is required, except for the current state. 
<p> Observe that solving decision problems can be thought of as formal 
language recognition. The problem instances are encoded as strings and strings 
in the language if and only if the answer to the decision problem is YES! &nbsp;
</p> 
<p> What kind of machine is necessary to recognize this language? A Turing 
Machine! &nbsp;</p> 
<p> A Turing machine has a finite-state-control (its program), a two way 
infinite tape (its memory) and a read-write head (its program counter)</p> 
<p> </p> <br>

<p>So, where are we?</p> 
<p> Each instance of an optimization or decision problem can be encoded as 
string on some alphabet. The set of all instances which return True for some 
problem define a language.</p> 
<p> Hence, any problem which solves this problem is equivalent to a machine 
which recognizes whether an instance is in the language!</p> 
<p> The goal of all this is going to be a formal way to talk about the set of 
problems which can be solved in polynomial time, and the set that cannot be.</p>
<p> </p> 
<p>Non-deterministic Turing Machines</p> 
<p> Suppose we buy a <em>guessing module</em> peripherial for our Turing 
machine, which looks at a Turing machine program and problem instance and in 
polynomial time writes something it says is an answer. To convince ourselves it 
really is an answer, we can run another program to check it. &nbsp;&nbsp;</p> 
<p> Ex: The Traveling Salesman Problem </p> 
<p> The guessing module can easily write a permutation of the vertices in 
polynomial time. We can check if it is correct by summing up the weights of the 
special edges in the permutation and see that it is less than<em>k</em>. </p> 
<p> </p> <br>
 The class of languages which we can recognize in time 
polynomial in the size of the string or a deterministic Turing Machine (without 
guessing module) is called<em>P</em>. 
<p> The class of languages we can recognize in time polynomial in the length 
of the string or a non-deterministic Turing Machine is called<em>NP</em>. </p> 
<p> Clearly,  , since for any DTM program we can run it on a non-deterministic 
machine, ignore what the guessing module is doing, and it will just as fast.</p>
<p> </p> 
<p>P ?= NP</p> 
<p> Observe that any NDTM program which takes time <i>P</i>(<i>n</i>) can 
simulated in time on a deterministic machine, by running the checking program  
times, once on each possible guessed string. &nbsp;</p> 
<p> The $10,000 question is whether a polynomial time simulation exists, or in 
other words whether<i>P</i>=<i>NP</i>?. Do there exist languages which can be 
verified in polynomial time and still take exponential time on deterministic 
machines?</p> 
<p> This is the most important question in computer science. Since proving an 
exponential time lower bound for a problem in<em>NP</em> would make us famous, 
we assume that we cannot do it.</p> 
<p> What we can do is prove that it is at least as hard as any problem in <em>
NP</em>. A problem in <em>NP</em> for which a polynomial time algorithm would 
imply all languages in<em>NP</em> are in <em>P</em> is called <em>NP</em>
-complete.</p> 
<p> </p> 
<p>Turing Machines and Cook's Theorem</p> 
<p> Cook's Theorem proves that satisfiability is <em>NP</em>-complete by 
reducing all non-deterministic Turing machines to<em>SAT</em>. &nbsp; </p> 
<p> Each Turing machine has access to a two-way infinite tape (read/write) and 
a finite state control, which serves as the program.</p> 
<p> </p> <br>
 A program for a non-deterministic TM is: 
<ol> 
<li> Space on the tape for guessing a solution and certificate to permit 
verification.</li> 
<li> A finite set of tape symbols</li> 
<li> A finite set of states  for the machine, including the start state  and 
final states </li> 
<li> A transition function, which takes the current machine state, and current 
tape symbol and returns the new state, symbol, and head position.</li> </ol> 
<p> We know a problem is in <em>NP</em> if we have a NDTM program to solve it 
in worst-case time<i>p</i>[<i>n</i>], where <em>p</em> is a polynomial and <em>n
</em> is the size of the input. </p> 
<p> </p> 
<p>Cook's Theorem - Satisfiability is NP-complete!</p> 
<p> <b>Proof:</b> We must show that any problem in <em>NP</em> is at least as 
hard as SAT. Any problem in<em>NP</em> has a non-deterministic TM program which 
solves it in polynomial time, specifically<i>P</i>(<i>n</i>). &nbsp; </p> 
<p> We will take this program and create from it an instance of satisfiability 
such that it is satisfiable if and only if the input string was in the language.
</p> 
<p> </p> <br>
 If a polynomial time transform exists, then SAT must be <em>NP
</em>-complete, since a polynomial solution to SAT gives a polynomial time 
algorithm to anything in<em>NP</em>. 
<p> Our transformation will use boolean variables to maintain the state of the 
TM:</p> 
<p> </p> 
<p></p>  Variable  Range  Intended meaning <br>
<i>Q</i>[<i>i</i>, <i>j</i>]  
At time<em>i</em>, M is in <br>
state <br>
<i>H</i>[<i>i</i>,<i>j</i>]  At time 
<em>i</em>, the read-write head <br>
 is scanning tape square <em>j</em> <br>

<i>S</i>[<i>i</i>,<i>j</i>,<i>k</i>]  At time <em>i</em>, the contents of <br>
 
tape square<em>j</em> is symbol <br>
<br>
<br>

<p></p> <br>

<p> Note that there are  literals, a polynomial number if <i>p</i>(<i>n</i>) 
is polynomial.</p> 
<p> We will now have to add clauses to ensure that these variables takes or 
the values as in the TM computation.</p> 
<p> The group <em>6</em> clauses enforce the transition function of the 
machine. If the read-write head is not on tape square<em>j</em> at time <em>i
</em>, it doesn't change .... </p> 
<p> There are  literals and  clauses in all, so the transformation is done in 
polynomial time!</p> 
<p> </p> 
<p>Polynomial Time Reductions</p> 
<p> A decision problem is <em>NP</em>-hard if the time complexity on a 
deterministic machine is within a polynomial factor of the complexity of any 
problem in<em>NP</em>. &nbsp; </p> 
<p> A problem is <em>NP</em>-complete if it is <em>NP</em>-hard and in <em>NP
</em>. Cook's theorem proved SATISFIABILITY was <em>NP</em>-hard by using a 
polynomial time reduction translating each problem in<em>NP</em> into an 
instance of SAT:</p> 
<p> </p> <br>
 Since a polynomial time algorithm for SAT would imply a 
polynomial time algorithm for everything in<em>NP</em>, SAT is <em>NP</em>
-hard. Since we can guess a solution to SAT, it is in<em>NP</em> and thus <em>NP
</em>-complete. 
<p> The proof of Cook's Theorem, while quite clever, was certainly difficult 
and complicated. We had to show that all problems in<em>NP</em> could be 
reduced to SAT to make sure we didn't miss a hard one.</p> 
<p> But now that we have a known <em>NP</em>-complete problem in SAT. For any 
other problem, we can prove it<em>NP</em>-hard by polynomially transforming SAT 
to it!</p> 
<p> </p> <br>
 Since the composition of two polynomial time reductions can be 
done in polynomial time, all we need show is that SAT, ie. any instance of SAT 
can be translated to an instance of<em>x</em> in polynomial time. 
<p> Listening to Part 27-7 </p> 
<p> </p> 
<p>Finding the Optimal Spouse</p>  &nbsp; 
<p> </p> 
<ol> 
<li> There are up to <em>n</em> possible candidates we will see over our 
lifetime, one at a time.</li> 
<li> We seek to maximize our probability of getting the single best possible 
spouse.</li> 
<li> Our assessment of each candidate is relative to what we have seen before.
</li> 
<li> We must decided either to marry or reject each candidate as we see them. 
There is no going back once we reject someone.</li> 
<li> Each candidate is ranked from 1 to <em>n</em>, and all permutations are 
equally likely.
<p> </p></li> </ol> 
<p> Listening to Part 27-8 </p> 
<p> For example, if the input permutation is </p> 
<p> </p> 
<p> we see (3,1,2) after three candidates. </p> 
<p> Picking the first or last candidate gives us a probability of 1/<i>n</i> 
of getting the best.</p> 
<p> Since we seek maximize our chances of getting the best, it never pays to 
pick someone who is not the best we have seen.</p> 
<p> The optimal strategy is clearly to sample some fraction of the candidates, 
then pick the first one who is better than the best we have seen.</p> 
<p> <em>But what is the fraction?</em> </p> 
<p> Listening to Part 27-9 </p> 
<p> For a given fraction 1/<i>f</i>, what is the probability of finding the 
best?</p> 
<p> Suppose <em>i+1</em> is the highest ranked person in the first <i>n</i>/<i>
f</i> candidates. We win whenever the best candidate occurs before any number 
from<em>2</em> to <em>i</em> in the last <i>n</i> (1- 1/<i>f</i>) / <i>f</i> 
candidates.</p> 
<p> There is a 1/<i>i</i> probability of that, so, </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> In fact, the optimal is obtained by sampling the first <i>n</i>/<i>e</i> 
candidates.</p> 
<p> </p> <b>Next:</b> none <b>Up:</b> Table of contents <b>Previous:</b> 
Lecture 22 - techniques 
<h1> About this document ... </h1> 
<p> <strong></strong></p> 
<p> This document was generated using the <strong>LaTeX</strong>2HTML 
translator Version 96.1 (Feb 5, 1996) Copyright &copy; 1993, 1994, 1995, 1996,
Nikos Drakos, Computer Based Learning Unit, University of Leeds. </p> 
<p> The command line arguments were: <br>
<strong>latex2html</strong> all.tex. 
</p> 
<p>The translation was initiated by Algorithms on Mon Jun 2 09:21:39 EDT 1997
<br> </p> 
<p></p> <i>Algorithms <br>
 Mon Jun 2 09:21:39 EDT 1997</i> 
</body>
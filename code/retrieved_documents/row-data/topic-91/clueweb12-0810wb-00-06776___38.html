<!doctype html>
<meta charset="utf-8">
<title>mloss |  



Projects that use python as the programming language.</title>
<body>
Login <br>
<br>

<ul> 
<li>About</li> 
<li>Software</li> 
<li>Forum</li> 
<li>Blog</li> 
<li>Workshop</li> 
<li>FAQ</li> </ul>  20 projects found that use python as the programming 
language.<br>

<h2>Search</h2> 
<dl> 
<dd> Go<br>
</dd> </dl> 
<h2>Manage</h2> 
<ul> 
<li>Login to submit a new project</li> </ul> 
<h2>Sort by</h2> 
<ul> 
<li>Last Update</li> 
<li>Publication Date</li> 
<li>Project Title</li> 
<li>Rating</li> 
<li>Number of Views</li> 
<li>Number of Downloads</li> </ul> 
<h2>Filter by</h2> 
<ul> 
<li>Author</li> 
<li>Submitter</li> 
<li>Tag</li> 
<li>License</li> 
<li>Programming Language</li> 
<li>Operating System</li> 
<li>Data Format</li> 
<li>Published in JMLR</li> </ul> 
<p> </p> 
<h2>Latest Thoughts</h2> 
<p> </p> 
<ul> 
<li>Open Access is very cheap  by <strong>Cheng Soon Ong</strong> on March 8, 
2012 </li> 
<li>Did the MathWorks Infringe the Competition Laws?  by <strong>Soeren 
Sonnenburg</strong> on March 2, 2012 </li> 
<li>Nature Editorial about Open Science  by <strong>Cheng Soon Ong</strong> on 
February 28, 2012 </li> 
<li>Tagging Project 'Published in JMLR'  by <strong>Soeren Sonnenburg</strong> 
on February 3, 2012 </li> 
<li>Improving mloss.org  by <strong>Soeren Sonnenburg</strong> on December 16, 
2011 </li> 
<li>Mendeley/PLoS API Binary Battle (winners)  by <strong>Cheng Soon Ong
</strong> on December 6, 2011 </li> 
<li>What is a file?  by <strong>Cheng Soon Ong</strong> on December 5, 2011 
</li> 
<li>Linus's Lessons on Software  by <strong>Cheng Soon Ong</strong> on 
September 27, 2011 </li> 
<li>Software Freedom Day  by <strong>Cheng Soon Ong</strong> on September 17, 
2011 </li> 
<li>Mendeley/PLoS API Binary Battle  by <strong>Cheng Soon Ong</strong> on 
August 29, 2011 </li> </ul> 
<p></p> 
<p></p> 
<h2>Recent forum posts</h2> 
<p> </p> 
<ul> 
<li>Forum <strong>Website design</strong>  Thread <i>Enhanced statistics...</i>
<p>[HTML_REMOVED]&lt;a [...] </p>  by <strong>lmanaron</strong> on December 
25, 2011 </li> 
<li>Forum <strong>Standards</strong>  Thread <i>file format for data exchange
</i> 
<p>One more recent source is: http://www.data.gov/ </p>  by <strong>ong
</strong> on March 29, 2010 </li> </ul> 
<p></p> 
<p> </p> 
<h3> RSS Feed - Blog</h3> 
<h3> RSS Feed - New Software</h3> 
<h3> </h3> 
<p></p> 
<h5> Showing Items 1-20 of 60 on page 1 of 3: 1 2 3 Next </h5> 
<h2>  MyMediaLite 2.99 </h2> 
<p> by zenog - March 3, 2012, 10:32:37 CET [  ] 11762 views, 2301 downloads, 
15 subscriptions</p> <br>
<br>

<p> <b>About:</b> MyMediaLite is a lightweight, multi-purpose library of 
recommender system algorithms.</p> <b>Changes:</b> 
<p>Use float (32 bit) instead of double (64 bit) to store ratings and model 
parameters; Incremental update API now accepts several feedback events at once; 
New rating predictor SVD++; Merge LogisticRegressionMatrixFactorization and 
MultiCoreMatrixFactorization into BiasedMatrixFactorization; plenty of small 
enhancements, fixes, polishing</p> 
<ul> 
<li><b>Authors:</b> Zeno Gantner, Steffen Rendle, Christoph Freudenthaler </li>
<li><b>License:</b> Gpl Version 3 Or Later </li> 
<li><b>Programming Language:</b> Python, Perl, Ruby, Csharp, Fsharp </li> </ul>
<ul> 
<li><b>Operating System:</b> Linux, Windows, Solaris, Mac Os X </li> 
<li><b>Data Formats:</b> Csv, Tab Separated, Sql </li> 
<li><b>Tags:</b> Gradient Based Learning, Large Scale Learning, Algorithms, 
Data Mining, Evaluation, Supervised Learning, Collaborative Filtering, Matrix 
Factorization, Recommender Systems, Knn, Library, Dotnet, Mono </li> </ul> <br>

<br> <br>
<br>

<h2>  Pattern 2.3 </h2> 
<p> by tomdesmedt - February 26, 2012, 00:27:51 CET [  ] 323 views, 54 
downloads, 7 subscriptions </p> <br>
<br>

<p> <b>About:</b> &quot;Pattern&quot; is a web mining module for Python. It 
bundles tools for data retrieval, text analysis, clustering and classification, 
and data visualization.</p> <b>Changes:</b> 
<p>Initial Announcement on mloss.org. </p> 
<ul> 
<li><b>Authors:</b> Tom De Smedt, Walter Daelemans </li> 
<li><b>License:</b> Bsd </li> 
<li><b>Programming Language:</b> Python, Javascript </li> </ul> 
<ul> 
<li><b>Operating System:</b> Linux, Windows, Mac Os X </li> 
<li><b>Data Formats:</b> Txt, Csv </li> 
<li><b>Tags:</b> Graph, Svm, Latent Semantic Analysis, Natural Language 
Processing, Information Extraction, Data Visualization, Tfidf, Csv, K Nearest 
Neighbor, Html </li> </ul> <br>
<br>
<br>
<br>

<h2>  Theano 0.5 </h2> 
<p> by jaberg - February 23, 2012, 23:14:38 CET [  ] 5248 views, 957 downloads
, 9 subscriptions</p> <br>
<br>

<p> <b>About:</b> A Python library that allows you to define, optimize, and 
evaluate mathematical expressions involving multi-dimensional arrays 
efficiently. Dynamically generates CPU and GPU modules for good performance. 
Deep Learning Tutorials illustrate deep learning with Theano.</p> <b>Changes:
</b> 
<p>Theano 0.5 (23 February 2012) </p> 
<p>Highlight: </p> 
<ul> 
<li> Moved to github: http://github.com/Theano/Theano/ </li> 
<li> Old trac ticket moved to assembla ticket: 
http://www.assembla.com/spaces/theano/tickets</li> 
<li> Theano vision: 
http://deeplearning.net/software/theano/introduction.html#theano-vision (Many 
people)</li> 
<li> Theano with GPU works in some cases on Windows now. Still experimental. 
(Sebastian Urban)</li> 
<li> Faster dot() call: New/Better direct call to cpu and gpu ger, gemv, gemm 
and dot(vector, vector). (James, Frederic, Pascal)</li> 
<li> C implementation of Alloc. (James, Pascal) </li> 
<li> theano.grad() now also work with sparse variable. (Arnaud) </li> 
<li> Macro to implement the Jacobian/Hessian with 
theano.tensor.{jacobian,hessian} (Razvan)</li> 
<li> See the Interface changes. </li> </ul> 
<p>Interface Behavior Changes: </p> 
<ul> 
<li> The current default value of the parameter axis of 
theano.{max,min,argmax,argmin,max_and_argmax} is now the same as numpy: None. 
i.e. operate on all dimensions of the tensor. (Frederic Bastien, Olivier 
Delalleau) (was deprecated and generated a warning since Theano 0.3 released 
Nov. 23rd, 2010)</li> 
<li> The current output dtype of sum with input dtype [u]int* is now always 
[u]int64. You can specify the output dtype with a new dtype parameter to sum. 
The output dtype is the one using for the summation. There is no warning in 
previous Theano version about this. The consequence is that the sum is done in 
a dtype with more precision than before. So the sum could be slower, but will 
be more resistent to overflow. This new behavior is the same as numpy. 
(Olivier, Pascal)</li> 
<li> When using a GPU, detect faulty nvidia drivers. This was detected when 
running Theano tests. Now this is always tested. Faulty drivers results in in 
wrong results for reduce operations. (Frederic B.)</li> </ul> 
<p>Interface Features Removed (most were deprecated): </p> 
<ul> 
<li> The string modes FAST_RUN_NOGC and STABILIZE are not accepted. They were 
accepted only by theano.function(). Use Mode(linker='c|py_nogc') or 
Mode(optimizer='stabilize') instead.</li> 
<li> tensor.grad(cost, wrt) now always returns an object of the &quot;same 
type&quot; as wrt (list/tuple/TensorVariable). (Ian Goodfellow, Olivier)</li> 
<li> A few tag.shape and Join.vec_length left have been removed. (Frederic) 
</li> 
<li> The .value attribute of shared variables is removed, use 
shared.set_value() or shared.get_value() instead. (Frederic)</li> 
<li> Theano config option &quot;home&quot; is not used anymore as it was 
redundant with &quot;base_compiledir&quot;. If you use it, Theano will now 
raise an error. (Olivier D.)</li> 
<li> scan interface changes: (Razvan Pascanu) 
<ul> 
<li> The use of <code>return_steps</code> for specifying how many entries of 
the output to return has been removed. Instead, apply a subtensor to the output 
returned by scan to select a certain slice.</li> 
<li> The inner function (that scan receives) should return its outputs and 
updates following this order: [outputs], [updates], [condition]. One can skip 
any of the three if not used, but the order has to stay unchanged.</li> </ul> 
</li> </ul> 
<p>Interface bug fix: </p> 
<ul> 
<li> Rop in some case should have returned a list of one Theano variable, but 
returned the variable itself. (Razvan)</li> </ul> 
<p>New deprecation (will be removed in Theano 0.6, warning generated if you 
use them):</p> 
<ul> 
<li> tensor.shared() renamed to tensor._shared(). You probably want to call 
theano.shared() instead! (Olivier D.)</li> </ul> 
<p>Bug fixes (incorrect results): </p> 
<ul> 
<li> On CPU, if the convolution had received explicit shape information, they 
where not checked at runtime. This caused wrong result if the input shape was 
not the one expected. (Frederic, reported by Sander Dieleman)</li> 
<li> Theoretical bug: in some case we could have GPUSum return bad value. We 
were not able to reproduce this problem
<ul> 
<li> patterns affected ({0,1}*nb dim, 0 no reduction on this dim, 1 reduction 
on this dim): 01, 011, 0111, 010, 10, 001, 0011, 0101 (Frederic)</li> </ul> 
</li> 
<li> div by zero in verify_grad. This hid a bug in the grad of Images2Neibs. 
(James)</li> 
<li> theano.sandbox.neighbors.Images2Neibs grad was returning a wrong value. 
The grad is now disabled and returns an error. (Frederic)</li> 
<li> An expression of the form &quot;1 / (exp(x) +- constant)&quot; was 
systematically matched to &quot;1 / (exp(x) + 1)&quot; and turned into a 
sigmoid regardless of the value of the constant. A warning will be issued if 
your code was affected by this bug. (Olivier, reported by Sander Dieleman)</li> 
<li> When indexing into a subtensor of negative stride (for instance, 
x[a:b:-1][c]), an optimization replacing it with a direct indexing (x[d]) used 
an incorrect formula, leading to incorrect results. (Pascal, reported by Razvan)
</li> 
<li> The tile() function is now stricter in what it accepts to allow for 
better error-checking/avoiding nonsensical situations. The gradient has been 
disabled for the time being as it only implemented (incorrectly) one special 
case. The<code>reps</code> argument must be a constant (not a tensor variable), 
and must have the same length as the number of dimensions in the<code>x</code> 
argument; this is now checked. (David)</li> </ul> 
<p>Scan fixes: </p> 
<ul> 
<li> computing grad of a function of grad of scan (reported by Justin Bayer, 
fix by Razvan) before : most of the time crash, but could be wrong value with 
bad number of dimensions (so a visible bug) now : do the right thing.</li> 
<li> gradient with respect to outputs using multiple taps (reported by 
Timothy, fix by Razvan) before : it used to return wrong values now : do the 
right thing. Note: The reported case of this bug was happening in conjunction 
with the save optimization of scan that give run time errors. So if you didn't 
manually disable the same memory optimization (number in the list4), you are 
fine if you didn't manually request multiple taps.</li> 
<li> Rop of gradient of scan (reported by Timothy and Justin Bayer, fix by 
Razvan) before : compilation error when computing R-op now : do the right thing.
</li> 
<li> save memory optimization of scan (reported by Timothy and Nicolas BL, fix 
by Razvan) before : for certain corner cases used to result in a runtime shape 
error now : do the right thing.</li> 
<li> Scan grad when the input of scan has sequences of different lengths. 
(Razvan, reported by Michael Forbes)</li> 
<li> Scan.infer_shape now works correctly when working with a condition for 
the number of loops. In the past, it returned n_steps as the length, which is 
not always true. (Razvan)</li> 
<li> Scan.infer_shape crash fix. (Razvan) </li> </ul> 
<p>New features: </p> 
<ul> 
<li> AdvancedIncSubtensor grad defined and tested (Justin Bayer) </li> 
<li> Adding 1D advanced indexing support to inc_subtensor and set_subtensor 
(James Bergstra)</li> 
<li> tensor.{zeros,ones}_like now support the dtype param as numpy (Frederic) 
</li> 
<li> Added configuration flag &quot;exception_verbosity&quot; to control the 
verbosity of exceptions (Ian)</li> 
<li> theano-cache list: list the content of the theano cache (Frederic) </li> 
<li> theano-cache unlock: remove the Theano lock (Olivier) </li> 
<li> tensor.ceil_int_div to compute ceil(a / float(b)) (Frederic) </li> 
<li> MaxAndArgMax.grad now works with any axis (The op supports only 1 axis) 
(Frederic)
<ul> 
<li> used by tensor.{max,min,max_and_argmax} </li> </ul> </li> 
<li> tensor.{all,any} (Razvan) </li> 
<li> tensor.roll as numpy: (Matthew Rocklin, David Warde-Farley) </li> 
<li> Theano with GPU works in some cases on Windows now. Still experimental. 
(Sebastian Urban)</li> 
<li> IfElse now allows to have a list/tuple as the result of the if/else 
branches.
<ul> 
<li> They must have the same length and corresponding type (Razvan) </li> </ul>
</li> 
<li> Argmax output dtype is now int64 instead of int32. (Olivier) </li> 
<li> Added the element-wise operation arccos. (Ian) </li> 
<li> Added sparse dot with dense grad output. (Yann Dauphin) 
<ul> 
<li> Optimized to Usmm and UsmmCscDense in some case (Yann) </li> 
<li> Note: theano.dot and theano.sparse.structured_dot() always had a gradient 
with the same sparsity pattern as the inputs. The new theano.sparse.dot() has a 
dense gradient for all inputs.</li> </ul> </li> 
<li> GpuAdvancedSubtensor1 supports broadcasted dimensions. (Frederic) </li> 
<li> TensorVariable.zeros_like() and SparseVariable.zeros_like() </li> 
<li> theano.sandbox.cuda.cuda_ndarray.cuda_ndarray.device_properties() 
(Frederic)</li> 
<li> theano.sandbox.cuda.cuda_ndarray.cuda_ndarray.mem_info() return free and 
total gpu memory (Frederic)</li> 
<li> Theano flags compiledir_format. Keep the same default as before: 
compiledir_%(platform)s-%(processor)s-%(python_version)s. (Josh Bleecher Snyder)
<ul> 
<li> We also support the &quot;theano_version&quot; substitution. </li> </ul> 
</li> 
<li> IntDiv c code (faster and allow this elemwise to be fused with other 
elemwise) (Pascal)</li> 
<li> Internal filter_variable mechanism in Type. (Pascal, Ian) 
<ul> 
<li> Ifelse works on sparse. </li> 
<li> It makes use of gpu shared variable more transparent with theano.function 
updates and givens parameter.</li> </ul> </li> 
<li> Added a_tensor.transpose(axes) axes is optional (James) 
<ul> 
<li> theano.tensor.transpose(a_tensor, kwargs) We where ignoring kwargs, now 
it is used as the axes.</li> </ul> </li> 
<li> a_CudaNdarray_object[*] = int, now works (Frederic) </li> 
<li> tensor_variable.size (as numpy) computes the product of the shape 
elements. (Olivier)</li> 
<li> sparse_variable.size (as scipy) computes the number of stored values. 
(Olivier)</li> 
<li> sparse_variable[N, N] now works (Li Yao, Frederic) </li> 
<li> sparse_variable[M:N, O:P] now works (Li Yao, Frederic, Pascal) M, N, O, 
and P can be Python int or scalar tensor variables, None, or omitted 
(sparse_variable[:, :M] or sparse_variable[:M, N:] work).</li> 
<li> tensor.tensordot can now be moved to GPU (Sander Dieleman, Pascal, based 
on code from Tijmen Tieleman's gnumpy, 
http://www.cs.toronto.edu/~tijmen/gnumpy.html)</li> 
<li> Many infer_shape implemented on sparse matrices op. (David W.F.) </li> 
<li> Added theano.sparse.verify_grad_sparse to easily allow testing grad of 
sparse op. It support testing the full and structured gradient.</li> 
<li> The keys in our cache now store the hash of constants and not the 
constant values themselves. This is significantly more efficient for big 
constant arrays. (Frederic B.)</li> 
<li> 'theano-cache list' lists key files bigger than 1M (Frederic B.) </li> 
<li> 'theano-cache list' prints an histogram of the number of keys per 
compiled module (Frederic B.)</li> 
<li> 'theano-cache list' prints the number of compiled modules per op class 
(Frederic B.)</li> 
<li> The Theano flag &quot;nvcc.fastmath&quot; is now also used for the 
cuda_ndarray.cu file.</li> 
<li> Add the header_dirs to the hard part of the compilation key. This is 
currently used only by cuda, but if we use library that are only headers, this 
can be useful. (Frederic B.)</li> 
<li> The Theano flag &quot;nvcc.flags&quot; is now included in the hard part 
of the key. This mean that now we recompile all modules for each value of 
&quot;nvcc.flags&quot;. A change in &quot;nvcc.flags&quot; used to be ignored 
for module that were already compiled. (Frederic B.)</li> 
<li> Alloc, GpuAlloc are not always pre-computed (constant_folding 
optimization) at compile time if all their inputs are constant. (Frederic B., 
Pascal L., reported by Sander Dieleman)</li> 
<li> New Op tensor.sort(), wrapping numpy.sort (Hani Almousli) </li> </ul> 
<p>New optimizations: </p> 
<ul> 
<li> AdvancedSubtensor1 reuses preallocated memory if available (scan, 
c|py_nogc linker) (Frederic)</li> 
<li> dot22, dot22scalar work with complex. (Frederic) </li> 
<li> Generate Gemv/Gemm more often. (James) </li> 
<li> Remove scan when all computations can be moved outside the loop. (Razvan) 
</li> 
<li> scan optimization done earlier. This allows other optimizations to be 
applied. (Frederic, Guillaume, Razvan)</li> 
<li> exp(x) * sigmoid(-x) is now correctly optimized to the more stable form 
sigmoid(x). (Olivier)</li> 
<li> Added Subtensor(Rebroadcast(x)) =&gt; Rebroadcast(Subtensor(x)) 
optimization. (Guillaume)</li> 
<li> Made the optimization process faster. (James) </li> 
<li> Allow fusion of elemwise when the scalar op needs support code. (James) 
</li> 
<li> Better opt that lifts transpose around dot. (James) </li> </ul> 
<p>Crashes fixed: </p> 
<ul> 
<li> T.mean crash at graph building time. (Ian) </li> 
<li> &quot;Interactive debugger&quot; crash fix. (Ian, Frederic) </li> 
<li> Do not call gemm with strides 0, some blas refuse it. (Pascal Lamblin) 
</li> 
<li> Optimization crash with gemm and complex. (Frederic) </li> 
<li> GPU crash with elemwise. (Frederic, some reported by Chris Currivan) </li>
<li> Compilation crash with amdlibm and the GPU. (Frederic) </li> 
<li> IfElse crash. (Frederic) </li> 
<li> Execution crash fix in AdvancedSubtensor1 on 32 bit computers. (Pascal) 
</li> 
<li> GPU compilation crash on MacOS X. (Olivier) </li> 
<li> Support for OSX Enthought Python Distribution 7.x. (Graham Taylor, 
Olivier)</li> 
<li> When the subtensor inputs had 0 dimensions and the outputs 0 dimensions. 
(Frederic)</li> 
<li> Crash when the step to subtensor was not 1 in conjunction with some 
optimization. (Frederic, reported by Olivier Chapelle)</li> 
<li> Runtime crash related to an optimization with subtensor of alloc 
(reported by Razvan, fixed by Frederic)</li> 
<li> Fix dot22scalar cast of integer scalars (Justin Bayer, Frederic, Olivier) 
</li> 
<li> Fix runtime crash in gemm, dot22. FB </li> 
<li> Fix on 32bits computer: make sure all shape are int64.(Olivier) </li> 
<li> Fix to deque on python 2.4 (Olivier) </li> 
<li> Fix crash when not using c code (or using DebugMode) (not used by 
default) with numpy 1.6*. Numpy has a bug in the reduction code that made it 
crash. (Pascal)</li> 
<li> Crashes of blas functions (Gemv on CPU; Ger, Gemv and Gemm on GPU) when 
matrices had non-unit stride in both dimensions (CPU and GPU), or when matrices 
had negative strides (GPU only). In those cases, we are now making copies. 
(Pascal)</li> 
<li> More cases supported in AdvancedIncSubtensor1. (Olivier D.) </li> 
<li> Fix crash when a broadcasted constant was used as input of an elemwise Op 
and needed to be upcasted to match the op's output. (Reported by John 
Salvatier, fixed by Pascal L.)</li> 
<li> Fixed a memory leak with shared variable (we kept a pointer to the 
original value) (Ian G.)</li> </ul> 
<p>Known bugs: </p> 
<ul> 
<li> CAReduce with nan in inputs don't return the good output (<code>Ticket 
&lt;https://www.assembla.com/spaces/theano/tickets/763&gt;</code>_). 
<ul> 
<li> This is used in tensor.{max,mean,prod,sum} and in the grad of 
PermuteRowElements.</li> </ul> </li> </ul> 
<p>Sandbox: </p> 
<ul> 
<li> cvm interface more consistent with current linker. (James) </li> 
<li> Now all tests pass with the linker=cvm flags. </li> 
<li> vm linker has a callback parameter. (James) </li> 
<li> review/finish/doc: diag/extract_diag. (Arnaud Bergeron, Frederic, Olivier)
</li> 
<li> review/finish/doc: AllocDiag/diag. (Arnaud, Frederic, Guillaume) </li> 
<li> review/finish/doc: MatrixInverse, matrix_inverse. (Razvan) </li> 
<li> review/finish/doc: matrix_dot. (Razvan) </li> 
<li> review/finish/doc: det (determinent) op. (Philippe Hamel) </li> 
<li> review/finish/doc: Cholesky determinent op. (David) </li> 
<li> review/finish/doc: ensure_sorted_indices. (Li Yao) </li> 
<li> review/finish/doc: spectral_radius_boud. (Xavier Glorot) </li> 
<li> review/finish/doc: sparse sum. (Valentin Bisson) </li> 
<li> review/finish/doc: Remove0 (Valentin) </li> 
<li> review/finish/doc: SquareDiagonal (Eric) </li> </ul> 
<p>Sandbox New features (not enabled by default): </p> 
<ul> 
<li> CURAND_RandomStreams for uniform and normal (not picklable, GPU only) 
(James)</li> 
<li> New sandbox.linalg.ops.pinv(pseudo-inverse) op (Razvan) </li> </ul> 
<p>Documentation: </p> 
<ul> 
<li> Many updates. (Many people) </li> 
<li> Updates to install doc on MacOS. (Olivier) </li> 
<li> Updates to install doc on Windows. (David, Olivier) </li> 
<li> Doc on the Rop function (Ian) </li> 
<li> Added how to use scan to loop with a condition as the number of 
iteration. (Razvan)</li> 
<li> Added how to wrap in Theano an existing python function (in numpy, scipy, 
...). (Frederic)</li> 
<li> Refactored GPU installation of Theano. (Olivier) </li> </ul> 
<p>Others: </p> 
<ul> 
<li> Better error messages in many places. (Many people) </li> 
<li> PEP8 fixes. (Many people) </li> 
<li> Add a warning about numpy bug when using advanced indexing on a tensor 
with more than 2<em></em>32 elements (the resulting array is not correctly 
filled and ends with zeros). (Pascal, reported by David WF)</li> 
<li> Added Scalar.ndim=0 and ScalarSharedVariable.ndim=0 (simplify code) 
(Razvan)</li> 
<li> New min_informative_str() function to print graph. (Ian) </li> 
<li> Fix catching of exception. (Sometimes we used to catch interrupts) 
(Frederic, David, Ian, Olivier)</li> 
<li> Better support for utf string. (David) </li> 
<li> Fix pydotprint with a function compiled with a ProfileMode (Frederic) 
<ul> 
<li> Was broken with change to the profiler. </li> </ul> </li> 
<li> Warning when people have old cache entries. (Olivier) </li> 
<li> More tests for join on the GPU and CPU. (Frederic) </li> 
<li> Do not request to load the GPU module by default in scan module. (Razvan) 
</li> 
<li> Fixed some import problems. (Frederic and others) </li> 
<li> Filtering update. (James) </li> 
<li> On Windows, the default compiledir changed to be local to the 
computer/user and not transferred with roaming profile. (Sebastian Urban)</li> 
<li> New theano flag &quot;on_shape_error&quot;. Defaults to &quot;warn&quot; 
(same as previous behavior): it prints a warning when an error occurs when 
inferring the shape of some apply node. The other accepted value is 
&quot;raise&quot; to raise an error when this happens. (Frederic)</li> 
<li> The buidbot now raises optimization/shape errors instead of just printing 
a warning. (Frederic)</li> 
<li> better pycuda tests (Frederic) </li> 
<li> check_blas.py now accept the shape and the number of iteration as 
parameter (Frederic)</li> 
<li> Fix opt warning when the opt ShapeOpt is disabled (enabled by default) 
(Frederic)</li> 
<li> More internal verification on what each op.infer_shape return. (Frederic, 
James)</li> 
<li> Argmax dtype to int64 (Olivier) </li> 
<li> Improved docstring and basic tests for the Tile Op (David). </li> </ul> 
<ul> 
<li><b>Authors:</b> Mostly Lisa Lab </li> 
<li><b>License:</b> Bsd </li> 
<li><b>Programming Language:</b> Python, C, Cuda </li> </ul> 
<ul> 
<li><b>Operating System:</b> Linux, Macosx, Windows </li> 
<li><b>Data Formats:</b> Agnostic </li> 
<li><b>Tags:</b> Cuda, Gpu, Symbolic Differentiation </li> </ul> <br>
<br>
<br>
<br> 
<h2>  LWPR 1.2.4 </h2> 
<p> by sklanke - February 6, 2012, 19:55:41 CET [  ] 17819 views, 2131 
downloads, 8 subscriptions </p> <br>
<br>

<p> <b>About:</b> Locally Weighted Projection Regression (LWPR) is a recent 
algorithm that achieves nonlinear function approximation in high dimensional 
spaces with redundant and irrelevant input dimensions. At its [...]</p> <b>
Changes:</b> 
<h2>Version 1.2.4</h2> 
<ul> 
<li> Corrected typo in lwpr.c (wrong function name for multi-threaded helper 
function on Unix systems) Thanks to Jose Luis Rivero</li> </ul> 
<ul> 
<li><b>Authors:</b> Stefan Klanke, Sethu Vijayakumar, Stefan Schaal </li> 
<li><b>License:</b> Lgpl </li> 
<li><b>Programming Language:</b> C++, Python, Matlab, Octave, C </li> </ul> 
<ul> 
<li><b>Operating System:</b> Linux, Macosx, Windows, Unix </li> 
<li><b>Data Formats:</b> None </li> 
<li><b>JMLR-MLOSS Publication:</b> JMLR Page </li> 
<li><b>Tags:</b> Regression, Online Learning </li> </ul> <br>
<br>
<br>
<br>

<h2>  scikitlearn 0.9 </h2> 
<p> by fabianp - February 3, 2012, 10:46:50 CET [  ] 3006 views, 884 downloads
, 10 subscriptions</p> Rating <br>
(based on 2 votes) <br>
<br>
<br>
<br>

<p> <b>About:</b> The scikit-learn aims to provide state of the art standard 
machine learning algorithms in Python.</p> <b>Changes:</b> 
<p>Initial Announcement on mloss.org. </p> 
<ul> 
<li><b>Authors:</b> Bertrand Thirion, Edouard Duschenay, Vincent Michel, Gael 
Varoquaux, Olivier Grisel, Jacob Vanderplas, Alexandre Granfort, Fabian 
Pedregosa </li> 
<li><b>License:</b> New Bsd </li> 
<li><b>Programming Language:</b> Python, C </li> </ul> 
<ul> 
<li><b>Operating System:</b> Linux, Macosx, Windows </li> 
<li><b>Data Formats:</b> Agnostic </li> 
<li><b>JMLR-MLOSS Publication:</b> JMLR Page </li> 
<li><b>Tags:</b> Icml2010 </li> </ul> <br>
<br>
<br>
<br>

<h2>  MLPY Machine Learning Py 3.4.0 </h2> 
<p> by albanese - January 9, 2012, 12:10:16 CET [  ] 28220 views, 5612 
downloads, 19 subscriptions </p> Rating <br>
(based on 3 votes) <br>
<br>
<br>

<br> 
<p> <b>About:</b> mlpy is a Python module for Machine Learning built on top of 
NumPy/SciPy and of GSL.</p> <b>Changes:</b> 
<p>New features: </p> 
<ul> 
<li> Standard DTW added </li> 
<li> Subsequence DTW added </li> 
<li> Standard LCS added </li> </ul> 
<p>Fix: </p> 
<ul> 
<li> LibSvm: fix error when x is a list in learn() method </li> 
<li> fix code for vc++ </li> 
<li> fix setup.py (cblas) </li> </ul> 
<ul> 
<li><b>Authors:</b> Davide Albanese, Stefano Merler, Giuseppe Jurman, Roberto 
Visintainer, Cesare Furlanello </li> 
<li><b>License:</b> Gpl 3 </li> 
<li><b>Programming Language:</b> C++, Python, C </li> </ul> 
<ul> 
<li><b>Operating System:</b> Linux, Macosx, Windows, Unix, Freebsd </li> 
<li><b>Data Formats:</b> None </li> 
<li><b>Tags:</b> Svm, Classification, Clustering, Regression, Rfe, Wavelet, Dtw
,Discriminant Analysis </li> </ul> <br>
<br>
<br>
<br>

<h2>  PyMVPA Multivariate Pattern Analysis in Python 2.0.0 </h2> 
<p> by yarikoptic - December 22, 2011, 01:36:32 CET [  ] 21074 views, 3881 
downloads, 8 subscriptions </p> Rating <br>
(based on 2 votes) <br>
<br>
<br>

<br> 
<p> <b>About:</b> Python module to ease pattern classification analyses of 
large datasets. It provides high-level abstraction of typical processing steps 
(e.g. data preparation, classification, feature selection, [...]</p> <b>Changes:
</b> 
<ul> 
<li> 2.0.0 (Mon, Dec 19 2011) </li> </ul> 
<p> This release aggregates all the changes occurred between official releases 
in 0.4 series and various snapshot releases (in 0.5 and 0.6 series). To get 
better overview of high level changes see :ref:<code>release notes for 0.5 
&lt;chap_release_notes_0.5&gt;</code> and :ref:<code>0.6 
&lt;chap_release_notes_0.6&gt;</code> as well as summaries of release 
candidates below</p> 
<ul> 
<li>
<p>Fixes (23 BF commits) </p> 
<ul> 
<li> significance level in the right tail was fixed to include the value 
tested -- otherwise resulted in optimistic bias (or absurdly high significance 
in improbable case if all estimates having the same value)</li> 
<li> compatible with the upcoming IPython 0.12 and renamed sklearn (Fixes #57) 
</li> 
<li> do not double-train <code>slave</code> classifiers while assessing 
sensitivities (Fixes #53)</li> </ul> </li> 
<li>
<p>Enhancements (30 ENH + 3 NF commits) </p> 
<ul> 
<li> resolving voting ties in kNN based on mean distance, and randomly in SMLR 
</li> 
<li> :class:<code>kNN</code>'s <code>ca.estimates</code> now contains 
dictionaries with votes for each class</li> 
<li> consistent zscoring in :class:<code>Hyperalignment</code> </li> </ul> 
</li> 
<li>
<p>2.0.0~rc5 (Wed, Oct 19 2011) </p> </li> 
<li>
<p><strong>Major</strong>: to allow easy co-existence of stable PyMVPA 0.4.x, 
0.6 development<code>mvpa</code> module was renamed into mod:<code>mvpa2</code>.
</p> </li> 
<li>
<p>Fixes </p> 
<ul> 
<li> compatible with the new Shogun 1.x series </li> 
<li> compatible with the new h5py 2.x series </li> 
<li> mvpa-prep-fmri -- various compatibility fixes and smoke testing </li> 
<li> deepcopying :class:<code>SummaryStatistics</code> during <strong>add
</strong> </li> </ul> </li> 
<li>
<p>Enhancements </p> 
<ul> 
<li> tutorial uses :mod:<code>mvpa2.tutorial_suite</code> now </li> 
<li> better suppression of R warnings when needed </li> 
<li> internal attributes of many classes were exposed as properties </li> 
<li> more unification of <code>__repr__</code> for many classes </li> </ul> 
</li> 
<li>
<p>0.6.0~rc4 (Wed, Jun 14 2011) </p> </li> 
<li>
<p>Fixes </p> 
<ul> 
<li> Finished transition to :mod:<code>nibabel</code> conventions in <code>
plot_lightbox</code> </li> 
<li> Addressed :mod:<code>matplotlib.hist</code> API change </li> 
<li> Various adjustments in the tests batteries (:mod:<code>nibabel</code> 
1.1.0 compatibility, etc)</li> </ul> </li> 
<li>
<p>New functionality </p> 
<ul> 
<li> Explicit new argument <code>flatten</code> to from_wizard -- default 
behavior changed if mapper was provided as well</li> </ul> </li> 
<li>
<p>Enhancements </p> 
<ul> 
<li> Elaborated <code>__str__</code> and <code>__repr__</code> for some 
Classifiers and Measures</li> </ul> </li> 
<li>
<p>0.6.0~rc3 (Thu, Apr 12 2011) </p> </li> 
<li>
<p>Fixes </p> 
<ul> 
<li> Bugfixes regarding the interaction of FlattenMapper and BoxcarMapper that 
affected event-related analyses.</li> 
<li> <code>Splitter</code> now handles attribute value <code>None</code> for 
splitting properly.</li> 
<li> <code>GNBSearchlight</code> handling of<br>
<code>roi_ids</code>. </li> 
<li> More robust detection of mod:<code>scikits.learn</code> and :mod:<code>
nipy</code> externals. </li> </ul> </li> 
<li>
<p>New functionality </p> 
<ul> 
<li> Added a <code>Repeater</code> node to yield a dataset multiple times and 
<br> <code>Sifter</code> node to exclude some datasets. Consequently, the 
&quot;nosplitting&quot; mode of<code>Splitter</code> got removed at the same 
time.</li> 
<li> :file:<code>tools/niils</code> -- little tool to list details 
(dimensionality, scaling, etc) of the files in nibabel-supported formats.</li> 
</ul> </li> 
<li>
<p>Enhancements </p> 
<ul> 
<li> Numerous documentation fixes. </li> 
<li> Various improvements and increased flexibility of null distribution 
estimation of Measures.</li> 
<li> All attribute are now reported in sorted order when printing a dataset. 
</li> 
<li> <code>fmri_dataset</code> now also stores the input image type. </li> 
<li> <code>Crossvalidation</code> can now take a custom <code>Splitter</code> 
instance. Moreover, the default splitter of CrossValidation is more robust in 
terms of number and type of created splits for common usage patterns (i.e. 
together with partitioners).</li> 
<li> <code>CrossValidation</code> takes any custom Node as <code>errorfx</code>
 argument.</li> 
<li> <code>ConfusionMatrix</code> can now be used as an <code>errorfx</code> 
in Crossvalidation.</li> 
<li> <code>LOE(ACC): Linear Order Effect in ACC</code> was added to <br>
<code>
ConfusionMatrix</code> to detect trends in performances across splits. </li> 
<li> A <code>Node</code> s postproc is now accessible as a property. </li> 
<li> <code>RepeatedMeasure</code> has a new 'concat_as' argument that allows 
results to be concatenated along the feature axis. The default behavior, 
stacking as multiple samples, is unchanged.</li> 
<li> <code>Searchlight</code> now has the ability to mark the center/seed of 
an ROI in with a feature attribute in the generated datasets.</li> 
<li> <code>debug</code> takes <code>args</code> parameter for delayed string 
comprehensions. It should reduce run-time impact of<code>debug()</code> calls 
in regular, non<code>-O</code> mode of Python operation. </li> 
<li> String summaries and representations (provided by <code>__str__</code> and
<code>__repr__</code>) were made more exhaustive and more coherent. Additional 
properties to access initial constructor arguments were added to variety of 
classes.</li> </ul> </li> 
<li>
<p>Internal changes </p> 
<ul> 
<li>
<p>New debug target <code>STDOUT</code> to allow attaching metrics (e.g. 
traceback, timestamps) to regular output printed to stdout</p> </li> 
<li>
<p>New set of decorators to help with unittests </p> </li> 
<li>
<p><code>@nodebug</code> to disable specific debug targets for the duration of 
the test.</p> </li> 
<li>
<p><code>@reseed_rng</code> to guarantee consistent random data given initial 
seeding.</p> </li> 
<li>
<p><code>@with_tempfile</code> to provide a tempfile name which would get 
removed upon completion (test success or failure)</p> </li> 
<li>
<p>Dropping daily testing of <code>maint/0.5</code> branch -- RIP. </p> </li> 
<li>
<p><code>Collection</code> s were provided with adequate <code>(deep|)copy
</code>. And <code>Dataset</code> was refactored to use <code>Collection</code> 
s<code>copy</code> method. </p> </li> 
<li>
<p><code>update-*</code> Makefile rules automatically should fast-forward 
corresponding<code>website-updates</code> branch </p> </li> 
<li>
<p><code>MVPA_TESTS_VERBOSITY</code> controls also :mod:<code>numpy</code> 
warnings now.</p> </li> 
<li>
<p><code>Dataset.__array__</code> provides original array instead of copy 
(unless dtype is provided)</p> </li> </ul> </li> </ul> 
<p> Also adapts changes from 0.4.6 and 0.4.7 (see corresponding changelogs). 
</p> 
<ul> 
<li>
<p>0.6.0~rc2 (Thu, Mar 3 2011) </p> </li> 
<li>
<p>Various fixes in the mvpa.atlas module. </p> </li> 
<li>
<p>0.6.0~rc1 (Thu, Feb 24 2011) </p> </li> 
<li>
<p>Many, many, many </p> </li> 
<li>
<p>For an overview of the most drastic changes :ref:<code>see constantly 
evolving release notes for 0.6 &lt;chap_release_notes_0.6&gt;</code> </p> </li> 
<li>
<p>0.5.0 (sometime in March 2010) </p> </li> </ul> 
<p> This is a special release, because it has never seen the general public. A 
summary of fundamental changes introduced in this development version can be 
seen in the :ref:<code>release notes &lt;chap_release_notes_0.5&gt;</code>. </p>
<p> Most notably, this version was to first to come with a comprehensive 
two-day workshop/tutorial.</p> 
<ul> 
<li> 0.4.7 (Tue, Mar 07 2011) (Total: 12 commits) </li> </ul> 
<p> A bugfix release </p> 
<ul> 
<li>
<p>Fixed </p> 
<ul> 
<li> Addressed the issue with input NIfTI files having <code>scl_</code> 
fields set: it could result in incorrect analyses and map2nifti-produced NIfTI 
files. Now input files account for scaling/offset if<code>scl_</code> fields 
direct to do so. Moreover upon map2nifti, those fields get reset.</li> 
<li> :file:<code>doc/examples/searchlight_minimal.py</code> - best error is 
the minimal one</li> </ul> </li> 
<li>
<p>Enhancements </p> 
<ul> 
<li> :class:<code>~mvpa.clfs.gnb.GNB</code> can now tolerate training datasets 
with a single label</li> 
<li> :class:<code>~mvpa.clfs.meta.TreeClassifier</code> can have trailing 
nodes with no classifier assigned</li> </ul> </li> 
<li>
<p>0.4.6 (Tue, Feb 01 2011) (Total: 20 commits) </p> </li> </ul> 
<p> A bugfix release </p> 
<ul> 
<li>
<p>Fixed (few BF commits): </p> 
<ul> 
<li> Compatibility with numpy 1.5.1 (histogram) and scipy 0.8.0 (workaround 
for a regression in legendre)</li> 
<li> Compatibility with libsvm 3.0 </li> 
<li> :class:<code>~mvpa.clfs.plr.PLR</code> robustification </li> </ul> </li> 
<li>
<p>Enhancements </p> 
<ul> 
<li> Enforce suppression of numpy warnings while running unittests. Also 
setting verbosity &gt;= 3 enables all warnings (Python, NumPy, and PyMVPA)</li> 
<li> :file:<code>doc/examples/nested_cv.py</code> example (adopted from 0.5) 
</li> 
<li> Introduced base class :class:<code>~mvpa.clfs.base.LearnerError</code> 
for classifiers' exceptions (adopted from 0.5)</li> 
<li> Adjusted example data to live upto nibabel's warranty of NIfTI 
standard-compliance</li> 
<li> More robust operation of MC iterations -- skip iterations where 
classifier experienced difficulties and raise an exception (e.g. due to 
degenerate data)</li> </ul> </li> </ul> 
<ul> 
<li><b>Authors:</b> Michael Hanke, Yaroslav Halchenko, Per B. Sederberg, 
Emanuele Olivetti </li> 
<li><b>License:</b> Mit </li> 
<li><b>Programming Language:</b> Python, C </li> </ul> 
<ul> 
<li><b>Operating System:</b> Agnostic </li> 
<li><b>Data Formats:</b> None </li> 
<li><b>Tags:</b> Shogun, Python, Eeg, Classification, Regression, Support 
Vector Machines, K Nearest Neighbor Classification, Pca, Rfe, Neuroscience, Fmri
,Framework, Gpr, Lars, Smlr, Meg </li> </ul> <br>
<br>
<br>
<br>

<h2>  OpenOpt 0.37 </h2> 
<p> by Dmitrey - December 15, 2011, 17:47:08 CET [  ] 18297 views, 4214 
downloads, 1 subscription </p> Rating <br>
(based on 2 votes) <br>
<br>
<br>

<br> 
<p> <b>About:</b> Universal Python-written numerical optimization toolbox. 
Problems: NLP, LP, QP, NSP, MILP, LSP, LLSP, MMP, GLP, SLE, etc; automatic 
differentiation is available</p> <b>Changes:</b> 
<p>http://openopt.org/Changelog </p> 
<ul> 
<li><b>Authors:</b> Dmitrey Kroshko </li> 
<li><b>License:</b> Bsd </li> 
<li><b>Programming Language:</b> Python </li> </ul> 
<ul> 
<li><b>Operating System:</b> Linux, Macosx, Windows, Macos, Unix, Solaris </li>
<li><b>Data Formats:</b> Mps </li> 
<li><b>Tags:</b> Python, Optimization </li> </ul> <br>
<br>
<br>
<br>

<h2>  GraphLab v1-1908 </h2> 
<p> by dannybickson - November 22, 2011, 12:50:00 CET [  ] 972 views, 140 
downloads, 1 subscription </p> <br>
<br>

<p> <b>About:</b> Multicore/distributed large scale machine learning framework.
</p> <b>Changes:</b> 
<p>Update version. </p> 
<ul> 
<li><b>Authors:</b> Joseph Gonzalez, Yucheng Low, Aapo Kyrola, Danny Bickson, 
Carlos Guestrin </li> 
<li><b>License:</b> Apache 2.0 </li> 
<li><b>Programming Language:</b> C++, Python, Matlab, C, Java </li> </ul> 
<ul> 
<li><b>Operating System:</b> Linux, Mac Os X </li> 
<li><b>Data Formats:</b> Matlab, Sparse Matrix Market </li> 
<li><b>Tags:</b> Machine Learning, Collaborative Filtering, Matrix 
Factorization, Linear Models </li> </ul> <br>
<br>
<br>
<br>

<h2>  peewit 0.7 </h2> 
<p> by lorenz - November 4, 2011, 19:54:09 CET [  ] 7079 views, 1262 downloads
, 1 subscription</p> <br>
<br>

<p> <b>About:</b> peewit provides workflow services for programming, running 
and result examination of machine learning experiments. It does not provide any 
ML algorithms, has no GUI, and it is restricted to experiments that fulfill a 
certain uniformity. On the other hand, it makes no assumptions on the type of 
task. The current Version is 0.7.8.</p> <b>Changes:</b> 
<p>semi-uniform v-cubes </p> 
<ul> 
<li><b>Authors:</b> Lorenz Weizsaecker </li> 
<li><b>License:</b> Gpl Version 3 </li> 
<li><b>Programming Language:</b> Python </li> </ul> 
<ul> 
<li><b>Operating System:</b> Posix </li> 
<li><b>Data Formats:</b> Agnostic </li> 
<li><b>Tags:</b> Workflow </li> </ul> <br>
<br>
<br>
<br>

<h2>  MDP Modular toolkit for Data Processing 3.2 </h2> 
<p> by otizonaizit - October 24, 2011, 15:43:59 CET [  ] 10753 views, 3002 
downloads, 1 subscription </p> Rating <br>
(based on 3 votes) <br>
<br>
<br>

<br> 
<p> <b>About:</b> MDP is a Python library of widely used data processing 
algorithms that can be combined according to a pipeline analogy to build more 
complex data processing software. The base of available algorithms includes 
signal processing methods (Principal Component Analysis, Independent Component 
Analysis, Slow Feature Analysis), manifold learning methods ([Hessian] Locally 
Linear Embedding), several classifiers, probabilistic methods (Factor Analysis, 
RBM), data pre-processing methods, and many others.</p> <b>Changes:</b> 
<h2>What's new in version 3.2?</h2> 
<ul> 
<li> improved sklearn wrappers </li> 
<li> update sklearn, shogun, and pp wrappers to new versions </li> 
<li> do not leave temporary files around after testing </li> 
<li> refactoring and cleaning up of HTML exporting features </li> 
<li> improve export of signature and doc-string to public methods </li> 
<li> fixed and updated FastICANode to closely resemble the original Matlab 
version (thanks to Ben Willmore)</li> 
<li> support for new numpy version </li> 
<li> new NeuralGasNode (thanks to Michael Schmuker) </li> 
<li> several bug fixes and improvements </li> </ul> 
<p>We recommend all users to upgrade. </p> 
<ul> 
<li><b>Authors:</b> Mdp Developers </li> 
<li><b>License:</b> Bsd </li> 
<li><b>Programming Language:</b> Python </li> </ul> 
<ul> 
<li><b>Operating System:</b> Agnostic </li> 
<li><b>Data Formats:</b> None </li> 
<li><b>Tags:</b> Algorithms, Classifiers, Developing Framework, Education, Fda,
Gng, Ica, Pca, Sfa, Nips2008 </li> </ul> <br>
<br>
<br>
<br>

<h2>  treelearn 1 </h2> 
<p> by iskander - September 21, 2011, 16:12:27 CET [  ] 614 views, 119 
downloads, 1 subscription </p> <br>
<br>

<p> <b>About:</b> A python implementation of Breiman's Random Forests. </p> <b>
Changes:</b> 
<p>Initial Announcement on mloss.org. </p> 
<ul> 
<li><b>Authors:</b> Alex Rubinsteyn </li> 
<li><b>License:</b> Lpgl </li> 
<li><b>Programming Language:</b> Python </li> </ul> 
<ul> 
<li><b>Operating System:</b> Agnostic </li> 
<li><b>Data Formats:</b> Agnostic, Numpy </li> 
<li><b>Tags:</b> Decision Trees, Decision Tree Learning, Random Forests, 
Ensemble Of Classifiers </li> </ul> <br>
<br>
<br>
<br>

<h2>  Maja Machine Learning Framework 1.0 </h2> 
<p> by jhm - September 13, 2011, 15:13:56 CET [  ] 6883 views, 1388 downloads, 
1 subscription</p> <br>
<br>

<p> <b>About:</b> The Maja Machine Learning Framework (MMLF) is a general 
framework for problems in the domain of Reinforcement Learning (RL) written in 
python. It provides a set of RL related algorithms and a set of benchmark 
domains. Furthermore it is easily extensible and allows to automate 
benchmarking of different agents.</p> <b>Changes:</b> 
<ul> 
<li> Experiments can now be invoked from the command line </li> 
<li> Experiments can now be &quot;scripted&quot; </li> 
<li> MMLF Experimenter contains now basic module for statistical hypothesis 
testing</li> 
<li> MMLF Explorer can now visualize the model that has been learned by an 
agent</li> </ul> 
<ul> 
<li><b>Authors:</b> Jan Hendrik Metzen, Mark Edgington </li> 
<li><b>License:</b> Gpl Version 3 Or Later </li> 
<li><b>Programming Language:</b> Python </li> </ul> 
<ul> 
<li><b>Operating System:</b> Agnostic </li> 
<li><b>Data Formats:</b> None </li> 
<li><b>Tags:</b> Reinforcement Learning, Optimization, Evolution, Toolbox, 
Neuroevolution </li> </ul> <br>
<br>
<br>
<br>

<h2>  FLANN, Fast Library for Approximate Nearest Neighbors 1.6.11 </h2> 
<p> by mariusmuja - September 12, 2011, 22:32:29 CET [  ] 12079 views, 1697 
downloads, 1 subscription </p> <br>
<br>

<p> <b>About:</b> FLANN is a library for performing fast approximate nearest 
neighbor searches in high dimensional spaces. It contains a collection of 
algorithms we found to work best for nearest neighbor search.</p> <b>Changes:
</b> 
<p>See project page for changes. </p> 
<ul> 
<li><b>Authors:</b> Marius Muja </li> 
<li><b>License:</b> Lgpl </li> 
<li><b>Programming Language:</b> C++, Python, Matlab, C </li> </ul> 
<ul> 
<li><b>Operating System:</b> Linux, Macosx, Windows </li> 
<li><b>Data Formats:</b> None </li> 
<li><b>Tags:</b> Clustering, Nips2008, Hierarchical Kmeans, Kdtree, Nearest 
Neighbors </li> </ul> <br>
<br>
<br>
<br>

<h2>  OptWok 0.2.1 </h2> 
<p> by ong - July 21, 2011, 20:39:12 CET [  ] 1872 views, 277 downloads, 1 
subscription</p> <br>
<br>

<p> <b>About:</b> A collection of python code to perform research in 
optimization. The aim is to provide reusable components that can be quickly 
applied to machine learning problems. Used for learning output kernels, that is 
kernels between the labels of a classifier.</p> <b>Changes:</b> 
<ul> 
<li> Fixed missing multiclass module. </li> 
<li> Slycot sources no longer distributed, using github project instead. </li> 
</ul> 
<ul> 
<li><b>Authors:</b> Cheng Soon Ong </li> 
<li><b>License:</b> Gpl Version 3 Or Later </li> 
<li><b>Programming Language:</b> Python, Cython, Fortran </li> </ul> 
<ul> 
<li><b>Operating System:</b> Agnostic </li> 
<li><b>Data Formats:</b> Matlab, Numpy </li> 
<li><b>Tags:</b> Kernel Learning </li> </ul> <br>
<br>
<br>
<br>

<h2>  libDAI 0.3.0 </h2> 
<p> by jorism - July 12, 2011, 17:08:54 CET [  ] 17881 views, 3383 downloads, 
2 subscriptions</p> Rating <br>
(based on 1 vote) <br>
<br>
<br>
<br>

<p> <b>About:</b> libDAI provides free &amp; open source implementations of 
various (approximate) inference methods for graphical models with discrete 
variables, including Bayesian networks and Markov Random Fields.</p> <b>Changes:
</b> 
<p>Release 0.3.0 bumps the version number because the license has changed: 
instead of the former GPL v2+ license, libDAI is now licensed under the BSD 
2-clause license (also known as the FreeBSD license). Further, various bugs 
have been fixed.</p> 
<ul> 
<li><b>Authors:</b> Joris Mooij, Martijn Leisink, Frederik Eaton, Charles Vaske
,Giuseppe Passino, Bastian Wemmenhove, Patrick Pletscher </li> 
<li><b>License:</b> Free Bsd </li> 
<li><b>Programming Language:</b> C++, Python, Matlab, Octave </li> </ul> 
<ul> 
<li><b>Operating System:</b> Cygwin, Linux, Macosx, Windows </li> 
<li><b>Data Formats:</b> Ascii </li> 
<li><b>JMLR-MLOSS Publication:</b> JMLR Page </li> 
<li><b>Tags:</b> Approximate Inference, Bayesian Networks, Factor Graphs, 
Generalized Belief Propagation, Graphical Models, Junction Tree, Loop 
Corrections, Loopy Belief Propagation, Markov Random Fields, Mean Field </li> 
</ul> <br>
<br>
<br>
<br>

<h2>  K tree 0.4.2 </h2> 
<p> by cdevries - July 4, 2011, 06:01:59 CET [  ] 3117 views, 751 downloads, 1 
subscription</p> <br>
<br>

<p> <b>About:</b> The K-tree is a scalable approach to clustering inspired by 
the B+-tree and k-means algorithms.</p> <b>Changes:</b> 
<p>Release of K-tree implementation in Python. This is targeted at a research 
and rapid prototyping audience.</p> 
<ul> 
<li><b>Authors:</b> Lance De Vine, Chris De Vries, Shlomo Geva, Ulf 
Grossekathofer </li> 
<li><b>License:</b> Gpl, Lgpl </li> 
<li><b>Programming Language:</b> Python, C, Java </li> </ul> 
<ul> 
<li><b>Operating System:</b> Agnostic </li> 
<li><b>Data Formats:</b> Ascii, Java Arrays </li> 
<li><b>Tags:</b> Clustering, Algorithm </li> </ul> <br>
<br>
<br>
<br>

<h2>  gensim 0.8.0 </h2> 
<p> by Radim - June 21, 2011, 01:20:53 CET [  ] 6607 views, 1353 downloads, 1 
subscription</p> <br>
<br>

<p> <b>About:</b> Python Framework for Vector Space Modelling that can handle 
unlimited datasets (streamed input, online algorithms work incrementally in 
constant memory).</p> <b>Changes:</b> 
<ul> 
<li> faster document similarity queries </li> 
<li> more optimizations to <strong>Latent Dirichlet Allocation</strong> 
(online LDA) and<strong>Latent Semantic Analysis</strong> (single pass online 
SVD) (Wikipedia experiments ) </li> 
<li> full change set here </li> </ul> 
<ul> 
<li><b>Authors:</b> Radim Rehurek </li> 
<li><b>License:</b> Lgpl </li> 
<li><b>Programming Language:</b> Python </li> </ul> 
<ul> 
<li><b>Operating System:</b> Platform Independent </li> 
<li><b>Data Formats:</b> Agnostic </li> 
<li><b>Tags:</b> Latent Semantic Analysis, Latent Dirichlet Allocation, Svd, 
Random Projections, Tfidf </li> </ul> <br>
<br>
<br>
<br>

<h2>  Milk 0.3.10 </h2> 
<p> by luispedro - May 11, 2011, 04:18:53 CET [  ] 10805 views, 2186 downloads
, 1 subscription</p> Rating <br>
(based on 2 votes) <br>
<br>
<br>
<br>

<p> <b>About:</b> Python Machine Learning Toolkit </p> <b>Changes:</b> 
<ul> 
<li>
<p>Added a new module: milk.ext.jugparallel to interface with jug 
(http://luispedro.org/software/jug). This makes it easy to parallelise things 
such as n-fold cross validation (each fold runs on its own processor) or 
multiple kmeans random starts.</p> </li> 
<li>
<p>Add some new functions: measures.curves.precision_recall, 
milk.unsupervised.kmeans.select_best.kmeans.</p> </li> 
<li>
<p>Fixed a tricky bug in SDA and a few minor issues elsewhere </p> </li> </ul> 
<ul> 
<li><b>Authors:</b> Luis Pedro Coelho </li> 
<li><b>License:</b> Mit </li> 
<li><b>Programming Language:</b> C++, Python, C </li> </ul> 
<ul> 
<li><b>Operating System:</b> Agnostic </li> 
<li><b>Data Formats:</b> None, Agnostic </li> 
<li><b>Tags:</b> Python, Svm, Feature Selection, Kmeans, Decision Tree Learning
,Random Forests, Supervised, Libsvm, Affinity Propagation, Nonnegative Matrix 
Factorization </li> </ul> <br>
<br>
<br>
<br>

<h2>  mldata.org svn-r1070-Apr-2011 </h2> 
<p> by sonne - April 8, 2011, 10:15:49 CET [  ] 1674 views, 219 downloads, 1 
subscription</p> <br>
<br>

<p> <b>About:</b> The source code of the mldata.org site - a community portal 
for machine learning data sets.</p> <b>Changes:</b> 
<p>Initial Announcement on mloss.org. </p> 
<ul> 
<li><b>Authors:</b> Soeren Sonnenburg, Cheng Soon Ong, Sebastian Henschel, 
Mikio Braun </li> 
<li><b>License:</b> Gpl Version 3 Or Later </li> 
<li><b>Programming Language:</b> Python, D </li> </ul> 
<ul> 
<li><b>Operating System:</b> Platform Independent </li> 
<li><b>Data Formats:</b> Ascii, Matlab, Arff, Octave, Hdf, R, Mldata </li> 
<li><b>Tags:</b> Data Sets, Django, Platform, Website </li> </ul> <br>
<br>

<br> <br>

<h5> Showing Items 1-20 of 60 on page 1 of 3: 1 2 3 Next </h5> <br>

<p>&copy; 2007-2012 Soeren Sonnenburg, Mikio Braun, Cheng Soon Ong | Impressum 
| Hosting graciously provided by theML Group of the TU Berlin | v0.1 | Thanks 
Max Planck Society for previously hosting the site. </p> 
</body>
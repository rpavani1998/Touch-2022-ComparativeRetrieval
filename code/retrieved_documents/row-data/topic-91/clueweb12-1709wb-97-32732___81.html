<!doctype html>
<meta charset="utf-8">
<title>Statistics Glossary: S</title>
<body>
<br>
 | 2 | 3 | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | 
Q | R | S | T | U | V | W | X | Y | Z | <br>

<ul> 
<li> 
<ul> 
<li></li>  2 
<li>2D Bar/Column Plots</li> 
<li>2D Box Plots</li> 
<li>2D Box Plots - Box Whiskers</li> 
<li>2D Box Plots - Boxes</li> 
<li>2D Box Plots - Columns</li> 
<li>2D Box Plots - Error Bars</li> 
<li>2D Box Plots - Whiskers</li> 
<li>2D Categorized Detrended Probability Plots</li> 
<li>2D Categorized Half-Norm. Probability Plots</li> 
<li>2D Categorized Normal Probability Plots</li> 
<li>2D Detrended Probability Plots</li> 
<li>2D Histograms</li> 
<li>2D Histograms - Hanging Bars</li> 
<li>2D Histograms - Double-Y</li> 
<li>2D Line Plots</li> 
<li>2D Line Plots - Aggregated</li> 
<li>2D Line Plots - Double-Y</li> 
<li>2D Line Plots - Multiple</li> 
<li>2D Line Plots - Regular</li> 
<li>2D Line Plots - XY Trace</li> 
<li>2D Range Plots - Error Bars</li> 
<li>2D Matrix Plots</li> 
<li>2D Matrix Plots - Columns</li> 
<li>2D Matrix Plots - Lines</li> 
<li>2D Matrix Plots - Scatterplot</li> 
<li>2D Normal Probability Plots</li> 
<li>2D Probability-Probability Plots</li> 
<li>2D Probability-Probability Plots-Categorized</li> 
<li>2D Quantile-Quantile Plots</li> 
<li>2D Quantile-Quantile Plots - Categorized</li> 
<li>2D Scatterplot</li> 
<li>2D Scatterplot - Categorized Ternary Graph</li> 
<li>2D Scatterplot - Double-Y</li> 
<li>2D Scatterplot - Frequency</li> 
<li>2D Scatterplot - Multiple</li> 
<li>2D Scatterplot - Regular</li> 
<li>2D Scatterplot - Voronoi</li> 
<li>2D Sequential/Stacked Plots</li> 
<li>2D Sequential/Stacked Plots - Area</li> 
<li>2D Sequential/Stacked Plots - Column</li> 
<li>2D Sequential/Stacked Plots - Lines</li> 
<li>2D Sequential/Stacked Plots - Mixed Line</li> 
<li>2D Sequential/Stacked Plots - Mixed Step</li> 
<li>2D Sequential/Stacked Plots - Step</li> 
<li>2D Sequential/Stacked Plots - Step Area</li> 
<li>2D Ternary Plots - Scatterplot</li> 
<li></li>  3 
<li>3D Bivariate Histogram</li> 
<li>3D Box Plots</li> 
<li>3D Box Plots - Border-style Ranges</li> 
<li>3D Box Plots - Double Ribbon Ranges</li> 
<li>3D Box Plots - Error Bars</li> 
<li>3D Box Plots - Flying Blocks</li> 
<li>3D Box Plots - Flying Boxes</li> 
<li>3D Box Plots - Points</li> 
<li>3D Categorized Plots - Contour Plot</li> 
<li>3D Categorized Plots - Deviation Plot</li> 
<li>3D Categorized Plots - Scatterplot</li> 
<li>3D Categorized Plots - Space Plot</li> 
<li>3D Categorized Plots - Spectral Plot</li> 
<li>3D Categorized Plots - Surface Plot</li> 
<li>3D Deviation Plots</li> 
<li>3D Range Plot - Error Bars</li> 
<li>3D Raw Data Plots - Contour/Discrete</li> 
<li>3D Scatterplots</li> 
<li>3D Scatterplots - Ternary Graph</li> 
<li>3D Space Plots</li> 
<li>3D Ternary Plots</li> 
<li>3D Ternary Plots - Categorized Scatterplot</li> 
<li>3D Ternary Plots - Categorized Space</li> 
<li>3D Ternary Plots - Categorized Surface</li> 
<li>3D Ternary Plots - Categorized Trace</li> 
<li>3D Ternary Plots - Contour/Areas</li> 
<li>3D Ternary Plots - Contour/Lines</li> 
<li>3D Ternary Plots - Deviation</li> 
<li>3D Ternary Plots - Space</li> 
<li>3D Trace Plots</li> 
<li></li>  A 
<li>Aberration, Minimum</li> 
<li>Abrupt Permanent Impact</li> 
<li>Abrupt Temporary Impact</li> 
<li>Accept-Support Testing</li> 
<li>Accept Threshold</li> 
<li>Activation Function (in Neural Networks)</li> 
<li>Additive Models</li> 
<li>Additive Season, Damped Trend</li> 
<li>Additive Season, Exponential Trend</li> 
<li>Additive Season, Linear Trend</li> 
<li>Additive Season, No Trend</li> 
<li>Adjusted means</li> 
<li>Aggregation</li> 
<li>AID</li> 
<li>Akaike Information Criterion (AIC)</li> 
<li>Algorithm</li> 
<li>Alpha</li> 
<li>Anderson-Darling Test</li> 
<li>ANOVA</li> 
<li>Append a Network</li> 
<li>Append Cases and/or Variables</li> 
<li>Application Programming Interface (API)</li> 
<li>Arrow</li> 
<li>Assignable Causes and Actions</li> 
<li>Association Rules</li> 
<li>Asymmetrical Distribution</li> 
<li>AT&amp;T Runs Rules</li> 
<li>Attribute (attribute variable)</li> 
<li>Augmented Product Moment Matrix</li> 
<li>Autoassociative Network</li> 
<li>Automatic Network Designer</li> 
<li></li>  B 
<li>B Coefficients</li> 
<li>Back Propagation</li> 
<li>Bagging (Voting, Averaging)</li> 
<li>Balanced ANOVA Design</li> 
<li>Banner Tables</li> 
<li>Bar/Column Plots, 2D</li> 
<li>Bar Dev Plot</li> 
<li>Bar Left Y Plot</li> 
<li>Bar Right Y Plot</li> 
<li>Bar Top Plot</li> 
<li>Bar X Plot</li> 
<li>Bartlett Window</li> 
<li>Basis Functions</li> 
<li>Batch algorithms in <i>STATISTICA Neural Net</i></li> 
<li>Bayesian Information Criterion (BIC)</li> 
<li>Bayesian Networks</li> 
<li>Bayesian Statistics</li> 
<li>Bernoulli Distribution</li> 
<li>Best Network Retention</li> 
<li>Best Subset Regression</li> 
<li>Beta Coefficients</li> 
<li>Beta Distribution</li> 
<li>Bimodal Distribution</li> 
<li>Binomial Distribution</li> 
<li>Bivariate Normal Distribution</li> 
<li>Blocking</li> 
<li>Bonferroni Adjustment</li> 
<li>Bonferroni Test</li> 
<li>Boosting</li> 
<li>Boundary Case</li> 
<li>Box Plot/Medians (Block Stats Graphs)</li> 
<li>Box Plot/Means (Block Stats Graphs)</li> 
<li>Box Plots, 2D</li> 
<li>Box Plots, 2D - Box Whiskers</li> 
<li>Box Plots, 2D - Boxes</li> 
<li>Box Plots, 2D - Whiskers</li> 
<li>Box Plots, 3D</li> 
<li>Box Plots, 3D - Border-Style Ranges</li> 
<li>Box Plots, 3D - Double Ribbon Ranges</li> 
<li>Box Plots, 3D - Error Bars</li> 
<li>Box Plots, 3D - Flying Blocks</li> 
<li>Box Plots, 3D - Flying Boxes</li> 
<li>Box Plots, 3D - Points</li> 
<li>Box-Ljung Q Statistic</li> 
<li>Breakdowns</li> 
<li>Breaking Down (Categorizing)</li> 
<li>Brown-Forsythe Homogeneity of Variances</li> 
<li>Brushing</li> 
<li>Burt Table</li> 
<li></li>  C 
<li>Canonical Correlation</li> 
<li>Cartesian Coordinates</li> 
<li>Casewise Missing Data Deletion</li> 
<li>Categorical Dependent Variable</li> 
<li>Categorical Predictor</li> 
<li>Categorized Graphs</li> 
<li>Categorized Plots, 2D-Detrended Prob. Plots</li> 
<li>Categorized Plots, 2D-Half-Normal Prob. Plots</li> 
<li>Categorized Plots, 2D - Normal Prob. Plots</li> 
<li>Categorized Plots, 2D - Prob.-Prob. Plots</li> 
<li>Categorized Plots, 2D - Quantile Plots</li> 
<li>Categorized Plots, 3D - Contour Plot</li> 
<li>Categorized Plots, 3D - Deviation Plot</li> 
<li>Categorized Plots, 3D - Scatterplot</li> 
<li>Categorized Plots, 3D - Space Plot</li> 
<li>Categorized Plots, 3D - Spectral Plot</li> 
<li>Categorized Plots, 3D - Surface Plot</li> 
<li>Categorized 3D Scatterplot (Ternary graph)</li> 
<li>Categorized Contour/Areas (Ternary graph)</li> 
<li>Categorized Contour/Lines (Ternary graph)</li> 
<li>Categorizing</li> 
<li>Cauchy Distribution</li> 
<li>Cause-and-Effect Diagram</li> 
<li>Censoring (Censored Observations)</li> 
<li>Censoring, Left</li> 
<li>Censoring, Multiple</li> 
<li>Censoring, Right</li> 
<li>Censoring, Single</li> 
<li>Censoring, Type I</li> 
<li>Censoring, Type II</li> 
<li>CHAID</li> 
<li>Characteristic Life</li> 
<li>Chernoff Faces (Icon Plots)</li> 
<li><i>Chi</i>-square Distribution</li> 
<li>Circumplex</li> 
<li>City-Block (Manhattan) Distance</li> 
<li>Classification</li> 
<li>Classification (in Neural Networks)</li> 
<li>Classification and Regression Trees</li> 
<li>Classification by Labeled Exemplars (in NN)</li> 
<li>Classification Statistics (in Neural Networks)</li> 
<li>Classification Thresholds (in Neural Networks) </li> 
<li>Classification Trees</li> 
<li>Class Labeling (in Neural Networks)</li> 
<li>Cluster Analysis</li> 
<li>Cluster Diagram (in Neural Networks)</li> 
<li>Cluster Networks (in Neural Networks)</li> 
<li>Coarse Coding</li> 
<li>Codes</li> 
<li>Coding Variable</li> 
<li>Coefficient of Determination</li> 
<li>Coefficient of Variation </li> 
<li>Column Sequential/Stacked Plot</li> 
<li>Columns (Box Plot)</li> 
<li>Columns (Icon Plot)</li> 
<li>Common Causes</li> 
<li>Communality</li> 
<li>Complex Numbers</li> 
<li>Conditional Probability</li> 
<li>Conditioning (Categorizing)</li> 
<li>Confidence Interval</li> 
<li>Confidence Interval for the Mean</li> 
<li>Confidence Interval vs. Prediction Interval</li> 
<li>Confidence Limits</li> 
<li>Confidence Value (Association Rules)</li> 
<li>Confusion Matrix (in Neural Networks)</li> 
<li>Conjugate Gradient Descent (in Neural Net)</li> 
<li>Continuous Dependent Variable</li> 
<li>Contour/Discrete Raw Data Plot</li> 
<li>Contour Plot</li> 
<li>Control, Quality</li> 
<li>Cook's Distance</li> 
<li>Correlation</li> 
<li>Correlation, Intraclass</li> 
<li>Correlation (Pearson r)</li> 
<li>Correlation Value (Association Rules)</li> 
<li>Correspondence Analysis</li> 
<li>Cox-Snell Gen. Coefficient Determination</li> 
<li>Cpk, Cp, Cr</li> 
<li>CRISP</li> 
<li>Cross Entropy (in Neural Networks)</li> 
<li>Cross Verification (in Neural Networks)</li> 
<li>Cross-Validation</li> 
<li>Crossed Factors</li> 
<li>Crosstabulations</li> 
<li>C-SVM Classification</li> 
<li>Cubic Spline Smoother</li> 
<li>&quot;Curse&quot; of Dimensionality</li> 
<li></li>  D 
<li>Daniell (or Equal Weight) Window</li> 
<li>Data Mining</li> 
<li>Data Preparation Phase</li> 
<li>Data Reduction</li> 
<li>Data Rotation (in 3D space)</li> 
<li>Data Warehousing</li> 
<li>Decision Trees</li> 
<li>Degrees of Freedom</li> 
<li>Deleted Residual</li> 
<li>Denominator Synthesis</li> 
<li>Dependent t-test</li> 
<li>Dependent vs. Independent Variables</li> 
<li>Deployment</li> 
<li>Derivative-Free Funct. Min. Algorithms</li> 
<li>Design, Experimental</li> 
<li>Design Matrix</li> 
<li>Desirability Profiles</li> 
<li>Detrended Probability Plots</li> 
<li>Deviance</li> 
<li>Deviance Residuals</li> 
<li>Deviation</li> 
<li>Deviation Assign. Algorithms (in Neural Net)</li> 
<li>Deviation Plot (Ternary Graph)</li> 
<li>Deviation Plots, 3D</li> 
<li>DFFITS</li> 
<li>DIEHARD Suite of Tests &amp; Randm. Num. Gen.</li> 
<li>Differencing (in Time Series)</li> 
<li>Dimensionality Reduction</li> 
<li>Discrepancy Function</li> 
<li>Discriminant Function Analysis</li> 
<li>Distribution Function</li> 
<li>DOE</li> 
<li>Document Frequency</li> 
<li>Double-Y Histograms</li> 
<li>Double-Y Line Plots</li> 
<li>Double-Y Scatterplot</li> 
<li>Drill-Down Analysis</li> 
<li>Drilling-down (Categorizing)</li> 
<li>Duncan's test</li> 
<li>Dunnett's test</li> 
<li>DV</li> 
<li></li>  E 
<li>Effective Hypothesis Decomposition</li> 
<li>Efficient Score Statistic</li> 
<li>Eigenvalues</li> 
<li>Ellipse, Prediction Area and Range</li> 
<li>EM Clustering</li> 
<li>Endogenous Variable</li> 
<li>Ensembles (in Neural Networks)</li> 
<li>Enterprise Resource Planning (ERP)</li> 
<li>Enterprise SPC</li> 
<li>Enterprise-Wide Software Systems</li> 
<li>Entropy</li> 
<li>Epoch in (Neural Networks)</li> 
<li>Eps</li> 
<li>EPSEM Samples</li> 
<li>ERP</li> 
<li>Error Bars (2D Box Plots)</li> 
<li>Error Bars (2D Range Plots)</li> 
<li>Error Bars (3D Box Plots)</li> 
<li>Error Bars (3D Range Plots)</li> 
<li>Error Function (in Neural Networks)</li> 
<li>Estimable Functions</li> 
<li>Euclidean Distance</li> 
<li>Euler's e</li> 
<li>Exogenous Variable</li> 
<li>Experimental Design</li> 
<li>Explained Variance</li> 
<li>Exploratory Data Analysis</li> 
<li>Exponential Distribution</li> 
<li>Exponential Family of Distributions</li> 
<li>Exponential Function</li> 
<li>Exponentially Weighted Moving Avg. Line</li> 
<li>Extrapolation</li> 
<li>Extreme Values (in Box Plots)</li> 
<li>Extreme Value Distribution</li> 
<li></li>  F 
<li>F Distribution</li> 
<li>FACT</li> 
<li>Factor Analysis</li> 
<li>Fast Analysis Shared Multidimensional Info. FASMI</li> 
<li>Feature Extraction (vs. Feature Selection)</li> 
<li>Feature Selection</li> 
<li>Feedforward Networks</li> 
<li>Fisher LSD</li> 
<li>Fixed Effects (in ANOVA)</li> 
<li>Free Parameter</li> 
<li>Frequencies, Marginal</li> 
<li>Frequency Scatterplot</li> 
<li>Frequency Tables</li> 
<li>Function Minimization Algorithms</li> 
<li></li>  G 
<li>g2 Inverse</li> 
<li>Gains Chart</li> 
<li>Gamma Coefficient</li> 
<li>Gamma Distribution</li> 
<li>Gaussian Distribution</li> 
<li>Gauss-Newton Method</li> 
<li>General ANOVA/MANOVA</li> 
<li>General Linear Model</li> 
<li>Generalization (in Neural Networks)</li> 
<li>Generalized Additive Models</li> 
<li>Generalized Inverse</li> 
<li>Generalized Linear Model</li> 
<li>Genetic Algorithm</li> 
<li>Genetic Algorithm Input Selection</li> 
<li>Geometric Distribution</li> 
<li>Geometric Mean</li> 
<li>Gibbs Sampler</li> 
<li>Gini Measure of Node Impurity</li> 
<li>Gompertz Distribution</li> 
<li>Goodness of Fit</li> 
<li>Gradient</li> 
<li>Gradient Descent</li> 
<li>Gradual Permanent Impact</li> 
<li>Group Charts</li> 
<li>Grouping (Categorizing)</li> 
<li>Grouping Variable</li> 
<li>Groupware</li> 
<li></li>  H 
<li>Half-Normal Probability Plots</li> 
<li>Half-Normal Probability Plots - Categorized</li> 
<li>Hamming Window</li> 
<li>Hanging Bars Histogram</li> 
<li>Harmonic Mean</li> 
<li>Hazard</li> 
<li>Hazard Rate</li> 
<li>Heuristic</li> 
<li>Heywood Case</li> 
<li>Hidden Layers (in Neural Networks)</li> 
<li>High-Low Close</li> 
<li>Histograms, 2D</li> 
<li>Histograms, 2D - Double-Y</li> 
<li>Histograms, 2D - Hanging Bars</li> 
<li>Histograms, 2D - Multiple</li> 
<li>Histograms, 2D - Regular</li> 
<li>Histograms, 3D Bivariate</li> 
<li>Histograms, 3D - Box Plots</li> 
<li>Histograms, 3D - Contour/Discrete</li> 
<li>Histograms, 3D - Contour Plot</li> 
<li>Histograms, 3D - Spikes</li> 
<li>Histograms, 3D - Surface Plot</li> 
<li>Hollander-Proschan Test</li> 
<li>Hooke-Jeeves Pattern Moves</li> 
<li>Hosmer-Lemeshow Test</li> 
<li>HTM</li> 
<li>HTML</li> 
<li>Hyperbolic Tangent (tanh)</li> 
<li>Hyperplane</li> 
<li>Hypersphere</li> 
<li></li>  I 
<li>Icon Plots</li> 
<li>Icon Plots - Chernoff Faces</li> 
<li>Icon Plots - Columns</li> 
<li>Icon Plots - Lines</li> 
<li>Icon Plots - Pies</li> 
<li>Icon Plots - Polygons</li> 
<li>Icon Plots - Profiles</li> 
<li>Icon Plots - Stars</li> 
<li>Icon Plots - Sun Rays</li> 
<li>Increment vs Non-Increment Learning Algr.</li> 
<li>Independent Events</li> 
<li>Independent t-test</li> 
<li>Independent vs. Dependent Variables</li> 
<li>Industrial Experimental Design</li> 
<li>Inertia</li> 
<li>Inlier</li> 
<li>In-Place Database Processing (IDP)</li> 
<li>Interactions</li> 
<li>Interpolation</li> 
<li>Interval Scale</li> 
<li>Intraclass Correlation Coefficient</li> 
<li>Invariance Const. Scale Factor ICSF</li> 
<li>Invariance Under Change of Scale (ICS)</li> 
<li>Inverse Document Frequency</li> 
<li>Ishikawa Chart</li> 
<li>Isotropic Deviation Assignment</li> 
<li>Item and Reliability Analysis</li> 
<li>IV</li> 
<li></li>  J 
<li>Jacobian Matrix</li> 
<li>Jogging Weights</li> 
<li>Johnson Curves</li> 
<li>Join</li> 
<li>Joining Networks (in Neural Networks)</li> 
<li>JPEG</li> 
<li>JPG</li> 
<li></li>  K 
<li>Kendall Tau</li> 
<li>Kernel Functions</li> 
<li><em>k</em>-Means Algorithm (in Neural Networks)</li> 
<li><em>k</em>-Nearest Algorithm</li> 
<li>Kohonen Algorithm (in Neural Networks)</li> 
<li>Kohonen Networks</li> 
<li>Kohonen Training</li> 
<li>Kolmogorov-Smirnov Test</li> 
<li>Kronecker Product</li> 
<li>Kruskal-Wallis Test</li> 
<li>Kurtosis</li> 
<li></li>  L 
<li>Lack of Fit</li> 
<li>Lambda Prime</li> 
<li>Laplace Distribution</li> 
<li>Latent Semantic Indexing</li> 
<li>Latent Variable</li> 
<li>Layered Compression</li> 
<li>Learned Vector Quantization (in Neural Net)</li> 
<li>Learning Rate (in Neural Networks)</li> 
<li>Least Squares (2D graphs)</li> 
<li>Least Squares (3D graphs)</li> 
<li>Least Squares Estimator</li> 
<li>Least Squares Means</li> 
<li>Left and Right Censoring</li> 
<li>Levenberg-Marquardt Algorithm (in Neural Net)</li> 
<li>Levene's Test for Homogeneity of Variances</li> 
<li>Leverage values</li> 
<li>Life Table</li> 
<li>Life, Characteristic</li> 
<li>Lift Charts</li> 
<li>Likelihood</li> 
<li>Lilliefors test</li> 
<li>Line Plots, 2D</li> 
<li>Line Plots, 2D - Aggregated</li> 
<li>Line Plots, 2D (Case Profiles)</li> 
<li>Line Plots, 2D - Double-Y</li> 
<li>Line Plots, 2D - Multiple</li> 
<li>Line Plots, 2D - Regular</li> 
<li>Line Plots, 2D - XY Trace</li> 
<li>Linear (2D graphs)</li> 
<li>Linear (3D graphs)</li> 
<li>Linear Activation function</li> 
<li>Linear Modeling</li> 
<li>Linear Units</li> 
<li>Lines (Icon Plot)</li> 
<li>Lines (Matrix Plot)</li> 
<li>Lines Sequential/Stacked Plot</li> 
<li>Link Function</li> 
<li>Local Minima</li> 
<li>Locally Weighted (Robust) Regression</li> 
<li>Logarithmic Function</li> 
<li>Logistic Distribution</li> 
<li>Logistic Function</li> 
<li>Logit Regression and Transformation</li> 
<li>Log-Linear Analysis</li> 
<li>Log-Normal Distribution</li> 
<li>Lookahead (in Neural Networks)</li> 
<li>Loss Function</li> 
<li>LOWESS Smoothing</li> 
<li></li>  M 
<li>Machine Learning</li> 
<li>Mahalanobis Distance</li> 
<li>Mallow's CP</li> 
<li>Manifest Variable</li> 
<li>Mann-Scheuer-Fertig Test</li> 
<li>MANOVA</li> 
<li>Marginal Frequencies</li> 
<li>Markov Chain Monte Carlo (MCMC)</li> 
<li>Mass</li> 
<li>Matching Moments Method</li> 
<li>Matrix Collinearity</li> 
<li>Matrix Ill-Conditioning</li> 
<li>Matrix Inverse</li> 
<li>Matrix Plots</li> 
<li>Matrix Plots - Columns</li> 
<li>Matrix Plots - Lines</li> 
<li>Matrix Plots - Scatterplot</li> 
<li>Matrix Rank</li> 
<li>Matrix Singularity</li> 
<li>Maximum Likelihood Loss Function</li> 
<li>Maximum Likelihood Method</li> 
<li>Maximum Unconfounding</li> 
<li>MD (Missing data)</li> 
<li>Mean</li> 
<li>Mean/S.D. Algorithm (in Neural Networks)</li> 
<li>Mean, Geometric</li> 
<li>Mean, Harmonic</li> 
<li>Mean Substitution of Missing Data</li> 
<li>Means, Adjusted</li> 
<li>Means, Unweighted</li> 
<li>Median</li> 
<li>Meta-Learning</li> 
<li>Method of Matching Moments</li> 
<li>Minimax</li> 
<li>Minimum Aberration</li> 
<li>Mining, Data</li> 
<li>Missing values</li> 
<li>Mixed Line Sequential/Stacked Plot</li> 
<li>Mixed Step Sequential/Stacked Plot</li> 
<li>Mode</li> 
<li>Model Profiles (in Neural Networks)</li> 
<li>Models for Data Mining</li> 
<li>Monte Carlo</li> 
<li>Multi-Pattern Bar</li> 
<li>Multicollinearity</li> 
<li>Multidimensional Scaling</li> 
<li>Multilayer Perceptrons</li> 
<li>Multimodal Distribution</li> 
<li>Multinomial Distribution</li> 
<li>Multinomial Logit and Probit Regression</li> 
<li>Multiple Axes in Graphs</li> 
<li>Multiple Censoring</li> 
<li>Multiple Dichotomies</li> 
<li>Multiple Histogram</li> 
<li>Multiple Line Plots</li> 
<li>Multiple Scatterplot</li> 
<li>Multiple R</li> 
<li>Multiple Regression</li> 
<li>Multiple Response Variables</li> 
<li>Multiple-Response Tables</li> 
<li>Multiple Stream Group Charts</li> 
<li>Multiplicative Season, Damped Trend</li> 
<li>Multiplicative Season, Exponential Trend</li> 
<li>Multiplicative Season, Linear Trend</li> 
<li>Multiplicative Season, No Trend</li> 
<li>Multivar. Adapt. Regres. Splines MARSplines</li> 
<li>Multi-way Tables</li> 
<li></li>  N 
<li>Nagelkerke Gen. Coefficient Determination</li> 
<li>Naive Bayes</li> 
<li>Neat Scaling of Intervals</li> 
<li>Negative Correlation</li> 
<li>Negative Exponential (2D graphs)</li> 
<li>Negative Exponential (3D graphs)</li> 
<li>Neighborhood (in Neural Networks)</li> 
<li>Nested Factors</li> 
<li>Nested Sequence of Models</li> 
<li>Neural Networks</li> 
<li>Neuron</li> 
<li>Newman-Keuls Test</li> 
<li>N-in-One Encoding</li> 
<li>Noise Addition (in Neural Networks)</li> 
<li>Nominal Scale</li> 
<li>Nominal Variables</li> 
<li>Nonlinear Estimation</li> 
<li>Nonparametrics</li> 
<li>Non-Outlier Range</li> 
<li>Nonseasonal, Damped Trend</li> 
<li>Nonseasonal, Exponential Trend</li> 
<li>Nonseasonal, Linear Trend</li> 
<li>Nonseasonal, No Trend</li> 
<li>Normal Distribution</li> 
<li>Normal Distribution, Bivariate</li> 
<li>Normal Fit</li> 
<li>Normality Tests</li> 
<li>Normalization</li> 
<li>Normal Probability Plots</li> 
<li>Normal Probability Plots (Computation Note)</li> 
<li>n Point Moving Average Line</li> 
<li></li>  O 
<li>ODBC</li> 
<li>Odds Ratio</li> 
<li>OLE DB</li> 
<li>On-Line Analytic Processing (OLAP)</li> 
<li>One-Off (in Neural Networks)</li> 
<li>One-of-N Encoding (in Neural Networks)</li> 
<li>One-Sample t-Test</li> 
<li>One-Sided Ranges Error Bars Range Plots</li> 
<li>One-Way Tables</li> 
<li>Operating Characteristic Curves</li> 
<li>Ordinal Multinomial Distribution</li> 
<li>Ordinal Scale</li> 
<li>Outer Arrays</li> 
<li>Outliers</li> 
<li>Outliers (in Box Plots)</li> 
<li>Overdispersion</li> 
<li>Overfitting</li> 
<li>Overlearning (in Neural Networks)</li> 
<li>Overparameterized Model</li> 
<li></li>  P 
<li>Pairwise Del. Missing Data vs Mean Subst.</li> 
<li>Pairwise MD Deletion</li> 
<li>Parametric Curve</li> 
<li>Pareto Chart Analysis</li> 
<li>Pareto Distribution</li> 
<li>Part Correlation</li> 
<li>Partial Correlation</li> 
<li>Partial Least Squares Regression</li> 
<li>Partial Residuals</li> 
<li>Parzen Window</li> 
<li>Pearson Correlation</li> 
<li>Pearson Curves</li> 
<li>Pearson Residuals</li> 
<li>Penalty Functions</li> 
<li>Percentiles</li> 
<li>Perceptrons (in Neural Networks)</li> 
<li>Pie Chart</li> 
<li>Pie Chart - Counts</li> 
<li>Pie Chart - Multi-Pattern Bar</li> 
<li>Pie Chart - Values</li> 
<li>Pies (Icon Plots)</li> 
<li>PMML (Predictive Model Markup Language)</li> 
<li>PNG Files</li> 
<li>Poisson Distribution</li> 
<li>Polar Coordinates</li> 
<li>Polygons (Icon Plots)</li> 
<li>Polynomial</li> 
<li>Population Stability Report</li> 
<li>Portable Network Graphics Files</li> 
<li>Positive Correlation</li> 
<li>Post hoc Comparisons</li> 
<li>Post Synaptic Potential (PSP) Function</li> 
<li>Posterior Probability</li> 
<li>Power (Statistical)</li> 
<li>Power Goal</li> 
<li>Ppk, Pp, Pr</li> 
<li>Prediction Interval Ellipse</li> 
<li>Prediction Profiles</li> 
<li>Predictive Data Mining</li> 
<li>Predictive Mapping</li> 
<li>Predictive Model Markup Language (PMML)</li> 
<li>Predictors</li> 
<li>PRESS Statistic</li> 
<li>Principal Components Analysis</li> 
<li>Prior Probabilities</li> 
<li>Probability</li> 
<li>Probability Plots - Detrended</li> 
<li>Probability Plots - Normal</li> 
<li>Probability Plots - Half-Normal</li> 
<li>Probability-Probability Plots</li> 
<li>Probability-Probability Plots - Categorized</li> 
<li>Probability Sampling</li> 
<li>Probit Regression and Transformation</li> 
<li>PROCEED</li> 
<li>Process Analysis</li> 
<li>Process Capability Indices</li> 
<li>Process Performance Indices</li> 
<li>Profiles, Desirability</li> 
<li>Profiles, Prediction</li> 
<li>Profiles (Icon Plots)</li> 
<li>Pruning (in Classification Trees)</li> 
<li>Pseudo-Components</li> 
<li>Pseudo-Inverse Algorithm</li> 
<li>Pseudo-Inverse-Singular Val. Decomp. NN</li> 
<li>PSP (Post Synaptic Potential) Function</li> 
<li>Pure Error</li> 
<li>p-Value (Statistical Significance)</li> 
<li></li>  Q 
<li>Quadratic</li> 
<li>Quality</li> 
<li>Quality Control</li> 
<li>Quantiles</li> 
<li>Quantile-Quantile Plots</li> 
<li>Quantile-Quantile Plots - Categorized</li> 
<li>Quartile Range</li> 
<li>Quartiles</li> 
<li>Quasi-Newton Method</li> 
<li>QUEST</li> 
<li>Quota Sampling</li> 
<li></li>  R 
<li>R Programming Language</li> 
<li>Radial Basis Functions</li> 
<li>Radial Sampling (in Neural Networks)</li> 
<li>Random Effects (in Mixed Model ANOVA)</li> 
<li>Random Forests</li> 
<li>Random Num. from Arbitrary Distributions</li> 
<li>Random Numbers (Uniform)</li> 
<li>Random Sub-Sampling in Data Mining</li> 
<li>Range Ellipse</li> 
<li>Range Plots - Boxes</li> 
<li>Range Plots - Columns</li> 
<li>Range Plots - Whiskers</li> 
<li>Rank</li> 
<li>Rank Correlation</li> 
<li>Ratio Scale</li> 
<li>Raw Data, 3D Scatterplot</li> 
<li>Raw Data Plots, 3D - Contour/Discrete</li> 
<li>Raw Data Plots, 3D - Spikes</li> 
<li>Raw Data Plots, 3D - Surface Plot</li> 
<li>Rayleigh Distribution</li> 
<li>Receiver Oper. Characteristic Curve</li> 
<li>Receiver Oper. Characteristic (in Neural Net)</li> 
<li>Rectangular Distribution</li> 
<li>Regression</li> 
<li>Regression (in Neural Networks)</li> 
<li>Regression, Multiple</li> 
<li>Regression Summary Statistics (in Neural Net)</li> 
<li>Regular Histogram</li> 
<li>Regular Line Plots</li> 
<li>Regular Scatterplot</li> 
<li>Regularization (in Neural Networks)</li> 
<li>Reject Inference</li> 
<li>Reject Threshold</li> 
<li>Relative Function Change Criterion</li> 
<li>Reliability</li> 
<li>Reliability and Item Analysis</li> 
<li>Representative Sample</li> 
<li>Resampling (in Neural Networks)</li> 
<li>Residual</li> 
<li>Resolution</li> 
<li>Response Surface</li> 
<li>Right Censoring</li> 
<li>RMS (Root Mean Squared) Error</li> 
<li>Robust Locally Weighted Regression</li> 
<li>ROC Curve</li> 
<li>ROC Curve (in Neural Networks)</li> 
<li>Root Cause Analysis</li> 
<li>Root Mean Square Stand. Effect RMSSE</li> 
<li>Rosenbrock Pattern Search</li> 
<li>Rotating Coordinates, Method of</li> 
<li>r (Pearson Correlation Coefficient)</li> 
<li>Runs Tests (in Quality Control)</li> 
<li></li>  S 
<li>Sampling Fraction</li> 
<li>Scalable Software Systems</li> 
<li>Scaling</li> 
<li>Scatterplot, 2D</li> 
<li>Scatterplot, 2D-Categorized Ternary Graph</li> 
<li>Scatterplot, 2D - Double-Y</li> 
<li>Scatterplot, 2D - Frequency</li> 
<li>Scatterplot, 2D - Multiple</li> 
<li>Scatterplot, 2D - Regular</li> 
<li>Scatterplot, 2D - Voronoi</li> 
<li>Scatterplot, 3D</li> 
<li>Scatterplot, 3D - Raw Data</li> 
<li>Scatterplot, 3D - Ternary Graph</li> 
<li>Scatterplot Smoothers</li> 
<li>Scheffe's Test</li> 
<li>Score Statistic</li> 
<li>Scree Plot, Scree Test</li> 
<li>S.D. Ratio</li> 
<li>Semi-Partial Correlation</li> 
<li>SEMMA</li> 
<li>Sensitivity Analysis (in Neural Networks)</li> 
<li>Sequential Contour Plot, 3D</li> 
<li>Sequential/Stacked Plots, 2D</li> 
<li>Sequential/Stacked Plots, 2D - Area</li> 
<li>Sequential/Stacked Plots, 2D - Column</li> 
<li>Sequential/Stacked Plots, 2D - Lines</li> 
<li>Sequential/Stacked Plots, 2D - Mixed Line</li> 
<li>Sequential/Stacked Plots, 2D - Mixed Step</li> 
<li>Sequential/Stacked Plots, 2D - Step</li> 
<li>Sequential/Stacked Plots, 2D - Step Area</li> 
<li>Sequential Surface Plot, 3D</li> 
<li>Sets of Samples in Quality Control Charts</li> 
<li>Shapiro-Wilks' W test</li> 
<li>Shewhart Control Charts</li> 
<li>Short Run Control Charts</li> 
<li>Shuffle, Back Propagation (in Neural Net)</li> 
<li>Shuffle Data (in Neural Networks)</li> 
<li>Sigma Restricted Model</li> 
<li>Sigmoid Function</li> 
<li>Signal Detection Theory</li> 
<li>Simple Random Sampling (SRS)</li> 
<li>Simplex Algorithm</li> 
<li>Single and Multiple Censoring</li> 
<li>Singular Value Decomposition</li> 
<li>Six Sigma (DMAIC)</li> 
<li>Six Sigma Process</li> 
<li>Skewness</li> 
<li>Slicing (Categorizing)</li> 
<li>Smoothing</li> 
<li>SOFMs Self-Organizing Maps Kohonen Net</li> 
<li>Softmax</li> 
<li>Space Plots 3D</li> 
<li>SPC</li> 
<li>Spearman R</li> 
<li>Special Causes</li> 
<li>Spectral Plot</li> 
<li>Spikes (3D graphs)</li> 
<li>Spinning Data (in 3D space)</li> 
<li>Spline (2D graphs)</li> 
<li>Spline (3D graphs)</li> 
<li>Split Selection (for Classification Trees)</li> 
<li>Splitting (Categorizing)</li> 
<li>Spurious Correlations</li> 
<li>SQL</li> 
<li>Square Root of the Signal to Noise Ratio (f)</li> 
<li>Stacked Generalization</li> 
<li>Stacking (Stacked Generalization)</li> 
<li>Standard Deviation</li> 
<li>Standard Error</li> 
<li>Standard Error of the Mean</li> 
<li>Standard Error of the Proportion</li> 
<li>Standardization</li> 
<li>Standardized DFFITS</li> 
<li>Standardized Effect (Es)</li> 
<li>Standard Residual Value</li> 
<li>Stars (Icon Plots)</li> 
<li>Stationary Series (in Time Series)</li> 
<li>STATISTICA Advanced Linear/Nonlinear</li> 
<li>STATISTICA Automated Neural Networks</li> 
<li>STATISTICA Base</li> 
<li>STATISTICA Data Miner</li> 
<li>STATISTICA Data Warehouse</li> 
<li>STATISTICA Document Management System</li> 
<li>STATISTICA Enterprise</li> 
<li>STATISTICA Enterprise/QC</li> 
<li>STATISTICA Enterprise Server</li> 
<li>STATISTICA Enterprise SPC</li> 
<li>STATISTICA Monitoring and Alerting Server</li> 
<li>STATISTICA MultiStream</li> 
<li>STATISTICA Multivariate Stat. Process Ctrl</li> 
<li>STATISTICA PI Connector</li> 
<li>STATISTICA PowerSolutions</li> 
<li>STATISTICA Process Optimization</li> 
<li>STATISTICA Quality Control Charts</li> 
<li>STATISTICA Sequence Assoc. Link Analysis</li> 
<li>STATISTICA Text Miner</li> 
<li>STATISTICA Variance Estimation Precision</li> 
<li>Statistical Power</li> 
<li>Statistical Process Control (SPC)</li> 
<li>Statistical Significance (p-value)</li> 
<li>Steepest Descent Iterations</li> 
<li>Stemming</li> 
<li>Steps</li> 
<li>Stepwise Regression</li> 
<li>Stiffness Parameter (in Fitting Options)</li> 
<li>Stopping Conditions</li> 
<li>Stopping Conditions (in Neural Networks)</li> 
<li>Stopping Rule (in Classification Trees)</li> 
<li>Stratified Random Sampling</li> 
<li>Stub and Banner Tables</li> 
<li>Studentized Deleted Residuals</li> 
<li>Studentized Residuals</li> 
<li>Student's t Distribution</li> 
<li>Sum-Squared Error Function</li> 
<li>Sums of Squares (Type I, II, III (IV, V, VI)) </li> 
<li>Sun Rays (Icon Plots)</li> 
<li>Supervised Learning (in Neural Networks)</li> 
<li>Support Value (Association Rules)</li> 
<li>Support Vector</li> 
<li>Support Vector Machine (SVM)</li> 
<li>Suppressor Variable</li> 
<li>Surface Plot (from Raw Data)</li> 
<li>Survival Analysis</li> 
<li>Survivorship Function</li> 
<li>Sweeping</li> 
<li>Symmetrical Distribution</li> 
<li>Symmetric Matrix</li> 
<li>Synaptic Functions (in Neural Networks)</li> 
<li></li>  T 
<li>Tables</li> 
<li>Tapering</li> 
<li>t Distribution (Student's)</li> 
<li>Tau, Kendall</li> 
<li>Ternary Plots, 2D - Scatterplot</li> 
<li>Ternary Plots, 3D</li> 
<li>Ternary Plots, 3D - Categorized Scatterplot</li> 
<li>Ternary Plots, 3D - Categorized Space</li> 
<li>Ternary Plots, 3D - Categorized Surface</li> 
<li>Ternary Plots, 3D - Categorized Trace</li> 
<li>Ternary Plots, 3D - Contour/Areas</li> 
<li>Ternary Plots, 3D - Contour/Lines</li> 
<li>Ternary Plots, 3D - Deviation</li> 
<li>Ternary Plots, 3D - Space</li> 
<li>Text Mining</li> 
<li>THAID</li> 
<li>Threshold</li> 
<li>Time Series</li> 
<li>Time Series (in Neural Networks)</li> 
<li>Time-Dependent Covariates</li> 
<li>Tolerance (in Multiple Regression)</li> 
<li>Topological Map</li> 
<li>Trace Plots, 3D</li> 
<li>Trace Plot, Categorized (Ternary Graph)</li> 
<li>Training/Test Error/Classification Accuracy</li> 
<li>Transformation (Probit Regression)</li> 
<li>Trellis Graphs</li> 
<li>Trimmed Means</li> 
<li>t-Test (independent &amp; dependent samples)</li> 
<li>Tukey HSD</li> 
<li>Tukey Window</li> 
<li>Two-State (in Neural Networks)</li> 
<li>Type I, II, III (IV, V, VI) Sums of Squares</li> 
<li>Type I Censoring</li> 
<li>Type II Censoring</li> 
<li>Type I Error Rate</li> 
<li></li>  U 
<li>Unconfounding, Maximum</li> 
<li>Unequal N HSD</li> 
<li>Uniform Distribution</li> 
<li>Unimodal Distribution</li> 
<li>Unit Penalty</li> 
<li>Unit Types (in Neural Networks)</li> 
<li>Unsupervised and Supervised Learning</li> 
<li>Unsupervised Learning (in Neural Networks)</li> 
<li>Unweighted Means</li> 
<li></li>  V 
<li>Variance</li> 
<li>Variance Components in Mixed Model ANOVA</li> 
<li>Variance Inflation Factor (VIF)</li> 
<li>V-fold Cross-validation</li> 
<li>Vintage Analysis</li> 
<li>Voronoi</li> 
<li>Voronoi Scatterplot</li> 
<li>Voting</li> 
<li></li>  W 
<li>Wald Statistic</li> 
<li>Warehousing, Data</li> 
<li>Weibull Distribution</li> 
<li>Weigend Weight Reg. (in Neural Networks)</li> 
<li>Weighted Least Squares</li> 
<li>Wilcoxon test</li> 
<li>Wilson-Hilferty Transformation</li> 
<li>Win Frequencies (in Neural Networks)</li> 
<li>Wire</li> 
<li></li>  X 
<li>X11 Output</li> 
<li>XML (Extensible Markup Language)</li> 
<li></li>  Y 
<li>Yates Corrected Chi-square</li> 
<li></li>  Z 
<li>Z Distribution (Standard Normal)</li> </ul> </li> <br>
<br>
<br>
<br>
<br>

<br> <br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

<br> <br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

<br> <br>
<br>
</ul> <br>
<br>
<b>Creators of <em>STATISTICA</em> Data Analysis 
Software and Services</b> <br>
<br>
 Welcome, Register &nbsp;|&nbsp; Login <br>

 Search the Electronic Statistics Textbook <br>
StatSoft.com TextbookStatistics 
GlossaryS <br>
<br>
<br>

<p>Sampling Fraction. In probability sampling, the sampling fraction is the 
(known) probability with which cases in the population are selected into the 
sample. For example, if we were to take a simple random sample with a sampling 
fraction of 1/10,000 from a population of 1,000,000 cases, each case would have 
a 1/10,000 probability of being selected into the sample, which will consist of 
approximately 1/10,000 * 1,000,000 = 100 observations.</p> 
<p>Scalable Software Systems. Software (e.g., a data base management system, 
such as MS SQL Server or Oracle) that can be expanded to meet future 
requirements without the need to restructure its operation (e.g., split data 
into smaller segments) to avoid a degradation of its performance. For example, 
a scalable network allows the network administrator to add many additional 
nodes without the need to redesign the basic system. An example of a 
non-scalable architecture is the DOS directory structure (adding files will 
eventually require splitting them into subdirectories). See alsoEnterprise-Wide 
Systems.</p> 
<p>Scaling. Altering original variable values (according to a specific 
function or an algorithm) into a range that meet particular criteria (e.g., 
positive numbers, fractions, numbers less than 10E12, numbers with a large 
relative variance).</p> 
<p>Scatterplot, 2D. The scatterplot visualizes a relation (correlation) 
between two variables<i>X</i> and <i>Y</i> (e.g., weight and height). 
Individual data points are represented in two-dimensional space (see below), 
where axes represent the variables (<i>X</i> on the horizontal axis and <i>Y</i>
 on the vertical axis).</p> 
<p></p> 
<p>The two coordinates (<i>X</i> and <i>Y</i>) that determine the location of 
each point correspond to its specific values on the two variables. See also,
Data Reduction.</p> 
<p>Scatterplot, 2D - Categorized Ternary Graph. The points representing the 
proportions of the component variables (<i>X, Y</i>, and <i>Z</i>) in a ternary 
graph are plotted in a 2-dimensional display for each level of thegrouping 
variable (or user-defined subset of data). One component graph is produced for 
each level of the grouping variable (or user-defined subset of data) and all 
the component graphs are arranged in one display to allow for comparisons 
between the subsets of data (categories). See also,Data Reduction.</p> 
<p>Scatterplot, 2D - Double-Y. This type of scatterplot can be considered to 
be a combination of two<i>multiple scatterplots</i> for one <i>X-variable</i> 
and two different sets (lists) of<i>Y-variables</i>. A scatterplot for the <i>
X-variable</i> and each of the selected <i>Y-variables</i> will be plotted, but 
the variables entered into the first list (called<i>Left-Y</i>) will be plotted 
against the<i>left-Y</i> axis, whereas the variables entered into the second 
list (called<i>Right-Y</i>) will be plotted against the <i>right-Y</i> axis. 
The names of all<i>Y-variables</i> from the two lists will be included in the 
legend followed either by the letter (<i>L</i>) or (<i>R</i>), denoting the <i>
left-Y</i> and <i>right-Y</i> axis, respectively.</p> 
<p></p> 
<p>The <i>Double-Y</i> scatterplot can be used to compare images of several 
correlations by overlaying them in a single graph. However, due to the 
independent scaling used for the two list of variables, it can facilitate 
comparisons between variables with values in different ranges. See also,Data 
Reduction.</p> 
<p>Scatterplot, 2D - Frequency. Frequency scatterplots display the frequencies 
of overlapping points between two variables in order to visually represent data 
point weight or other measurable characteristics of individual data points.</p> 
<p></p> 
<p>See also, Data Reduction.</p> 
<p>Scatterplot, 2D - Multiple. Unlike the regular scatterplot in which one 
variable is represented by the horizontal axis and one by the vertical axis, 
the multiple scatterplot consists of multiple plots and represents multiple 
correlations: one variable (<i>X</i>) is represented by the horizontal axis, 
and several variables (<i>Y</i>'s) are plotted against the vertical axis. A 
different point marker and color is used for each of the multiple<i>Y-variables
</i> and referenced in the legend so that individual plots representing 
different variables can be discriminated in the graph.</p> 
<p></p> 
<p>The <i>Multiple</i> scatterplot is used to compare images of several 
correlations by overlaying them in a single graph that uses one common set of 
scales (e.g., to reveal the underlying structure of factors or dimensions in
Discriminant Function Analysis). See also, Data Reduction.</p> 
<p>Scatterplot, 2D - Regular. The <i>regular</i> scatterplot visualizes a 
relation between two variables<i>X</i> and <i>Y</i> ( e.g., weight and height). 
Individual data points are represented by point markers in two-dimensional 
space, where axes represent the variables. The two coordinates (<i>X</i> and <i>
Y</i>)&nbsp;that determine the location of each point correspond to its 
specific values on the two variables. If the two variables are strongly 
related, then the data points form a systematic shape (e.g., a straight line or 
a clear curve). If the variables are not related, then the points form an 
irregular &quot;cloud&quot; (see the categorized scatterplot below for examples 
of both types of data sets).</p> 
<p></p> 
<p>Fitting functions to scatterplot data helps identify the patterns of 
relations between variables (see example below).</p> 
<p></p> 
<p>For more examples of how scatterplot data helps identify the patterns of 
relations between variables, seeOutliers and Brushing. See also, Data Reduction.
</p> 
<p>Scatterplot, 3D. <i>3D Scatterplots</i> visualize a relationship between 
three or more variables, representing the<i>X, Y</i>, and one or more <i>Z</i> 
(vertical) coordinates of each point in 3-dimensional space (see graph below).
</p> 
<p></p> 
<p>See also, 3D Scatterplot - Custom Ternary Graph, Data Reduction and Data 
Rotation (in 3D space).</p> 
<p>Scatterplot, 3D - Raw Data. An unsmoothed surface (no smoothing function is 
applied) is drawn through the points in the 3D scatterplot. See also,Data 
Reduction.</p> 
<p>Scatterplot, 3D - Ternary Graph. In this type of ternary graph, the 
triangular coordinate systems are used to plot four (or more) variables (the 
components<i>X, Y</i>, and <i>Z</i>, and the responses <i>V1, V2</i>, etc.) in 
three dimensions (ternary 3D scatterplots or surface plots). Here, the 
responses (<i>V1, V2</i>, etc.) associated with the proportions of the 
component variables (<i>X, Y</i>, and <i>Z</i>) in a ternary graph are plotted 
as the heights of the points. See also,Data Reduction.</p> 
<p>Scatterplot Smoothers. In 2D scatterplots, various smoothing methods are 
available to fit a function through the points to best represent (summarize) 
the relationship between the variables.</p> 
<p>Scheffe's Test. This post hoc test can be used to determine the significant 
differences between group means in an analysis of variance setting.<i>Scheffe's 
test</i> is considered to be one of the most conservative post hoc tests (for a 
detailed discussion of different post hoc tests, see Winer, Michels, &amp; 
Brown (1991). For more details, seeGeneral Linear Models. See also, Post Hoc 
Comparisons. For a discussion of statistical significance, see Elementary 
Concepts.</p> 
<p>Score Statistic. This statistic is used to evaluate the statistical 
significance of parameter estimates computed viamaximum likelihood methods. It 
is also sometimes called the<i>efficient score statistic.</i> The test is based 
on the behavior of the log-likelihood function at the point where the 
respective parameter estimate is equal to<i>0.0</i> (zero); specifically, it 
uses the derivative (slope) of the log-likelihood function evaluated at the 
null hypothesis value of the parameter (parameter =<i>0.0</i>). While this test 
is not as accurate as explicit likelihood-ratio test statistics based on the 
ratio of the likelihoods of the model that includes the parameter of interest, 
over the likelihood of the model that does not, its computation is usually much 
faster. It is therefore the preferred method for evaluating the statistical 
significance of parameter estimates in stepwise or best-subset model building 
methods. An alternative statistic is the<i>Wald statistic. </i></p> 
<p>Scree Plot, Scree Test. The eigenvalues for successive factors can be 
displayed in a simple line plot. Cattell (1966) proposed that this<i>scree plot
</i> can be used to graphically determine the optimal number of factors to 
retain.</p> 
<p></p> 
<p>The <i>scree test</i> involves finding the place where the smooth decrease 
of eigenvalues appears to level off to the right of the plot. To the right of 
this point, presumably,&nbsp;we find only &quot;factorial scree&quot; &ndash; 
&quot;scree&quot; is the geological term referring to the debris&nbsp;that 
collects on the lower part of a rocky slope. Thus, no more than the number of 
factors to the left of this point should be retained.</p> 
<p>For more information on procedures for determining the optimal number of 
factors to retain, see the section onReviewing the Results of a Principal 
Components Analysis in Factor Analysis and How Many Dimensions to Specify in 
Multi-dimensional Scaling.</p> 
<p>S.D. Ratio. In a regression problem, the ratio of the prediction error 
standard deviation to the original output data standard deviation. A lower S.D. 
ratio indicates a better prediction. This is equivalent to one minus the 
explained variance of the model. SeeMultiple Regression, Neural Networks.</p> 
<p>Semi-Partial (or Part) Correlation. The semi-partial or part correlation is 
similar to thepartial correlation statistic. Like the, partial correlation, it 
is a measure of the correlation between two variables that remains after 
controlling for (i.e., &quot;partialling&quot; out) the effects of one or more 
other predictor variables. However, while the squared partial correlation 
between a predictor<i>X1</i> and a response variable <i>Y</i> can be 
interpreted as the proportion of (unique) variance accounted for by<i>X1</i>, 
in the presence of other predictors<i>X2</i>, ... , <i>Xk</i>, relative to the 
residual or unexplained variance that cannot be accounted for by<i>X2</i>, ... ,
<i>Xk</i>, the squared semi-partial or part correlation is the proportion of 
(unique) variance accounted for by the predictor<i>X1</i>, relative to the 
total variance of<i>Y</i>. Thus, the semi-partial or part correlation is a 
better indicator of the &quot;practical relevance&quot; of a predictor, because 
it is scaled to (i.e., relative to) the total variability in the dependent 
(response) variable.</p> 
<p>See also Correlation, Spurious Correlations, partial correlation, <i>Basic 
Statistics</i>, <i>Multiple Regression</i>, <i>General Linear Models</i>, <i>
General Stepwise Regression</i>, <i>Structural Equation Modeling (SEPATH)</i>.
</p> 
<p>SEMMA. See Models for Data Mining. See also, Data Mining Techniques.</p> 
<p>Sensitivity Analysis (in Neural Networks). A sensitivity analysis indicates 
which input variables are considered most important by that particular neural 
network. Sensitivity analysis can be used purely for informative purposes, or 
to perform input pruning.</p> 
<p>Sensitivity analysis can give important insights into the usefulness of 
individual variables. It often identifies variables that can be safely ignored 
in subsequent analyses, and key variables that must always be retained. 
However, it must be deployed with some care, for reasons that are explained 
below.</p> 
<p>Input variables are not, in general, independent; that is, there are 
interdependencies between variables. Sensitivity analysis rates variables 
according to the deterioration in modeling performance that occurs if that 
variable is no longer available to the model. In so doing, it assigns a single 
rating value to each variable. However, the interdependence between variables 
means that no scheme of single ratings per variable can ever reflect the 
subtlety of the true situation.</p> 
<p>Consider, for example, the case where two input variables encode the same 
information (they might even be copies of the same variable). A particular 
model might depend wholly on one, wholly on the other, or on some arbitrary 
combination of them. Then sensitivity analysis produces an arbitrary relative 
sensitivity to them. Moreover, if either is eliminated the model may compensate 
adequately because the other still provides the key information. It may 
therefore rate the variables as of low sensitivity, even though they might 
encode key information. Similarly, a variable that encodes relatively 
unimportant information, but is the only variable to do so, may have higher 
sensitivity than any number of variables that mutually encode more important 
information.</p> 
<p>There may be interdependent variables that are useful only if included as a 
set. If the entire set is included in a model, they can be accorded significant 
sensitivity, but this does not reveal the interdependency. Worse, if only part 
of the interdependent set is included, their sensitivity will be zero, as they 
carry no discernable information.</p> 
<p>In summary, sensitivity analysis does not rate the &quot;usefulness&quot; 
of variables in modeling in a reliable or absolute manner.&nbsp;We must be 
cautious in the conclusions we draw about the importance of variables. 
Nonetheless, in practice it is extremely useful. If a number of models are 
studied, it is often possible to identify key variables that are always of high 
sensitivity, others that are always of low sensitivity, and 
&quot;ambiguous&quot; variables that change ratings and probably carry mutually 
redundant information.</p> 
<p>How does sensitivity analysis work? Each input variable is treated in turn 
as if it were &quot;unavailable&quot; (Hunter, 2000). There is a missing value 
substitution procedure, which is used to allow predictions to be made in the 
absence of values for one or more inputs. To define the sensitivity of a 
particular variable,v, we first run the network on a set of test cases, and 
accumulate the network error.&nbsp; We then run the network again using the 
same cases, but this time replacing the observed values ofv with the value 
estimated by the missing value procedure, and again accumulate the network 
error.</p> 
<p>Given that we have effectively removed some information that presumably the 
network uses (i.e. one of its input variables), we would reasonably expect some 
deterioration in error to occur. The basic measure of sensitivity is the ratio 
of the error with missing value substitution to the original error. The more 
sensitive the network is to a particular input, the greater the deterioration 
we can expect, and therefore the greater the ratio.</p> 
<p>If the ratio is one or lower, making the variable &quot;unavailable&quot; 
either has no effect on the performance of the network, or actually enhances 
it. Once sensitivities have been calculated for all variables, they may be 
ranked in order.</p> 
<p>Sequential Contour Plot, 3D. This contour plot presents a 2-dimensional 
projection of the spline-smoothed surface fit to the data (see3D Sequential 
Surface Plot. Successive values of each series are plotted along the <i>X</i>
-axis, with each successive series represented along the<i>Y</i>-axis.</p> 
<p>Sequential/Stacked Plots. In this type of graph, the sequence of values 
from each selected variable is stacked on one another.</p> 
<p></p> 
<p>Sequential/Stacked Plots, 2D - Area. The sequence of values from each 
selected variable will be represented by consecutive areas stacked on one 
another in this type of graph.</p> 
<p>Sequential/Stacked Plots, 2D - Column. The sequence of values from each 
selected variable will be represented by consecutive segments of vertical 
columns stacked on one another in this type of graph.</p> 
<p>Sequential/Stacked Plots, 2D - Lines. The sequence of values from each 
selected variable will be represented by consecutive lines stacked on one 
another in this type of graph.</p> 
<p>Sequential/Stacked Plots, 2D - Mixed Line. In this type of graph, the 
sequences of values of variables selected in the first list will be represented 
by consecutive areas stacked on one another while the sequences of values of 
variables selected in the second list will be represented by consecutive<i>lines
</i> stacked on one another (over the area representing the last variable from 
the first list).</p> 
<p>Sequential/Stacked Plots, 2D - Mixed Step. In this type of graph, the 
sequences of values of variables selected in the first list will be represented 
by consecutive<i>step areas</i> stacked on one another while the sequences of 
values of variables selected in the second list will be represented by 
consecutive<i>step lines</i> stacked on one another (over the step area 
representing the last variable from the first list).</p> 
<p>Sequential/Stacked Plots, 2D - Step. The sequence of values from each 
selected variable will be represented by consecutive step lines stacked on one 
another in this type of graph.</p> 
<p>Sequential/Stacked Plots, 2D - Step Area. The sequence of values from each 
selected variable will be represented by consecutive step areas stacked on one 
another in this type of graph.</p> 
<p>Sequential Surface Plot, 3D. In this sequential plot, a spline-smoothed 
surface is fit to each data point. Successive values of each series are plotted 
along the<i>X</i>-axis, with each successive series represented along the <i>Y
</i>-axis.</p> 
<p>Sets of Samples in Quality Control Charts. While monitoring an ongoing 
process, it often becomes necessary to adjust the center line values or control 
limits, as those values are being refined over time. Also,&nbsp;we may want to 
compute the control limits and center line values from a<i>set of samples</i> 
that are known to be in control, and apply those values to all subsequent 
samples. Thus, each set is defined by a set of computation samples (from which 
various statistics are computed, e.g.,<i>sigma</i>, means, etc.) and a set of 
application samples (to which the respective statistics, etc. are applied). Of 
course, the computation samples and application samples can be (and often are) 
not the same. To reiterate,&nbsp;we may want to estimate<i>sigma</i> from a set 
of samples that are known to be in control (the computation set), and use that 
estimate for establishing control limits for all remaining and new samples (the 
application set).</p> 
<p>Note that each sample must be uniquely assigned to one application set; in 
other words, each sample has control limits based on statistics (e.g.,<i>sigma
</i>) computed for one particular set. The assignment of application samples to 
sets proceeds in a hierarchical manner, i.e., each sample is assigned to the 
first set where it &quot;fits&quot; (where the definition of the application 
sample set would include the respective sample). This hierarchical search 
always begins at the last set that the user specified, and not with the 
all-samples set. Hence, if the user-specified sets encompass all valid samples, 
the default all-samples set will actually become empty (since all samples will 
be assigned to one of the user-defined sets).</p> 
<p>Shapiro-Wilk W Test. The Shapiro-Wilk W test is used in testing for 
normality. If the W statistic is significant, then the hypothesis that the 
respective distribution is normal should be rejected. The Shapiro-Wilk W test 
is the preferred test of normality because of its good power properties as 
compared to a wide range of alternative tests (Shapiro, Wilk, &amp; Chen, 
1968). Some software programs implement an extension to the test described by 
Royston (1982), which allows it to be applied to large samples (with up to 5000 
observations). See alsoKolmogorov-Smirnov test and Lilliefors test.</p> 
<p>Shewhart Control Charts. This is a standard graphical tool widely used in 
statistical Quality Control. The general approach to quality control charting 
is straightforward:&nbsp;We extract samples of a certain size from the ongoing 
production process.&nbsp;We then produce line charts of the variability in 
those samples, and consider their closeness to target specifications. If a 
trend emerges in those lines, or if samples fall outside pre-specified limits, 
then the process is declared to be out of control and the operator will take 
action to find the cause of the problem. These types of charts are sometimes 
also referred to as Shewhart control charts (named after W. A. Shewhart who is 
generally credited as being the first to introduce these methods; see Shewhart, 
1931). For additional information, see alsoQuality Control charts; Assignable 
causes and actions.</p> 
<p>Short Run Control Charts. The short run quality control chart , for short 
production runs, plots transformations of the observations of variables or 
attributes for multiple parts, each of which constitutes a distinct 
&quot;run,&quot; on the same chart. The transformations rescale the variable 
values of interest such that they are of comparable magnitudes across the 
different short production runs (or parts). The control limits computed for 
those transformed values can then be applied to determine if the production 
process is in control, to monitor continuing production, and to establish 
procedures for continuous quality improvement.</p> 
<p>Shuffle, Back Propagation (in Neural Networks). Presenting training cases 
in a random order on eachepoch to prevent various undesirable effects that can 
otherwise occur (such as oscillation and convergence to local minima). See,
Neural Networks.</p> 
<p>Shuffle Data (in Neural Networks). Randomly assigning cases to the training 
and verification sets, so that these are (as far as possible) statistically 
unbiased. See,Neural Networks.</p> 
<p>Sigma Restricted Model. A <i>sigma restricted model </i>uses the 
sigma-restricted coding to represent effects for categorical predictor 
variables in general linear models and generalized linear models. To illustrate 
the sigma-restricted coding, suppose that a categorical predictor variable 
called<em>Gender </em>has two levels (i.e., <em>male </em>and <em>female</em>). 
Cases in the two groups would be assigned values of 1 or -1, respectively, on 
the coded predictor variable, so that if the regression coefficient for the 
variable is positive, the group coded as 1 on the predictor variable will have 
a higher predicted value (i.e., a higher group mean) on the dependent variable, 
and if the regression coefficient is negative, the group coded as -1 on the 
predictor variable will have a higher predicted value on the dependent 
variable. This coding strategy is aptly called the sigma-restricted 
parameterization, because the values used to represent group membership (1 and 
-1) sum to zero. See also,categorical predictor variables, design matrix; or <i>
General Linear Models</i>.</p> 
<p>Sigmoid Function. An S-shaped curve, with a near-linear central response 
and saturating limits. See also,logistic function and hyperbolic tangent 
function.</p> 
<p>Signal Detection Theory (SDT). <i>Signal detection theory </i>(SDT) is an 
application of statistical decision theory used to detect a signal embedded in 
noise. SDT is used in psychophysical studies of detection, recognition, and 
discrimination, and in other areas such as medical research, weather 
forecasting, survey research, and marketing research.</p> 
<p>A general approach to estimating the parameters of the signal detection 
model is via the use of thegeneralized linear model. For example, DeCarlo 
(1998) shows how<i>signal detection </i>models based on different underlying 
distributions can easily be considered by using the generalized linear model 
with differentlink functions.</p> 
<p>For discussion of the generalized linear model and the link functions it 
uses, see the<i>Generalized Linear Models</i> topic.</p> 
<p>Simple Random Sampling (SRS). Simple random sampling is a type of 
probability sampling where observations are randomly selected from a population 
with a known probability orsampling fraction. Typically,&nbsp;we begin with a 
list of<em>N</em> observations that comprises the entire population from 
which&nbsp;we wish to extract a simple random sample (e.g., a list of 
registered voters);&nbsp;we can then generate<em>k</em> random case numbers 
(without replacement) in the range from 1 to<em>N</em>, and select the 
respective cases into the final sample (with a sampling fraction or known 
selection probability of k/N). Refer to Kish (1965) for a detailed discussion 
of the advantages and characteristics of probability samples andEPSEM samples.
</p> 
<p>Simplex Algorithm. A nonlinear estimation algorithm that does not rely on 
the computation or estimation of the derivatives of theloss function. Instead, 
at each iteration the function will be evaluated at m+1 points in the m 
dimensional parameter space. For example, in two dimensions (i.e., when there 
are two parameters to be estimated), the program will evaluate the function at 
three points around the current optimum. These three points would define a 
triangle; in more than two dimensions, the &quot;figure&quot; produced by these 
points is called a<i>Simplex</i>.</p> 
<p>Single and Multiple Censoring. There are situations in which censoring can 
occur at different times (<i>multiple censoring</i>), or only at a particular 
point in time (<i>single censoring</i>). Consider an example experiment where 
we start with 100 light bulbs, and terminate the experiment after a certain 
amount of time. If the experiment is terminated at a particular point in time, 
then a single point of censoring exists, and the data set is said to be<i>
single-censored</i>. However, in biomedical research <i>multiple censoring</i> 
often exists, for example, when patients are discharged from a hospital after 
different amounts (times) of treatment, and the researcher knows that the 
patient survived up to those (differential) points of censoring.</p> 
<p>Data sets with censored observations can be analyzed via Survival Analysis 
orWeibull and Reliability/Failure Time Analysis. See also, Type I and II 
Censoring and Left and Right Censoring.</p> 
<p>Singular Value Decomposition. An efficient algorithm for optimizing a 
linear model. See also, pseudo-inverse.</p> 
<p> Six Sigma (DMAIC). Six Sigma is a well-structured, data-driven methodology 
for eliminating defects, waste, or quality control problems of all kinds in 
manufacturing, service delivery, management, and other business activities.Six 
Sigma methodology is based on the combination of well-established statistical 
quality control techniques, simple and advanced data analysis methods, and the 
systematic training of all personnel at every level in the organization 
involved in the activity or process targeted bySix Sigma.</p> 
<p>Six Sigma methodology and management strategies provide an overall 
framework for organizing company wide quality control efforts. These methods 
have recently become very popular, due to numerous success stories from major 
US-based as well as international corporations. For reviews ofSix Sigma 
strategies, refer to Harry and Schroeder (2000), or Pyzdek (2001).</p> 
<p>These are organized into the categories of activities that make up the Six 
Sigma effort: Define (D), Measure (M), Analyze (A), Improve (I), Control (C); or
DMAIC for short.</p> 
<p>Define. The Define phase is concerned with the definition of project goals 
and boundaries, and the identification of issues that need to be addressed to 
achieve the higher sigma level.</p> 
<p>Measure. The goal of the Measure phase is to gather information about the 
current situation, to obtain baseline data on current process performance, and 
to identify problem areas.</p> 
<p>Analyze. The goal of the Analyze phase is to identify the root cause(s) of 
quality problems, and to confirm those causes using the appropriate data 
analysis tools.</p> 
<p>Improve. The goal of the Improve phase is to implement solutions that 
address the problems (root causes) identified during the previous (Analyze) 
phase.</p> 
<p>Control. The goal of the Control phase is to evaluate and monitor the 
results of the previous phase (Improve).</p> 
<p>Six Sigma Process. A six sigma process is one that can be expected to 
produce only 3.4 defects per one million opportunities. The concept of the six 
sigma process is important inSix Sigma quality improvement programs. The idea 
can best be summarized with the following graphs.</p> 
<p></p> 
<p>The term Six Sigma derives from the goal to achieve a process variation, so 
that &plusmn; 6 * sigma (the estimate of the population standard deviation) 
will &quot;fit&quot; inside the lower and upper specification limits for the 
process. In that case, even if the process mean shifts by 1.5 * sigma in one 
direction (e.g., to +1.5 sigma in the direction of the upper specification 
limit), then the process will still produce very few defects.</p> 
<p>For example, suppose we expressed the area above the upper specification 
limit in terms of one million opportunities to produce defects. The 6 * sigma 
process shifted upwards by 1.5 * sigma will only produce 3.4 defects (i.e., 
&quot;parts&quot; or &quot;cases&quot; greater than the upper specification 
limit) per one million opportunities.</p> 
<p>Shift. An ongoing process that at some point was centered will shift over 
time. Motorola, in their implementation of Six Sigma strategies, determined 
that it is reasonable to assume that a process will shift over time by 
approximately 1.5 * sigma (see, for example, Harry and Schroeder, 2000). Hence, 
most standard Six Sigma calculators will be based on a 1.5 * sigma shift.</p> 
<p>One-sided vs. two-sided limits. In the illustration shown above the area 
outside the upper specification limit (greater than USL) is defined as one 
million opportunities to produce defects. Of course, in many cases any 
&quot;outcomes&quot; (e.g., parts) that are produced that fall below the 
specification limit can be equally defective. In that case,&nbsp;we may want to 
consider the lower tail of the respective (shifted) normal distribution as well.
&nbsp;However, in practice,&nbsp;we usually ignore the lower tail of the normal 
curve because (1) in many cases, the process &quot;naturally&quot; has 
one-sided specification limits (e.g., very low delay times are not really a 
defect, only very long times; very few customer complaints are not a problem, 
only very many, etc.), and (2) when a 6 * sigma process has been achieved, the 
area under the normal curve below the lower specification limit is negligible.
</p> 
<p>Yield. The illustration shown above focuses on the number of defects that a 
process produces. The number of non-defects can be considered the Yield of the 
process. Six Sigma calculators will compute the number of defects per million 
opportunities (DPMO) as well as the yield, expressed as the percent of the area 
under the normal curve that falls below the upper specification limit (in the 
illustration above).</p> 
<p>Skewness. Skewness (this term was first used by Pearson, 1895) measures the 
deviation of the distribution from symmetry. If the skewness is clearly 
different from 0, then that distribution isasymmetrical, while normal 
distributions are perfectlysymmetrical.</p> 
<p>Skewness = n*M3/[(n-1)*(n-2)*3] </p> 
<p>where<br>
M3 &nbsp;&nbsp;&nbsp;&nbsp;is equal to: (xi-Meanx)3<br>
3 
&nbsp;&nbsp;&nbsp;&nbsp;is the standard deviation (sigma) raised to the third 
power<br>
n &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;is the valid number of 
cases.</p> 
<p>See also, Descriptive Statistics Overview.</p> 
<p>Smoothing. <i>Smoothing</i> techniques can be used in two different 
situations. Smoothing techniques for3D Bivariate Histograms allow&nbsp;us to 
fit surfaces to 3D representations of bivariate frequency data. Thus, every 3D 
histogram can be turned into a smoothed surface providing a sensitive method 
for revealing non-salient overall patterns of data and/or identifying patterns 
to use in developing quantitative models of the investigated phenomenon.</p> 
<p></p> 
<p>In Time Series analysis, the general purpose of <i>smoothing</i> techniques 
is to &quot;bring out&quot; the major patterns or trends in a time series, 
while de-emphasizing minor fluctuations (random noise). Visually, as a result 
of smoothing, a jagged line pattern should be transformed into a smooth curve.
</p> 
<p></p> 
<p>See also, Exploratory Data Analysis and Data Mining Techniques and 
Smoothing Bivariate Distributions.</p> 
<p>SOFMs (Self-Organizing Feature Maps; Kohonen Networks). Neural networks 
based on the topological properties of the human brain, also known as Kohonen 
Networks (Kohonen, 1982; Fausett, 1994,; Haykin, 1994; Patterson, 1996).</p> 
<p>Softmax. A specialized activation function for one-of-N encoded 
classification networks. Performs a normalized exponential (i.e. the outputs 
add up to 1). In combination with thecross entropy error function, allows 
multilayer perceptron networks to be modified for class probability estimation 
(Bishop, 1995; Bridle, 1990). See,Neural Networks.</p> 
<p>Space Plots. This type of graph offers a distinctive means of representing 
3D Scatterplot&nbsp;data through the use of a separate <i>X-Y plane</i> 
positioned at a user-selectable level of the vertical<i>Z</i>-axis (which 
&quot;sticks up&quot; through the middle of the plane).</p> 
<p></p> 
<p>The <i>Space Plots</i> specific layout may facilitate exploratory 
examination of specific types of three-dimensional data. It is recommended to 
assign variables to axes such that the variable that is most likely to 
discriminate between patterns of relation among the other two is specified as<i>
Z</i>. See also, Data Rotation (in 3D space) in the Graphical Techniques topic.
</p> 
<p>Spearman R. <em>Spearman R</em> can be thought of as the regular Pearson 
product-moment correlation coefficient (Pearson<i>r</i>); that is, in terms of 
the proportion of variability accounted for, except that Spearman<i>R</i> is 
computed from ranks. As mentioned above, Spearman<i>R</i> assumes that the 
variables under consideration were measured on at least an<i>ordinal</i> (rank 
order) scale; that is, the individual observations (cases) can be ranked into 
two ordered series. Detailed discussions of the Spearman<i>R</i> statistic, its 
power and efficiency can be found in Gibbons (1985), Hays (1981), McNemar 
(1969), Siegel (1956), Siegel and Castellan (1988), Kendall (1948), Olds 
(1949), or Hotelling and Pabst (1936).</p> 
<p>Spectral Plot. The original application of this type of plot was in the 
context ofspectral analysis in order to investigate the behavior of 
non-stationary time series. On the horizontal axes,&nbsp;we can plot the 
frequency of the spectrum against consecutive time intervals, and indicate on 
the Z-axis the spectral densities at each interval (see for example, Shumway, 
1988, page 82).</p> 
<p></p> 
<p>Spectral plots have clear advantages over the regular 3D Scatterplots 
when&nbsp;we are interested in examining how a relationship between two 
variables changes across the levels of a third variable, as is shown in the 
next illustration. The advantage of Spectral Plots over regular 3D Scatterplots 
is well-illustrated in the comparison of the two displays of the same data set 
shown below.</p> 
<p></p> 
<p>The <i>Spectral Plot</i> makes it easier to see that the relationship 
between<i>Pressure</i> and <i>Yield</i> changes from an &quot;inverted U&quot; 
to a &quot;U&quot;. See also,Data Rotation (in 3D space) in Graphical Techniques
.</p> 
<p>Spikes (3D Graphs). In this type of graph, individual values of one or more 
series of data are represented along the<i>X</i>-axis as a series of 
&quot;spikes&quot; (point symbols with lines descending to the base plane). 
Each series to be plotted is spaced along the<i>Y</i>-axis. The 
&quot;height&quot; of each spike is determined by the respective value of each 
series.</p> 
<p>Spline (2D Graphs). A curve is fitted to the XY coordinate data using the 
bicubic spline smoothing procedure.</p> 
<p>Spline (3D Graphs). A surface is fitted to the XYZ coordinate data using 
the bicubic spline smoothing procedure.</p> 
<p>Split Selection (for Classification Trees). <i>Split selection</i> for 
classification trees refers to the process of selecting the splits on the 
predictor variables&nbsp;that are used to predict membership in the classes of 
the dependent variable for the cases or objects in the analysis. Given the 
hierarchical nature of classification trees, these splits are selected one at 
time, starting with the split at the root node, and continuing with splits of 
resulting child nodes until splitting stops, and the child nodes&nbsp;that have 
not been split become terminal nodes. The split selection process is described 
in theComputational Methods section of Classification Trees.</p> 
<p>Spurious Correlations. Correlations that are due mostly to the influences 
of one or more &quot;other&quot; variables. For example, there is a correlation 
between the total amount of losses in a fire and the number of firemen that 
were putting out the fire; however, what this correlation does not indicate is 
that if&nbsp;we call fewer firemen,&nbsp;we would lower the losses. There is a 
third variable (the initial size of the fire) that influences both the amount 
of losses and the number of firemen. If&nbsp;we &quot;control&quot; for this 
variable (e.g., consider only fires of a fixed size), the correlation will 
either disappear or perhaps even change its sign. The main problem with 
spurious correlations is that we typically do not know what the 
&quot;hidden&quot; agent is. However, in cases when we know where to look, we 
can usepartial correlations that control for (i.e., partial out) the influence 
of specified variables. See also,Correlation, Partial Correlation, Basic 
Statistics, Multiple Regression, Structural Equation Modeling (SEPATH).</p> 
<p>SQL. SQL (Structured Query Language) enables&nbsp;us to query an outside 
data source about the data it contains.&nbsp;We can use a SQL statement in 
order to specify the desired tables, fields, rows, etc. to return as data. For 
information onSQL syntax, consult an SQL manual.</p> 
<p>Square Root of the Signal to Noise Ratio (f). This standardized measure of 
effect size is used in the Analysis of Variance to characterize the overall 
level of population effects, and is very similar to theRMSSE. It is the square 
root of the sum of squared standardized effects divided by the number of 
effects. For example, in a 1-Way ANOVA, with J groups, f is calculated as</p> 
<p></p> 
<p>For more information, see Power Analysis.</p> 
<p>Stacked Generalization. See Stacking.</p> 
<p>Stacking (Stacked Generalization). The concept of stacking (short for 
Stacked Generalization) applies to the area ofpredictive data mining, to 
combine the predictions from multiple models. It is particularly useful when 
the types of models included in the project are very different.</p> 
<p>Suppose your data mining project includes tree classifiers, such as C&amp;RT
 and&nbsp; CHAID, linear discriminant analysis (e.g., see GDA), and Neural 
Networks. Each computes predicted classifications for a crossvalidation sample, 
from which overall goodness-of-fit statistics (e.g., misclassification rates) 
can be computed. Experience has shown that combining the predictions from 
multiple methods often yields more accurate predictions than can be derived 
from any one method (e.g., see Witten and Frank, 2000).</p> 
<p>In stacking, the predictions from different classifiers are used as input 
into ameta-learner, which attempts to combine the predictions to create a final 
best predicted classification. So, for example, the predicted classifications 
from the tree classifiers, linear model, and the neural network classifier(s) 
can be used as input variables into a neural network meta-classifier, which 
will attempt to &quot;learn&quot; from the data how to combine the predictions 
from the different models to yield maximum classification accuracy.</p> 
<p>Other methods for combining the prediction from multiple models or methods 
(e.g., from multiple datasets used for learning) areBoosting and Bagging (Voting
).</p> 
<p>Standard Deviation. The standard deviation (this term was first used by 
Pearson, 1894) is a commonly-used measure of variation. The standard deviation 
of a population of values is computed as:</p> 
<p> = [(xi-&micro;)2/N]1/2 </p> 
<p>where<br>
&micro;&nbsp;&nbsp;&nbsp;&nbsp; is the population mean<br>
N
&nbsp;&nbsp;&nbsp; is the population size.<br>
 The sample estimate of the 
population standard deviation is computed as:</p> 
<p>s = [(xi-x-bar)2/n-1]1/2 </p> 
<p>where<br>
xbar&nbsp;&nbsp; is the sample mean<br>
n 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; is the sample size.</p> 
<p>See also, Descriptive Statistics Overview.</p> 
<p>Standard Error. The <i>standard error</i> (this term was first used by 
Yule, 1897) is the standard deviation of a mean and is computed as:</p> 
<p>std.err. = &Ouml;(s2/n)</p> 
<p>where<br>
 s2 is the sample variance<br>
 n is the sample size.</p> 
<p>Standard Error of the Mean. The standard error of the mean (first used by 
Yule, 1897) is the theoretical standard deviation of all sample means of size<i>
n</i> drawn from a population and depends on both the population variance 
(sigma) and the sample size (<i>n</i>) as indicated below:</p> 
<p> = (2/n)1/2 where<br>
2&nbsp;&nbsp;&nbsp;is the population variance and <br>
n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;is the sample size.</p> 
<p>Since the population variance is typically unknown, the best estimate for 
the standard error of the mean is then calculated as:</p> 
<p> = (s2/n)1/2</p> 
<p>where<br>
s2&nbsp;&nbsp;&nbsp; is the sample variance (our best estimate of 
the population variance) and<br>
n&nbsp;&nbsp;&nbsp;&nbsp;is the sample size.
</p> 
<p>See also, Descriptive Statistics Overview.</p> 
<p>Standard Error of the Proportion. This is the standard deviation of the 
distribution of the sample proportion over repeated samples. If the population 
proportion is, and the sample size is N, the standard error of the proportion 
when sampling from an infinite population is</p> 
<p>sp = (p(1-p)/N)**1/2</p> 
<p>For more information, see Power Analysis.</p> 
<p>Standardization.While in the everyday language, the term 
&quot;standardization&quot; means converting to a common standard or making 
something conform to a standard (i.e., its meaning is similar to the term
normalization in data analysis), in statistics, this term has a very specific 
meaning and refers to the transformation of data by subtracting each value from 
some reference value (typically a sample mean) and diving it by the standard 
deviation (typically a sample SD). This important transformation will bring all 
values (regardless of their distributions and original units of measurement) to 
compatible units from a distribution with a mean of 0 and a standard deviation 
of 1.</p> 
<p>This transformation has a wide variety of applications because it makes the 
distributions of values easy to compare across variables and/or subsets. If 
applied to the input data, standardization also makes the results of a variety 
of statistical techniques entirely independent of the ranges of values or the 
units of measurements (see the discussion of these issues inElementary Concepts,
Basic Statistics, Multiple Regression, Factor Analysis, and others).</p> 
<p>Standardized DFFITS. This is another measure of impact of the respective 
case on the regression equation. The formula for<i>standardized DFFITS</i> is
</p> 
<p>SDFITi = DFFITi/(si(i)1/2) where hi is the leverage for the ith case<br>
 
and</p> 
<p>i = 1/N + hi See also, DFFITS, studentized residuals, and studentized 
deleted residuals. For more information see Hocking (1996) and Ryan (1997).</p> 
<p>Standardized Effect (Es). A statistical effect expressed in convenient 
standardized units. For example, the standardized effect in a 2 Sample<i>t</i>
-test is the difference between the two means, divided by the standard 
deviation, i.e.,</p> 
<p>Es = (&micro;1 - &micro;2)/s</p> 
<p>For more information, see Power Analysis.</p> 
<p>Standard Residual Value. This is the standardized residual value (observed 
minus predicted divided by the square root of the residual mean square). See 
also,Mahalanobis distance, deleted residual and Cook&rsquo;s distance.</p> 
<p>Stationary Series (in Time Series). In Time Series analysis, a stationary 
series has a constant mean, variance, and autocorrelation through time (i.e., 
seasonal dependencies have been removed viaDifferencing).</p> 
<p><i>STATISTICA </i>Advanced Linear/Nonlinear Models. StatSoft's <em>
STATISTICA Advanced Linear/Nonlinear Models</em> offers a wide array of the 
most advanced linear and nonlinear modeling tools on the market, supports 
continuous and categorical predictors, interactions, hierarchical models; 
automatic model selection facilities; also, includes variance components, time 
series, and many other methods; all analyses include extensive, interactive 
graphical support and built-in complete Visual Basic scripting.</p> 
<ul> 
<li>General Linear Models</li> 
<li>Generalized Linear/Nonlinear Models</li> 
<li>General Regression Models</li> 
<li>General Partial Least Squares Models</li> 
<li>Variance Components</li> 
<li>Survival Analysis</li> 
<li>Nonlinear Estimation</li> 
<li>Fixed Nonlinear Regression</li> 
<li>Log-Linear Analysis of Frequency Tables</li> 
<li>Time Series/Forecasting</li> 
<li>Structural Equation Modeling, and more</li> </ul> 
<p><i><em>STATISTICA </em></i>Automated Neural Networks. StatSoft's <em>
STATISTICA Automated Neural Networks (SANN)</em> contains a comprehensive array 
of statistics, charting options, network architectures, and training 
algorithms; C and PMML (Predictive Model Markup Language) code generators. The 
C code generator is an add-on.</p> 
<ul> 
<li>Automatic Search for Best Architecture and Network Solutions</li> 
<li>Multilayer Perceptrons</li> 
<li>Radial Basis Function Networks</li> 
<li>Self-Organizing Feature Maps</li> 
<li>Time Series Neural Networks for both Regression and Classification problems
</li> 
<li>A variety of algorithms for fast and efficient training of Neural Network 
Models including Gradient Descent, Conjugate Gradient, and BFGS</li> 
<li>Numerous analytical graphs to aid in generating results and drawing 
conclusions</li> 
<li>Sampling of data into subsets for optimizing network performance and 
enhancing the generalization ability</li> 
<li>Sensitivity Analysis, Lift Charts, and ROC Curves</li> 
<li>Creation of Ensembles out of already existing standalone networks</li> 
<li>C-code and PMML (Predictive Model Markup Language) Neural Network Code 
Generators that are easy to deploy</li> </ul> 
<p><i>STATISTICA </i>Base. StatSoft's <em>STATISTICA</em> Base offers a 
comprehensive set of essential statistics in a user-friendly package with 
flexible output management and Web enablement features; it also includes all<em>
STATISTICA</em> graphics tools and a comprehensive Visual Basic development 
environment.</p> 
<ul> 
<li>All <em>STATISTICA</em> graphics tools</li> 
<li>Basic Statistics, Breakdowns, and Tables</li> 
<li>Distribution Fitting</li> 
<li>Multiple Linear Regression</li> 
<li>Analysis of Variance</li> 
<li>Nonparametrics, and more</li> </ul> 
<p><i>STATISTICA </i>Data Miner. StatSoft's <em>STATISTICA Data Miner</em> 
contains the most comprehensive selection of data mining solutions on the 
market, with an icon-based, extremely easy-to-use user interface. It features a 
selection of completely integrated, and automated, ready to deploy &quot;as 
is&quot; (but also easily customizable) specific data mining solutions for a 
wide variety of business applications. The data mining solutions are driven by 
powerful procedures from five modules, which can also be used interactively 
and/or used to build, test, and deploy new solutions:</p> 
<ul> 
<li>General Slicer/Dicer Explorer (with optional OLAP)</li> 
<li>General Classifier</li> 
<li>General Modeler/Multivariate Explorer</li> 
<li>General Forecaster</li> 
<li>General Neural Networks Explorer, and more</li> </ul> 
<p><i>STATISTICA </i>Data Warehouse. StatSoft's <em>STATISTICA Data Warehouse
</em> is the ultimate high-performance, scalable system for intelligent 
management of unlimited amounts of data, distributed across locations worldwide.
<em>STATISTICA Data Warehouse</em> consists of a suite of powerful, flexible 
component applications, including:</p> 
<ul> 
<li><em>STATISTICA</em> Data Warehouse Server Database</li> 
<li><em>STATISTICA</em> Data Warehouse Query (featuring <em>STATISTICA Query
</em>)</li> 
<li><em>STATISTICA</em> Data Warehouse Analyzer (featuring <em>STATISTICA Data 
Miner</em>, <em>STATISTICA Text Miner</em>, <em>STATISTICA Process Optimization 
and Root Cause Analysis</em>, or the complete set of <em>STATISTICA Enterprise 
Server</em> analytics)</li> 
<li><em>STATISTICA</em> Data Warehouse Reporter (featuring <em>STATISTICA 
Enterprise Server Knowledge Portal</em>)</li> 
<li><em>STATISTICA</em> Data Warehouse Document Repository (featuring <em>
STATISTICA Document Management System</em>)</li> 
<li><em>STATISTICA</em> Data Warehouse Scheduler</li> 
<li><em>STATISTICA</em> Data Warehouse Real Time Monitor and Reporter 
(featuring<em>STATISTICA Enterprise</em> Server)</li> </ul> 
<p><i>STATISTICA </i>Document Management System (SDMS). StatSoft's <em>
STATISTICA Document Management System (SDMS)</em> is a scalable solution for 
flexible, productivity-enhancing management of local or Web-based document 
repositories (FDA/ISO compliant).</p> 
<ul> 
<li>Extremely transparent and easy to use</li> 
<li>Flexible, customizable (optionally browser/Web-enabled) user interface</li>
<li>Electronic signatures</li> 
<li>Comprehensive auditing trails, approvals</li> 
<li>Optimized searches</li> 
<li>Document comparison tools</li> 
<li>Security</li> 
<li>Satisfies the FDA 21 CFR Part 11 requirements</li> 
<li>Satisfies ISO 9000 (9001, 14001) documentation requirements</li> 
<li>Unlimited scalability (from desktop or network Client-Server versions, to 
the ultimate size, Web-based worldwide systems)</li> 
<li>Open architecture and compatibility with industry standards</li> </ul> 
<p><i>STATISTICA </i>Enterprise. StatSoft's <em>STATISTICA Enterprise</em> is 
an integrated multi-user software system designed for general purpose data 
analysis and business intelligence applications in research, marketing, 
finance, and other industries.<i>STATISTICA Enterprise</i> provides an 
efficient interface to enterprise-wide data repositories and a means for 
collaborative work as well as all the statistical functionality available in 
any or all<em>STATISTICA</em> products.</p> 
<ul> 
<li>Integration with data warehouses</li> 
<li>Intuitive query and filtering tools</li> 
<li>Easy-to use administration tools</li> 
<li>Automatic report distribution</li> 
<li>Alarm notification, and more</li> </ul> 
<p><i>STATISTICA </i>Enterprise/QC. StatSoft's <em>STATISTICA Enterprise/QC
</em> is designed for local and global enterprise quality control and 
improvement applications including Six Sigma.<em>STATISTICA Enterprise/QC</em> 
offers a high-performance database (or an optimized interface to existing 
databases), real-time monitoring and alarm notification for the production 
floor, a comprehensive set of analytical tools for engineers, sophisticated 
reporting features for management, Six Sigma reporting options, and much more.
</p> 
<ul> 
<li>Web-enabled user interface and reporting tools; interactive querying tools
</li> 
<li>User-specific interfaces for operators, engineers, etc.</li> 
<li>Groupware functionality for sharing queries, special applications, etc.
</li> 
<li>Open-ended alarm notification including cause/action prompts</li> 
<li>Scalable, customizable, and can be integrated into existing database/ERP 
systems, and more</li> </ul> 
<p><i>STATISTICA Enterprise Server</i>. StatSoft's <em>STATISTICA Enterprise 
Server</em> is the ultimate enterprise system that offers full Web enablement, 
including the ability to run<em>STATISTICA</em> interactively or in batch from 
a Web browser on any computer (including Linux, UNIX), offload time consuming 
tasks to the servers (using distributed processing), use multi-tier 
Client-Server architecture, and manage projects over the Web (supporting 
multithreading and distributed/parallel processing that scales to multiple 
server computers).</p> 
<p><i>STATISTICA </i>Monitoring and Alerting Server (MAS).StatSoft's <em>
STATISTICA Monitoring and Alerting Server (MAS)</em> is a system that enables 
users to automate the continual monitoring of hundreds or thousands of critical 
process and product parameters. The ongoing monitoring is an automated and 
efficient method for:</p> 
<ul> 
<li>Monitoring many critical parameters simultaneously</li> 
<li>Providing status &quot;snapshots&quot; from the results of these 
monitoring activities to personnel based on their responsibilities.</li> 
<li>Dashboards associated with User/Group</li> </ul> 
<p><i>STATISTICA </i>MultiStream. StatSoft's <em>STATISTICA MultiStream</em> 
is a solution package for identifying and implementing effective strategies for 
advanced multivariate process monitoring and control.<i>STATISTICA MultiStream
</i> was designed for process industries in general. <em>MultiStream </em>is 
well suited to help pharmaceutical manufacturers and power generation 
facilities leverage the data collected into their existing specialized process 
data bases for multivariate and predictive process control, for actionable 
advisory systems.</p> 
<p><em>STATISTICA MultiStream</em> is a complete enterprise system built on a 
robust, advanced client-server (and fully Web-enabled) architecture, offers 
central administration and management of deployment of models, as well as 
cutting edge root-cause analysis and predictive data mining technology, and its 
analytics are seamlessly integrated with a built-in document management system.
</p> 
<p><i>STATISTICA </i>Multivariate Statistical Process Control (MSPC).StatSoft's
<em>STATISTICA Multivariate Statistical Process Control (MSPC)</em> is a 
complete solution for multivariate statistical process control, deployed within 
a scalable, secure analytics software platform.</p> 
<ul> 
<li>Univariate and multivariate statistical methods for quality control, 
predictive modeling, and data reduction</li> 
<li>Functions to determine the most critical process, raw materials, and 
environment factors and their optimal settings for delivering products of the 
highest quality</li> 
<li>Monitoring of process characteristics interactively or automatically 
during production stages</li> 
<li>Building, evaluating, and deploying predictive models based on the known 
outcomes from historical data</li> 
<li>Historical analysis, data exploration, data visualization, predictive 
model building and evaluation, model deployment to monitoring server</li> 
<li>Interactive monitoring with dashboard summary displays and 
automatic-updating results</li> 
<li>Automated monitoring with rules, alarm events, and configurable actions
</li> 
<li>Multivariate techniques including Partial Least Squares, Principal 
Components, Neural Networks, Recursive Partitioning (Tree) Methods, Support 
Vector Machines, Independent Components Analysis, Cluster Analysis, and more
</li> </ul> 
<p><i>STATISTICA </i>PI Connector. StatSoft's <em>STATISTICA PI Connector</em> 
is an optional<em>STATISTICA</em> add-on component that allows for direct 
integration to data stored in the PI data historian. The<em>STATISTICA</em> PI 
Connector utilizes the PI user access control and security model, allows for 
interactive browsing of tags, and takes advantages of dedicated PI 
functionality for interpolation and snapshot data.<em>STATISTICA</em> 
integrated with the PI system is being used for streamlined and automated 
analyses for applications such as Process Analytical Technology (PAT) in 
FDA-regulated industries, Advanced Process Control (APC) systems in Chemical 
and Petrochemical industries, and advisory systems for process optimization and 
compliance in the Energy Utility industry.</p> 
<p><i>STATISTICA </i>PowerSolutions. StatSoft's <em>STATISTICA PowerSolutions
</em> is a solution package aimed for use at power generation companies to 
optimize power plant performance, increase efficiency, and reduce emissions. 
This product offers a highly economical alternative to multimillion dollar 
investments in new or upgraded equipment (hardware). Based on more than 20 
years of experience in applying advanced data driven, predictive data 
mining/optimization technologies for process optimization in various industries,
<em>STATISTICA PowerSolutions</em> enables power plants to get the most out of 
their existing equipment and control systems by leveraging all data collected 
at their sites to identify opportunities for improvement, even for older 
designs such as coal-fired Cyclone furnaces (as well as wall-fired or T-fired 
designs).</p> 
<p><i>STATISTICA </i>Process Optimization. StatSoft's <em>STATISTICA Process 
Optimization</em> is a powerful software solution designed to monitor processes 
and identify and anticipate problems related to quality control and improvement 
with unmatched sensitivity and effectiveness.<em>STATISTICA Process Optimization
</em> integrates all Quality Control Charts, Process Capability Analyses, 
Experimental Design procedures, and Six Sigma methods with a comprehensive 
library of cutting-edge techniques for exploratory and predictive data mining.
</p> 
<ul> 
<li>Predict QC problems with cutting edge data mining methods</li> 
<li>Discover root causes of problem areas</li> 
<li>Monitor and improve ROI (Return On Investment)</li> 
<li>Generate suggestions for improvement</li> 
<li>Monitor processes in real time over the Web</li> 
<li>Create and deploy QC/SPC solutions over the Web</li> 
<li>Use multithreading and distributed processing to rapidly process extremely 
large streams of data</li> </ul> 
<p><i>STATISTICA </i>Quality Control Charts. StatSoft's <em>STATISTICA Quality 
Control Charts</em> offers fully customizable (e.g., callable from other 
environments), easy and quick to use, versatile charts with a selection of 
automation options and user-interface shortcuts to simplify routine work (a 
comprehensive tool for Six Sigma methods).</p> 
<ul> 
<li>Multiple Chart (Six Sigma Style) Reports and displays</li> 
<li>X-bar and R Charts; X-bar and S Charts; Np, P, U, C Charts</li> 
<li>Pareto Charts</li> 
<li>Process Capability and Performance Indices</li> 
<li>Moving Average/Range Charts, EWMA Charts</li> 
<li>Short Run Charts (including Nominal and Target)</li> 
<li>CuSum (Cumulative Sum) Charts</li> 
<li>Runs Tests</li> 
<li>Interactive</li> 
<li>Causes and actions, customizable alarms, analytic brushing, and more</li> 
</ul> 
<p><i>STATISTICA </i>Sequence, Association and Link Analysis (SAL).
&nbsp;StatSoft's S<em>TATISTICA Sequence, Association and Link Analysis (SAL)
</em> is designed to address the needs of clients in retailing, banking, 
insurance, etc., industries by implementing the fastest known highly scalable 
algorithm with the ability to drive Association and Sequence rules in one 
single analysis. The program represents a stand-alone module that can be used 
for both model building and deployment. All tools in<em>STATISTICA Data Miner
</em> can be quickly and effortlessly leveraged to analyze and &quot;drill 
into&quot; results generated via<em>STATISTICA SAL</em>.</p> 
<ul> 
<li>Uses a tree-building technique to extract Association and Sequence rules 
from data</li> 
<li>Uses efficient and thread-safe local relational database technology to 
store Association and Sequence models</li> 
<li>Handles multiple response, multiple dichotomy, and continuous variables in 
one analysis</li> 
<li>Performs Sequence Analysis while mining for Association rules in a single 
analysis</li> 
<li>Simultaneously extracts Association and Sequence rules for more than one 
dimension</li> 
<li>Given the ability to perform multidimensional Association and Sequence 
mining and the capacity to extract only rules for specific items, the program 
can be used for Predictive Data Mining</li> 
<li>Performs Hierarchical Single-Linkage Cluster Analysis, which can detect 
the more likely cluster of items that can occur. This has extremely useful, 
practical real-world applications, e.g., in retailing</li> </ul> 
<p><i>STATISTICA </i>Text Miner. StatSoft's <em>STATISTICA Text Miner</em> is 
an optional extension of<em>STATISTICA Data Miner.</em> The program features a 
large selection of text retrieval, pre-processing, and analytic and 
interpretive mining procedures for unstructured text data (including Web 
pages), with numerous options for converting text into numeric information (for 
mapping, clustering, predictive data mining, etc.), language-specific stemming 
algorithms. Because<em>STATISTICA</em>&rsquo;s flexible data import options, 
the methods available in<em>STATISTICA Text Miner</em> can also be useful for 
processing other unstructured input (e.g., image files imported as data 
matrices, etc.).</p> 
<p><i>STATISTICA </i>Variance Estimation and Precision. StatSoft's <em>
STATISTICA Variance Estimation and Precision</em>offers a comprehensive set of 
techniques for analyzing data from experiments that include both fixed and 
random effects using REML (Restricted Maximum Likelihood Estimation). With<em>
STATISTICA Variance Estimation and Precision</em>, you can obtain estimates of 
variance components and use them to make precision statements while at the same 
time comparing fixed effects in the presence of multiple sources of variation.
</p> 
<ul> 
<li>Variability plots</li> 
<li>Multiple plot layouts to allow direct comparison of multiple dependent 
variables</li> 
<li>Expected mean squares and variance components with confidence intervals
</li> 
<li>Flexible handling of multiple dependent variables</li> 
<li>Graph displays of variance components</li> </ul> 
<p>Statistical Power. The probability of rejecting a false statistical null 
hypothesis. For more information, seePower Analysis.</p> 
<p>Statistical Process Control (SPC).The term <i>Statistical Process Control
</i> (<i>SPC</i>) is typically used in context of manufacturing processes 
(although it may also pertain to services and other activities), and it denotes 
statistical methods used to monitor and improve the quality of the respective 
operations. By gathering information about the various stages of the process 
and performing statistical analysis on that information, the<i>SPC</i> engineer 
is able to take necessary action (often preventive) to ensure that the overall 
process stays in-control and to allow the product to meet all desired 
specifications.</p> 
<p><i>SPC</i> involves monitoring processes, identifying problem areas, 
recommending methods to reduce variation and verifying that they work, 
optimizing the process, assessing the reliability of parts, and other analytic 
operations.<i>SPC</i> uses such basic statistical quality control methods as 
quality control charts (Sheward, Pareto, and others), capability analysis, gage 
repeatability/reproducibility analysis, and reliability analysis. However, also 
specialized experimental methods (DOE) and other advanced statistical 
techniques are often part of global<i>SPC</i> systems. Important components of 
effective, modern SPC systems are real-time access to data and facilities to 
document and respond to incoming QC data on-line, efficient central QCdata 
warehousing, and groupware facilities allowing QC engineers to share data and 
reports (see also,Enterprise SPC).</p> 
<p>See also, Quality Control and Process Analysis<i>. </i>For more information 
on process control systems, see the ASQC/AIAG's<i>Fundamental statistical 
process control reference manual</i> (1991).</p> 
<p>Statistical Significance (<em>p</em>-value). The statistical significance 
of a result is an estimated measure of the degree to which it is 
&quot;true&quot; (in the sense of &quot;representative of the 
population&quot;). More technically, the value of the<i>p-</i>value represents 
a decreasing index of the reliability of a result. The higher the<i>p-</i>
value, the less we can believe that the observed relation between variables in 
the sample is a reliable indicator of the relation between the respective 
variables in the population. Specifically, the<em>p</em>-value represents the 
probability of error that is involved in accepting our observed result as 
valid, that is, as &quot;representative of the population.&quot; For example, 
the<i>p-</i>value of .05 (i.e.,1/20) indicates that there is a 5% probability 
that the relation between the variables found in our sample is a 
&quot;fluke.&quot; In other words, assuming that in the population there was no 
relation between those variables whatsoever, and we were repeating experiments 
like ours one after another, we could expect that approximately in every 20 
replications of the experiment there would be one in which the relation between 
the variables in question would be equal or stronger than in ours. In many 
areas of research, the<i>p-</i>value of .05 is customarily treated as a 
&quot;border-line acceptable&quot; error level. See also,Elementary Concepts.
</p> 
<p>Steepest Descent Iterations. When initial values for the parameters are far 
from the ultimate minimum, the approximate Hessian used in the Gauss-Newton 
procedure may fail to yield a proper step direction during iteration. In this 
case, the program may iterate into a region of the parameter space from which 
recovery (i.e., successful iteration to the true minimum point) is not 
possible. One option offered byStructural Equation Modeling is to precede the 
Gauss-Newton procedure with a few iterations utilizing the &quot;method of 
steepest descent.&quot; In the steepest descent approach, values of the 
parameter vector q on each iteration are obtained as</p> 
<p>k+1 = k + kgk </p> 
<p>In simple terms, what this means is that the Hessian is not used to help 
find the direction for the next step. Instead, only the first derivative 
information in the gradient is used.</p> 
<p><b>Hint for beginners.</b> Inserting a few <i>Steepest Descent Iterations
</i> may help in situations where the iterative routine &quot;gets lost&quot; 
after only a few iterations.</p> 
<p>Stemming. An important pre-processing step before indexing input documents 
fortext mining is the <i>stemming</i> of words. The term stemming refers to the 
reduction of words to their roots so that, for example, different grammatical 
forms or declinations of verbs are identified and indexed (counted) as the same 
word. For example, stemming will ensure that both &quot;travel&quot; and 
&quot;traveled&quot; will be recognized by the program as the same word. For 
more information, see Manning and Sch&uuml;tze (2002).</p> 
<p>Steps. Repetitions of a particular analytic or computational operation or 
procedure. For example in theneural network time series analysis, the number of 
consecutive time steps from which input variable values should be drawn to be 
fed into the neural network input units.</p> 
<p>Stepwise Regression. A model-building technique&nbsp;that finds subsets of 
predictor variables that most adequately predict responses on a dependent 
variable by linear (or nonlinear) regression, given the specified criteria for 
adequacy of model fit.</p> 
<p>For an overview of <i>stepwise regression </i>and <i>model fit</i> criteria 
see<i>General Stepwise Regression</i> or <i>Multiple Regression</i>; for 
nonlinear stepwise and best subset regression, see <i>Generalized Linear Models
</i>.</p> 
<p>Stiffness Parameter (in Fitting Options). The function that controls the 
weight is determined by theStiffness parameter, which can be modified. Thus, the
stiffness parameter determines the degree to which the fitted curve depends on 
local configurations of the analyzed values.</p> 
<p></p> 
<p>The lower the coefficient, the more the shape of the curve is influenced by 
individual data points (i.e., the curve &quot;bends&quot; more to accommodate 
individual values and subsets of values). The range of the stiffness parameters 
is 0 &lt; s &lt; 1. Large values of the parameter produce smoother curves that 
adequately represent the overall pattern in the data set at the expense of 
local details. See also, McLain, 1974.</p> 
<p>Stopping Conditions. During an iterative process (e.g., fitting, searching, 
training), the conditions&nbsp;that must be true for the process to stop. (For 
example, inneural networks, the stopping conditions include the maximum number 
ofepochs, target error performance and the minimum error improvement thresholds.
</p> 
<p>Stopping Conditions (in Neural Networks). The iterative gradient-descent 
training algorithms (back propagation, Quasi-Newton, conjugate gradient descent,
Levenberg-Marquardt, quick propagation, Delta-bar-Delta, and Kohonen) all 
attempt to reduce the training error on each epoch.</p> 
<p>We&nbsp;specify a maximum number of epochs for these iterative algorithms. 
However,&nbsp;we can also define stopping conditions that may cause training to 
determine earlier.</p> 
<p>Specifically, training may be stopped when:</p> 
<ul> 
<li> 
<p>the error drops below a given level;</p> </li> 
<li> 
<p>the error fails to improve by a given amount over a given number of epochs.
</p> </li> </ul> 
<p>The conditions are cumulative; i.e., if several stopping conditions are 
specified, training ceases when any one of them is satisfied. In particular, a 
maximum number of epochs must always be specified. The error-based stopping 
conditions can also be specified independently for the error on the training 
set and the error on the selection set (if any).</p> 
<p>Target Error.&nbsp;We can specify a target error level, for the training 
subset, the selection subset, or both. If the RMS falls below this level, 
training ceases.</p> 
<p>Minimum Improvement. Specifies that the RMS error on the training subset, 
the selection subset, or both must improve by at least this amount, or training 
will cease (if the Window parameter is non-zero).</p> 
<p>Sometimes error improvement may slow down for a while or even rise 
temporarily (particularly if the shuffle option is used with back propagation, 
or non-zero noise is specified, as these both introduce an element of noise 
into the training process). To prevent this option from aborting the run 
prematurely, specify a longer Window. It is particularly recommended to monitor 
the selection error for minimum improvement, as this helps to prevent 
over-learning.</p> 
<p>Specify a negative improvement threshold if you want to stop training only 
when a significant deterioration in the error is detected. The algorithm will 
stop when a number of generations pass during which the error is always the 
given amount worse than the best it ever achieved.</p> 
<p>Window. The window factor is the number of epochs across which the error 
must fail to improve by the specified amount, before the algorithm is deemed to 
have slowed down too much and is stopped. By default the window is zero, which 
means that the minimum improvement stopping condition is not used at all.</p> 
<p>Stopping Rule (in Classification Trees). The <i>stopping rule</i> for a 
classification tree refers to the criteria that are used for determining the 
&quot;right-sized&quot; classification tree, that is, a classification tree 
with an appropriate number of splits and optimal predictive accuracy. The 
process of determining the &quot;right-sized&quot; classification tree is 
described in theComputational Methods section of Classification Trees.</p> 
<p>Stratified Random Sampling. In general, random sampling is the process of 
randomly selecting observations from a population, to create a subsample that 
&quot;represents&quot; the observations in that population (see Kish, 1965; see 
alsoProbability Sampling, Simple Random Sampling, EPSEM Samples; see also 
Representative Sample for a brief exploration of this often misunderstood 
notion). In stratified sampling,&nbsp;we usually apply specific (identical or 
different) sampling fractions to different groups (strata) in the population to 
draw the sample.</p> 
<p><b>Over-sampling particular strata to over-represent rare events. </b>In 
somepredictive data mining applications, it is often necessary to apply 
stratified sampling to systematically over-sample (apply a greater sampling 
fraction) to particular &quot;rare events&quot; of interest. For example, in 
catalog retailing the response rate to particular catalog offers can be below 
1%, and when analyzing historical data (from prior campaigns) to build a model 
for targeting potential customers more successfully, it is desirable to 
over-sample past respondents (i.e., the &quot;rare&quot; respondents who 
ordered from the catalog);&nbsp;we can then apply the various model building 
techniques for classification (seeData Mining) to a sample consisting of 
approximately 50% responders and 50% non-responders. Otherwise, if&nbsp;we were 
to draw a simple random sample for the analysis (with 1% of responders), then 
practically all model building techniques would likely predict a simple 
&quot;no-response&quot; for all cases and would be (trivially) correct in 99% 
of the cases.</p> 
<p>Stub and Banner Tables (Banner Tables). Stub-and-banner tables are 
essentially two-way tables, except that two lists of categorical variables 
(instead of just two individual variables) are crosstabulated. In the<i>
Stub-and-banner table</i>, one list will be tabulated in the columns 
(horizontally) and the second list will be tabulated in the rows (vertically) 
of the Scrollsheet. For more information, see theStub and Banner Tables section 
ofBasic Statistics.</p> 
<p>Studentized Deleted Residuals. In addition to standardized residuals 
several methods (includingstudentized residuals, studentized deleted residuals, 
DFFITS, and standardized DFFITS) are available for detecting outlying values 
(observations with extreme values on the set of predictor variables or the 
dependent variable). The formula for studentized deleted residuals is given by
</p> 
<p>SDRESIDi = DRESIDi/ s(i) </p> 
<p>for</p> 
<p>DRESID = ei/(1-i ) </p> 
<p>and where</p> 
<p>s(i) = 1/(C-p-1)1/2 * ((C-p)s2/1-hi) - DRESIDi2)1/2 ei
&nbsp;&nbsp;&nbsp;&nbsp;is the error for the ith case<br>
hi
&nbsp;&nbsp;&nbsp;&nbsp;is the leverage for the ith case<br>
p
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;is the number of coefficients in the model</p> 
<p>and</p> 
<p>i = 1/N + hi For more information see Hocking (1996) and Ryan (1997).</p> 
<p>Studentized Residuals. In addition to standardized residuals several 
methods (including studentized residuals,studentized deleted residuals, DFFITS, 
andstandardized DFFITS) are available for detecting outlying values 
(observations with extreme values on the set of predictor variables or the 
dependent variable). The formula for<i>studentized residuals</i> is</p> 
<p>SRESi = (ei/s)/(1-i)1/2 where<br>
ei&nbsp;&nbsp;&nbsp;&nbsp;is the error 
for the ith case<br>
hi&nbsp;&nbsp;&nbsp;&nbsp;is the leverage for the ith case
</p> 
<p>and i = 1/N + hi For more information see Hocking (1996) and Ryan (1997).
</p> 
<p>Student's <i>t</i> Distribution. The Student's t distribution has density 
function (for = 1, 2, ...):</p> 
<p><br>
 where<br>
&nbsp;&nbsp;&nbsp;&nbsp; is the degrees of freedom<br>

&nbsp;&nbsp;&nbsp; (gamma) is the Gamma function<br>
&nbsp;&nbsp;&nbsp; is the 
constant Pi (3.14...)</p> 
<p><br>
 &nbsp;</p> 
<p>The animation above shows various tail areas (p-values) for a Student's t 
distribution with 15 degrees of freedom.</p> 
<p>Sum-Squared Error Function. An error function composed by squaring the 
difference between sets of target and actual values, and adding these together. 
See also,loss function.</p> 
<p>Supervised and Unsupervised Learning. An important distinction in machine 
learning, and also applicable to data mining, is that between supervised and 
unsupervised learning algorithms. The term &quot;supervised&quot; learning is 
usually applied to cases in which a particular classification is already 
observed and recorded in a training sample, and you want to build a model to 
predict those classifications (in a new testing sample). For example, you may 
have a data set that contains information about who from among a list of 
customers targeted for a special promotion responded to that offer. The purpose 
of the classification analysis would be to build a model to predict who (from a 
different list of new potential customers) is likely to respond to the same (or 
a similar) offer in the future. You may want to review the methods discussed in
General Classification and Regression Trees (GC&amp;RT), General CHAID Models 
(GCHAID), Discriminant Function Analysis and General Discriminant Analysis (GDA)
,MARSplines (Multivariate Adaptive Regression Splines), and neural networks to 
learn about different techniques that can be used to build or fit models to 
data where the outcome variable of interest (e.g., customer did or did not 
respond to an offer) was observed. These methods are called supervised learning 
algorithms because the learning (fitting of models) is &quot;guided&quot; or 
&quot;supervised&quot; by the observed classifications recorded in the data 
file.</p> 
<p>In unsupervised learning, the situation is different. Here the outcome 
variable of interest is not (and perhaps cannot be) directly observed. Instead, 
we want to detect some &quot;structure&quot; or clusters in the data that may 
not be trivially observable. For example, you may have a database of customers 
with various demographic indicators and variables potentially relevant to 
future purchasing behavior. Your goal would be to find market segments, i.e., 
groups of observations that are relatively similar to each other on certain 
variables; once identified, you could then determine how best to reach one or 
more clusters by providing certain goods or services you think may have some 
special utility or appeal to individuals in that segment (cluster). This type 
of task calls for an unsupervised learning algorithm, because learning (fitting 
of models) in this case cannot be guided by previously known classifications. 
Only after identifying certain clusters can you begin to assign labels, for 
example, based on subsequent research (e.g., after identifying one group of 
customers as &quot;young risk takers&quot;).</p> 
<p>There are several methods available for unsupervised learning, including 
Principal Components and Classification Analysis, Factor Analysis, 
Multidimensional Scaling, Correspondence Analysis, Neural Networks, 
Self-Organizing Feature Maps (SOFM, Kohonen networks);particularly powerful 
algorithms for pattern recognition and clustering are the<i>EM</i> and <i>k</i>
-Means clustering algorithms.</p> 
<p>Support Value (Association Rules). When applying (in data or text mining) 
algorithms for derivingassociation rules of the general form <i>If Body then 
Head</i> (e.g., <i>If (Car=Porsche and Age&lt;20) then (Risk=High and 
Insurance=High</i>)), the Support value is computed as the joint probability 
(relative frequency of co-occurrence) of the<i>Body</i> and <i>Head</i> of each 
association rule.</p> 
<p>Support Vector. A set of points in the feature space that determines the 
boundary between objects of different class memberships.</p> 
<p>Support Vector Machine (SVM) Support Vector Machine (SVM) A classification 
method based on the maximum margin hyperplane.</p> 
<p>Suppressor Variable. A <i>suppressor variable</i> (in Multiple Regression ) 
has zero (or close to zero) correlation with the criterion but is correlated 
with one or more of the predictor variables, and therefore, it will suppress 
irrelevant variance of independent variables. For example, you are trying to 
predict the times of runners in a 40 meter dash. Your predictors are<i>Height
</i> and <i>Weight</i> of the runner. Now, assume that <i>Height</i> is not 
correlated with<i>Time</i>, but <i>Weight</i> is. Also assume that <i>Weight</i>
 and<i>Height</i> are correlated. If <i>Height</i> is a suppressor variable, 
then it will suppress, or control for, irrelevant variance (i.e., variance that 
is shared with the predictor and not the criterion), thus increasing the 
partial correlation. This can be viewed as ridding the analysis of noise.</p> 
<p>Let <i>t</i> = Time, <i>h</i> = Height, <i>w</i> - Weight, rth = 0.0, rtw = 
0.5, and rhw = 0.6.</p> 
<p>Weight in this instance accounts for 25% (Rtw**2 = 0.5**2) of the 
variability of Time. However, if Height is included in the model, then an 
additional 14% of the variability of Time is accounted for even though Height 
is not correlated with Time (see below):</p> 
<p>Rt.hw**2 = 0.5**2/(1 - 0.6**2) = 0.39 </p> 
<p>For more information, please refer to Pedhazur, 1982.</p> 
<p>Surface Plot (from Raw Data). This sequential plot fits a spline-smoothed 
surface to each data point. Successive values of each series are plotted along 
the X-axis, with each successive series represented along the Y-axis.</p> 
<p>Survival Analysis. <i>Survival analysis</i> (exploratory and hypothesis 
testing) techniques include descriptive methods for estimating the distribution 
of survival times from a sample, methods for comparing survival in two or more 
groups, and techniques for fitting linear or non-linear regression models to 
survival data. A defining characteristic of survival time data is that they 
usually include so-calledcensored observations, e.g., observations that 
&quot;survived&quot; to a certain point in time, and then dropped out from the 
study (e.g., patients who are discharged from a hospital). Instead of 
discarding such observations from the data analysis all together (i.e., 
unnecessarily loose potentially useful information) survival analysis 
techniques can accommodate censored observations, and &quot;use&quot; them in 
statistical significance testing and model fitting.</p> 
<p>Typical <i>survival analysis</i> methods include <i>life table</i>, <i>
survival distribution</i>, and <i>Kaplan-Meier</i> survival function 
estimation, and additional techniques for comparing the survival in two or more 
groups. Finally,<i>Survival analysis</i> includes the use of regression models 
for estimating the relationship of (multiple) continuous variables to survival 
times. For more information, seeSurvival Analysis.</p> 
<p>Survivorship Function. The <i>survivorship function</i> (commonly denoted as
<i>R(t)</i>) is the complement to the cumulative distribution function (i.e., 
<i>R(t)=1-F(t)</i>); the <i>survivorship function</i> is also referred to as 
the reliability or survival function (since it describes the probability of not 
failing or of surviving until a certain time<i>t</i>; e.g., see Lee, 1992).</p> 
<p>For additional information see Survival Analysis or the Weibull and 
Reliability/Failure Time Analysis section&nbsp;of Process Analysis.</p> 
<p>Sweeping. The sweeping transformation of matrices is commonly used to 
efficiently perform stepwise multiple regression (see Dempster, 1969, Jennrich, 
1977) or similar analyses; a modified version of this transformation is also 
used to compute theg2 generalized inverse<i>. </i>The forward sweeping 
transformation for a column k can be summarized in the following four steps 
(where the e's refer to the elements of a symmetric matrix):</p> 
<ol> 
<li>eij = eij - ejk * ekj / ekk for i&lt;&gt;k, j&lt;&gt;k </li> 
<li>ekj = ekj / ekk</li> 
<li>eik = eik / ekk</li> 
<li>ekk = -1 / ekk</li> </ol> 
<p>The reverse sweeping operation reverses the changes effected by these 
transformations. The sweeping operator is used extensively inGeneral Linear 
Models, Multiple Regression, and similar techniques.</p> 
<p>Symmetrical Distribution. If you split the distribution in half at its mean 
(or median), then the distribution of values would be a &quot;mirror 
image&quot; about this central point. See also,Descriptive Statistics Overview.
</p> 
<p>Symmetric Matrix. A matrix is symmetric if the transpose of the matrix is 
itself (i.e., A = A'). In other words, the lower triangle of the square matrix 
is a &quot;mirror image&quot; of the upper triangle with 1's on the diagonal 
(see below).</p> |1 2 3 4|<br>
 |2 1 5 6|<br>
 |3 5 1 7|<br>
 |4 6 7 1| <br>

<br> 
<p>Synaptic Functions (in Neural Networks).</p> 
<p>Dot product. Dot product units perform a weighted sum of their inputs, 
minus the threshold value. In vector terminology, this is the dot product of 
the weight vector with the input vector, plus a bias value. Dot product units 
have equal output values along hyperplanes in pattern space. They attempt to 
perform classification by dividing pattern space into sections using 
intersecting hyperplanes.</p> 
<p>Radial. Radial units calculate the square of the distance between the two 
points in<i>N</i> dimensional space (where <i>N</i> is the number of inputs) 
represented by the input pattern vector and the unit's weight vector. Radial 
units have equal output values lying on hyperspheres in pattern space. They 
attempt to perform classification by measuring the distance of normalized cases 
from exemplar points in pattern space (the exemplars being stored by the 
units). The squared distance is multiplied by the threshold (which is, 
therefore, actually a deviation value in radial units) to produce the post 
synaptic value of the unit (which is then passed to the unit's activation 
function).</p> 
<p>Dot product units are used in multilayer perceptron and linear networks, 
and in the final layers ofradial basis function, PNN, and GRNN networks.</p> 
<p>Radial units are used in the second layer of Kohonen, radial basis function,
Clustering, and probabilistic and generalized regression networks. &nbsp; They 
are not used in any other layers of any standard network architecture.</p> 
<p>Division. This is specially designed for use in generalized regression 
networks, and should not be employed elsewhere. It expects one incoming weight 
to equal +1, one to equal -1, and the others to equal zero. The post-synaptic 
value is the +1 input divided by the -1 input.</p> 
<p>&nbsp;</p> <br>
<br>

</body>
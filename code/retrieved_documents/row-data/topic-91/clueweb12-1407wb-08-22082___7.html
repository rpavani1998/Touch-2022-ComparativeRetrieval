<!doctype html>
<meta charset="utf-8">
<title>Data Mining Survivor: Contents - Random Forests</title>
<body>
<b>DATA MINING</b> <br>
<b>Desktop Survival Guide</b> <br>
by <i>Graham Williams
</i> <br>
Desktop Survival Project Home Introduction Getting Started The 
Business Problem Data Loading Data Exploring Data Interactive Graphics 
Statistical Tests Models Network Analysis Text Mining Decision Trees Random 
Forests Boosting Bagging Support Vector Machine Linear Regression Neural Network
Naive Bayes Survival Analysis Evaluation and Deployment Transforming Data 
Deployment Troubleshooting Issues Moving into R R Getting Help Data Graphics in 
R Understanding Data Preparing Data Issues Evaluating Models Reporting Fraud 
Analysis Archetype Analysis Algorithms Bayes Classifier K-Nearest Neighbours 
Linear Models Open Products AlphaMiner Borgelt Data Mining Suite KNime R Rattle 
Weka Closed Products Clementine Equbits Foresight GhostMiner InductionEngine ODM
Enterprise Miner Statistica Data Miner TreeNet Virtual Predict Installing Rattle
Projects Bibliography Index Preface Goals Organisation Features Audience 
Typographical Conventions A Note on Languages Currency Acknowledgements 
Introduction Data Mining The Business Problem Types of Analysis Data Mining 
Applications A Framework for Modelling Agile Data Mining R Rattle Why R and 
Rattle? Data Preparation Number of Algorithms Repeatability Performance Open 
Source Data Mining Business Case Sample Business Case Pros and Cons Books on R 
Getting Started Initial Interaction with R Quitting Rattle and R First Contact 
Loading a Dataset Building a Model Understanding Our Data Evaluating the Model 
Evaluating the Model Interacting with R Interacting with Rattle Projects Toolbar
Menus Interacting with Plots Keyboard Navigation Summary Command Summary The 
Business Problem Solar Panel Efficiency Water Collection Others Other Business 
Problems Fraud Detection Loan Approval Documenting the Business Problem Summary 
Resources Exercises Data Data Nomenclature Loading Data into Rattle CSV Data 
Datasets Reading Direct from URL Play Golf Weather Data Other Data Sources ARFF 
Data ODBC Sourced Data Setting Up a Data Source Name Netezza Setup Teradata 
Setup R Data R Dataset Data Entry Data Tab Options in Rattle Sampling Data 
Variable Roles Automatic Role Identification Weights Calculator Manipulating 
Data Loading Data CSV Data Locating and Loading Data Loading the File CSV 
Options Basic Data Summary ARFF Data ODBC Sourced Data R Dataset R Data Library 
Data Options Sampling Data Variable Roles Automatic Role Identification Weights 
Calculator Command Summary Exploring Data Summarising Data Summary Describe 
Basics Kurtosis Skewness Missing Exploring Distributions Box Plot Histogram 
Cumulative Distribution Plot Benford's Law Other Digits Stratified Benford Plots
Bar Plot Dot Plot Mosaic Plot GGobi Scatterplot Data Viewer Brushing Identify 
Multivariate Outliers Other Options Quality Plots Using R Further GGobi 
Documentation Correlation Analysis Hierarchical Correlation Principal Components
Single Variable Overviews Interactive Graphics Interactive Visualisations 
Latticist GGobi Scatterplot Multiple Plots Brushing Other Plots Data Viewer 
Brushing Identify Multivariate Outliers Other Options Quality Plots Using R 
Further GGobi Documentation Documenting Interactive Explorations Code Review 
Chapter Exercises Command Summary Statistical Tests Documenting Interactive 
Explorations Code Review Further Resources Chapter Exercises Command Summary 
Models A Framework for Modelling Descriptive Analytics Predictive Analytics 
Documenting Models Summary Code Review Exercises Further Resources Command 
Summary Cluster Analysis Summary Clusters Basic Clustering Hot Spots 
Alternative Clustering Other Cluster Examples KMeans Export KMeans Clusters 
Discriminant Coordinates Plot Number of Clusters Hierarchical Clusters Other 
Cluster Algorithms Association Analysis Summary Overview Algorithm Usage Read 
Transactions file format sep cols rm.duplicates Summary Apriori data parameter 
appearance control Inspect Examples Video Marketing Survey Data Other Examples 
Resources and Further Reading Basket Analysis General Rules Network Analysis 
Documenting Interactive Explorations Code Review Chapter Exercises Command 
Summary Text Mining Application to Text Text Mining with R Decision Trees 
Knowledge Representation Search Heuristic Measures Tutorial Example Rattle R 
Tuning Parameters Min Split (Rarg[]minsplit) Min Bucket (minbucket) Priors 
(prior) Loss Matrix Complexity (cp) Other Options Simple Example Convert Tree 
to Rules Predicting Salary Group Issues Summary Code Review Iris Wine Exercises 
Resources Command Summary Random Forests Formalities Tutorial Example Tuning 
Parameters Number of Trees Sample Size Number of Variables Summary Overview 
Algorithm Usage Random Forest importance classwt Examples Resources and Further 
Reading Summary Overview Example Algorithm Resources and Further Reading 
Boosting Formalities Tutorial Example Tuning Parameters Summary Overview 
AdaBoost Algorithm Examples Step by Step Using gbm Extensions and Variations 
Alternating Decision Tree Resources and Further Reading Documenting Code Review 
Further Resources Chapter Exercises Command Summary Bootstrapping Summary Usage 
Further Information Summary Overview Example Algorithm Resources and Further 
Reading Bagging Support Vector Machine Formalities Tutorial Example Tuning 
parameters Examples Resources and Further Reading Overview Examples Resources 
and Further Reading Linear Regression Linear Regression Formalities Tutorial 
Example Tuning parameters Generalized Regression Formalities Tutorial Example 
Tuning parameters Logistic Regression Formalities Tutorial Example Tuning 
parameters Discussion Probit Regression Formalities Tutorial Example Tuning 
Parameters Multinomial Regression Formalities Tutorial Example Tuning Parameters
Neural Network Formalities Tutorial Example Tuning parameters Documenting Code 
Review Further Resources Chapter Exercises Command Summary Naive Bayes Summary 
Code Review Resources Exercises Command Summary Survival Analysis Sample Data 
Simple Lung Descriptive Analysis Regression survreg Simple Lung coxph Simple 
Lung Apply to New Data More Input Variables Decision Tree Example from Singer 
and Willett Other Approaches Design Package Random Survival Forests Prediction 
on Test Data Evaluation The Evaluate Tab Confusion Matrix Measures Graphical 
Measures Risk Charts Cost Curves Lift ROC Curves Area Under Curve Precision 
versus Recall Sensitivity versus Specificity Predicted versus Observed Scoring 
Documenting Interactive Explorations Code Review Chapter Exercises Command 
Summary Transforming Data Rescale Data Recenter Scale [0,1] Rank Median/MAD 
Peer Relativity Profiling Index Impute Zero/Missing Mean/Median/Mode Constant 
Remap Binning Indicator Variables Join Categorics Math Transforms Outliers 
Cleanup Delete Ignored Delete Selected Delete Missing Delete Obs with Missing 
Other Transformations Removing Duplicates Command Summary Deployment 
Documenting Deployment Code Review Chapter Exercises Command Summary 
Troubleshooting Cairo A factor has new levels Issues Model Selection Overfitting
Imbalanced Classification Sampling Cost Based Learning Model Deployment and 
Interoperability SQL PMML XML for Data Bibliographic Notes Documenting Code 
Review Chapter Exercises Command Summary Moving into R Interacting with R Basic 
Command Line Windows, Icons, Mouse, Pointer--WIMP The Current Rattle State 
Samples Projects The Rattle Log Further Tuning Models Emacs and ESS Documenting 
Code Review Chapter Exercises Command Summary R Evaluation Exercises Assignment 
Libraries and Packages Searching for Objects Package Management Information 
About a Package Testing Package Availability Packages and Namespaces Basic 
Programming in R Principles Folders and Files Flow Control If Statement For Loop
Functions Apply Methods Objects System Running System Commands System Parameters
Misc Internet Memory Management Memory Usage Garbage Collection Errors Frivolous
Sudoku Further Resources Using R Specific Purposes Survey Analysis Getting Help 
R Documentation Data Data Types Numbers Strings Building Strings Splitting 
Strings Substitution Trim Whitespace Evaluating Strings Logical Dates and Times 
Space Data Structures Vectors Arrays Lists Sets Matricies Exercises Data Frames 
Accessing Columns Removing Columns Exercises General Manipulation Factors 
Elements Rows and Columns Finding Index of Elements Partitions Head and Tail 
Reverse a List Sorting Unique Values Loading Data Interactive Responses 
Interactive Data Entry Available Datasets The Iris Dataset CSV Data Used In The 
Book The Wine Dataset The Cardiac Arrhythmia Dataset The Adult Survey Dataset 
Foreign Formats Stata Data Conversions Reading Variable Width Data Saving Data 
Formatted Output Automatically Generate Filenames Reading a Large File 
Manipulating Data Manipulating Data As SQL Using SQLite ODBC Data Database 
Connection Excel Access Clipboard Data Spatial Data Simple Map A Density Map 
Overlays and Point in Polygon Other Data Formats Fixed Width Data Global 
Positioning System Documenting a Dataset Common Data Problems Graphics in R 
Basic Plot Controlling Axes Arrow Axes Legends and Points Tables Within Plots 
Colour Labels in Plots Axis Labels Legend Labels Within Plots Maths in Labels 
Multiple Plots MatPlot Multiple Plots Using ggplot2 Using GGPlot Networks 
Symbols Other Graphic Elements Making an Animation Animated Mandelbrot Adding a 
Logo to a Graphic Graphics Devices Setup Screen Devices Multiple Devices File 
Devices Multiple Plots Copy and Print Devices Graphics Parameters Plotting 
Region Locating Points on a Plot Scientific Notation and Plots Understanding 
Data Single Variable Overviews Textual Summaries Multiple Line Plots Separate 
Line Plots Pie Chart Fan Plot Stem and Leaf Plots Histogram Barplot Trellis 
Histogram Histogram Uneven Distribution Bump Chart Density Plot Basic Histogram 
Basic Histogram with Density Curve Practical Histogram Multiple Variable 
Overviews Scatterplot Scatterplot with Marginal Histograms Multi-Dimension 
Scatterplot Correlation Plot Colourful Correlations Fluctuation Plot Heat Map 
Projection Pursuit RADVIZ Parallel Coordinates Categoric and Numeric Measuring 
Data Distributions Textual Summaries Boxplot Multiple Boxplots Boxplot by Class 
Tuning a Boxplot Boxplot Using Lattice Boxplot Using ggplot Violin Plot What 
Distribution Miscellaneous Plots Line and Point Plots Matrix Data Multiple Plots
Aligned Plots Probability Scale Network Plot Sunflower Plot Stairs Plot 
Graphing Means and Error Bars Bar Charts With Segments Bar Plot With Means 3d 
Bar Plot Stacks Versus Lines Multi-Line Title Mathematics Plots for Normality 
Basic Bar Chart Bar Chart Displays Multiple Dot Plots Alternative Multiple Dot 
Plots 3D Plot Clustered Box Plot Perspective Plots Star Plot Residuals Plot 
Waterfall Plots Dates and Times Simple Time Series Multiple Time Series Plot 
Time Series Plot Time Series with Axis Labels Grouping Time Series for Box Plot 
Time Series Heatmap Textual Summaries Stem and Leaf Plots Histogram Barplot 
Density Plot Basic Histogram Basic Histogram with Density Curve Practical 
Histogram Correlation Plot Colourful Correlations Measuring Data Distributions 
Textual Summaries Boxplot Multiple Boxplots Boxplot by Class Box and Whisker 
Plot Box and Whisker Plot Clustered Box Plot Further Resources Map Displays 
Further Resources Preparing Data Data Selection and Extraction Training and 
Test Datasets Data Cleaning Review Data Selectively Changing Vector Values 
Replace Indices By Names Missing Values Remove Levels from a Factor Variable 
Manipulations Remove Columns Reorder Columns Remove Non-Numeric Columns Remove 
Variables with no Variance Cleaning the Wine Dataset Cleaning the Cardiac 
Dataset Cleaning the Survey Dataset Imputation Nearest Neighbours Multiple 
Imputation Data Linking Simple Linking Record Linkage Data Transformation 
Aggregation Sum of Columns Pivot Tables Normalising Data Binning Interpolation 
Variable Selection Classification Classification Classification Issues 
Incremental or Online Modelling Model Tuning Tuning rpart Unbalanced 
Classification Building Models Temporal Analysis Evaluation Basics Basic 
Measures Cross Validation Graphical Performance Measures Lift The ROC Curve 
Other Examples 10 Fold Cross Validation Area Under Curve Calibration Curves 
Reporting Generating Open Document Format Getting Started with odfWeave 
OpenOffice.org Macro Support Generating HTML Generating PDF with LATEX 
Configuration Figure Sizes Fraud Analysis Archetype Analysis <br>
<br>
<br>
<br>
<br> 
<h1> <br>
 Random Forests </h1> <br>
<br>
<br>

<p> </p>  A random forest is an ensemble (i.e., a collection) of unpruned 
decision trees. Random forests are often used when we have very large training 
datasets and a very large number of input variables (hundreds or even thousands 
of input variables). A random forest model is typically made up of tens or 
hundreds of decision trees.
<p> The generalisation error rate from random forests tends to compare 
favourably to boosting approaches, yet the approach tends to be more robust to 
noise in the training dataset, and so tends to be a very stable model builder, 
not suffering the sensitivity to noise in a dataset that single decision tree 
induction does. The general observation is that the random forest model builder 
is very competitive with nonlinear classifiers such as artificial neural nets 
and support vector machines. However, performance is often dataset dependent 
and so it remains useful to try a suite of approaches.</p> 
<p> Each decision tree is built from a random subset of the training dataset, 
using what is called replacement (thus it is doing what is known as bagging), 
in performing this sampling. That is, some entities will be included more than 
once in the sample, and others won't appear at all. Generally, about two thirds 
of the entities will be included in the subset of the training dataset, and one 
third will be left out.</p> 
<p> In building each decision tree model based on a different random subset of 
the training dataset a random subset of the available variables is used to 
choose how best to partition the dataset at each node. Each decision tree is 
built to its maximum size, with no pruning performed.</p> 
<p> Together, the resulting decision tree models of the forest represent the 
final ensemble model where each decision tree votes for the result, and the 
majority wins. (For a regression model the result is the average value over the 
ensemble of regression trees.)</p> 
<p> In building the random forest model we have options to choose the number 
of trees to build, to choose the training dataset sample size to use for 
building each decision tree, and to choose the number of variables to randomly 
select when considering how to partition the training dataset at each node. The 
random forest model builder can also report on the input variables that are 
actually most important in determining the values of the output variable.</p> 
<p> By building each decision tree to its maximal depth (i.e., by not pruning 
the decision tree) we can end up with a model that is less biased.</p> 
<p> The randomness introduced by the random forest model builder in the 
dataset selection and in the variable selection delivers considerable 
robustness to noise, outliers, and over-fitting, when compared to a single tree 
classifier.</p> 
<p> The randomness also delivers substantial computational efficiencies. In 
building a single decision tree the model builder may select a random subset of 
the training dataset. Also, at each node in the process of building the 
decision tree, only a small fraction of all of the available variables are 
considered when determining how to best partition the dataset. This 
substantially reduces the computational requirement.</p> 
<p> In summary, a random forest model is a good choice for model building for 
a number of reasons. First, just like decision trees, very little, if any, 
pre-processing of the data needs to be performed. The data does not need to be 
normalised and the approach is resiliant to outliers. Second, if we have many 
input variables, we generally do not need to do any variable selection before 
we begin model building. The random forest model builder is able to target the 
most useful variables. Thirdly, because many trees are built and there are two 
levels of randomness and each tree is effectively an independent model, the 
model builder tends not to overfit to the training dataset.</p> 
<p> <br>
</p> <strong>Subsections</strong> 
<ul> 
<li> 
<ul> 
<li>Formalities </li> 
<li>Tutorial Example </li> 
<li>Tuning Parameters 
<ul> 
<li>Number of Trees </li> 
<li>Sample Size </li> 
<li>Number of Variables </li> </ul> </li> 
<li>Summary </li> 
<li>Overview </li> 
<li>Algorithm </li> 
<li>Usage </li> 
<li>Random Forest 
<ul> 
<li>importance </li> 
<li>classwt </li> </ul> </li> 
<li>Examples </li> 
<li>Resources and Further Reading </li> </ul> <br>
</li> 
<li>Bagging: <br>
 Meta Algorithm </li> 
<li>Summary </li> 
<li>Overview </li> 
<li>Example </li> 
<li>Algorithm </li> 
<li>Resources and Further Reading </li> </ul> <br>
 Copyright &copy; 2004-2010 
Togaware Pty Ltd Support further development through the purchase of the PDF 
version of the book.<br>
 The PDF version is a formatted comprehensive draft 
book (with over 800 pages).<br>
 Brought to you by Togaware. This page 
generated: Sunday, 22 August 2010 
</body>
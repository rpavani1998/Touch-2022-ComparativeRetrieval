<!doctype html>
<meta charset="utf-8">
<title>Interoperability Happens - Development Processes</title>
<body>
<br>
 &nbsp;&nbsp;&nbsp;&nbsp;JOB REFERRALS <br>
<br>
<br>
 
&nbsp;&nbsp;&nbsp;&nbsp;ON THIS PAGE <br>
Is Programming Less Exciting Today?...
<br> Tech Predictions, 2012 Edition <br>
Changes, changes, changes <br>

Managing Talks: An F#/Office Love S... <br>
Tech Predictions, 2011 Edition <br>

Thoughts on my first Startup Weeken... <br>
VMWare help <br>
Architectural Katas
<br> Emotional commitment colors everyth... <br>
Code Kata: Compressing Lists 
<br> 10 Things To Improve Your Developme... <br>
2010 Predictions, 2009 
Predictions ... <br>
Book Review: Debug It! (Paul Butche... <br>
Haacked, but 
not content; agile sti... <br>
SDWest, SDBestPractices, SDArch&amp;... <br>
As 
for Peer Review, Code Review? <br>
Phishing attacks know no boundaries... <br>

2009 Predictions, 2008 Predictions ... <br>
Ruminations on Women in IT <br>
The 
Myth of Discovery <br>
Apparently I'm #25 on the Top 100 B... <br>
An 
Announcement <br>
The Never-Ending Debate of Speciali... <br>
From the 
&quot;Team-Building Exerci... <br>
From the &quot;Gosh, You Wanted Me ... <br>

From the &quot;You Must Be Trolling... <br>
More on Paradise <br>
Developer 
paradise? <br>
Reminder <br>
The reason for conferences <br>
Mort means 
productivity <br>
The Fallacies Remain.... <br>
Can Dynamic Languages Scale? 
<br> My Open Wireless Network <br>
I Refused to be Terrorized <br>
Let the JDK 
Hacking Begin... <br>
Quotes on writing <br>
A Dozen Levels of Done <br>

Anybody know of a good WebDAV clien... <br>
Welcome to the Shitty Code Support 
... <br>
A Book Every Developer Must Read <br>
Hard Questions About Architects 
<br> The First Strategy: Declare War on ... <br>
Yellow Journalism Meets The 
Web... ... <br>
The Strategies of Software Developm... <br>
Would you still 
love AJAX if you kn... <br>
Important/Not-so-important <br>
More on Ethics <br>

Programming Promises (or, the Profe... <br>
The Root of All Evil <br>
A Time 
for a Change <br>
Why programmers shouldn't fear offs... <br>
Check it out... 
<br> Don't fall prey to the latest socia... <br>
2006 Tech Predictions <br>

Porting legacy code <br>
No, John, software really *does* ev... <br>
Best 
practices, redux <br>
<br>
<br>
<br>
 &nbsp;&nbsp;&nbsp;&nbsp;ARCHIVES <br>

January, 2012 (4) <br>
December, 2011 (2) <br>
October, 2011 (2) <br>
August, 
2011 (1) <br>
May, 2011 (1) <br>
April, 2011 (1) <br>
February, 2011 (1) <br>

January, 2011 (1) <br>
December, 2010 (1) <br>
November, 2010 (1) <br>
October, 
2010 (3) <br>
September, 2010 (4) <br>
August, 2010 (2) <br>
July, 2010 (1) <br>
June, 2010 (1) <br>
May, 2010 (3) <br>
March, 2010 (5) <br>
February, 2010 (1) 
<br> January, 2010 (4) <br>
December, 2009 (1) <br>
November, 2009 (3) <br>

October, 2009 (3) <br>
August, 2009 (2) <br>
July, 2009 (4) <br>
June, 2009 (3) 
<br> May, 2009 (6) <br>
April, 2009 (4) <br>
March, 2009 (4) <br>
February, 
2009 (5) <br>
January, 2009 (11) <br>
December, 2008 (3) <br>
November, 2008 (9)
<br> October, 2008 (1) <br>
September, 2008 (2) <br>
August, 2008 (4) <br>

July, 2008 (10) <br>
June, 2008 (5) <br>
May, 2008 (10) <br>
April, 2008 (13) 
<br> March, 2008 (11) <br>
February, 2008 (18) <br>
January, 2008 (17) <br>

December, 2007 (12) <br>
November, 2007 (2) <br>
October, 2007 (6) <br>

September, 2007 (1) <br>
August, 2007 (2) <br>
July, 2007 (7) <br>
June, 2007 
(1) <br>
May, 2007 (1) <br>
April, 2007 (2) <br>
March, 2007 (2) <br>
February, 
2007 (1) <br>
January, 2007 (16) <br>
December, 2006 (3) <br>
November, 2006 (7)
<br> October, 2006 (5) <br>
September, 2006 (1) <br>
June, 2006 (4) <br>
May, 
2006 (3) <br>
April, 2006 (3) <br>
March, 2006 (17) <br>
February, 2006 (5) <br>
January, 2006 (13) <br>
December, 2005 (2) <br>
November, 2005 (6) <br>

October, 2005 (15) <br>
September, 2005 (16) <br>
August, 2005 (17) <br>
<br>

<br> <br>
 &nbsp;&nbsp;&nbsp;&nbsp;CATEGORIES <br>
&nbsp;.NET <br>
&nbsp;Android
<br> &nbsp;Azure <br>
&nbsp;C# <br>
&nbsp;C++ <br>
&nbsp;Conferences <br>
&nbsp;
Development Processes <br>
&nbsp;F# <br>
&nbsp;Flash <br>
&nbsp;Industry <br>

&nbsp;iPhone <br>
&nbsp;Java/J2EE <br>
&nbsp;Languages <br>
&nbsp;LLVM <br>

&nbsp;Mac OS <br>
&nbsp;Objective-C <br>
&nbsp;Parrot <br>
&nbsp;Personal <br>

&nbsp;Python <br>
&nbsp;Reading <br>
&nbsp;Review <br>
&nbsp;Ruby <br>
&nbsp;
Scala <br>
&nbsp;Security <br>
&nbsp;Social <br>
&nbsp;Solaris <br>
&nbsp;
Visual Basic <br>
&nbsp;VMWare <br>
&nbsp;WCF <br>
&nbsp;Windows <br>
&nbsp;XML 
Services <br>
&nbsp;XNA <br>
<br>
<br>
<br>
 &nbsp;&nbsp;&nbsp;&nbsp;BLOGROLL 
<br> &nbsp;Ajaxian <br>
&nbsp;Amazon Web Services Blog <br>
&nbsp;Clemens 
Vasters <br>
&nbsp;Harry Pierson <br>
&nbsp;Haus News <br>
&nbsp;IBM Alphaworks 
<br> &nbsp;java.net blogs Recent Entries <br>
&nbsp;Lambda-the-Ultimate <br>

&nbsp;Microsoft Research News <br>
&nbsp;MSDN Blogs <br>
&nbsp;Scott Hanselman 
<br> &nbsp;Stephen Forte <br>
<br>
<br>
<br>
 &nbsp;&nbsp;&nbsp;&nbsp;LINKS <br>
Home <br>
Book: Effective Enterprise Java <br>
Book: SSCLI Essentials <br>

Book: C# In a Nusthell (2nd Ed) <br>
Book: Server-Based Java Programming <br>

<br> <br>
<br>
 &nbsp;&nbsp;&nbsp;&nbsp;SEARCH <br>
<br>
<br>
<br>
 
&nbsp;&nbsp;&nbsp;&nbsp;MY BOOKS <br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

<br>  &nbsp;&nbsp;&nbsp;&nbsp;DISCLAIMER <br>

<p> Powered by: newtelligence dasBlog 1.9.7067.0 </p> 
<p> The opinions expressed herein are my own personal opinions and do not 
represent my employer's view in any way.</p> 
<p> &copy; Copyright 2012 , Ted Neward </p> 
<p>  E-mail</p> <br>
<br>
Sign In <br>
<br>
<br>
 &nbsp; <br>
&nbsp;Wednesday, 
25 January 2012<br>
Is Programming Less Exciting Today? <br>

<p>As discriminatory as this is going to sound, this one is for the 
old-timers. If you started programming after the turn of the milennium, I 
don&rsquo;t know if you&rsquo;re going to be able to follow the trend of this 
post&mdash;not out of any serious deficiency on your part, hardly that. But I 
think this is something only the old-timers are going to identify with. (And 
thus, do I alienate probably 80% of my readership, but so be it.)</p> 
<p>Is it me, or is programming just less interesting today than it was two 
decades ago?</p> 
<p>By all means, shake your smartphones and other mobile devices at me and 
say, &ldquo;Dude, how can you say that?&rdquo;, but in many ways programming 
for Android and iOS reminds me of programming for Windows and Mac OS two 
decades ago. HTML 5 and JavaScript remind me of ten years ago, the first time 
HTML and JavaScript came around. The discussions around programming languages 
remind me of the discussions around C++. The discussions around NoSQL remind me 
of the arguments both for and against relational databases. It all feels like 
we&rsquo;ve been here before, with only the names having changed.</p> 
<p>Don&rsquo;t get me wrong&mdash;if any of you comment on the differences 
between HTML 5 now and HTML 3.2 then, or the degree of the various browser 
companies agreeing to the standard today against the &ldquo;browser wars&rdquo; 
of a decade ago, I&rsquo;ll agree with you. This isn&rsquo;t so much of a 
rational and logical discussion as it is an emotive and intuitive one. It just
<em>feels</em> similar.</p> 
<p>To be honest, I get this sense that across the entire industry right now, 
there&rsquo;s a sort of malaise, a general sort of &ldquo;Bah, nothing really 
all that new is going on anymore&rdquo;. NoSQL is re-introducing storage ideas 
that had been around before but were discarded (perhaps injudiciously and too 
quickly) in favor of the relational model. Functional languages have obviously 
been in place since the 50&rsquo;s (in Lisp). And so on.</p> 
<p>More importantly, look at the Java community: what truly innovative ideas 
have emerged here in the last five years? Every new open-source project or 
commercial endeavor either seems to be a refinement of an idea before it (how 
many different times are we going to create a new Web framework, guys?) or an 
attempt to leverage an idea coming from somewhere else (be it from .NET or from 
Ruby or from JavaScript or&hellip;.). With the upcoming .NET 4.5 release and 
Windows 8, Microsoft is holding out very little &ldquo;new and exciting&rdquo; 
bits for the community to invest emotionally in: we hear about 
&ldquo;async&rdquo; in C# 5 (something that F# has had already, thank you), and 
of course there is WinRT (another platform or virtual machine&hellip; sort of), 
and&hellip; well, honestly, didn&rsquo;t we just do this a decade ago? Where is 
the WCFs, the WPFs, the Silverlights, the things that would get us fired up? 
Hell, even a new approach to data access might stir some excitement. Node.js 
feels like an attempt to reinvent the app server, but if you look back far 
enough you see that the app server itself was reinvented once (in the Java 
world) in Spring and other lightweight frameworks, and before that by people 
who actually thought to write their own web servers in straight Java. (And, for 
the record, the whole event-driven I/O thing is something that&rsquo;s been 
done in both Java and .NET a long time before now.)</p> 
<p>And as much as this is going to probably just throw fat on the fire, all 
the excitement around JavaScript as a language reminds me of the excitement 
about Ruby as a language. Does nobody remember that Sun did this once already, 
with Phobos? Or that Netscape did this with LiveScript? JavaScript on the 
server end is not new, folks. It&rsquo;s just new to the people who&rsquo;d 
never seen it before.</p> 
<p>In years past, there has always seemed to be something deeper, something 
more exciting and more innovative that drives the industry in strange ways. 
Artificial Intelligence was one such thing: the search to try and bring 
computers to a state of human-like sentience drove a lot of interesting ideas 
and concepts forward, but over the last decade or two, AI seems to have lost 
almost all of its luster and momentum. User interfaces&mdash;specifically, 
GUIs&mdash;were another force for a while, until GUIs got to the point where 
they were so common and so deeply rooted in their chosen pasts (the 
single-button of the Mac, the menubar-per-window of Windows, etc) that they 
left themselves so little room for maneuver. At least this is one area where 
Microsoft is (maybe) putting the fatted sacred cow to the butcher&rsquo;s 
knife, with their Metro UI moves in Windows 8&hellip; but only up to a point.
</p> 
<p>Maybe I&rsquo;m just old and tired and should hang up my keyboard and go 
take up farming, then go retire to my front porch&rsquo;s rocking chair and 
practice my<em>Hey you kids! Getoffamylawn!</em> or something. But before you 
dismiss me entirely, do me a favor and tell me: what gets you excited these 
days? If you&rsquo;ve been programming for twenty years, what about the 
industry today gets your blood moving and your mind sharpened?</p> <br>

<p></p> .NET&nbsp;|&nbsp; Android&nbsp;|&nbsp; Azure&nbsp;|&nbsp; C#
&nbsp;|&nbsp;C++&nbsp;|&nbsp; Development Processes&nbsp;|&nbsp; F#&nbsp;|&nbsp;
Flash&nbsp;|&nbsp; Industry&nbsp;|&nbsp; iPhone&nbsp;|&nbsp; Java/J2EE
&nbsp;|&nbsp;Languages&nbsp;|&nbsp; LLVM&nbsp;|&nbsp; Mac OS&nbsp;|&nbsp; 
Objective-C&nbsp;|&nbsp; Parrot&nbsp;|&nbsp; Personal&nbsp;|&nbsp; Python
&nbsp;|&nbsp;Ruby&nbsp;|&nbsp; Scala&nbsp;|&nbsp; Security&nbsp;|&nbsp; Social
&nbsp;|&nbsp;Solaris&nbsp;|&nbsp; Visual Basic&nbsp;|&nbsp; VMWare&nbsp;|&nbsp; 
WCF&nbsp;|&nbsp; Windows&nbsp;|&nbsp; XML Services&nbsp;|&nbsp; XNA <br>
<br>
 
Wednesday, 25 January 2012 15:24:43 (Pacific Standard Time, UTC-08:00)<br>

Comments [33] &nbsp;|&nbsp; <br>
<br>
<br>
&nbsp;Sunday, 01 January 2012 <br>

Tech Predictions, 2012 Edition <br>

<p>Well, friends, another year has come and gone, and it's time for me to put 
my crystal ball into place and see what the upcoming year has for us. But, of 
course, in the long-standing tradition of these predictions, I also need to put 
my spectacles on (I did turn 40 last year, after all) and have a look at how 
well I didin this same activity twelve months ago.</p> 
<p>Let's see what unbelievable gobs of hooey I slung last year came even 
remotely to pass. For 2011, I said....</p> 
<ul> 
<li> <strong>THEN:</strong> <em>Android&rsquo;s penetration into the mobile 
space is going to rise, then plateau around the middle of the year.</em> 
Android phones, collectively, have outpaced iPhone sales. That&rsquo;s a pretty 
significant statistic&mdash;and it means that there&rsquo;s fewer customers 
buying smartphones in the coming year. More importantly, the first generation 
of Android slates (including the Galaxy Tab, which I own), are 
less-than-sublime, and not really an &ldquo;iPad Killer&rdquo; device by any 
stretch of the imagination. And I think that will slow down people buying 
Android slates and phones, particularly since Google has all but promised that 
Android releases will start slowing down.
<ul> 
<li><strong>NOW:</strong> Well, I think I get a point for saying that 
Android's penetration will rise... but then I lose it for suggesting that it 
would slow down. Wow, was I wrong on that. Once Amazon put the Kindle Fire out, 
suddenly for the first time Android tablets began to appear in peoples' hands 
in record numbers. The drawback here is that most people using the Fire don't 
realize it's an Android tablet, which certainly hurts Google's brand-awareness 
(not that Amazon really seems to mind), but the upshot is simple: people are 
still buying devices, even though they may already own one. Which amazes me.
</li> </ul> </li> 
<li> <strong>THEN:</strong> <em>Windows Phone 7 penetration into the mobile 
space will appear huge, then slow down towards the middle of the year.</em> 
Microsoft is getting some pretty decent numbers now, from what I can piece 
together, and I think that&rsquo;s largely the &ldquo;I love Microsoft&rdquo; 
crowd buying in. But it&rsquo;s a pretty crowded place right now with Android 
and iPhone, and I&rsquo;m not sure if the much-easier Office and/or Exchange 
integration is enough to woo consumers (who care about Office) or business 
types (who care about Exchange) away from their Androids and iPhones.
<ul> 
<li><strong>NOW:</strong> Despite the catastrophic implosion of RIM (thus 
creating a huge market of people looking to trade their Blackberrys in for 
other mobile phones, ones which won't all go down when a RIM server implodes), 
WP7 has definitely not emerged as the &quot;third player&quot; in the mobile 
space; or, perhaps more precisely, they feel like a distant third, rather than 
a creditable alternative to the other two. In fact, more and more it just feels 
like this is a two-horse race and Microsoft is in it still because they're 
willing to throw loss after loss to stay in it. (For what reason, I'm not 
sure--it's not clear to me that they can ever reach a point of profitability 
here, even once Nokia makes the transition to WP7, which is supposedly going to 
take years. On the order of a half-decade or so.) Even living here in Redmon, 
where I would expect the WP7 concentration to be much, much higher than 
anywhere else in the world, it's still more common to see iPhones and 'droids 
in peoples' hands than it is to see WP7 phones.</li> </ul> </li> 
<li> <strong>THEN:</strong> <em>Android, iOS and/or Windows Phone 7 becomes a 
developer requirement.</em> Developers, if you haven&rsquo;t taken the time to 
learn how to program one of these three platforms, you are electing to remove 
yourself from a growing market that desperately wants people with these skills. 
I see the &ldquo;mobile native app development&rdquo; space as every bit as hot 
as the &ldquo;Internet/Web development&rdquo; space was back in 2000. If you 
don&rsquo;t have a device, buy one. If you have a device, get the 
tools&mdash;in all three cases they&rsquo;re free downloads&mdash;and start 
writing stupid little apps that nobody cares about, so you can have some skills 
on the platform when somebody cares about it.
<ul> 
<li><strong>NOW:</strong> Wow, yes. Right now, if you are a developer and you 
haven't spent at least a little time learning mobile development, you are 
excluding yourself from a development &quot;boom&quot; that rivals the one 
around Web sites in the mid-90's. Seriously: remember when everybody had to 
have a website? That's the mentality right now with a ton of different 
companies--&quot;we have to have a mobile app!&quot; &quot;But we sell condom 
lubricant!&quot; &quot;Doesn't matter! We need a mobile app! Build us 
something! Go go go go go!&quot;</li> </ul> </li> 
<li> <strong>THEN:</strong> <em>The Windows 7 slates will suck.</em> This 
isn&rsquo;t a prediction, this is established fact. I played with an 
&ldquo;ExoPC&rdquo; 10&rdquo; form factor slate running Windows 7 (Dell I think 
was the manufacturer), and it was a horrible experience. Windows 7, like most 
OSes, really expects a keyboard to be present, and a slate doesn&rsquo;t have 
one&mdash;so the OS was hacked to put a &ldquo;keyboard&rdquo; button at the 
top of the screen that would slide out to let you touch-type on the slate. I 
tried to fire up Notepad and type out a haiku, and it was an unbelievably 
awkward process. Android and iOS clearly own the slate market for the 
forseeable future, and if Dell has any brains in its corporate head, it will 
phone up Google tomorrow and start talking about putting Android on that 
hardware.
<ul> 
<li><strong>NOW:</strong> Yeah, that was something of a &quot;gimme&quot; 
point (but I'll take it). Windows7 on a slate was a Bad Idea, and I'm pretty 
sure the sales reflect that. Conduct your own anecdotal poll: see if you can 
find a store somewhere in your town or city that will actually sell you a 
Windows7 slate. Can't find one? I can--it's the Microsoft store in town, and 
I'm not entirely sure they still stock them. Certainly our local Best Buy 
doesn't.</li> </ul> </li> 
<li> <strong>THEN:</strong> <em>DSLs mostly disappear from the buzz.</em> I 
still see no strawman (no &ldquo;pet store&rdquo; equivalent), and none of the 
traditional builders-of-strawmen (Microsoft, Oracle, etc) appear interested in 
DSLs much anymore, so I think 2010 will mark the last year that we spent any 
time talking about the concept.
<ul> 
<li><strong>NOW:</strong> I'm going to claim a point here, too. DSLs have 
pretty much left us hanging. Without a strawman for developers to 
&quot;get&quot;, the DSL movement has more or less largely died out. I still 
sometimes hear people refer to something that isn't a programming language but 
does something technical as a &quot;DSL&quot; (&quot;That shipping label? 
That's a DSL!&quot;), and that just tells me that the concept never really took 
root.</li> </ul> </li> 
<li> <strong>THEN:</strong> <em>Facebook becomes more of a developer 
requirement than before.</em> I don&rsquo;t like Mark Zuckerburg. I don&rsquo;t 
like Facebook&rsquo;s privacy policies. I don&rsquo;t particularly like the way 
Facebook approaches the Facebook Connect experience. But Facebook owns enough 
people to be the fourth-largest nation on the planet, and probably commands an 
economy of roughly that size to boot. If your app is aimed at the Facebook 
demographic (that is, everybody who&rsquo;s not on Twitter), you have to know 
how to reach these people, and that means developing at least some part of your 
system to integrate with it.
<ul> 
<li><strong>NOW:</strong> Facebook, if anything, has become more important 
through 2011, particularly for startups looking to get some exposure and 
recognition. Facebook continues to screw with their user experience, though, 
and they keep screwing with their security policies, and as &quot;big&quot; a 
presence as they have, it's not invulnerable, and if they're not careful, 
they're going to find themselves on the other side of the relevance curve.</li> 
</ul> </li> 
<li> <strong>THEN:</strong> Twitter becomes more of a developer requirement, 
too. Anybody who&rsquo;s not on Facebook is on Twitter. Or dead. So to reach 
the other half of the online community, you have to know how to connect out 
with Twitter.
<ul> 
<li><strong>NOW:</strong> Twitter's impact has become deeper, but more muted 
in some ways--people don't think of Twitter as a &quot;new&quot; channel, but 
one that they've come to expect and get used to. At the same time, how Twitter 
is supposed to factor into different applications isn't always clear, which 
hinders Twitter's acceptance and &quot;must-have&quot;-ness. Of course, Twitter 
could care less, it seems, though it still confuses me how they actually make 
money.</li> </ul> </li> 
<li> <strong>THEN:</strong> XMPP becomes more of a developer requirement. XMPP 
hasn&rsquo;t crossed a lot of people&rsquo;s radar screen before, but Facebook 
decided to adopt it as their chat system communication protocol, and 
Google&rsquo;s already been using it, and suddenly there&rsquo;s a whole lotta 
traffic going over XMPP. More importantly, it offers a two-way communication 
experience that is in some scenarios vastly better than what HTTP offers, yet 
running in a very &ldquo;Internet-friendly&rdquo; way just as HTTP does. I 
suspect that XMPP is going to start cropping up in a number of places as a 
useful alternative and/or complement to using HTTP.
<ul> 
<li><strong>NOW:</strong> Well, unfortunately, XMPP still hides underneath 
other names and still doesn't come to mind when people are thinking about 
communication, leaving this one way unfulfilled. *sigh* Maybe someday we will 
learn that not everything has to go over HTTP, but it didn't happen in 2011.
</li> </ul> </li> 
<li> <strong>THEN:</strong> &ldquo;Gamification&rdquo; starts making serious 
inroads into non-gaming systems. Maybe it&rsquo;s just because I&rsquo;ve been 
talking more about gaming, game design, and game implementation last year, but 
all of a sudden &ldquo;gamification&rdquo;&mdash;the process of putting 
game-like concepts into non-game applications&mdash;is cresting in a big way. 
FourSquare, Yelp, Gowalla, suddenly all these systems are offering achievement 
badges and scoring systems for people who want to play in their worlds. How 
long is it before a developer is pulled into a meeting and told that &ldquo;we 
need to put achievement badges into the call-center support application&rdquo;? 
Or the online e-commerce portal? It&rsquo;ll start either this year or next.
<ul> 
<li><strong>NOW:</strong> Gamification is emerging, but slowly and under the 
radar. It's certainly not as strong as I thought it would be, but gamification 
concepts are sneaking their way into a variety of different scenarios (beyond 
games themselves). Probably can't claim a point here, no.</li> </ul> </li> 
<li> <strong>THEN:</strong> Functional languages will hit a make-or-break 
point. I know, I said it last year. But the buzz keeps growing, and when that 
happens, it usually means that it&rsquo;s either going to reach a critical mass 
and explode, or it&rsquo;s going to implode&mdash;and the longer the buzz 
grows, the faster it explodes or implodes, accordingly. My personal guess is 
that the &ldquo;F/O hybrids&rdquo;&mdash;F#, Scala, etc&mdash;will continue to 
grow until they explode, particularly since the suggested v.Next changes to 
both Java and C# have to be done as language changes, whereas futures for F# 
frequently are either built as libraries masquerading as syntax (such as 
asynchronous workflows, introduced in 2.0) or as back-end library hooks that 
anybody can plug in (such as type providers, introduced at PDC a few months 
ago), neither of which require any language revs&mdash;and no concerns about 
backwards compatibility with existing code. This makes the F/O hybrids vastly 
more flexible and stable. In fact, I suspect that within five years or so, 
we&rsquo;ll start seeing a gradual shift away from pure O-O systems, into 
systems that use a lot more functional concepts&mdash;and that will propel the 
F/O languages into the center of the developer mindshare.
<ul> 
<li><strong>NOW:</strong> More than any of my other predictions (or subjects 
of interest), functional languages stump me the most. On the one hand, there 
doesn't seem to be a drop-off of interest in the subject, based on a variety of 
anecdotal evidence (books, articles, etc), but on the other hand, they don't 
seem to be crossing over into the &quot;mainstream&quot; programming worlds, 
either. At best, we can say that they are entering the mindset of senior 
programmers and/or project leads and/or architects, but certainly they don't 
seem to be turning in to the &quot;go-to&quot; language for projects being done 
in 2011.</li> </ul> </li> 
<li> <strong>THEN:</strong> <em>The Microsoft Kinect will lose its shine.</em> 
I hate to say it, but I just don&rsquo;t see where the excitement is coming 
from. Remember when the Wii nunchucks were the most amazing thing anybody had 
ever seen? Frankly, after a slew of initial releases for the Wii that made use 
of them in interesting ways, the buzz has dropped off, and more importantly, 
the nunchucks turned out to be just another way to move an arrow around on the 
screen&mdash;in other words, we haven&rsquo;t found particularly novel and 
interesting/game-changing ways to use the things. That&rsquo;s what I think 
will happen with the Kinect. Sure, it&rsquo;s really freakin&rsquo; cool that 
you can use your body as the controller&mdash;but how precise is it, how 
quickly can it react to my body movements, and most of all, what new user 
interface metaphors are people going to have to come up with in order to avoid 
the &ldquo;me-too&rdquo; dancing-game clones that are charging down the 
pipeline right now?
<ul> 
<li><strong>NOW:</strong> Kinect still makes for a great Christmas or birthday 
present, but nobody seems to be all that amazed by the idea anymore. Certainly 
we aren't seeing a huge surge in using Kinect as a general user interface 
device, at least not yet. Maybe it needed more time for people to develop those 
new metaphors, but at the same time, I would've expected at least a few more 
games to make use of it, and I haven't seen any this past year.</li> </ul> </li>
<li> <strong>THEN:</strong> <em>There will be no clear victor in the 
Silverlight-vs-HTML5 war.</em> And make no mistake about it, a war is brewing. 
Microsoft, I think, finds itself in the inenviable position of having two very 
clearly useful technologies, each one&rsquo;s &ldquo;sphere of utility&rdquo; 
(meaning, the range of answers to the &ldquo;where would I use it?&rdquo; 
question) very clearly overlapping. It&rsquo;s sort of like being a football 
team with both Brett Favre and Tom Brady on your roster&mdash;both of them are 
superstars, but you know, deep down, that you have to cut one, because you 
can&rsquo;t devote the same degree of time and energy to both. Microsoft is 
going to take most of 2011 and probably part of 2012 trying to support both, 
making a mess of it, offering up conflicting rationale and reasoning, in the 
end achieving nothing but confusing developers and harming their relationship 
with the Microsoft developer community in the process. Personally, I think 
Microsoft has no choice but to get behind HTML 5, but I like a lot of the 
features of Silverlight and think that it has a lot of mojo that HTML 5 lacks, 
and would actually be in favor of Microsoft keeping both&mdash;so long as they 
make it very clear to the developer community when and where each should be 
used. In other words, the executives in charge of each should be locked into a 
room and not allowed out until they&rsquo;ve hammered out a business strategy 
that is then printed and handed out to every developer within a 3-continent 
radius of Redmond. (Chances of this happening: .01%)
<ul> 
<li><strong>NOW:</strong> Well, this was accurate all the way up until the 
last couple of months, when Microsoft made it fairly clear that Silverlight was 
being effectively &quot;put behind&quot; HTML 5, despite shipping another 
version of Silverlight. In the meantime, though, they've tried to support both 
(and some Silverlighters tell me that the Silverlight team is still looking 
forward to continuing supporting it, though I'm not sure at this point what is 
rumor and what is fact anymore), and yes, they confused the hell out of 
everybody. I'm surprised they pulled the trigger on it in 2011, though--I 
expected it to go a version or two more before they finally pulled the rug out.
</li> </ul> </li> 
<li> <strong>THEN:</strong> <em>Apple starts feeling the pressure to deliver a 
developer experience that isn&rsquo;t mired in mid-90&rsquo;s metaphor.</em> 
Don&rsquo;t look now, Apple, but a lot of software developers are coming to 
your platform from Java and .NET, and they&rsquo;re bringing their expectations 
for what and how a developer IDE should look like, perform, and do, with them. 
Xcode is not a modern IDE, all the Apple fan-boy love for it notwithstanding, 
and this means that a few things will happen:
<ul> 
<li><em>Eclipse gets an iOS plugin.</em> Yes, I know, it wouldn&rsquo;t work 
(for the most part) on a Windows-based Eclipse installation, but if Eclipse can 
have a native C/C++ developer experience, then there&rsquo;s no reason why a 
Mac Eclipse install couldn&rsquo;t have an Objective-C plugin, and that opens 
up the idea of using Eclipse to write iOS and/or native Mac apps (which will be 
critical when the Mac App Store debuts somewhere in 2011 or 2012).</li> 
<li><em>Rumors will abound about Microsoft bringing Visual Studio to the Mac.
</em> Silverlight already runs on the Mac; why not bring the native development 
experience there? I&rsquo;m not saying they&rsquo;ll actually do it, and 
certainly not in 2011, but the rumors, they will be flyin&hellip;.</li> 
<li><em>Other third-party alternatives to Xcode will emerge and/or grow.</em> 
MonoTouch is just one example. There&rsquo;s opportunity here, just as the 
fledgling Java IDE market looked back in &lsquo;96, and people will come to 
fill it.</li> 
<li><strong>NOW:</strong> Xcode 4 is &quot;better&quot;, but it's still not 
what I would call comparable to the Microsoft Visual Studio or JetBrains IDEA 
experience. LLVM is definitely a better platform for the company's development 
efforts, long-term, and it's encouraging that they're investing so heavily into 
it, but I still wish the overall development experience was stronger. 
Meanwhile, though, no Eclipse plugin has emerged (that I'm aware of), which 
surprised me, and neither did we see Microsoft trying to step into that world, 
which doesn't surprise me, but disappoints me just a little. I realize that 
Microsoft's developer tools are generally designed to support the Windows 
operating system first, but Microsoft has to cut loose from that perspective if 
they're going to survive as a company. More on that later.</li> </ul> </li> 
<li> <strong>THEN:</strong> <em>NoSQL buzz grows.</em> The NoSQL movement, 
which sort of got started last year, will reach significant states of buzz this 
year. NoSQL databases have a lot to offer, particularly in areas that 
relational databases are weak, such as hierarchical kinds of storage 
requirements, for example. That buzz will reach a fever pitch this year, and 
the relational database moguls (Microsoft, Oracle, IBM) will start to fight 
back.
<ul> 
<li><strong>NOW:</strong> Well, the buzz certainly grew, and it surprised me 
that the big storage guys (Microsoft, IBM, Oracle) didn't do more to address 
it; I was expecting features to emerge in their database products to address 
some of the features present in MongoDB or CouchDB or some of the others, such 
as &quot;schemaless&quot; or map/reduce-style queries. Even just incorporating 
JavaScript into the engine somewhere would've generated a reaction.</li> </ul> 
</li> </ul> 
<p>Overall, it appears I'm running at about my usual 50/50 levels of 
prognostication. So be it. Let's see what the ol' crystal ball has in mind for 
2012:</p> 
<ul> 
<li><em>Lisps will be the languages to watch.</em> With Clojure leading the 
way, Lisps (that is, languages that are more or less loosely based on Common 
Lisp or one of its variants) are slowly clawing their way back into the 
limelight. Lisps are both functional languages as well as dynamic languages, 
which gives them a significant reason for interest. Clojure runs on top of the 
JVM, which makes it highly interoperable with other JVM languages/systems, and 
Clojure/CLR is the version of Clojure for the CLR platform, though there seems 
to be less interest in it in the .NET world (which is a mistake, if you ask me).
</li> 
<li><em>Functional languages will.... I have no idea.</em> As I said above, 
I'm kind of stymied on the whole functional-language thing and their future. I 
keep thinking they will either &quot;take off&quot; or &quot;drop off&quot;, 
and they keep tacking to the middle, doing neither, just sort of hanging in 
there as a concept for programmers to take and run with. Mind you, I like 
functional languages, and I want to see them become mainstream, or at least 
more so, but I keep wondering if the mainstream programming public is ready to 
accept the ideas and concepts hiding therein. So this year, let's try something 
different: I predict that they will remain exactly where they are, neither 
&quot;done&quot; nor &quot;accepted&quot;, but continue next year to sort of 
hang out in the middle.</li> 
<li><em>F#'s type providers will show up in C# v.Next.</em> This one is 
actually a &quot;gimme&quot;, if you look across the history of F# and C#: for 
almost every version of F# v.&quot;N&quot;, features from that version show up 
in C# v.&quot;N+1&quot;. More importantly, F# 3.0's type provider feature is an 
amazing idea, and one that I think will open up language research in some<em>
very</em> interesting ways. (Not sure what F#'s type providers are or what 
they'll do for you? Check outDon Syme's talk on it at BUILD last year.)</li> 
<li><em>Windows8 will generate a lot of chatter.</em> As 2012 progresses, 
Microsoft will try to force a lot of buzz around it by keeping things under 
wraps until various points in the year that feel strategic (TechEd, BUILD, 
etc). In doing so, though, they will annoy a number of people by not talking 
about them more openly or transparently. What's more....</li> 
<li><em>Windows8 (&quot;Metro&quot;)-style apps won't impress at first.</em> 
The more I think about it, the more I'm becoming convinced that Metro-style 
apps on a desktop machine are going to collectively underwhelm. The UI simply 
isn't designed for keyboard-and-mouse kinds of interaction, and that's going to 
be the hardware setup that most people first experience Windows8 on--contrary 
to what (I think) Microsoft thinks, people do not just have tablets laying 
around waiting for Windows 8 to be installed on it, nor are they going to buy a 
Windows8 tablet just to try it out, at least not until it's gathered some mojo 
behind it. Microsoft is going to have to finesse the messaging here very, very 
finely, and that's not something they've shown themselves to be particularly 
good at over the last half-decade.</li> 
<li><em>Scala will get bigger, thanks to Heroku.</em> With the adoption of 
Scala and Play for their Java apps, Heroku is going to make Scala look 
attractive as a development platform, and the adoption of Play by Typesafe (the 
same people who brought you Akka) means that these four--Heroku, Scala, Play 
and Akka--will combine into a very compelling and interesting platform. I'm 
looking forward to seeing what comes of that.</li> 
<li><em>Cloud will continue to whip up a lot of air.</em> For all the hype and 
money spent on it, it doesn't really seem like cloud is gathering commensurate 
amounts of traction, across all the various cloud providers with the possible 
exception of Amazon's cloud system. But, as the different cloud platforms start 
to diversify their platform technology (Microsoft seems to be leading the way 
here, ironically, with the introduction of Java, Hadoop and some limited NoSQL 
bits into their Azure offerings), and as we start to get more experience with 
the pricing and costs of cloud, 2012 might be the year that we start to see 
mainstream cloud adoption, beyond &quot;just&quot; the usage patterns we've 
seen so far (as a backing server for mobile apps and as an easy way to spin up 
startups).</li> 
<li><em>Android tablets will start to gain momentum.</em> Amazon's Kindle Fire 
has hit the market strong, definitely better than any other Android-based 
tablet before it. The Nooq (the Kindle's principal competitor, at least in the 
e-reader world) is also an Android tablet, which means that right now, 
consumers can get into the Android tablet world for far, far less than what an 
iPad costs. Apple rumors suggest that they may have a 7&quot; form factor 
tablet that will price competitively (in the $200/$300 range), but that's just 
rumor right now, and Apple has never shown an interest in that form factor, 
which means the 7&quot; world will remain exclusively Android's (at least for 
now), and that's a nice form factor for a lot of things. This translates well 
into more sales of Android tablets in general, I think.</li> 
<li><em>Apple will release an iPad 3, and it will be &quot;more of the 
same&quot;.</em> Trying to predict Apple is generally a lost cause, 
particularly when it comes to their vaunted iOS lines, but somewhere around the 
middle of the year would be ripe for a new iPad, at the very least. (With the 
iPhone 4S out a few months ago, it's hard to imagine they'd cannibalize those 
sales by releasing a new iPhone, until the end of the year at the earliest.) 
Frankly, though, I don't expect the iPad 3 to be all that big of a boost, just 
a faster processor, more storage, and probably about the same size. Probably 
the only thing I'd want added to the iPad would be a USB port, but that 
conflicts with the Apple desire to present the iPad as a &quot;device&quot;, 
rather than as a &quot;computer&quot;. (USB ports smack of 
&quot;computers&quot;, not self-contained &quot;devices&quot;.)</li> 
<li><em>Apple will get hauled in front of the US government for... something.
</em> Apple's recent foray in the legal world, effectively informing Samsung 
that they can't make square phones and offering advice as to what will avoid 
future litigation, smacks of such hubris and arrogance, it makes Microsoft look 
like a Pollyanna Pushover by comparison. It is pretty much a given, it seems to 
me, that a confrontation in the legal halls is not far removed, either with the 
US or with the EU, over anti-cometitive behavior. (And if this kind of behavior 
continues, and there is no legal action, it'll be pretty apparent that Apple 
has a pretty good set of US Congressmen and Senators in their pocket, something 
they probably learned from watching Microsoft and IBM slug it out rather than 
just buy them off.)</li> 
<li><em>IBM will be entirely irrelevant again.</em> Look, IBM's main 
contribution to the Java world is/was Eclipse, and to a much lesser degree, 
Harmony. With Eclipse more or less &quot;done&quot; (aside from all the work on 
plugins being done by third parties), and with IBM abandoning Harmony in favor 
of OpenJDK, IBM more or less removes themselves from the game, as far as 
developers are concerned. Which shouldn't really be surprising--they've been 
more or less irrelevant pretty much ever since the mid-2000s or so.</li> 
<li><em>Oracle will &quot;screw it up&quot; at least once.</em> Right now, the 
Java community is poised, like a starving vulture, waiting for Oracle to do 
something else that demonstrates and befits their Evil Emperor status. The 
community has already been quick (far too quick, if you ask me) to highlight 
Oracle's supposed missteps, such as the JVM-crashing bug (which has already 
been fixed in the _u1 release of Java7, which garnered no attention from the 
various Java news sites) and the debacle around 
Hudson/Jenkins/whatever-the-heck-we-need-to-call-it-this-week. I'll grant you, 
the Hudson/Jenkins debacle was deserving of ire, but Oracle is hardly the Evil 
Emperor the community makes them out to be--at least, so far. (I'll admit it, 
though, I'm a touch biased, both because Brian Goetz is a friend of mine and 
because Oracle TechNet has asked me to write a column for them next year. 
Still, in the spirit of &quot;innocent until proven guilty&quot;....)</li> 
<li><em>VMWare/SpringSource will start pushing their cloud solution in a major 
way.</em> Companies like Microsoft and Google are pushing cloud solutions 
because Software-as-a-Service is a reoccurring revenue model, generating 
revenue even in years when the product hasn't incremented. VMWare, being a 
product company, is in the same boat--the only time they make money is when 
they sell a new copy of their product, unless they can start pushing their 
virtualization story onto hardware on behalf of clients--a.k.a. &quot;the 
cloud&quot;. With SpringSource as the software stack, VMWare has a more-or-less 
complete cloud play, so it's surprising that they didn't push it harder in 
2011; I suspect they'll start cramming it down everybody's throats in 2012. 
Expect to see Rod Johnson talking a lot about the cloud as a result.</li> 
<li><em>JavaScript hype will continue to grow, and by years' end will be at 
near-backlash levels.</em> JavaScript (more properly known as ECMAScript, not 
that anyone seems to care but me) is gaining all kinds of steam as a mainstream 
development language (as opposed to just-a-browser language), particularly with 
the release of NodeJS. That hype will continue to escalate, and by the end of 
the year we may start to see a backlash against it. (Speaking personally, 
NodeJS is an interesting solution, but suggesting that it will replace your 
Tomcat or IIS server is a bit far-fetched; event-driven I/O is something both 
of those servers have been doing for years, and the rest of it is 
&quot;just&quot; a language discussion. We could pretty easily use JavaScript 
as the development language inside both servers, as Sun demonstrated years ago 
with their &quot;Phobos&quot; project--not that anybody really cared back then.)
</li> 
<li><em>NoSQL buzz will continue to grow, and by years' end will start to 
generate a backlash.</em> More and more companies are jumping into NoSQL-based 
solutions, and this trend will continue to accelerate, until some extremely 
public failure will start to generate a backlash against it. (This seems to be 
a pattern that shows up with a lot of technologies, so it seems entirely 
realistic that it'll happen here, too.) Mind you, I don't mean to suggest that 
the backlash will be factual or correct--usually these sorts of things come 
from misuing the tool, not from any intrinsic failure in it--but it'll generate 
some bad press.</li> 
<li><em>Ted will thoroughly rock the house during his CodeMash keynote.</em> 
Yeah, OK, that's more of a fervent wish than a prediction, but hey, keep a 
positive attitude and all that, right?</li> 
<li><em>Ted will continue to enjoy his time working for Neudesic.</em> So far, 
it's been great working for these guys, and I'm looking forward to a great 2012 
with them. (Hopefully this will be a prediction I get to tack on for many years 
to come, too.)</li> </ul> 
<p>I hope that all of you have enjoyed reading these, and I wish you and yours 
a very merry, happy, profitable and fulfilling 2012. Thanks for reading.</p> 
<br> 
<p></p> .NET&nbsp;|&nbsp; Android&nbsp;|&nbsp; C#&nbsp;|&nbsp; C++&nbsp;|&nbsp;
Conferences&nbsp;|&nbsp; Development Processes&nbsp;|&nbsp; F#&nbsp;|&nbsp; 
Flash&nbsp;|&nbsp; Industry&nbsp;|&nbsp; iPhone&nbsp;|&nbsp; Java/J2EE
&nbsp;|&nbsp;Languages&nbsp;|&nbsp; LLVM&nbsp;|&nbsp; Mac OS&nbsp;|&nbsp; 
Objective-C&nbsp;|&nbsp; Parrot&nbsp;|&nbsp; Personal&nbsp;|&nbsp; Ruby
&nbsp;|&nbsp;Scala&nbsp;|&nbsp; VMWare&nbsp;|&nbsp; Windows <br>
<br>
 Sunday, 
01 January 2012 22:17:28 (Pacific Standard Time, UTC-08:00)<br>
Comments [2] 
&nbsp;|&nbsp;<br>
<br>
<br>
&nbsp;Tuesday, 27 December 2011 <br>
Changes, 
changes, changes <br>

<p>Many of you have undoubtedly noticed that my blogging has dropped off 
precipitously over the last half-year. The reason for that is multifold, 
ranging from the usual &ldquo;I just don&rsquo;t seem to have the time for 
it&rdquo; rationale, up through the realization that I have a couple of regular 
(paid) columns (one with CoDe Magazine, one with MSDN) that consume a lot of my 
ideas that would otherwise go into the blog.</p> 
<p>But most of all, the main reason I&rsquo;m finding it harder these days to 
blog is that as of July of this year, I have joined forces withNeudesic, LLC, 
as a full-time employee, working as an Architectural Consultant for them.</p> 
<p>Neudesic is a Microsoft partner (as a matter of fact, as I understand it we 
were Microsoft&rsquo;s Partner of the Year not too long ago), with several 
different technology practices, including a Mobile practice, a User Experience 
practice, a Connected Systems practice, and a Custom Application Development 
practice, among others. The company is (as of this writing) about 400 
consultants strong, with a number of Microsoft MVPs and Regional Directors on 
staff, including a personal friend of mine, Simon Guest, who heads up the 
Mobile Practice, and another friend, Rick Garibay, who is the Practice Director 
for Connected Systems. And that doesn&rsquo;t include the other friends I have 
within the company, as well as the people within the company who are quickly 
becoming new friends. I&rsquo;m even more tickled that I was instrumental in 
bringing Steven &ldquo;Doc&rdquo; List in, to bring his agile experience and 
perspective to our projects nationwide. (Plus I just like working with Doc.)</p>
<p>It&rsquo;s been a great partnership so far: they ask me to continue doing 
the speaking and writing that I love to do, bringing fame and glory (I hope!) 
to the Neudesic name, and in turn I get to jump in on a variety of different 
projects as an architect and mentor. The people I&rsquo;m working with are 
great, top-notch technology experts and just some of the nicest people 
I&rsquo;ve met. Plus, yes, it&rsquo;s nice to draw a regular bimonthly paycheck 
and benefits after being an independent for a decade or so.</p> 
<p>The fact that they&rsquo;re principally a .NET shop may lead some to 
conclude that this is my farewell letter to the Java community, but in fact the 
opposite is the case. I&rsquo;m actively engaged with our Mobile practice 
around Android (and iOS) development, and I&rsquo;m subtly and covertly (sssh! 
Don&rsquo;t tell the partners!) trying to subvert the company into expanding 
our technology practices into the Java (and Ruby/Rails) space.</p> 
<p>With the coming new year, I think one of my upcoming responsibilities will 
be to blog more, so don&rsquo;t be too surprised if you start to see more 
activity on a more regular basis here. But in the meantime, I&rsquo;m working 
on my end-of-year predictions and retrospective, so keep an eye out for that in 
the next few days.</p> 
<p>(Oh, and that link that appears across the bottom of my blog posts? Someday 
I&rsquo;m going to remember how to change the text for that in the blog engine 
and modify it to read something more Neudesic-centric. But for now, it&rsquo;ll 
work.)</p> <br>

<p></p> .NET&nbsp;|&nbsp; Android&nbsp;|&nbsp; Azure&nbsp;|&nbsp; C#
&nbsp;|&nbsp;C++&nbsp;|&nbsp; Conferences&nbsp;|&nbsp; Development Processes
&nbsp;|&nbsp;F#&nbsp;|&nbsp; Flash&nbsp;|&nbsp; Industry&nbsp;|&nbsp; iPhone
&nbsp;|&nbsp;Java/J2EE&nbsp;|&nbsp; Languages&nbsp;|&nbsp; Mac OS&nbsp;|&nbsp; 
Personal&nbsp;|&nbsp; Ruby&nbsp;|&nbsp; Scala&nbsp;|&nbsp; Security&nbsp;|&nbsp;
Social&nbsp;|&nbsp; Visual Basic&nbsp;|&nbsp; WCF&nbsp;|&nbsp; XML Services <br>
<br> Tuesday, 27 December 2011 13:53:14 (Pacific Standard Time, UTC-08:00) <br>

Comments [0] &nbsp;|&nbsp; <br>
<br>
<br>
&nbsp;Tuesday, 26 April 2011 <br>

Managing Talks: An F#/Office Love Story (Part 1) <br>

<p>Those of you who&rsquo;ve seen me present at conferences probably 
won&rsquo;t be surprised by this, but I do a lot of conference talks. In fact, 
I&rsquo;m doing an average of 10 or so talks at the NFJS shows alone. When you 
combine that with all the talks I&rsquo;ve done over the past decade, 
it&rsquo;s reached a point where maintaining them all has begun to approach the 
unmanageable. For example, when the publication of<em>Professional F# 2.0</em> 
went final, I found myself going through slide decks trying to update all the 
&ldquo;Credentials&rdquo; slides to reflect the new publication date (and 
title, since it changed to<em>Professional F# 2.0</em> fairly late in the 
game), and frankly, it&rsquo;s becoming something of a pain in the ass. I 
figured, in the grand traditions of &ldquo;itch-scratching&rdquo;, to try and 
solve it.</p> 
<p>Since (as many past attendees will tell you) my slides are generally not 
the focus of my presentations, I realized that my slide-building needs are 
pretty modest. In particular, I want:</p> 
<ul> 
<li>a fairly easy mechanism for doing text, including bulleted and 
non-bulleted (yet still indented) lists</li> 
<li>a &ldquo;section header&rdquo; scheme that allows me to put a slide in 
place that marks a new section of slides</li> 
<li>some simple metadata, from which I can generate a &ldquo;list of all my 
talks&rdquo; page, such as what&rsquo;s behind the listing at
http://www.tedneward.com/speaking.aspx. (Note that I realize it&rsquo;s a pain 
in the *ss to read through them all; a later upgrade to the site is going to 
add a categorization/filter feature to the HTML, probably using jQuery or 
something.)</li> </ul> 
<p>So far, this is pretty simple stuff. For reasons of easy parsing, I want to 
start with an XML format, but keep the verbosity to a minimum; in other words, 
I&rsquo;m OK with XML so long as it merely reflects the structure of the slide 
deck, and doesn&rsquo;t create a significant overhead in creating the text for 
the slides.</p> 
<p>And note that I&rsquo;m deliberately targeting PowerPoint with this tool, 
since that&rsquo;s what I use, but there&rsquo;s nothing offhand that prevents 
somebody from writing a tool to drive Apple&rsquo;s Keynote (presumably using 
Applescript and/or Apple events) or OpenOffice (using their Java SDK). Because 
the conferences I speak to are all more or less OK with PowerPoint (or PDF, 
which is easy to generate from PPT) format, that&rsquo;s what I&rsquo;m using. 
(If you feel like I&rsquo;m somehow cheating you by not supporting either of 
the other two, consider this a challenge to generate something similar for 
either or both. But I feel no such challenge, so don&rsquo;t be looking at me 
any time soon.)</p> 
<p>(OK, I admit it, I may get around to it someday. But not any time soon.)</p>
<p>(Seriously.)</p> 
<p>As a first cut, I want to work from a format like the following:</p> 
<pre>&lt;presentation xmlns:xi=&quot;http://www.w3.org/2003/XInclude&quot;&gt; 
&lt;head&gt; &lt;title&gt;Busy Java Developer's Guide|to 
Android:Basics&lt;/title&gt; &lt;abstract&gt; This is the abstract for a sample 
talk. This is the second paragraph for an abstract. &lt;/abstract&gt; 
&lt;audience&gt;For any intermediate Java (2 or more years) 
audience&lt;/audience&gt; &lt;/head&gt; &lt;xi:include 
href=&quot;Testing/external-1.xml&quot; parse=&quot;xml&quot; /&gt; &lt;!-- 
Test bullets --&gt; &lt;slide title=&quot;Concepts&quot;&gt; * Activities * 
Intents * Services * Content Providers &lt;/slide&gt; &lt;!-- Test up to three- 
four- and five-level nesting --&gt; &lt;slide title=&quot;Tools&quot;&gt; * 
Android tooling consists of: ** JDK 1.6.latest ** Android SDK *** Android SDK 
installer/updater **** Android libraries &amp; documentation (versioned) ***** 
Android emulator ***** ADB ** an Android device (optional, sort of) ** IDE 
w/Android plugins (optional) *** Eclipse is the oldest; I don&rsquo;t 
particularly care for it *** IDEA 10 rocks; Community Edition is free *** Even 
NetBeans has an Android plugin &lt;/slide&gt; &lt;!-- Test bulletless indents 
--&gt; &lt;slide title=&quot;Objectives&quot;&gt; My job... - ... is to test 
this tool -- ... is to show you enough Android to make you dangerous --- ... 
because I can't exhaustively cover the entire platform in just one conference 
session ---- ... I will show you the (barebones) tools ----- ... I will show 
you some basics &lt;/slide&gt; &lt;!-- Test section header --&gt; &lt;section 
title=&quot;Getting Dirty&quot; quote=&quot;In theory, there's no 
difference|between theory and practice.|In practice, however...&quot; 
attribution=&quot;Yogi Berra&quot; /&gt; &lt;/presentation&gt;</pre> 
<p>You&rsquo;ll notice the XInclude namespace declaration in the top-level 
element; its purpose there is pretty straightforward, demonstrated in the 
&ldquo;credentials&rdquo; slide a few lines later, so that not only can I 
modify the &ldquo;credentials&rdquo; slide that appears in all my decks, but 
also do a bit of slide-deck reuse, using slides to describe concepts that apply 
to multiple decks (like a set of slides describing functional concepts for 
talks on F#, Scala,Clojure or others). Given that it&rsquo;s (mostly) an XML 
document, it&rsquo;s not that hard to imagine the XML parsing parts of it. 
We&rsquo;ll look at that later.</p> 
<p>The interesting part of this is the other part of this, the PowerPoint 
automation used to drive the generation of the slides. Like all .NET languages, 
F# can drive Office just as easily as C# can. Thus, it&rsquo;s actually pretty 
reasonable to imagine a simple F# driver that opens the XML file, parses it, 
and uses what it finds there to drive the creation of slides.</p> 
<p>But before I immediately dive into creating slides, one of the things I 
want my slide decks to have is a common look-and-feel to them; in some cases, 
PowerPoint gurus will start talking about &ldquo;themes&rdquo;, but I&rsquo;ve 
found it vastly easier to simply start from an empty PPT deck that has some 
&ldquo;slide masters&rdquo; set up with the layouts, fonts, colors, and so on, 
that I want. This approach will be no different: I want a class that will open 
a &ldquo;template&rdquo; PPT, modify the heck out of it, and save it as the new 
PPT.</p> 
<p>Thus, one of the first places to start is with an F# type that does this 
precise workflow:</p> 
<pre>type PPTGenerator(inputPPTFilename : string) = let app = 
ApplicationClass(Visible = MsoTriState.msoTrue, DisplayAlerts = 
PpAlertLevel.ppAlertsAll) let pres = app.Presentations.Open(inputPPTFilename) 
member this.Title(title : string) : unit = let workingTitle = 
title.Replace(&quot;|&quot;, &quot;\n&quot;) let slides = pres.Slides let slide 
= slides.Add(1, PpSlideLayout.ppLayoutTitle) slide.Layout &lt;- 
PpSlideLayout.ppLayoutTitle let textRange = 
slide.Shapes.Item(1).TextFrame.TextRange textRange.Text &lt;- workingTitle 
textRange.Font.Size &lt;- 30.0f let infoRng = 
slide.Shapes.Item(2).TextFrame.TextRange infoRng.Text &lt;- &quot;\rTed 
Neward\rNeward &amp; Associates\rhttp://www.tedneward.com | 
ted@tedneward.com&quot; infoRng.Font.Size &lt;- 20.0f let copyright = 
&quot;Copyright (c) &quot; + System.DateTime.Now.Year.ToString() + &quot; 
Neward &amp; Associates. All rights reserved.\r&quot; + &quot;This presentation 
is intended for informational use only.&quot; 
pres.HandoutMaster.HeadersFooters.Header.Text &lt;- &quot;Neward &amp; 
Associates&quot; pres.HandoutMaster.HeadersFooters.Footer.Text &lt;- copyright
</pre> 
<p>The class isn&rsquo;t done, obviously, but it gives a basic feel to 
what&rsquo;s happening here: app and pres are members used to represent the 
PowerPoint application itself, and the presentation I&rsquo;m modifying, 
respectively. Notice the use of F#&rsquo;s ability to modify properties as part 
of the new() call when creating an instance of app; this is so that I can watch 
the slides being generated (which is useful for debugging, plus I&rsquo;ll want 
to look them over during generation, just as a sanity-check, before saving the 
results).</p> 
<p>The Title() method is used to do exactly what its name implies: generate a 
title slide, using the built-in slide master for that purpose. This is probably 
the part that functional purists are going to go ballistic over&mdash;clearly 
I&rsquo;m using tons of mutable property references, rather than a more 
functional transformation, but to be honest, this is just how Office works. It 
was either this, or try generating PPTX files (which are intrinsically XML) by 
hand, and thank you, no, I&rsquo;m not<em>that</em> zealous about functional 
purity that I&rsquo;m going to sign up for that gig.</p> 
<p>One thing that was a royal pain to figure out: the block of text (infoRng) 
is a single TextRange, but to control the formatting a little, I wanted to make 
sure the three lines were line-breaks in just the right places. I tried doing 
multiple TextRanges, but that became a problem when working with bulleted 
lists. After much, much frustration, I finally discovered that PowerPoint 
really wants you to embed &ldquo;\r&rdquo; carriage-return-line-feeds into the 
text directly.</p> 
<p>You&rsquo;ll also notice that I use the &ldquo;|&rdquo; character in the 
raw title to embed a line break as well; this is because I frequently use long 
titles like &ldquo;The Busy .NET Developer&rsquo;s Guide to Underwater 
Basketweaving.NET&rdquo;, and want to break the title right after 
&ldquo;Guide&rdquo;. Other than that, it&rsquo;s fairly easy to see 
what&rsquo;s going on here&mdash;two TextRanges, corresponding to the yellow 
center right-justified section and the white bottom center-justified section, 
each of which is set to a particular font size (which must be specified in 
float32 values) and text, and we&rsquo;re good.</p> 
<p>(To be honest, this particular method could be replaced by a different 
mechanism I&rsquo;m going to show later, but this gives a feel for what driving 
the PowerPoint model looks like.)</p> 
<p>Let&rsquo;s drive what we&rsquo;ve got so far, using a simple main:</p> 
<pre>let main (args : string array) : unit = let pptg = new 
PPTGenerator(&quot;C:\Projects\Presentations\Templates\__Template.ppt&quot;) 
pptg.Title(&quot;Howdy, PowerPoint!&quot;) () 
main(System.Environment.GetCommandLineArgs())</pre> 
<p>When run, this is what the slide (using my template, though any base 
PowerPoint template *should* work) looks like:</p> 
<p></p> 
<p>Not bad, for a first pass.</p> 
<p>I&rsquo;ll go over more of it as I build out more of it (actually, truth be 
told, much more of it is already built out, but I want to show it in manageable 
chunks), but as a highlight, here&rsquo;s some of the features I either have 
now or I&rsquo;m going to implement:</p> 
<ul> 
<li>as mentioned, XIncluding from other XML sources to allow for reusable 
sections. I have this already.</li> 
<li>&ldquo;code slides&rdquo;: slides with code fragments on them. Ideally, 
the font will be color syntax highlighted according to the language, but 
that&rsquo;s way down the road. Also ideally, the code would be sucked in from 
compilable source files, so that I could make sure the code compiles before 
embedding it on the page, but that&rsquo;s also way down the road, too.</li> 
<li>markup formats supporting *bold*, _underline_ and ^italic^ inline text. If 
I don&rsquo;t get here, it won&rsquo;t be the end of the world, but it&rsquo;d 
be nice.</li> 
<li>the ability to &ldquo;import&rdquo; an existing set of slides (in PPT 
format) into this presentation. This is my &ldquo;escape hatch&rdquo;, to get 
to functionality that I don&rsquo;t use often enough that I want to try and 
capture it in text files yet still want to use. I have this already, too.</li> 
</ul> 
<p>I&rsquo;m not advocating this as a generalized replacement of PowerPoint, 
by the way, which is why importing existing slides is so critical: for anything 
that&rsquo;s outside of the 20% of the core functionality I need (animations 
and/or very particular layout come to mind), I&rsquo;ll just write a slide in 
PowerPoint and import it directly into the results. The goal here is to make it 
ridiculously easy to whip a slide deck up and reuse existing material as I 
desire, without going too far and trying to solve that last mile.</p> 
<p>And if you find it inspirational or useful, well&hellip; cool.</p> <br>

<p></p> .NET&nbsp;|&nbsp; Conferences&nbsp;|&nbsp; Development Processes
&nbsp;|&nbsp;F#&nbsp;|&nbsp; Windows <br>
<br>
 Tuesday, 26 April 2011 23:15:13 
(Pacific Daylight Time, UTC-07:00)<br>
Comments [0] &nbsp;|&nbsp; <br>
<br>
<br>
&nbsp;Saturday, 01 January 2011<br>
Tech Predictions, 2011 Edition <br>

<p>Long-time readers of this blog know what&rsquo;s coming next: it&rsquo;s 
time for Ted to prognosticate on what the coming year of tech will bring us. 
But I believe strongly in accountability, even in my offered-up-for-free 
predictions, so one of the traditions of this space is to go back and revisit 
my predictions from this time last year. So, without further ado, let&rsquo;s 
look back atTed&rsquo;s 2010 predictions, and see how things played out; 2010 
predictions are prefixed with &ldquo;THEN&rdquo;, and my thoughts on my 
predictions are prefixed with &ldquo;NOW&rdquo;:</p> 
<p>For 2010, I predicted....</p> 
<ul> 
<li><strong>THEN: </strong><em>... I will offer 3- and 4-day training classes 
on F# and Scala, among other things.</em> OK, that's not fair&mdash;yes, I have 
the materials, I just need to work out locations and times. Contact me if 
you're interested in a private class, by the way.</li> 
<ul> 
<li><strong>NOW:</strong> Well, I offered them&hellip; I just didn&rsquo;t do 
much to advertise them or sell them. I got plenty busy just with the other 
things I had going on. Besides, this and the next prediction were pretty much 
all advertisement anyway, so I don&rsquo;t know if anybody really counts these 
two.</li> </ul> 
<li><strong>THEN: </strong><em>... I will publish two books, one on F# and one 
on Scala.</em> OK, OK, another plug. Or, rather, more of a resolution. One will 
be the &quot;Professional F#&quot; I'm doing for Wiley/Wrox, the other isn't 
yet finalized. But it'll either be published through a publisher, or 
self-published, by JavaOne 2010.</li> 
<ul> 
<li><strong>NOW:</strong> &ldquo;Professional F# 2.0&rdquo; shipped in Q3 of 
2010; the other Scala book I decided not to pursue&mdash;too much stuff going 
on to really put the necessary time into it. (Cue sad trombone.)</li> </ul> 
<li><strong>THEN: </strong><em>... DSLs will either &quot;succeed&quot; this 
year, or begin the short slide into the dustbin of obscure programming ideas.
</em> Domain-specific language advocates have to put up some kind of strawman 
for developers to learn from and poke at, or the whole concept will just fade 
away. Martin's book will help, if it ships this year, but even that might not 
be enough to generate interest if it doesn't have some kind of large-scale 
applicability in it. Patterns and refactoring and enterprise containers all had 
a huge advantage in that developers could see pretty easily what the problem 
was they solved; DSLs haven't made that clear yet.</li> 
<ul> 
<li><strong>NOW:</strong> To be honest, this one is hard to call. Martin 
Fowler published his DSL book, which many people consider to be a good sign of 
what&rsquo;s happening in the world, but really, the DSL buzz seems to have 
dropped off significantly. The strawman hasn&rsquo;t appeared in any meaningful 
public way (I still don&rsquo;t see an example being offered up from anybody), 
and that leads me to believe that the fading-away has started.</li> </ul> 
<li><strong>THEN: </strong><em>... functional languages will start to see a 
backlash.</em> I hate to say it, but &quot;getting&quot; the functional mindset 
is hard, and there's precious few resources that are making it easy for 
mainstream (read: O-O) developers make that adjustment, far fewer than there 
was during the procedural-to-object shift. If the functional community doesn't 
want to become mainstream, then mainstream developers will find ways to take 
functional's most compelling gateway use-case (parallel/concurrent programming) 
and find a way to &quot;git 'er done&quot; in the traditional O-O approach, 
probably through software transactional memory, and functional languages like 
Haskell and Erlang will be relegated to the &quot;What Might Have Been&quot; of 
computer science history. Not sure what I mean? Try this: walk into a 
functional language forum, and ask what a monad is. Nobody yet has been able to 
produce an answer that doesn't involve math theory, or that does involve a 
practical domain-object-based example. In fact, nobody has really said why (or 
if) monads are even still useful. Or catamorphisms. Or any of the other 
dime-store words that the functional community likes to toss around.</li> 
<ul> 
<li><strong>NOW:</strong> I think I have to admit that this hasn&rsquo;t 
happened&mdash;at least, there&rsquo;s been no backlash that I&rsquo;ve seen. 
In fact, what&rsquo;s interesting is that there&rsquo;s been some movement to 
bring those functional concepts&mdash;including monads, which surprised me 
completely&mdash;into other languages like C# or Java for discussion and use. 
That being said, though, I don&rsquo;t see Haskell and Erlang taking center 
stage as application languages&mdash;instead, I see them taking supporting-cast 
kinds of roles building other infrastructure that applications in turn make use 
of, a la CouchDB (written in Erlang). Monads still remain a mostly-opaque 
subject for most developers, however, and it&rsquo;s still unclear if monads 
are something that people should think about applying in code, or if they are 
one of those &ldquo;in theory&rdquo; kinds of concepts. (You know, one of those 
ideas that change your brain forever, but you never actually use directly in 
code.)</li> </ul> 
<li><strong>THEN: </strong><em>... Visual Studio 2010 will ship on time, and 
be one of the buggiest and/or slowest releases in its history.</em> I hate to 
make this prediction, because I really don't want to be right, but there's just 
so much happening in the Visual Studio refactoring effort that it makes me 
incredibly nervous. Widespread adoption of VS2010 will wait until SP1 at the 
earliest. In fact....</li> 
<ul> 
<li><strong>NOW: </strong>Wow, did I get a few people here in Redmond annoyed 
with me about that one. And, as it turned out, I was pretty off-base about its 
stability. (It shipped pretty close if not exactly on the ship date Microsoft 
promised, as I recall, though I admit I wasn&rsquo;t paying too much attention 
to it.)&nbsp; I&rsquo;ve been using VS 2010 for a lot of .NET work in the last 
six months, and I&rsquo;ve yet (knock on wood) to have it crash on me. /bow 
Visual Studio team.</li> </ul> 
<li><strong>THEN: </strong><em>... Visual Studio 2010 SP 1 will ship within 
three months of the final product.</em> Microsoft knows that people wait until 
SP 1 to think about upgrading, so they'll just plan for an eager SP 1 release, 
and hope that managers will be too hung over from the New Year (still) to 
notice that the necessary shakeout time hasn't happened.</li> 
<ul> 
<li><strong>NOW:</strong> Uh&hellip;. nope. In fact, SP 1 has just reached a 
beta/CTP state. As for managers being too hung over, well&hellip;</li> </ul> 
<li><strong>THEN: </strong><em>... Apple will ship a tablet with multi-touch 
on it, and it will flop horribly.</em> Not sure why I think this, but I just 
don't think the multi-touch paradigm that Apple has cooked up for the iPhone 
will carry over to a tablet/laptop device. That won't stop them from shipping 
it, and it won't stop Apple fan-boiz from buying it, but that's about where the 
interest will end.</li> 
<ul> 
<li><strong>NOW:</strong> Oh, WOW did I come so close and yet missed the mark 
by a mile. Of course, the &ldquo;tablet&rdquo; that Apple shipped was the iPad, 
and it did pretty much everything<em>except</em> flop horribly. Apple fan-boys 
bought it&hellip; and then about 24 hours later, so did everybody else. My<em>
mom</em> got one, for crying out loud. And folks, the iPad&mdash;along with the 
whole &ldquo;slate&rdquo; concept&mdash;is pretty clearly here to stay.</li> 
</ul> 
<li><strong>THEN: </strong><em>... JDK 7 closures will be debated for a few 
weeks, then become a fait accompli as the Java community shrugs its collective 
shoulders.</em> Frankly, I think the Java community has exhausted its interest 
in debating new language features for Java. Recent college grads and 
open-source groups with an axe to grind will continue to try and make an issue 
out of this, but I think the overall Java community just... doesn't... care. 
They just want to see JDK 7 ship someday.</li> 
<ul> 
<li><strong>NOW:</strong> Pretty close&mdash;except that closures won&rsquo;t 
ship as part of JDK 7, largely due to the Oracle acquisition in the middle of 
the year here. And I was spot-on vis-&agrave;-vis the &ldquo;they want to see 
JDK 7 ship someday&rdquo;; when given the chance to wait for a year or so for a 
Java-with-closures to ship, the community overwhelmingly voted to get something 
sooner rather than later.</li> </ul> 
<li><strong>THEN: </strong><em>... Scala either &quot;pops&quot; in 2010, or 
begins to fall apart.</em> By &quot;pops&quot;, I mean reaches a critical mass 
of developers interested in using it, enough to convince somebody to create a 
company around it, a la G2One.</li> 
<ul> 
<li><strong>NOW:</strong> &hellip; and by &ldquo;somebody&rdquo;, it turns out 
I meant Martin Odersky. Scala is pretty clearly a hot topic in the Java space, 
its buzz being disturbed only by Clojure. Scala and/or Clojure, plus Groovy, 
makes a really compelling JVM-based stack.</li> </ul> 
<li><strong>THEN: </strong><em>... Oracle is going to make a serious 
&quot;cloud&quot; play, probably by offering an Oracle-hosted version of Azure 
or AppEngine.</em> Oracle loves the enterprise space too much, and derives too 
much money from it, to not at least appear to have some kind of offering here. 
Now that they own Java, they'll marry it up against OpenSolaris, the Oracle 
database, and throw the whole thing into a series of server centers all over 
the continent, and call it &quot;Oracle 12c&quot; (c for Cloud, of course) or 
something.</li> 
<ul> 
<li><strong>NOW:</strong> Oracle made a play, but it was to continue to 
enhance Java, not build a cloud space. It surprises me that they haven&rsquo;t 
made a more forceful move in this space, but I suspect that a huge amount of 
time and energy went into folding Sun into their corporate environment.</li> 
</ul> 
<li><strong>THEN: </strong><em>... Spring development will slow to a crawl and 
start to take a left turn toward cloud ideas.</em> VMWare bought SpringSource 
for a reason, and I believe it's entirely centered around VMWare's movement 
into the cloud space&mdash;they want to be more than &quot;just&quot; a 
virtualization tool. Spring + Groovy makes a compelling development stack, 
particularly if VMWare does some interesting hooks-n-hacks to make Spring a 
virtualization environment in its own right somehow. But from a practical 
perspective, any community-driven development against Spring is all but 
basically dead. The source may be downloadable later, like the VMWare Player 
code is, but making contributions back? Fuhgeddabowdit.</li> 
<ul> 
<li><strong>NOW:</strong> The Spring One show definitely played up Cloud 
stuff, and springsource.com seems to be emphasizing cloud more in a couple of 
subtle ways. Not sure if I call this one a win or not for me, though.</li> </ul>
<li><strong>THEN: </strong><em>... the explosion of e-book readers brings the 
Kindle 2009 edition way down to size.</em> The era of the e-book reader is 
here, and honestly, while I'm glad I have a Kindle, I'm expecting that I'll be 
dusting it off a shelf in a few years. Kinda like I do with my iPods from a few 
years ago.</li> 
<ul> 
<li><strong>NOW:</strong> Honestly, can&rsquo;t say that I&rsquo;m using my 
Kindle a lot, but I am reading using the Kindle app on non-Kindle hardware more 
than I thought I would be. That said, I am eyeing the new Kindle hardware 
generation with an acquisitive eye&hellip;</li> </ul> 
<li><strong>THEN: </strong><em>... &quot;social networking&quot; becomes the 
&quot;Web 2.0&quot; of 2010.</em> In other words, using the term will basically 
identify you as a tech wannabe and clearly out of touch with the bleeding edge.
</li> 
<ul> 
<li><strong>NOW: </strong>Um&hellip;. yeah.</li> </ul> 
<li><strong>THEN: </strong><em>... Facebook becomes a developer platform 
requirement.</em> I don't pretend to know anything about Facebook&mdash;I'm not 
even on it, which amazes my family to no end&mdash;but clearly Facebook is one 
of those mechanisms by which people reach each other, and before long, it'll 
start showing up as a developer requirement for companies looking to hire. If 
you're looking to build out your resume to make yourself attractive to 
companies in 2010, mad Facebook skillz might not be a bad investment.</li> 
<ul> 
<li><strong>NOW:</strong> I&rsquo;m on Facebook, I&rsquo;ve written some code 
for it, and given how much the startup scene loves the &ldquo;Like&rdquo; 
button, I think developers who knew Facebook in 2010 did pretty well for 
themselves.</li> </ul> 
<li><strong>THEN: </strong><em>... Nintendo releases an open SDK for building 
games for its next-gen DS-based device.</em> With the spectacular success of 
games on the iPhone, Nintendo clearly must see that they're missing a huge 
opportunity every day developers can't write games for the Nintendo DS that are 
easily downloadable to the device for playing. Nintendo is not stupid&mdash;if 
they don't open up the SDK and promote &quot;casual&quot; games like those on 
the iPhone and those that can now be downloaded to the Zune or the XBox, they 
risk being marginalized out of existence.</li> 
<ul> 
<li><strong>NOW:</strong> Um&hellip; yeah. Maybe this was me just being 
hopeful.</li> </ul> </ul> 
<p>In general, it looks like I was more right than wrong, which is not a bad 
record to have. Of course, a couple of those &ldquo;wrong&rdquo;s were 
&ldquo;giving up the big play&rdquo; kind of wrongs, so while I may have a 
winning record, I still may have a defense that&rsquo;s given up too many 
points to be taken seriously. *shrug* Oh, well.</p> 
<p>What portends for 2011?</p> 
<ul> 
<li><em>Android&rsquo;s penetration into the mobile space is going to rise, 
then plateau around the middle of the year.</em> Android phones, collectively, 
have outpaced iPhone sales. That&rsquo;s a pretty significant 
statistic&mdash;and it means that there&rsquo;s fewer customers buying 
smartphones in the coming year. More importantly, the first generation of 
Android slates (including the Galaxy Tab, which I own), are less-than-sublime, 
and not really an &ldquo;iPad Killer&rdquo; device by any stretch of the 
imagination. And I think that will slow down people buying Android slates and 
phones, particularly since Google has all but promised that Android releases 
will start slowing down.</li> 
<li><em>Windows Phone 7 penetration into the mobile space will appear huge, 
then slow down towards the middle of the year.</em> Microsoft is getting some 
pretty decent numbers now, from what I can piece together, and I think 
that&rsquo;s largely the &ldquo;I love Microsoft&rdquo; crowd buying in. But 
it&rsquo;s a pretty crowded place right now with Android and iPhone, and 
I&rsquo;m not sure if the much-easier Office and/or Exchange integration is 
enough to woo consumers (who care about Office) or business types (who care 
about Exchange) away from their Androids and iPhones.</li> 
<li><em>Android, iOS and/or Windows Phone 7 becomes a developer requirement.
</em> Developers, if you haven&rsquo;t taken the time to learn how to program 
one of these three platforms, you are electing to remove yourself from a 
growing market that desperately wants people with these skills. I see the 
&ldquo;mobile native app development&rdquo; space as every bit as hot as the 
&ldquo;Internet/Web development&rdquo; space was back in 2000. If you 
don&rsquo;t have a device, buy one. If you have a device, get the 
tools&mdash;in all three cases they&rsquo;re free downloads&mdash;and start 
writing stupid little apps that nobody cares about, so you can have some skills 
on the platform when somebody cares about it.</li> 
<li><em>The Windows 7 slates will suck.</em> This isn&rsquo;t a prediction, 
this is established fact. I played with an &ldquo;ExoPC&rdquo; 10&rdquo; form 
factor slate running Windows 7 (Dell I think was the manufacturer), and it was 
a horrible experience. Windows 7, like most OSes, really expects a keyboard to 
be present, and a slate doesn&rsquo;t have one&mdash;so the OS was hacked to 
put a &ldquo;keyboard&rdquo; button at the top of the screen that would slide 
out to let you touch-type on the slate. I tried to fire up Notepad and type out 
a haiku, and it was an unbelievably awkward process. Android and iOS clearly 
own the slate market for the forseeable future, and if Dell has any brains in 
its corporate head, it will phone up Google tomorrow and start talking about 
putting Android on that hardware.</li> 
<li><em>DSLs mostly disappear from the buzz.</em> I still see no strawman (no 
&ldquo;pet store&rdquo; equivalent), and none of the traditional 
builders-of-strawmen (Microsoft, Oracle, etc) appear interested in DSLs much 
anymore, so I think 2010 will mark the last year that we spent any time talking 
about the concept.</li> 
<li><em>Facebook becomes more of a developer requirement than before.</em> I 
don&rsquo;t like Mark Zuckerburg. I don&rsquo;t like Facebook&rsquo;s privacy 
policies. I don&rsquo;t particularly like the way Facebook approaches the 
Facebook Connect experience. But Facebook owns enough people to be the 
fourth-largest nation on the planet, and probably commands an economy of 
roughly that size to boot. If your app is aimed at the Facebook demographic 
(that is, everybody who&rsquo;s not on Twitter), you have to know how to reach 
these people, and that means developing at least some part of your system to 
integrate with it.</li> 
<li><em>Twitter becomes more of a developer requirement, too.</em> Anybody 
who&rsquo;s not on Facebook is on Twitter. Or dead. So to reach the other half 
of the online community, you have to know how to connect out with Twitter.</li> 
<li><em>XMPP becomes more of a developer requirement.</em> XMPP hasn&rsquo;t 
crossed a lot of people&rsquo;s radar screen before, but Facebook decided to 
adopt it as their chat system communication protocol, and Google&rsquo;s 
already been using it, and suddenly there&rsquo;s a whole lotta traffic going 
over XMPP. More importantly, it offers a two-way communication experience that 
is in some scenarios vastly better than what HTTP offers, yet running in a very 
&ldquo;Internet-friendly&rdquo; way just as HTTP does. I suspect that XMPP is 
going to start cropping up in a number of places as a useful alternative and/or 
complement to using HTTP.</li> 
<li><em>&ldquo;Gamification&rdquo; starts making serious inroads into 
non-gaming systems.</em> Maybe it&rsquo;s just because I&rsquo;ve been talking 
more about gaming, game design, and game implementation last year, but all of a 
sudden &ldquo;gamification&rdquo;&mdash;the process of putting game-like 
concepts into non-game applications&mdash;is cresting in a big way. FourSquare, 
Yelp, Gowalla, suddenly all these systems are offering achievement badges and 
scoring systems for people who want to play in their worlds. How long is it 
before a developer is pulled into a meeting and told that &ldquo;we need to put 
achievement badges into the call-center support application&rdquo;? Or the 
online e-commerce portal? It&rsquo;ll start either this year or next.</li> 
<li><em>Functional languages will hit a make-or-break point.</em> I know, I 
said it last year. But the buzz keeps growing, and when that happens, it 
usually means that it&rsquo;s either going to reach a critical mass and 
explode, or it&rsquo;s going to implode&mdash;and the longer the buzz grows, 
the faster it explodes or implodes, accordingly. My personal guess is that the 
&ldquo;F/O hybrids&rdquo;&mdash;F#, Scala, etc&mdash;will continue to grow 
until they explode, particularly since the suggested v.Next changes to both 
Java and C# have to be done as language changes, whereas futures for F# 
frequently are either built as libraries masquerading as syntax (such as 
asynchronous workflows, introduced in 2.0) or as back-end library hooks that 
anybody can plug in (such as type providers, introduced at PDC a few months 
ago), neither of which require any language revs&mdash;and no concerns about 
backwards compatibility with existing code. This makes the F/O hybrids vastly 
more flexible and stable. In fact, I suspect that within five years or so, 
we&rsquo;ll start seeing a gradual shift away from pure O-O systems, into 
systems that use a lot more functional concepts&mdash;and that will propel the 
F/O languages into the center of the developer mindshare.</li> 
<li><em>The Microsoft Kinect will lose its shine.</em> I hate to say it, but I 
just don&rsquo;t see where the excitement is coming from. Remember when the Wii 
nunchucks were the most amazing thing anybody had ever seen? Frankly, after a 
slew of initial releases for the Wii that made use of them in interesting ways, 
the buzz has dropped off, and more importantly, the nunchucks turned out to be 
just another way to move an arrow around on the screen&mdash;in other words, we 
haven&rsquo;t found particularly novel and interesting/game-changing ways to 
use the things. That&rsquo;s what I think will happen with the Kinect. Sure, 
it&rsquo;s really freakin&rsquo; cool that you can use your body as the 
controller&mdash;but how precise is it, how quickly can it react to my body 
movements, and most of all, what new user interface metaphors are people going 
to have to come up with in order to avoid the &ldquo;me-too&rdquo; dancing-game 
clones that are charging down the pipeline right now?</li> 
<li><em>There will be no clear victor in the Silverlight-vs-HTML5 war.</em> 
And make no mistake about it, a war is brewing. Microsoft, I think, finds 
itself in the inenviable position of having two very clearly useful 
technologies, each one&rsquo;s &ldquo;sphere of utility&rdquo; (meaning, the 
range of answers to the &ldquo;where would I use it?&rdquo; question) very 
clearly overlapping. It&rsquo;s sort of like being a football team with both 
Brett Favre and Tom Brady on your roster&mdash;both of them are superstars, but 
you<em>know</em>, deep down, that you have to cut one, because you can&rsquo;t 
devote the same degree of time and energy to both. Microsoft is going to take 
most of 2011 and probably part of 2012 trying to support both, making a mess of 
it, offering up conflicting rationale and reasoning, in the end achieving 
nothing but confusing developers and harming their relationship with the 
Microsoft developer community in the process. Personally, I think Microsoft has 
no choice but to get behind HTML 5, but I like a lot of the features of 
Silverlight and think that it has a lot of mojo that HTML 5 lacks, and would 
actually be in favor of Microsoft keeping both&mdash;so long as they make it<em>
very</em> clear to the developer community when and where each should be used. 
In other words, the executives in charge of each should be locked into a room 
and not allowed out until they&rsquo;ve hammered out a business strategy that 
is then printed and handed out to every developer within a 3-continent radius 
of Redmond. (Chances of this happening: .01%)</li> 
<li><em>Apple starts feeling the pressure to deliver a developer experience 
that isn&rsquo;t mired in mid-90&rsquo;s metaphor.</em> Don&rsquo;t look now, 
Apple, but a lot of software developers are coming to your platform from Java 
and .NET, and they&rsquo;re bringing their expectations for what and how a 
developer IDE should look like, perform, and do, with them. Xcode is not a 
modern IDE, all the Apple fan-boy love for it notwithstanding, and this means 
that a few things will happen:</li> 
<ul> 
<li><em>Eclipse gets an iOS plugin.</em> Yes, I know, it wouldn&rsquo;t work 
(for the most part) on a Windows-based Eclipse installation, but if Eclipse can 
have a native C/C++ developer experience, then there&rsquo;s no reason why a 
Mac Eclipse install couldn&rsquo;t have an Objective-C plugin, and that opens 
up the idea of using Eclipse to write iOS and/or native Mac apps (which will be 
critical when the Mac App Store debuts somewhere in 2011 or 2012).</li> 
<li><em>Rumors will abound about Microsoft bringing Visual Studio to the Mac
</em>. Silverlight already runs on the Mac; why not bring the native 
development experience there? I&rsquo;m not saying they&rsquo;ll actually do 
it, and certainly not in 2011, but the rumors, they will be flyin&hellip;.</li> 
<li><em>Other third-party alternatives to Xcode will emerge and/or grow.</em> 
MonoTouch is just one example. There&rsquo;s opportunity here, just as the 
fledgling Java IDE market looked back in &lsquo;96, and people will come to 
fill it.</li> </ul> 
<li><em>NoSQL buzz grows.</em> The NoSQL movement, which sort of got started 
last year, will reach significant states of buzz this year. NoSQL databases 
have a lot to offer, particularly in areas that relational databases are weak, 
such as hierarchical kinds of storage requirements, for example. That buzz will 
reach a fever pitch this year, and the relational database moguls (Microsoft, 
Oracle, IBM) will start to fight back.</li> </ul> 
<p>I could probably go on making a few more, but I think these are enough to 
get me into trouble for the year.</p> 
<p>To all of you who&rsquo;ve been readers of this blog for the past year, I 
thank you&mdash;blog-gathered statistics tell me that I get, on average, about 
7,000 hits a day, which just stuns me&mdash;and it is a New Years&rsquo; 
Resolution that I blog more and give you even more reason to stick around. 
Happy New Year, and may your 2011 be just as peaceful, prosperous, and eventful 
as you want it to be.</p> <br>

<p></p> .NET&nbsp;|&nbsp; Android&nbsp;|&nbsp; Azure&nbsp;|&nbsp; C#
&nbsp;|&nbsp;C++&nbsp;|&nbsp; Conferences&nbsp;|&nbsp; Development Processes
&nbsp;|&nbsp;F#&nbsp;|&nbsp; Flash&nbsp;|&nbsp; Industry&nbsp;|&nbsp; iPhone
&nbsp;|&nbsp;Java/J2EE&nbsp;|&nbsp; Languages&nbsp;|&nbsp; LLVM&nbsp;|&nbsp; 
Mac OS&nbsp;|&nbsp; Objective-C&nbsp;|&nbsp; Parrot&nbsp;|&nbsp; Python
&nbsp;|&nbsp;Reading&nbsp;|&nbsp; Review&nbsp;|&nbsp; Ruby&nbsp;|&nbsp; Scala
&nbsp;|&nbsp;Security&nbsp;|&nbsp; Social&nbsp;|&nbsp; Solaris&nbsp;|&nbsp; 
Visual Basic&nbsp;|&nbsp; VMWare&nbsp;|&nbsp; WCF&nbsp;|&nbsp; Windows
&nbsp;|&nbsp;XML Services&nbsp;|&nbsp; XNA <br>
<br>
 Saturday, 01 January 2011 
02:27:11 (Pacific Standard Time, UTC-08:00)<br>
Comments [6] &nbsp;|&nbsp; <br>

<br> <br>
&nbsp;Monday, 13 December 2010 <br>
Thoughts on my first Startup 
Weekend <br>

<p>Startup Weekend came to Redmond this weekend, and as I write this it is all 
of three hours over. In the spirit of capturing post-mortem thoughts as quickly 
as possible, I thought I&rsquo;d blog my reactions and thoughts from it, both 
as a reference for myself for the next one, and as a guide/warning/data point 
for others considering doing it.</p> 
<p>A few weeks ago, emails started crossing the Seattle Tech Startup mailing 
list about this thing called &ldquo;Startup Weekend&rdquo;. I didn&rsquo;t do a 
whole lot of research around it&mdash;just glanced at the website, and asked a 
few questions of the organizer in an email. Specifically, I wanted to know that 
as a tech guy, with no specific startup ideas, I would find something to do. I 
was reassured immediately that, in fact, as a tech guy, I would be 
&ldquo;heavily recruited&rdquo; by others at the event who were business types.
</p> 
<blockquote> 
<p><em>First takeaway: I can&rsquo;t speak for all the events, this being my 
first, but it was a surprise, nay, a shock, to me just how many 
&ldquo;business&rdquo; and/or &ldquo;marketing&rdquo; types were at this event. 
I seriously expected that tech folks would outnumber the non-tech folks by a 
substantial margin, but it was exactly the opposite, probably on the order of 2 
to 1. As a developer, I was definitely being courted, rather than hunting for a 
team to find a way to make room for me. It was refreshing, exciting and a 
little overwhelming at the same time.</em></p> </blockquote> 
<p>The format of the event is interesting: anybody can pitch an idea, then 
everybody in the room is free to &ldquo;attach&rdquo; themselves to that idea 
to form a team to implement it somehow, sort of a &ldquo;Law of Two Feet&rdquo; 
applied to team-building.</p> 
<blockquote> 
<p><em>Second takeaway: Have a pretty clear idea of what you want to do here. 
The ideas initially all sound pretty good, and choosing between them can 
actually be quite painful and difficult. Have a clear goal for yourself what 
you want out of the weekend&mdash;to socialize, to stretch yourself, to build a 
business, whatever. Mine were (1) just to be here and experience the event, (2) 
to socialize and network more deeply with the startup scene, (3) to hack on 
some code and try to ship something, and (4) to learn some new tech that I 
hadn&rsquo;t had the chance to use beyond a &ldquo;Hello World&rdquo; demo 
before. There was always the chance I wouldn&rsquo;t get any of those things, 
in which case I accepted a consolation prize of simply watching how the event 
was structured and run, since it operates in many ways on the same basic 
concept that&nbsp; GiveCamp does, which is something I want to see done in 
Seattle sooner rather than later. So just by going and watching the event as a 
uninvolved observer was worth the price of admission, so once I&rsquo;d walked 
through the door, I&rsquo;d already met my #1 win condition.</em></p> 
</blockquote> 
<p>I realized as I was choosing which team to join that I didn&rsquo;t want to 
be paired alone with the project-pitching person (whomever that would be), 
since I had no idea how this event worked or what we were going for, so I 
deliberately turned away from a few projects that sounded interesting. I ended 
up as part of a team that was pretty well spread-out in terms of 
skillsets/interests (Chris, developer and &ldquo;original idea guy&rdquo;, 
Libby, business development, Maizer, also business development, Mohammed, small 
businessman, and Aaron, graphic designer), working on an idea around 
&ldquo;social bar gaming&rdquo;. In other words, we had a nebulous fuzzy idea 
about using games on a mobile device to help people in bars connect to other 
people in bars via some kind of &ldquo;scavenger hunt&rdquo; or similar 
social-engagement game. I had suggested that maybe one feature or idea would be 
to help groups of hard-drinking souls chart their path between bars (something 
like a Traveling Saleman&rsquo;s Problem meets a PubCrawl), and Chris thought 
that was definitely something to consider. We laid out a brief idea of what it 
was we wanted to build, then parted ways Friday night about midnight or so, 
except for Chris and myself, who headed out to Denny&rsquo;s to mull over some 
technology ideas for a while, until about 3 AM in the morning.</p> 
<blockquote> 
<p><em>Third takeaway: Hoard the nighttime hours on Friday, to spend them 
working on the app the rest of the weekend. Even though you&rsquo;re full of 
energy Friday night, rarin&rsquo; to go, bank it because you&rsquo;ll need to 
be well-rested for the marathon that is Saturday and Sunday.</em></p> 
</blockquote> 
<p>Chris and I briefly discussed the technology approaches we could use, and 
settled in on using Azure for the backplane, mostly because I felt it would be 
the quickest way to get us provisioned on a server, and it was an excuse for me 
to play with Azure, something I haven&rsquo;t had much of a chance to do beyond 
simple demos. We also thought that having some kind of Facebook integration 
would be a good idea, depending on what we actually wanted to do with the idea. 
I thought to myself, &ldquo;OK, so this is going to be interesting for 
me&mdash;I&rsquo;m going to be actually &lsquo;stretching&rsquo; on three 
things simultaneously: Azure, Facebook, and whatever Web framework we use to 
build this&rdquo;, since I haven&rsquo;t done much Web work in .NET in many, 
many years, and don&rsquo;t consider myself &ldquo;up to speed&rdquo; on either 
ASP.NET or ASP.NET MVC. Chris was a &ldquo;front to middle tier&rdquo; guy, 
though, so I figured I&rsquo;d focus on the Azure back-end parts&mdash;storage, 
queueing, etc&mdash;and maybe the Facebook integration, and we&rsquo;d be good.
</p> 
<p>By Saturday morning, thanks to a few other things I had to do after Chris 
left, I got there a bit late&mdash;about 10:30&mdash;fully expecting that the 
team had a pretty clear app vision laid out and ready for Chris and I to 
execute on. Alas, not quite&mdash;we were still sort of thrashing on what 
exactly we wanted to build&mdash;specifically, we kept bouncing back and forth 
between what the game would be and how it would be monetized. If we wanted to 
sell to bars as a way to get more bodies in the door, then we needed some kind 
of &ldquo;check-in&rdquo; game where people earned points for bringing friends 
to the bar. Or we could sell to bars by creating a game that was a kind of 
&ldquo;scavenger hunt&rdquo;, forcing patrons to discover things about the bar 
or about new drinks the bar sells, and so on. But we also wanted a game that 
was intrinsically social, forcing peoples&rsquo; eyes away from the screens and 
up towards the other patrons&mdash;otherwise why play the game?</p> 
<p>Aaron, a two-time veteran of Startup Weekend, suggested that we finalize 
our vision by 11 AM so we could start hacking. By 11 AM, we had a 
vision&hellip; until about an hour later, when I realized that Libby, Chris, 
Maizer, and Mohammed were changing the game to suit new monetization ideas. We 
set another deadline for 2 PM, at which point we had a vision&hellip;. until 
about an hour later again, when I looked up again and found them discussing 
again what kind of game we wanted to build. In the end, it wasn&rsquo;t until 7 
or 8 PM Saturday when we finally nailed down some kind of game app 
idea&mdash;and then only because Aaron came out of his shell a little and 
politely yelled at the group for wasting all of our time.</p> 
<blockquote> 
<p><em>Fourth takeaway: Know what&rsquo;s clear and unclear about your 
vision/idea. I think we didn&rsquo;t realize how nebulous our ideas were until 
we started trying to put game mechanics around it, and that was what led to all 
the thrashing on ideas.</em></p> 
<p><em>Fifth takeaway: Put somebody in charge. Have a dictator in place. Yes, 
everybody wants to be polite and yes, choosing a leader can be a bit 
uncomfortable, but having that final unambiguous deciding vote&mdash;a 
leader&mdash;who can make decisions and isn&rsquo;t afraid to do so would have 
saved us a lot of headache and gotten us much more quickly down the path. Libby 
said it best in our little post-mortem at the bar afterwards: Don&rsquo;t you 
dare leave Friday night until everybody is 100% clear on what you&rsquo;re 
building.</em></p> </blockquote> 
<p>Meanwhile, on the technical front, another warm front was meeting another 
cold front and developing into a storm. When we&rsquo;d decided to use Azure, I 
had suggested it over Google App Engine because Chris had said he&rsquo;d done 
some development with it before, so I figured he was comfortable with it and 
ready to go. As we started pulling out laptops to start working, though, Chris 
mentioned that he needed to spin up a virtual machine with Windows 7, Visual 
Studio, and the Azure tools in it. No worries&mdash;I needed some time to read 
up on Azure provisioning, data storage, and so on.</p> 
<p>Unfortunately, setting up the VM took until about 8 PM Saturday night, 
meaning we lost 11 of our 15 hours (9 AM to midnight) for that day.</p> 
<blockquote> 
<p><em>Sixth takeaway: Have your tools ready to go before you get there. Find 
a hosting provider&mdash;come on, everybody is going to need a hosting 
provider, even if you build a mobile app&mdash;and have a virtual machine or 
laptop configured with every dev tool you can think of, ready to go. Getting 
stuff downloaded and installed is burning a very precious commodity that you 
don&rsquo;t have nearly enough of: time.</em></p> 
<p><em>Seventh takeaway: Be clear about your personal motivation/win 
conditions for the weekend. Yes, I wanted to explore a new tech, but Chris took 
that to mean that I wasn&rsquo;t going to succeed if we abandoned Azure, and as 
a result, we burned close to 50% of our development cycles prepping a VM just 
so I could put Azure on my resume. I would&rsquo;ve happily redacted that line 
on my resume in exchange for getting us up and running by 11 AM Saturday 
morning, particularly because it became clear to me that others in the group 
were running with win conditions of &ldquo;spin up a legitimate business 
startup idea&rdquo;, and I had already met most of my win conditions for the 
weekend by this point. I should&rsquo;ve mentioned this much earlier, but 
didn&rsquo;t realize what was happening until a chance comment Chris made in 
passing Saturday night when we left for the night.</em></p> </blockquote> 
<p>Sunday I got in about noonish, owing to a long day, short night, and 
forgotten cell phone (alarm clock) in the car. By the time I got there, tempers 
were starting to flare because we were clearly well behind the curve. Chris had 
been up all night working on HTML forms for the game, Aaron had been up all 
night creating some (amazing!) graphics for the game, I had been up a 
significant part of the night diving into Facebook APIs, and I think we all 
sensed that this was in real danger of falling apart. Unfortunately, we 
couldn&rsquo;t even split the work between Chris and I, because we had 
(foolishly) not bothered to get some kind of source-control server going for 
the code so we could work in parallel.</p> 
<blockquote> 
<p><em>See the sixth takeaway. It applies to source-control servers, too. And 
source-control clients, while we&rsquo;re at it.</em></p> </blockquote> 
<p>We were slotted to present our app and business idea first, as it turned 
out, which was my preference&mdash;I figured that if we went first, we might 
set a high bar that other groups would have a hard time matching. (That turned 
out to be a really false hope&mdash;the other groups&rsquo; work was amazing.) 
The group asked me to make the pitch, which was fine with me&mdash;when have I 
ever turned down the chance to work a crowd?</p> 
<p>But our big concern was the demo&mdash;we&rsquo;d originally called for a 
&ldquo;feature freeze&rdquo; at 4PM, so we would have time to put the app on 
the server and test it, but by 4:15 Chris was still stitching pages together 
and putting images on pages. In fact, the push to the Azure server for v0.1 of 
our app happened about about 5:15, a full 30 seconds before I started the pitch.
</p> 
<p>The pitch itself was deliberately simple: we put Libby on a bar stool 
facing the crowd, Mohammed standing against a wall, and said, &ldquo;Ever been 
in a bar, wanting to strike up a conversation with that cute girl at the far 
table? With Pubbn, we give you an excuse&mdash;a social scavenger hunt&mdash;to 
strike up a conversation with her, or earn some points, or win a discount from 
the bar, or more. We want to take the usual social networking 
premise&mdash;pushing socialization into the network&mdash;and instead flip it 
on its ear&mdash;using the network to make it easier to socialize.&rdquo; It 
was a nice pitch, but I forgot to tell people to download the app and try it 
during the demo, which left some people thinking we never actually finished 
anything. ACK.</p> 
<blockquote> 
<p><em>Pubbn, by the way, was our app name, derived (obviously) from 
&ldquo;going pubbing&rdquo;, as in, going out to drink and socialize. I loved 
this name. It&rsquo;s up athttp://www.pubbn.com, but I&rsquo;ll warn you now, 
it&rsquo;s a static mockup and a far cry from what we wanted to end up 
with&mdash;in fact, I&rsquo;ll go out on a limb and say that of all the 
projects, ours was by far the weakest technical achievement, and I lay the 
blame for that at my feet. I should&rsquo;ve stepped up and taken more firm 
control of the development so Chris could focus more on the overall picture.
</em></p> </blockquote> 
<p>The eventual winners for the weekend were &ldquo;Doodle-A-Doodle&rdquo;, a 
fantastic learn-to-draw app for kids on the iPad; &ldquo;Hold It!&rdquo;, a 
game about standing in line in the mens&rsquo; room; and 
&ldquo;CamBadge&rdquo;, a brilliant little iPhone app for creating a conference 
badge on your phone, hanging your phone around your neck, and snapping a 
picture of the person standing in front of you with a single touch to the 
screen (assuming, of course, you have an iPhone 4 with its front-facing camera).
</p> 
<blockquote> 
<p><em>&ldquo;CamBadge&rdquo; was one of the apps I thought about working on, 
but passed on it because it didn&rsquo;t seem challenging enough 
technologically. Clearly that was a foolish choice from a business perspective, 
but this is why knowing what your win conditions for the weekend are so 
important&mdash;I didn&rsquo;t necessarily want to build a new business out of 
this weekend, and, to me, the more important &ldquo;win&rdquo; was to make a 
social connection with the people who looked like good folks to know in this 
space&mdash;and the &ldquo;CamBadge&rdquo; principal, Adam, clearly fit that 
bill. Drinking with him was far more important&mdash;to me&mdash;than building 
an app with him. Next Startup Weekend, my win conditions might be different, 
and if so, I&rsquo;d make an entirely different decision.</em></p> </blockquote>
<p>In the end, Startup Weekend was a blast, and something I thoroughly 
recommend every developer who&rsquo;s ever thought of going independent do. The 
cost is well, well worth the experience, and if you fail miserably, well, 
better to do that here, with so little invested, than to fail later in a 
&ldquo;real&rdquo; startup with millions attached.</p> 
<p>By the way, Startup Weekend Redmond was/is #swred on Twitter, if you want 
to see the buzz that came out of it. Particularly good reading are the Tweets 
starting at about 5 PM tonight, because that&rsquo;s when the presentations 
started.</p> <br>

<p></p> .NET&nbsp;|&nbsp; Android&nbsp;|&nbsp; Azure&nbsp;|&nbsp; C#
&nbsp;|&nbsp;Conferences&nbsp;|&nbsp; Development Processes&nbsp;|&nbsp; 
Industry&nbsp;|&nbsp; iPhone&nbsp;|&nbsp; Java/J2EE&nbsp;|&nbsp; Mac OS
&nbsp;|&nbsp;Objective-C&nbsp;|&nbsp; Review&nbsp;|&nbsp; Ruby&nbsp;|&nbsp; 
VMWare&nbsp;|&nbsp; XML Services&nbsp;|&nbsp; XNA <br>
<br>
 Monday, 13 
December 2010 01:53:24 (Pacific Standard Time, UTC-08:00)<br>
Comments [0] 
&nbsp;|&nbsp;<br>
<br>
<br>
&nbsp;Wednesday, 08 September 2010 <br>
VMWare help 
<br> 
<p>Hey, anybody who&rsquo;s got significant VMWare mojo, help out a bro?</p> 
<p>I&rsquo;ve got a Win7 VM (one of many) that appears to be exhibiting weird 
disk behavior&mdash;the vmdk, a growable single-file VMDK, is almost precisely 
twice the used space. It&rsquo;s a 120GB growable disk, and the Win7 guest 
reports about 35GB used, but the VMDK takes about 70GB on host disk. CHKDSK 
inside Windows says everything&rsquo;s good, and the VMWare &ldquo;Disk 
Cleanup&rdquo; doesn&rsquo;t change anything, either. It doesn&rsquo;t seem to 
be a Windows7 thing, because I&rsquo;ve got a half-dozen other Win7 VMs that 
operate&hellip; well, normally (by which I mean, 30GB used in the VMDK means 
30GB used on disk). It&rsquo;s a VMWare Fusion host, if that makes any 
difference. Any other details that might be relevant, let me know and 
I&rsquo;ll post.</p> 
<p>Anybody got any ideas what the heck is going on inside this disk?</p> <br>

<p></p> .NET&nbsp;|&nbsp; Android&nbsp;|&nbsp; C#&nbsp;|&nbsp; C++&nbsp;|&nbsp;
Conferences&nbsp;|&nbsp; Development Processes&nbsp;|&nbsp; F#&nbsp;|&nbsp; 
Flash&nbsp;|&nbsp; Industry&nbsp;|&nbsp; iPhone&nbsp;|&nbsp; Java/J2EE
&nbsp;|&nbsp;Languages&nbsp;|&nbsp; LLVM&nbsp;|&nbsp; Mac OS&nbsp;|&nbsp; 
Objective-C&nbsp;|&nbsp; Parrot&nbsp;|&nbsp; Python&nbsp;|&nbsp; Reading
&nbsp;|&nbsp;Review&nbsp;|&nbsp; Ruby&nbsp;|&nbsp; Scala&nbsp;|&nbsp; Security
&nbsp;|&nbsp;Social&nbsp;|&nbsp; Solaris&nbsp;|&nbsp; Visual Basic&nbsp;|&nbsp; 
VMWare&nbsp;|&nbsp; WCF&nbsp;|&nbsp; Windows&nbsp;|&nbsp; XML Services
&nbsp;|&nbsp;XNA <br>
<br>
 Wednesday, 08 September 2010 20:53:01 (Pacific 
Daylight Time, UTC-07:00)<br>
Comments [5] &nbsp;|&nbsp; <br>
<br>
<br>

&nbsp;Thursday, 17 June 2010<br>
Architectural Katas <br>

<p>By now, the Twitter messages have spread, and the word is out: at Uberconf 
this year, I did a session (&quot;Pragmatic Architecture&quot;), which I've 
done at other venues before, but this time we made it into a 180-minute 
workshop instead of a 90-minute session, and the workshop included breaking the 
room up into small (10-ish, which was still a teensy bit too big) groups and 
giving each one an &quot;architectural kata&quot; to work on.</p> 
<p>The architectural kata is a take on PragDave's coding kata, except taken to 
a higher level: the architectural kata is an exercise in which the group seeks 
to create an architecture to solve the problem presented. The inspiration for 
this came from Frederick Brooks' latest book,<em>The Design of Design</em>, in 
which he points out that the only way to get great designers is to get them to 
design. The corollary, of course, is that in order to create great architects, 
we have to get them to architect. But few architects get a chance to architect 
a system more than a half-dozen times or so over the lifetime of a career, and 
that's only for those who are fortunate to be given the opportunity to 
architect in the first place. Of course, the problem here is, you have to be an 
architect in order to get hired as an architect, but if you're not an 
architect, then how can you architect in order to become an architect?</p> 
<p>Um... hang on, let me make sure I wrote that right.</p> 
<p>Anyway, the &quot;rules&quot; around the kata (which makes it more 
difficult to consume the kata but makes the scenario more realistic, IMHO):</p> 
<ul> 
<li>you may ask the instructor questions about the project</li> 
<li>you must be prepared to present a rough architectural vision of the 
project and defend questions about it</li> 
<li>you must be prepared to ask questions of other participants' presentations
</li> 
<li>you may safely make assumptions about technologies you don't know well as 
long as those assumptions are clearly defined and spelled out</li> 
<li>you may not assume you have hiring/firing authority over the development 
team</li> 
<li>any technology is fair game (but you must justify its use)</li> 
<li>any other rules, you may ask about</li> </ul> 
<p>The groups were given 30 minutes in which to formulate some ideas, and then 
three of them were given a few minutes to present their ideas and defend it 
against some questions from the crowd.</p> 
<p>An example kata is below:</p> 
<blockquote> 
<p><strong>Architectural Kata #5: I'll have the BLT</strong></p> 
<p>a national sandwich shop wants to enable &quot;fax in your order&quot; but 
over the Internet instead</p> 
<p>users: millions+</p> 
<p>requirements: users will place their order, then be given a time to pick up 
their sandwich and directions to the shop (which must integrate with Google 
Maps); if the shop offers a delivery service, dispatch the driver with the 
sandwich to the user; mobile-device accessibility; offer national daily 
promotionals/specials; offer local daily promotionals/specials; accept payment 
online or in person/on delivery</p> </blockquote> 
<p>As you can tell, it's vague in some ways, and this is somewhat 
deliberate&mdash;as one group discovered, part of the architect's job is to ask 
questions of the project champion (me), and they didn't, and felt like they 
failed pretty miserably. (In their defense, the kata they 
drew&mdash;randomly&mdash;was pretty much universally thought to be the hardest 
of the lot.) But overall, the exercise was well-received, lots of people found 
it a great opportunity to try being an architect, and even the team that failed 
felt that it was a valuable exercise.</p> 
<p>I'm definitely going to do more of these, and refine the whole thing a 
little. (Thanks to everyone who participated and gave me great feedback on how 
to make it better.) If you're interested in having it done as a practice 
exercise for your development team before the start of a big project, ping me. 
I think this would be a *great* exercise to do during a user group meeting, too.
</p> <br>

<p></p> .NET&nbsp;|&nbsp; Android&nbsp;|&nbsp; C#&nbsp;|&nbsp; C++&nbsp;|&nbsp;
Conferences&nbsp;|&nbsp; Development Processes&nbsp;|&nbsp; F#&nbsp;|&nbsp; 
Flash&nbsp;|&nbsp; Industry&nbsp;|&nbsp; iPhone&nbsp;|&nbsp; Java/J2EE
&nbsp;|&nbsp;Languages&nbsp;|&nbsp; LLVM&nbsp;|&nbsp; Mac OS&nbsp;|&nbsp; 
Objective-C&nbsp;|&nbsp; Parrot&nbsp;|&nbsp; Python&nbsp;|&nbsp; Ruby
&nbsp;|&nbsp;Scala&nbsp;|&nbsp; Security&nbsp;|&nbsp; Social&nbsp;|&nbsp; 
Solaris&nbsp;|&nbsp; Visual Basic&nbsp;|&nbsp; WCF&nbsp;|&nbsp; XML Services
&nbsp;|&nbsp;XNA <br>
<br>
 Thursday, 17 June 2010 01:42:47 (Pacific Daylight 
Time, UTC-07:00)<br>
Comments [0] &nbsp;|&nbsp; <br>
<br>
<br>
&nbsp;Friday, 14 
May 2010<br>
Emotional commitment colors everything <br>

<p>As a part of my program to learn how to use the Mac OS more effectively 
(mostly to counteract my lack of Mac-command-line kung fu, but partly to get 
Neal Ford off my back ), I set the home page in Firefox to point to the OSX 
Daily website. This morning, this particular page popped up as the &quot;tip of 
the day&quot;, and a particular thing about it struck my fancy. Go ahead and 
glance at it before you continue on.</p> 
<p>On its own merits, there's nothing particularly interesting about 
it&mdash;it's a tip about how to do a screen-capture in OS X, which is hardly a 
breakthrough feature. But something about the tenor struck me: 
&quot;You&rsquo;ve probably noticed there is no &lsquo;Print Screen&rsquo; 
button on a Mac keyboard, this is to both simplify the keyboard and also 
because it&rsquo;s unnecessary. Instead of hitting a &ldquo;Print Screen&rdquo; 
button, you&rsquo;ll hit one of several keyboard combination shortcuts, 
depending on the exact screen capture action you want taken. ... 
Command+Shift+3 takes a screenshot of the full screen ... Command+Shift+4 
brings up a selection box .... Command+Shift+4, then spacebar, then click a 
window takes a screenshot of the window....&quot;</p> 
<p>Wait a second. This is <em>simpler</em>?</p> 
<p>If &quot;you're a PC&quot;, you're probably rolling on the floor with 
laughter at this moment, determined to go find a Mac fanboi and Lord it over 
him that it requires the use of no less than three keystrokes to take a 
friggin' screenshot.</p> 
<p>If, on the other hand, you love the Mac, you're probably chuckling at the 
idiocy of PC manufacturers who continue to keep a key on the keyboard dating 
back from the terminal days (right next to &quot;Scroll Lock&quot;) that 
rarely, if ever, gets used.</p> 
<p>Who's right? Who's the idiot?</p> 
<p>You both are.</p> 
<p>See, the fact is, your perceptions of a particular element of the different 
platforms (the menubar at the top of the screen vs. in the main window of the 
app, the one-button vs. two-button mouse, and so on) colors your response. If 
you have emotionally committed to the Mac, then anything it does is naturally 
right and obvious; if you've emotionally committed to Windows, then ditto. This 
is a natural psychological response&mdash;it happens to everybody, to some 
degree or another. We need, at a subconscious level, to know that our decisions 
were the right ones to have made, so we look for those facts which confirm the 
decision, and avoid the facts that question it. (It's this same psychological 
drive that causes battered wives to defend their battering husbands to the 
police and intervening friends/family, and for people who've already committed 
to one political party or the other to see huge gaping holes in logic in the 
opponents' debate responses, but to gloss over their own candidates'.)</p> 
<p>Why bring it up? Because this also is what drives developers to justify the 
decisions they've made in developing software&mdash;when a user or another 
developer questions a particular decision, the temptation is to defend it to 
the dying breath, because it was a decision we made. We start looking for 
justifications to back it, we start aggressively questioning the challenger's 
competency or right to question the decision, you name it. It's a hard thing, 
to admit we might have been wrong, and even harder to admit that even though we 
might have been right, we were for the wrong reasons, or the decision still was 
the wrong one, or&mdash;perhaps hardest of all&mdash;the users simply like it 
the other way, even though this way is vastly more efficient and sane.</p> 
<p>Have you admitted you were wrong lately?</p> 
<p>(Check out <em>Predictably Irrational</em>, <em>How We Decide</em>, and <em>
Why We Make Mistakes</em> for more details on the psychology of 
decision-making.)</p> <br>

<p></p> Conferences&nbsp;|&nbsp; Development Processes&nbsp;|&nbsp; Industry
&nbsp;|&nbsp;Mac OS&nbsp;|&nbsp; Reading&nbsp;|&nbsp; Solaris&nbsp;|&nbsp; 
Windows <br>
<br>
 Friday, 14 May 2010 03:40:33 (Pacific Daylight Time, 
UTC-07:00)<br>
Comments [2] &nbsp;|&nbsp; <br>
<br>
<br>
&nbsp;Thursday, 06 May 
2010<br>
Code Kata: Compressing Lists <br>

<p>Code Katas are small, relatively simple exercises designed to give you a 
problem to try and solve. I like to use them as a way to get my feet wet and 
help write something more interesting than &quot;Hello World&quot; but less 
complicated than &quot;The Internet's Next Killer App&quot;.</p> 
<p>&nbsp;</p> 
<p>Rick Minerich mentioned this one on his blog already, but here is the 
original &quot;problem&quot;/challenge as it was presented to me and which I in 
turn shot to him over a Twitter DM:</p> 
<p>&nbsp;</p> 
<p>I have a list, say something like [4, 4, 4, 4, 2, 2, 2, 3, 3, 2, 2, 2, 2, 
1, 1, 1, 5, 5], which consists of varying repetitions of integers. (We can 
assume that it's always numbers, and the use of the term &quot;list&quot; here 
is generic&mdash;it could be a list, array, or some other collection class, 
your choice.) The goal is to take this list of numbers, and 
&quot;compress&quot; it down into a (theoretically smaller) list of numbers in 
pairs, where the first of the pair is the occurrence number of the value, which 
is the second number. So, since the list above has four 4's, followed by three 
2's, two 3's, four 2's, three 1's and two 5's, it should compress into [4, 4, 
3, 2, 2, 3, 3, 1, 2, 5].</p> 
<blockquote> 
<p><strong>Update:</strong> Typo! It should compress into [4, 4, 3, 2, 2, 3, 
4, 2, 3, 1, 2, 5], not [4, 4, 3, 2, 2, 3, 3, 1, 2, 5]. Sorry!</p> </blockquote> 
<p>Using your functional language of choice, implement a solution. (No looking 
at Rick's solution first, by the way&mdash;that's cheating!) Feel free to post 
proposed solutions here as comments, by the way.</p> 
<p>&nbsp;</p> 
<p>This is a pretty easy challenge, but I wanted to try and solve it in a 
functional mindset, which the challenger had never seen before. I also thought 
it made for an interesting challenge for people who've never programming in 
functional languages before, because it requires a very different approach than 
the imperative solution.</p> 
<p>&nbsp;</p> 
<p>Extensions to the kata (a.k.a. &quot;extra credit&quot;):</p> 
<ul> 
<li>How does the implementation change (if any) to generalize it to a list of 
any particular type? (Assume the list is of homogenous type&mdash;always 
strings, always ints, always whatever.)</li> 
<li>How does the implementation change (if any) to generalize it to a list of 
any type? (In other words, a list of strings, ints, Dates, whatever, mixed 
together within the list: [1, 1, &quot;one&quot;, &quot;one&quot;, 
&quot;one&quot;, ...] .)</li> 
<li>How does the implementation change (if any) to generate a list of two-item 
tuples (the first being the occurence, the second being the value) as the 
result instead? Are there significant advantages to this?</li> 
<li>How does the implementation change (if any) to parallelize/multi-thread 
it? For your particular language how many elements have to be in the list 
before doing so yields a significant payoff?</li> </ul> 
<p>By the way, some of the extension questions make the Kata somewhat 
interesting even for the imperative/O-O developer; have at, and let me know 
what you think.</p> <br>

<p></p> .NET&nbsp;|&nbsp; Android&nbsp;|&nbsp; C#&nbsp;|&nbsp; C++&nbsp;|&nbsp;
Development Processes&nbsp;|&nbsp; F#&nbsp;|&nbsp; Flash&nbsp;|&nbsp; Industry
&nbsp;|&nbsp;iPhone&nbsp;|&nbsp; Java/J2EE&nbsp;|&nbsp; Languages&nbsp;|&nbsp; 
LLVM&nbsp;|&nbsp; Mac OS&nbsp;|&nbsp; Parrot&nbsp;|&nbsp; Python&nbsp;|&nbsp; 
Ruby&nbsp;|&nbsp; Scala&nbsp;|&nbsp; Visual Basic <br>
<br>
 Thursday, 06 May 
2010 14:42:09 (Pacific Daylight Time, UTC-07:00)<br>
Comments [13] &nbsp;|&nbsp;
<br> <br>
<br>
&nbsp;Tuesday, 19 January 2010 <br>
10 Things To Improve Your 
Development Career <br>

<p>Cruising the Web late last night, I ran across &quot;10 things you can do 
to advance your career as a developer&quot;, summarized below:</p> 
<ol> 
<li>Build a PC </li> 
<li>Participate in an online forum and help others </li> 
<li>Man the help desk </li> 
<li>Perform field service </li> 
<li>Perform DBA functions </li> 
<li>Perform all phases of the project lifecycle </li> 
<li>Recognize and learn the latest technologies </li> 
<li>Be an independent contractor </li> 
<li>Lead a project, supervise, or manage </li> 
<li>Seek additional education </li> </ol> 
<p>I agreed with some of them, I disagreed with others, and in general felt 
like they were a little too high-level to be of real use. For example, 
&quot;Seek additional education&quot; seems entirely too vague: In what? How 
much? How often? And &quot;Recognize and learn the latest technologies&quot; is 
something like offering advice to the Olympic fencing silver medalist and 
saying, &quot;You should have tried harder&quot;.</p> 
<p>So, in the great spirit of &quot;Not Invented Here&quot;, I present my own 
list; as usual, I welcome comment and argument. And, also as usual, caveats 
apply, since not everybody will be in precisely the same place and be looking 
for the same things. In general, though, whether you're looking to kick-start 
your career or just &quot;kick it up a notch&quot;, I believe this list will 
help, because these ideas have been of help to me at some point or another in 
my own career.</p> 
<h3><strong><em>10: Build a PC.</em></strong> </h3> 
<p>Yes, even developers have to know about hardware. More importantly, a 
developer at a small organization or team will find himself in a position where 
he has to take on some system administrator roles, and sometimes that means 
grabbing a screwdriver, getting a little dusty and dirty, and swapping hardware 
around. Having said this, though, once you've done it once or twice, leave it 
alone&mdash;the hardware game is an ever-shifting and ever-changing game (much 
like software is, surprise surprise), and it's been my experience that most of 
us only really have the time to pursue one or the other.</p> 
<p>By the way, &quot;PC&quot; there is something of a generic term&mdash;build 
a Linux box, build a Windows box, or &quot;build&quot; a Mac OS box (meaning, 
buy a Mac Pro and trick it out a little&mdash;add more memory, add another hard 
drive, and so on), they all get you comfortable with snapping parts together, 
and discovering just how ridiculously simple the whole thing really is.</p> 
<p>And for the record, once you've done it, go ahead and go back to buying 
pre-built systems or laptops&mdash;I've never found building a PC to be any 
cheaper than buying one pre-built. Particularly for PC systems, I prefer to use 
smaller local vendors where I can customize and trick out the box. If you're a 
Mac, that's not really an option unless you're into the &quot;Hackintosh&quot; 
thing, which is quite possibly the logical equivalent to &quot;Build a 
PC&quot;. Having never done it myself, though, I can't say how useful that is 
as an educational action.</p> 
<h3></h3> 
<h3></h3> 
<h3></h3> 
<h3><strong><em>9: Pick a destination</em></strong></h3> 
<p>Do you want to run a team of your own? Become an independent contractor? 
Teach programming classes? Speak at conferences? Move up into higher management 
and get out of the programming game altogether? Everybody's got a different 
idea of what they consider to be the &quot;ideal&quot; career, but it's amazing 
how many people don't really think about what they want their career path to be.
</p> 
<p>A wise man once said, &quot;The journey of a thousand miles begins with a 
single step.&quot; I disagree: The journey of a thousand miles begins with the 
damn map. You have to know where you want to go, and a rough idea of how to get 
there, before you can really start with that single step. Otherwise, you're 
just wandering, which in itself isn't a bad thing, but isn't going to get you 
to a destination except by random chance. (Sometimes that's not a bad result, 
but at least then you're openly admitting that you're leaving your career in 
the hands of chance. If you're OK with that, skip to the next item. If you're 
not, read on.)</p> 
<p>Lay out explicitly (as in, write it down someplace) what kind of job you're 
wanting to grow into, and then lay out a couple of scenarios that move you 
closer towards that goal. Can you grow within the company you're in? (Have 
others been able to?) Do you need to quit and strike out on your own? Do you 
want to lead a team of your own? (Are there new projects coming in to the 
company that you could put yourself forward as a potential tech lead?) And so 
on.</p> 
<p>Once you've identified the destination, now you can start thinking about 
steps to get there.</p> 
<p>If you want to become a speaker, put your name forward to give some 
presentations at the local technology user group, or volunteer to hold a 
&quot;brown bag&quot; session at the company. Sign up with Toastmasters to hone 
your speaking technique. Watch other speakers give technical talks, and see 
what they do that you don't, and vice versa.</p> 
<p>If you want to be a tech lead, start by quietly assisting other members of 
the team get their work done. Help them debug thorny problems. Answer questions 
they have. Offer yourself up as a resource for dealing with hard problems.</p> 
<p>If you want to slowly move up the management chain, look to get into the 
project management side of things. Offer to be a point of contact for the 
users. Learn the business better. Sit down next to one of your users and watch 
their interaction with the existing software, and try to see the system from 
their point of view.</p> 
<p>And so on.</p> 
<h3><strong><em>8: Be a bell curve</em></strong></h3> 
<p>Frequently, at conferences, attendees ask me how I got to know so much on 
so many things. In some ways, I'm reminded of the story of a world-famous 
concert pianist giving a concert at Carnegie Hall&mdash;when a gushing fan 
said, &quot;I'd give my life to be able to play like that&quot;, the pianist 
responded quietly, &quot;I did&quot;. But as much as I'd like to leave you with 
the impression that I've dedicated my entire life to knowing everything I could 
about this industry, that would be something of a lie. The truth is, I don't 
know anywhere near as much as I'd like, and I'm always poking my head into new 
areas. Thank God for my ADD, that's all I can say on that one.</p> 
<p>For the rest of you, though, that's not feasible, and not really practical, 
particularly since I have an advantage that the &quot;working&quot; programmer 
doesn't&mdash;I have set aside weeks or months in which to do nothing more than 
study a new technology or language.</p> 
<p>Back in the early days of my career, though, when I was holding down the 
9-to-5, I was a Windows/C++ programmer. I was working with the Borland C++ 
compiler and its associated framework, the ObjectWindows Library (OWL), 
extending and maintaining applications written in it. One contracting client 
wanted me to work with Microsoft MFC instead of OWL. Another one was storing 
data into a relational database using ODBC. And so on. Slowly, over time, I 
built up a &quot;bell curve&quot;-looking collection of skills that sort of 
&quot;hovered&quot; around the central position of C++/Windows.</p> 
<p>Then, one day, a buddy of mine mentioned the team on which he was a project 
manager was looking for new blood. They were doing web applications, something 
with which I had zero experience&mdash;this was completely outside of my bell 
curve. HTML, HTTP, Cold Fusion, NetDynamics (an early Java app server), this 
was way out of my range, though at least NetDynamics was a<em>little</em> 
similar, since it was basically a server-side application framework, and I had 
some experience with app frameworks from my C++ days. So, resting on my C++ 
experience, I started flirting with Java, and so on.</p> 
<p>Before long, my &quot;bell curve&quot; had been readjusted to have Java 
more or less at its center, and I found that experience in C++ still worked out 
here&mdash;what I knew about ODBC turned out to be incredibly useful in 
understanding JDBC, what I knew about DLLs from Windows turned out to be 
helpful in understanding Java's dynamic loading model, and of course 
syntactically Java looked a lot like C++ even though it behaved a little bit 
differently under the hood. (One article author suggested that Java was closer 
to Smalltalk than C++, and that prompted me to briefly flirt with Smalltalk 
before I concluded said author was out of his frakking mind.)</p> 
<p>All of this happened over roughly a three-year period, by the way.</p> 
<p>The point here is that you won't be able to assimilate the entire industry 
in a single sitting, so pick something that's relatively close to what you 
already know, and use your experience as a springboard to learn something 
that's new, yet possibly-if-not-probably useful to your current job. You don't 
have to be a deep expert in it, and the further away it is from what you do, 
the less you really need to know about it (hence the bell curve metaphor), but 
you're still exposing yourself to new ideas and new concepts and new 
tools/technologies that still could be applicable to what you do on a daily 
basis. Over time the &quot;center&quot; of your bell curve may drift away from 
what you've done to include new things, and that's OK.</p> 
<h3><strong><em>7: Learn one new thing every year</em></strong></h3> 
<p>In the last tip, I told you to branch out slowly from what you know. In 
this tip, I'm telling you to go throw a dart at something entirely unfamiliar 
to you and learn it. Yes, I realize this sounds contradictory. It's because 
those who stick to only what they know end up missing the radical shifts of 
direction that the industry hits every half-decade or so until it's mainstream 
and commonplace and &quot;everybody's doing it&quot;.</p> 
<p>In their amazing book &quot;The Pragmatic Programmer&quot;, Dave Thomas and 
Andy Hunt suggest that you learn one new programming language every year. I'm 
going to amend that somewhat&mdash;not because there aren't enough languages in 
the world to keep you on that pace for the rest of your life&mdash;far from it, 
if that's what you want, go learn Ruby, F#, Scala, Groovy, Clojure, Icon, Io, 
Erlang, Haskell and Smalltalk, then come back to me for the list for 
2020&mdash;but because languages aren't the only thing that we as developers 
need to explore. There's a lot of movement going on in areas beyond languages, 
and you don't want to be the last kid on the block to know they're happening.
</p> 
<p>Consider this list: object databases (db4o) and/or the &quot;NoSQL&quot; 
movement (MongoDB). Dependency injection and composable architectures (Spring, 
MEF). A dynamic language (Ruby, Python, ECMAScript). A functional language (F#, 
Scala, Haskell). A Lisp (Common Lisp, Clojure, Scheme, Nu). A mobile platform 
(iPhone, Android). &quot;Space&quot;-based architecture (Gigaspaces, 
Terracotta). Rich UI platforms (Flash/Flex, Silverlight). Browser enhancements 
(AJAX, jQuery, HTML 5) and how they're different from the rich UI platforms. 
And this is without adding any of the &quot;obvious&quot; stuff, like Cloud, to 
the list.</p> 
<p>(I'm not convinced Cloud is something worth learning this year, anyway.)</p>
<p>You get through that list, you're operating outside of your comfort zone, 
and chances are, your boss' comfort zone, which puts you into the enviable 
position of being somebody who can advise him around those technologies.<em>DO 
NOT TAKE THIS TO MEAN YOU MUST KNOW THEM DEEPLY.</em> Just having a passing 
familiarity with them can be enough.<em>DO NOT TAKE THIS TO MEAN YOU SHOULD 
PROPOSE USING THEM ON THE NEXT PROJECT.</em> In fact, sometimes the most 
compelling evidence that you really know where and when they should be used is 
when you suggest stealing ideas from the thing, rather than trying to force-fit 
the thing onto the project as a whole.</p> 
<h3><strong><em>6: Practice, practice, practice</em></strong></h3> 
<p>Speaking of the concert pianist, somebody once asked him how to get to 
Carnegie Hall. HIs answer: &quot;Practice, my boy, practice.&quot;</p> 
<p>The same is true here. You're not going to get to be a better developer 
without practice. Volunteer some time&mdash;even if it's just an hour a 
week&mdash;on an open-source project, or start one of your own. Heck, it 
doesn't even have to be an &quot;open source&quot; project&mdash;just create 
some requirements of your own, solve a problem that a family member is having, 
or rewrite the project you're on as an interesting side-project. Do the Nike 
thing and &quot;Just do it&quot;. Write some Scala code. Write some F# code. 
Once you're past &quot;hello world&quot;, write the Scala code to use db4o as a 
persistent storage. Wire it up behind Tapestry. Or write straight servlets in 
Scala. And so on.</p> 
<h3><strong><em>5: Turn off the TV</em></strong></h3> 
<p>Speaking of marketing slogans, if you're like most Americans, surveys have 
shown that you watch about four hours of TV a day, or 28 hours of TV a week. In 
that same amount of time (28 hours over 1 week), you could read the entire set 
of poems by Maya Angelou, one F. Scott Fitzgerald novel, all poems by 
T.S.Eliot, 2 plays by Thornton Wilder, or all 150 Psalms of the Bible. An 
average reader, reading just one hour a day, can finish an 
&quot;average-sized&quot; book (let's assume about the size of a novel) in a 
week, which translates to 52 books a year.</p> 
<p>Let's assume a technical book is going to take slightly longer, since it's 
a bit deeper in concept and requires you to spend some time experimenting and 
typing in code; let's assume that reading and going through the exercises of an 
average technical book will require 4 weeks (a month) instead of just one week. 
That's 12 new tools/languages/frameworks/ideas you'd be learning per year.</p> 
<p>All because you stopped watching David Caruso turn to the camera, whip his 
sunglasses off and say something stupid. (I guess it's not his fault;<em>
CSI:Miami</em> is a crap show. The other two are actually not bad, but <em>Miami
</em> just makes me retch.) </p> 
<p>After all, when's the last time that David Caruso or the rest of that show 
did anything that was even remotely realistic from a computer perspective? (I 
always laugh out loud every time they run a database search against some 
national database on a completely non-indexable criteria&mdash;like a partial 
license plate number&mdash;and it comes back in seconds. What the hell database 
are THEY using? I want it!) Soon as you hear The Who break into that riff, flip 
off the TV (or set it to mute) and pick up the book on the nightstand and boost 
your career. (And hopefully sink Caruso's.)</p> 
<p>Or, if you just can't give up your weekly dose of Caruso, then put the book 
in the bathroom. Think about it&mdash;how much time do you spend in there a 
week?</p> 
<p>And this gets even better when you get a Kindle or other e-reader that 
accepts PDFs, or the book you're interested in is natively supported in the 
e-readers' format. Now you have it with you for lunch, waiting at dinner for 
your food to arrive, or while you're sitting guard on your 10-year-old so he 
doesn't sneak out of his room after his bedtime to play more XBox.</p> 
<h3><strong><em>4: Have a life</em></strong></h3> 
<p>Speaking of XBox, don't slave your life to work. Pursue other things. 
Scientists have repeatedly discovered that exercise helps keep the mind in 
shape, so take a couple of hours a week (buh-bye,<em>American Idol</em>) and go 
get some exercise. Pick up a new sport you've never played before, or just go 
work out at the gym. (This year I'm doing Hopkido and fencing.) Read some 
nontechnical books. (I recommend anything by Malcolm Gladwell as a starting 
point.) Spend time with your family, if you have one&mdash;mine spends at least 
six or seven hours a week playing &quot;family games&quot; likeSettlers of Catan
,Dominion, To Court The King, Munchkin, and other non-traditional games, 
usually over lunch or dinner. I also belong to an informal &quot;Game Night 
club&quot; in Redmond consisting of several Microsoft employees and their 
families, as well as outsiders. And so on. Heck, go to a local bar and watch 
the game, and you'll meet some really interesting people. And some boring 
people, too, but you don't have to talk to them during the next game if you 
don't want.</p> 
<p>This isn't just about maintaining a healthy work-life balance&mdash;it's 
also about having interests that other people can latch on to, qualities that 
will make you more &quot;human&quot; and more interesting as a person, and make 
you more attractive and &quot;connectable&quot; and stand out better in their 
mind when they hear that somebody they know is looking for a software 
developer. This will also help you connect better with your users, because like 
it or not, they do<em>not</em> get your puns involving Klingon. (Besides, the 
geek stereotype is SO 90's, and it's time we let the world know that.)</p> 
<p>Besides, you never know when having some depth in other 
areas&mdash;philosophy, music, art, physics, sports, whatever&mdash;will help 
you create an analogy that will explain some thorny computer science concept to 
a non-technical person and get past a communication roadblock.</p> 
<h3><strong><em>3: Practice on a cadaver</em></strong></h3> 
<p>Long before they scrub up for their first surgery on a human, medical 
students practice on dead bodies. It's grisly, it's not something we really 
want to think about, but when you're the one going under the general 
anesthesia, would you rather see the surgeon flipping through the 
&quot;How-To&quot; manual, &quot;just to refresh himself&quot;?</p> 
<p>Diagnosing and debugging a software system can be a hugely puzzling trial, 
largely because there are so many possible &quot;moving parts&quot; that are 
creating the problem. Compound that with certain bugs that only appear when 
multiple users are interacting at the same time, and you've got a recipe for 
disaster when a production bug suddenly threatens to jeopardize the company's 
online revenue stream. Do you really want to be sitting in the production 
center, flipping through &quot;How-To&quot;'s and FAQs online while your boss 
looks on and your CEO is counting every minute by the thousands of dollars?</p> 
<p>Take a tip from the med student: long before the thing goes into 
production, introduce a bug, deploy the code into a virtual machine, then hand 
it over to a buddy and let him try to track it down. Have him do the same for 
you. Or if you can't find a buddy to help you, do it to yourself (but try not 
to cheat or let your knowledge of where the bug is color your reactions). How 
do you know the bug is there? Once you know it's there, how do you determine 
what kind of bug it is? Where do you start looking for it? How would you track 
it down without attaching a debugger or otherwise disrupting the system's 
operations? (Remember, we can't always just attach an IDE and step through the 
code on a production server.) How do you patch the running system? And so on.
</p> 
<p>Remember, you can either learn these things under controlled circumstances, 
learn them while you're in the &quot;hot seat&quot;, so to speak, or not learn 
them at all and see how long the company keeps you around.</p> 
<h3><strong><em>2: Administer the system</em></strong></h3> 
<p>Take off your developer hat for a while&mdash;a week, a month, a quarter, 
whatever&mdash;and be one of those thankless folks who have to keep the system 
running. Wear the pager that goes off at 3AM when a server goes down. Stay all 
night doing one of those &quot;server upgrades&quot; that have to be done in 
the middle of the night because the system can't be upgraded while users are 
using it. Answer the phones or chat requests of those hapless users who can't 
figure out why they can't find the record they just entered into the system, 
and after a half-hour of thinking it must be a bug, ask them if they remembered 
to check the &quot;Save this record&quot; checkbox on the UI (which had to be 
there because the developers were told it had to be there) before submitting 
the form. Try adding a user. Try removing a user. Try changing the user's 
password. Learn what a real joy having seven different 
properties/XML/configuration files scattered all over the system really is.</p> 
<p>Once you've done that, particularly on a system that you built and tossed 
over the fence into production and thought that was the end of it, you'll 
understand just why it's so important to keep the system administrators in mind 
when you're building a system for production. And why it's critical to be able 
to have a system that tells you when it's down, instead of having to go hunting 
up the answer when a VP tells you it is (usually because he's just gotten an 
outage message from a customer or client).</p> 
<h3><strong><em>1: Cultivate a peer group</em></strong></h3> 
<p>Yes, you can join an online forum, ask questions, answer questions, and 
learn that way, but that's a poor substitute for physical human contact once in 
a while. Like it or not, various sociological and psychological studies confirm 
that a &quot;connection&quot; is really still best made when eyeballs meet 
flesh. (The &quot;disassociative&quot; nature of email is what makes it so easy 
to be rude or flamboyant or downright violent in email when we would never say 
such things in person.) Go to conferences, join a user group, even start one of 
your own if you can't find one. Yes, the online avenues are still open to 
you&mdash;read blogs, join mailing lists or newsgroups&mdash;but don't lose 
sight of human-to-human contact.</p> 
<p>While we're at it, don't create a peer group of people that all look to you 
for answers&mdash;as flattering as that feels, and as much as we do learn by 
providing answers, frequently we rise (or fall) to the level of our 
peers&mdash;have at least one peer group that's overwhelmingly smarter than 
you, and as scary as it might be, venture to offer an answer or two to that 
group when a question comes up. You don't have to be right&mdash;in fact, it's 
often vastly more educational to be wrong. Just maintain an attitude that says 
&quot;I have no ego wrapped up in being right or wrong&quot;, and take the 
entire experience as a learning opportunity.</p> <br>

<p></p> .NET&nbsp;|&nbsp; C#&nbsp;|&nbsp; C++&nbsp;|&nbsp; Conferences
&nbsp;|&nbsp;Development Processes&nbsp;|&nbsp; F#&nbsp;|&nbsp; Flash
&nbsp;|&nbsp;Industry&nbsp;|&nbsp; Java/J2EE&nbsp;|&nbsp; Languages&nbsp;|&nbsp;
LLVM&nbsp;|&nbsp; Mac OS&nbsp;|&nbsp; Parrot&nbsp;|&nbsp; Python&nbsp;|&nbsp; 
Reading&nbsp;|&nbsp; Ruby&nbsp;|&nbsp; Scala&nbsp;|&nbsp; Security&nbsp;|&nbsp; 
Social&nbsp;|&nbsp; Solaris&nbsp;|&nbsp; Visual Basic&nbsp;|&nbsp; VMWare
&nbsp;|&nbsp;WCF&nbsp;|&nbsp; Windows&nbsp;|&nbsp; XML Services <br>
<br>
 
Tuesday, 19 January 2010 02:02:01 (Pacific Standard Time, UTC-08:00)<br>

Comments [2] &nbsp;|&nbsp; <br>
<br>
<br>
&nbsp;Tuesday, 05 January 2010 <br>

2010 Predictions, 2009 Predictions Revisited <br>

<p>Here we go again&mdash;another year, another set of predictions revisited 
and offered up for the next 12 months. And maybe, if I'm feeling really 
ambitious, I'll take that shot I thought about last year and try predicting for 
the decade. Without further ado, I'll go back and revisit, unedited, my 
predictions for 2009 (&quot;<strong>THEN</strong>&quot;), and pontificate on 
those subjects for 2010 before adding any new material/topics. Just for 
convenience,here's a link back to last years' predictions.</p> 
<p>Last year's predictions went something like this (complete with 
basketball-scoring):</p> 
<ul> 
<li><strong>THEN: </strong>&quot;Cloud&quot; will become the next 
&quot;ESB&quot; or &quot;SOA&quot;, in that it will be something that everybody 
will talk about, but few will understand and even fewer will do anything with. 
(Considering the widespread disparity in the definition of the term, this seems 
like a no-brainer.)<strong>NOW:</strong> Oh, yeah. Straight up. I get two 
points for this one. Does<em>anyone</em> have a working definition of 
&quot;cloud&quot; that applies to all of the major vendors' implementations?<em>
Ted, 2; Wrongness, 0</em>.</li> 
<li><strong>THEN: </strong>Interest in Scala will continue to rise, as will 
the number of detractors who point out that Scala is too hard to learn.<strong>
NOW:</strong> Two points for this one, too. Not a hard one, mind you, but one 
of those &quot;pass-and-shoot&quot; jumpers from twelve feet out. James 
Strachan even tweeted about this earlier today, pointing out this comparison. 
As more Java developers who think of themselves as smart people try to pick up 
Scala and fail, the numbers of sour grapes responses like &quot;Scala's too 
complex, and who needs that functional stuff anyway?&quot; will continue to 
rise in 2010.<em>Ted, 4; Wrongness, 0</em>.</li> 
<li><strong>THEN</strong>: Interest in F# will continue to rise, as will the 
number of detractors who point out that F# is too hard to learn. (Hey, the two 
really are cousins, and the fortunes of one will serve as a pretty good 
indication of the fortunes of the other, and both really seem to be on the same 
arc right now.)<strong>NOW:</strong> Interestingly enough, I haven't heard as 
many F# detractors as Scala detractors, possibly because I think F# hasn't 
really reached the masses of .NET developers the way that Scala has managed to 
find its way in front of Java developers. I think that'll change mighty quickly 
in 2010, though, once VS 2010 hits the streets.<em>Ted, 4; Wrongness 2</em>.
</li> 
<li><strong>THEN</strong><em>:</em> Interest in all kinds of functional 
languages will continue to rise, and more than one person will take a hint from 
Bob &quot;crazybob&quot; Lee and liken functional programming to AOP, for good 
and for ill. People who took classes on Haskell in college will find themselves 
reaching for their old college textbooks again.<strong>NOW:</strong> Yep, I'm 
claiming two points on this one, if only because a bunch of Haskell books 
shipped this year, and they'll be the last to do so for about five years after 
this. (By the way, does anybody still remember aspects?) But I'm going the 
opposite way with this one now; yes, there's Haskell, and yes, there's Erlang, 
and yes, there's a lot of other functional languages out there, but who cares? 
They're hard to learn, they don't always translate well to other languages, and 
developers want languages that work on the platform they use on a daily basis, 
and that means F# and Scala or Clojure, or its simply not an option.<em>Ted 6; 
Wrongness 2</em>.</li> 
<li><strong>THEN</strong><em>:</em> The iPhone is going to be hailed as 
&quot;the enterprise development platform of the future&quot;, and companies 
will be rolling out apps to it. Look for Quicken iPhone edition, PowerPoint 
and/or Keynote iPhone edition, along with connectors to hook the iPhone up to a 
presentation device, and (I'll bet) a World of Warcraft iPhone client (legit or 
otherwise). iPhone is the new hotness in the mobile space, and people will 
flock to it madly.<strong>NOW:</strong> Two more points, but let's be 
honest&mdash;this was a fast-break layup, no work required on my part.<em>Ted 
8; Wrongness 2.</em></li> 
<li><strong>THEN</strong>: Another Oslo CTP will come out, and it will bear 
only a superficial resemblance to the one that came out in October at PDC. 
Betting on Oslo right now is a fools' bet, not because of any inherent weakness 
in the technology, but just because it's way too early in the cycle to be 
thinking about for anything vaguely resembling production code.<strong>NOW:
</strong> If you've worked at all with Oslo, you might argue with me, but I'm 
still taking my two points. The two CTPs were pretty different in a number of 
ways.<em>Ted 10; Wrongness 2.</em></li> 
<li><strong>THEN</strong>: The IronPython and IronRuby teams will find some 
serious versioning issues as they try to manage the DLR versioning story 
between themselves and the CLR as a whole. An initial hack will result, which 
will be codified into a standard practice when .NET 4.0 ships. Then the next 
release of IPy or IRb will have to try and slip around its restrictions in 
2010/2011. By 2012, IPy and IRb will have to be shipping as part of Visual 
Studio just to put the releases back into lockstep with one another (and the 
rest of the .NET universe).<strong>NOW:</strong> Pressure is still building. 
Let's see what happens by the time VS 2010 ships, and then see what the IPy/IRb 
teams start to do to adjust to the versioning issues that arise.<em>Ted 8; 
Wrongness 2.</em></li> 
<li><strong>THEN</strong>: The death of JSR-277 will spark an uprising among 
the two leading groups hoping to foist it off on the Java community--OSGi and 
Maven--while the rest of the Java world will breathe a huge sigh of relief and 
look to see what &quot;modularity&quot; means in Java 7. Some of the alpha 
geeks in Java will start using--if not building--JDK 7 builds just to get a 
heads-up on its impact, and be quietly surprised and, I dare say, perhaps even 
pleased.<strong>NOW:</strong> Ah, Ted, you really should never underestimate 
the community's willingness to take a bad idea, strip all the goodness out of 
it, and then cycle it back into the mix as something completely different yet 
somehow just as dangerous and crazy. I give you Project Jigsaw.<em>Ted 10; 
Wrongness 2;</em></li> 
<li><strong>THEN</strong>: The invokedynamic JSR will leapfrog in importance 
to the top of the list.<strong>NOW:</strong> The invokedynamic JSR begat 
interest in other languages on the JVM. The interest in other languages on the 
JVM begat the need to start thinking about how to support them in the Java 
libraries. The need to start thinking about supporting those languages begat a 
&quot;Holy sh*t moment&quot; somewhere inside Sun and led them to (re-)propose 
closures for JDK 7. And in local sports news, Ted notched up two more points on 
the scoreboard.<em>Ted 12; Wrongness 2.</em></li> 
<li><strong>THEN</strong>: Another Windows 7 CTP will come out, and it will 
spawn huge media interest that will eventually be remembered as Microsoft 
promises, that will eventually be remembered as Microsoft guarantees, that will 
eventually be remembered as Microsoft FUD and &quot;promising much, delivering 
little&quot;. Microsoft ain't always at fault for the inflated expectations 
people have--sometimes, yes, perhaps even a lot of times, but not always.
<strong>NOW:</strong> And then, just when the game started to turn into a 
runaway, airballs started to fly. The Windows7 release shipped, and contrary to 
what I expected, the general response to it was pretty warm. Yes, there were a 
few issues that emerged, but overall the media liked it, the masses liked it, 
and Microsoft seemed to have dodged a bullet.<em>Ted 12; Wrongness 5.</em></li> 
<li><strong>THEN</strong>: Apple will begin to legally threaten the clone 
market again, except this time somebody's going to get the DOJ involved. (Yes, 
this is the iPhone/iTunes prediction from last year, carrying over. I still 
expect this to happen.)<strong>NOW:</strong> What clones? The only people 
trying to clone Macs are those who are building Hackintosh machines, and Apple 
can't sue them so long as they're using licensed copies of Mac OS X (as far as 
I know). Which has never stopped them from trying, mind you, and I still think 
Steve has some part of his brain whispering to him at night, calculating all 
the hardware sales lost to Hackintosh netbooks out there. But in any event, 
that's another shot missed.<em>Ted 12; Wrongness 7.</em></li> 
<li><strong>THEN</strong>: Alpha-geek developers will start creating their own 
languages (even if they're obscure or bizarre ones like Shakespeare or Ook#) 
just to have that listed on their resume as the DSL/custom language buzz 
continues to build.<strong>NOW:</strong> I give you Ioke. If I'd extended this 
to include outdated CPU interpreters, I'd have made that three-pointer from 
half-court instead of just the top of the key.<em>Ted 14; Wrongness 7.</em></li>
<li><strong>THEN</strong>: Roy Fielding will officially disown most of the 
&quot;REST&quot;ful authors and software packages available. Nobody will 
care--or worse, somebody looking to make a name for themselves will proclaim 
that Roy &quot;doesn't really understand REST&quot;. And they'll be right--Roy 
doesn't understand what<em>they</em> consider to be REST, and the fact that he 
created the term will be of no importance anymore. Being &quot;REST&quot;ful 
will equate to &quot;I did it myself!&quot;, complete with expectations of a 
gold star and a lollipop.<strong>NOW:</strong> Does anybody in the REST 
community care what Roy Fielding wrote way back when? I keep seeing 
&quot;REST&quot;ful systems that seem to have designers who've never heard of 
Roy, or his thesis. Roy hasn't officially disowned them, but damn if he doesn't 
seem close to it. Still.... No points.<em>Ted 14; Wrongness 9.</em></li> 
<li><strong>THEN</strong>: The Parrot guys will make at least one more minor 
point release. Nobody will notice or care, except for a few doggedly stubborn 
Perl hackers. They will find themselves having nightmares of previous lives 
carrying around OS/2 books and Amiga paraphernalia. Perl 6 will celebrate it's 
seventh... or is it eighth?... anniversary of being announced, and nobody will 
notice.<strong>NOW:</strong> Does anybody still follow Perl 6 development? Has 
the spec even been written yet? Google on &quot;Perl 6 release&quot;, and you 
get varying reports: &quot;It'll ship 'when it's ready'&quot;, &quot;There are 
no such dates because this isn't a commericially-backed effort&quot;, and 
&quot;Spring 2010&quot;. Swish&mdash;nothin' but net.<em>Ted 16; Wrongness 9.
</em></li> 
<li><strong>THEN</strong>: The debate around &quot;Scrum Certification&quot; 
will rise to a fever pitch as short-sighted money-tight companies start looking 
for reasons to cut costs and either buy into agile at a superficial level and 
watch it fail, or start looking to cut the agilists from their company in order 
to replace them with cheaper labor.<strong>NOW:</strong> Agile has become 
another adjective meaning &quot;best practices&quot;, and as such, has 
essentially lost its meaning. Just ask Scott Bellware.<em>Ted 18; Wrongness 9.
</em></li> 
<li><strong>THEN</strong>: Adobe will continue to make Flex and AIR look more 
like C# and the CLR even as Microsoft tries to make Silverlight look more like 
Flash and AIR. Web designers will now get to experience the same fun that 
back-end web developers have enjoyed for near-on a decade, as shops begin to 
artificially partition themselves up as either &quot;Flash&quot; shops or 
&quot;Silverlight&quot; shops.<strong>NOW:</strong> Not sure how to score this 
one&mdash;I haven't seen the explicit partitioning happen yet, but the two 
environments definitely still seem to be looking to start tromping on each 
others' turf, particularly when we look at the rapid releases coming from the 
Silverlight team.<em>Ted 16; Wrongness 11.</em></li> 
<li><strong>THEN</strong>: Gartner will still come knocking, looking to hire 
me for outrageous sums of money to do nothing but blog and wax prophetic.
<strong>NOW:</strong> Still no job offers. Damn. Ah, well. <em>Ted 16; 
Wrongness 13.</em></li> </ul> 
<p>A close game. Could've gone either way. *shrug* Ah, well. It was silly to 
try and score it in basketball metaphor, anyway&mdash;that's the last time I 
watch ESPN before writing this.</p> 
<p>For 2010, I predict....</p> 
<ul> 
<li><em>... I will offer 3- and 4-day training classes on F# and Scala, among 
other things.</em> OK, that's not fair&mdash;yes, I have the materials, I just 
need to work out locations and times. Contact me if you're interested in a 
private class, by the way.</li> 
<li><em>... I will publish two books, one on F# and one on Scala.</em> OK, OK, 
another plug. Or, rather, more of a resolution. One will be the 
&quot;Professional F#&quot; I'm doing for Wiley/Wrox, the other isn't yet 
finalized. But it'll either be published through a publisher, or 
self-published, by JavaOne 2010.</li> 
<li><em>... DSLs will either &quot;succeed&quot; this year, or begin the short 
slide into the dustbin of obscure programming ideas.</em> Domain-specific 
language advocates have to put up some kind of strawman for developers to learn 
from and poke at, or the whole concept will just fade away. Martin's book will 
help, if it ships this year, but even that might not be enough to generate 
interest if it doesn't have some kind of large-scale applicability in it. 
Patterns and refactoring and enterprise containers all had a huge advantage in 
that developers could see pretty easily what the problem was they solved; DSLs 
haven't made that clear yet.</li> 
<li><em>... functional languages will start to see a backlash.</em> I hate to 
say it, but &quot;getting&quot; the functional mindset is hard, and there's 
precious few resources that are making it easy for mainstream (read: O-O) 
developers make that adjustment, far fewer than there was during the 
procedural-to-object shift. If the functional community doesn't want to become 
mainstream, then mainstream developers will find ways to take functional's most 
compelling gateway use-case (parallel/concurrent programming) and find a way to 
&quot;git 'er done&quot; in the traditional O-O approach, probably through 
software transactional memory, and functional languages like Haskell and Erlang 
will be relegated to the &quot;What Might Have Been&quot; of computer science 
history. Not sure what I mean? Try this: walk into a functional language forum, 
and ask what a monad is. Nobody yet has been able to produce an answer that 
doesn't involve math theory, or that does involve a practical 
domain-object-based example. In fact, nobody has really said why (or if) monads 
are even still useful. Or catamorphisms. Or any of the other dime-store words 
that the functional community likes to toss around.</li> 
<li><em>... Visual Studio 2010 will ship on time, and be one of the buggiest 
and/or slowest releases in its history.</em> I hate to make this prediction, 
because I really don't want to be right, but there's just so much happening in 
the Visual Studio refactoring effort that it makes me incredibly nervous. 
Widespread adoption of VS2010 will wait until SP1 at the earliest. In fact....
</li> 
<li><em>... Visual Studio 2010 SP 1 will ship within three months of the final 
product.</em> Microsoft knows that people wait until SP 1 to think about 
upgrading, so they'll just plan for an eager SP 1 release, and hope that 
managers will be too hung over from the New Year (still) to notice that the 
necessary shakeout time hasn't happened.</li> 
<li><em>... Apple will ship a tablet with multi-touch on it, and it will flop 
horribly.</em> Not sure why I think this, but I just don't think the 
multi-touch paradigm that Apple has cooked up for the iPhone will carry over to 
a tablet/laptop device. That won't stop them from shipping it, and it won't 
stop Apple fan-boiz from buying it, but that's about where the interest will 
end.</li> 
<li><em>... JDK 7 closures will be debated for a few weeks, then become a fait 
accompli as the Java community shrugs its collective shoulders.</em> Frankly, I 
think the Java community has exhausted its interest in debating new language 
features for Java. Recent college grads and open-source groups with an axe to 
grind will continue to try and make an issue out of this, but I think the 
overall Java community just... doesn't... care. They just want to see JDK 7 
ship someday.</li> 
<li><em>... Scala either &quot;pops&quot; in 2010, or begins to fall apart.
</em> By &quot;pops&quot;, I mean reaches a critical mass of developers 
interested in using it, enough to convince somebody to create a company around 
it, a la G2One.</li> 
<li><em>... Oracle is going to make a serious &quot;cloud&quot; play, probably 
by offering an Oracle-hosted version of Azure or AppEngine.</em> Oracle loves 
the enterprise space too much, and derives too much money from it, to not at 
least appear to have some kind of offering here. Now that they own Java, 
they'll marry it up against OpenSolaris, the Oracle database, and throw the 
whole thing into a series of server centers all over the continent, and call it 
&quot;Oracle 12c&quot; (c for Cloud, of course) or something.</li> 
<li><em>... Spring development will slow to a crawl and start to take a left 
turn toward cloud ideas.</em> VMWare bought SpringSource for a reason, and I 
believe it's entirely centered around VMWare's movement into the cloud 
space&mdash;they want to be more than &quot;just&quot; a virtualization tool. 
Spring + Groovy makes a compelling development stack, particularly if VMWare 
does some interesting hooks-n-hacks to make Spring a virtualization environment 
in its own right somehow. But from a practical perspective, any 
community-driven development against Spring is all but basically dead. The 
source may be downloadable later, like the VMWare Player code is, but making 
contributions back? Fuhgeddabowdit.</li> 
<li><em>... the explosion of e-book readers brings the Kindle 2009 edition way 
down to size.</em> The era of the e-book reader is here, and honestly, while 
I'm glad I have a Kindle, I'm expecting that I'll be dusting it off a shelf in 
a few years. Kinda like I do with my iPods from a few years ago.</li> 
<li><em>... &quot;social networking&quot; becomes the &quot;Web 2.0&quot; of 
2010.</em> In other words, using the term will basically identify you as a tech 
wannabe and clearly out of touch with the bleeding edge.</li> 
<li><em>... Facebook becomes a developer platform requirement.</em> I don't 
pretend to know anything about Facebook&mdash;I'm not even on it, which amazes 
my family to no end&mdash;but clearly Facebook is one of those mechanisms by 
which people reach each other, and before long, it'll start showing up as a 
developer requirement for companies looking to hire. If you're looking to build 
out your resume to make yourself attractive to companies in 2010, mad Facebook 
skillz might not be a bad investment.</li> 
<li><em>... Nintendo releases an open SDK for building games for its next-gen 
DS-based device.</em> With the spectacular success of games on the iPhone, 
Nintendo clearly must see that they're missing a huge opportunity every day 
developers can't write games for the Nintendo DS that are easily downloadable 
to the device for playing. Nintendo is not stupid&mdash;if they don't open up 
the SDK and promote &quot;casual&quot; games like those on the iPhone and those 
that can now be downloaded to the Zune or the XBox, they risk being 
marginalized out of existence.</li> </ul> 
<p>And for the next decade, I predict....</p> 
<ul> 
<li><em>... colleges and unversities will begin issuing e-book reader devices 
to students.</em> It's a helluvalot cheaper than issuing laptops or netbooks, 
and besides....</li> 
<li><em>... netbooks and e-book readers will merge before the decade is out.
</em> Let's be honest&mdash;if the e-book reader could do email and browse the 
web, you have almost the perfect paperback-sized mobile device. As for the 
credit-card sized mobile device....</li> 
<li><em>... mobile phones will all but disappear as they turn into what PDAs 
tried to be.</em> &quot;The iPhone makes calls? Really? You mean Voice-over-IP, 
right? No, wait, over cell signal? It can<em>do </em>that? Wow, there's really 
an app for everything, isn't there?&quot;</li> 
<li><em>... wireless formats will skyrocket in importance all around the 
office and home.</em> Combine the iPhone's Bluetooth (or something similar yet 
lower-power-consuming) with an equally-capable (Bluetooth or otherwise) 
projector, and suddenly many executives can leave their netbook or laptop at 
home for a business presentation. Throw in the Whispersync-aware e-book 
reader/netbook-thing, and now most executives have absolutely zero reason to 
carry anything but their e-book/netbook and their phone/PDA. The day somebody 
figures out an easy way to combine Bluetooth with PayPal on the iPhone or 
Android phone, we will have more or less made pocket change irrelevant. And 
believe me, that day will happen before the end of the decade.</li> 
<li><em>... either Android or Windows Mobile will gain some serious market 
share against the iPhone the day they figure out how to support an open and 
unrestricted AppStore-like app acquisition model.</em> Let's be honest, the 
attraction of iTunes and AppStore is that I can see an &quot;Oh, cool!&quot; 
app on a buddy's iPhone, and have it on mine less than 30 seconds later. If 
Android or WinMo can figure out how to offer that same kind of experience 
without the draconian AppStore policies to go with it, they'll start making up 
lost ground on iPhone in a hurry.</li> 
<li><em>... Apple becomes the DOJ target of the decade.</em> Microsoft was it 
in the 2000's, and Apple's stunning rising success is going to put it squarely 
in the sights of monopolist accusations before long. Coupled with the 
unfortunate health distractions that Steve Jobs has to deal with, Apple's going 
to get hammered pretty hard by the end of the decade, but it will have mastered 
enough market share and mindshare to weather it as Microsoft has.</li> 
<li><em>... Google becomes the next Microsoft.</em> It won't be anything the 
founders do, but Google will do &quot;something evil&quot;, and it will be 
loudly and screechingly pointed out by all of Google's corporate opponents, and 
the star will have fallen.</li> 
<li>... <em>Microsoft finds its way again.</em> Microsoft, as a company, has 
lost its way. This is a company that's not used to losing, and like Bill 
Belichick's Patriots, they will find ways to adapt and adjust to the changed 
circumstances of their position to find a way to win again. What that'll be, I 
have no idea, but historically, the last decade notwithstanding, betting 
against Microsoft has historically been a bad idea. My gut tells me they'll 
figure something new to get that mojo back.</li> 
<li><em>... a politician will make himself or herself famous by standing up to 
the TSA.</em> The scene will play out like this: during a Congressional hearing 
on airline security, after some nut/terrorist tries to blow up another plane 
through nitroglycerine-soaked underwear, the TSA director will suggest all 
passengers should fly naked in order to preserve safety, the congressman/woman 
will stare open-mouthed at this suggestion, proclaim, &quot;Have you no sense 
of decency, sir?&quot; and immediately get a standing ovation and never have to 
worry about re-election again. Folks, if we want to prevent any chance of loss 
of life from a terrorist act on an airplane, we have to prevent passengers from 
getting on them. Otherwise, just accept that it might happen, do a reasonable 
job of preventing it from happening, and let private insurance start offering 
flight insurance against the possibility to reassure the paranoid.</li> </ul> 
<p>See you all next year.</p> <br>

<p></p> .NET&nbsp;|&nbsp; C#&nbsp;|&nbsp; C++&nbsp;|&nbsp; Conferences
&nbsp;|&nbsp;Development Processes&nbsp;|&nbsp; F#&nbsp;|&nbsp; Flash
&nbsp;|&nbsp;Industry&nbsp;|&nbsp; Java/J2EE&nbsp;|&nbsp; Languages&nbsp;|&nbsp;
LLVM&nbsp;|&nbsp; Mac OS&nbsp;|&nbsp; Parrot&nbsp;|&nbsp; Python&nbsp;|&nbsp; 
Reading&nbsp;|&nbsp; Review&nbsp;|&nbsp; Ruby&nbsp;|&nbsp; Scala&nbsp;|&nbsp; 
Security&nbsp;|&nbsp; Social&nbsp;|&nbsp; Solaris&nbsp;|&nbsp; Visual Basic
&nbsp;|&nbsp;VMWare&nbsp;|&nbsp; WCF&nbsp;|&nbsp; Windows&nbsp;|&nbsp; XML 
Services <br>
<br>
 Tuesday, 05 January 2010 01:45:59 (Pacific Standard Time, 
UTC-08:00)<br>
Comments [5] &nbsp;|&nbsp; <br>
<br>
<br>
&nbsp;Sunday, 22 
November 2009<br>
Book Review: Debug It! (Paul Butcher, Pragmatic Bookshelf) 
<br> 
<p>Paul asked me to review this, his first book, and my comment to him was 
that he had a pretty high bar to match; being of the same &quot;series&quot; as
<em>Release It!</em>, Mike Nygard's take on building software ready for 
production (and, in my repeatedly stated opinion, the most important-to-read 
book of the decade),<em>Debug It!</em> had some pretty impressive shoes to 
fill. Paul's comment was pretty predictable: &quot;Thanks for keeping the 
pressure to a minimum.&quot;</p> 
<p>My copy arrived in the mail while I was at the NFJS show in Denver this 
past weekend, and with a certain amount of dread and excitement, I opened the 
envelope and sat down to read for a few minutes. I managed to get halfway 
through it before deciding I had to post a review before I get too caught up in 
my next trip and forget.</p> 
<h4><em>Short version</em></h4> 
<p><em>Debug It!</em> is a great resource for anyone looking to learn the 
science of good debugging. It is entirely language- and platform-agnostic, 
preferring to focus entirely on the<em>process</em> and <em>mindset</em> of 
debugging, rather than on edge cases or command-line switches in a tool or 
language. Overall, the writing is clear and straightforward without being 
preachy or judgmental, and is liberally annotated with real-life case stories 
from both the authors' and the Pragmatic Programmers' own history, which keeps 
the tone lighter and yet still proving the point of the text. Highly 
recommended for the junior developers on the team; senior developers will 
likely find some good tidbits in here as well.</p> 
<h4><em>Long version</em></h4> 
<p><em>Debug It!</em> is an excellently-written and to-the-point description 
of the process of not only identifying and fixing defects in software, but also 
of the attitudes required to keep software from failing. Rather than simply 
tossing off old maxims or warming them over with new terminology (&quot;You 
should always verify the parameters to your procedure calls&quot; replaced with 
&quot;You should always verify the parameters entering a method and ensure the 
fields follow the invariants established in the specification&quot;), Paul 
ensures that when making a point, his prose is clear, the rationale carefully 
explained, and the consequences of not following this advice are clearly 
spelled out. His advice is pragmatic, and takes into account that developers 
can't always follow the absolute rules we'd like to&mdash;he talks about some 
of his experiences with &quot;bug priorities&quot; and how users pretty quickly 
figured out to always set the bug's priority at the highest level in order to 
get developer attention, for example, and some ways to try and address that 
all-too-human failing of bug-tracking systems.</p> 
<p>It needs to be said, right from the beginning, that <em>Debug It!</em> will 
not teach you how to use the debugging features of your favorite IDE, however. 
This is because Paul (deliberately, it seems) takes a platform- and 
language-agnostic approach to the book&mdash;there are no examples of how to 
set breakpoints in gdb, or how to attach the Visual Studio IDE to a running 
Windows service, for example. This will likely weed out those readers who are 
looking for &quot;Google-able&quot; answers to their common debugging problems, 
and that's a shame, because those are probably the very readers that need to 
read this book. Having said that, however, I like this agnostic approach, 
because these ideas and thought processes, the ones that are entirely 
independent of the language or platform, are exactly the kinds of things that 
senior developers carry over with them from one platform to the next. Still, 
the junior developer who picks this book up is going to still need a reference 
manual or the user manual for their IDE or toolchain, and will need to practice 
some with both books in hand if they want to maximize the effectiveness of 
what's in here.</p> 
<p>One of the things I like most about this book is that it is liberally 
adorned with real-life discussions of various scenarios the author team has 
experienced; the reason I say &quot;author team&quot; here is because although 
the stories (for the most part) remain unattributed, there are obvious 
references to &quot;Dave&quot; and &quot;Andy&quot;, which I assume pretty 
obviously refer to Dave Thomas and Andy Hunt, the Pragmatic Programmers and the 
owners of Pragmatic Bookshelf. Some of the stories are humorous, and some of 
them probably would be humorous if they didn't strike so close to my own 
bitterly-remembered experiences. All of them do a good job of reinforcing the 
point, however, thus rendering the prose more effective in communicating the 
idea without getting to be too preachy or bombastic.</p> 
<p>The book obviously intends to target a junior developer audience, because 
most senior developers have already intuitively (or experientially) figured out 
many of the processes described in here. But, quite frankly, I think it would 
be a shame for senior developers to pass on this one; though the temptation 
will be to simply toss it aside and say, &quot;I already do all this 
stuff&quot;, senior developers should resist that urge and read it through 
cover to cover. If nothing else, it'll help reinforce certain ideas, bring some 
of the intuitive process more to light and allow us to analyze what we do right 
and what we do wrong, and perhaps most importantly, give us a common backdrop 
against which we can mentor junior developers in the science of debugging.</p> 
<p>One of the chapters I like in particular, &quot;Chapter 7: Pragmatic Zero 
Tolerance&quot;, is particularly good reading for those shops that currently 
suffer from a deficit of management support for writing good software. In it, 
Paul talks specifically about some of the triage process about bugs (&quot;When 
to fix bugs&quot;), the mental approach developers should have to fixing bugs 
(&quot;The debugging mind-set&quot;) and how to get started on creating good 
software out of bad (&quot;How to dig yourself out of a quality hole&quot;). 
These are techniques that a senior developer can bring to the team and 
implement at a grass-roots level, in many cases without management even being 
aware of what's going on. (It's a sad state of affairs that we sometimes have 
to work behind management's back to write good-quality code, but I know that 
some developers out there are in exactly that situation, and simply saying, 
&quot;Quit and find a new job&quot;, although pithy and good for a laugh on a 
panel, doesn't really offer much in the way of help. Paul doesn't take that 
route here, and that alone makes this book worth reading.)</p> 
<p>Another of the chapters that resonates well with me is the first one in 
Part III (&quot;Debug Fu&quot;), Chapter 8, entitled &quot;Special Cases&quot;, 
in which he tackles a number of &quot;advanced&quot; debugging topics, such as 
&quot;Patching Existing Releases&quot; and &quot;Hesenbugs&quot; 
(Concurrency-related bugs). I won't spoil the punchline for you, but suffice it 
to say that I wish I'd had that chapter on hand to give out to teammates on a 
few projects I've worked on in the past.</p> 
<p>Overall, this book is going to be a huge win, and I think it's a worthy 
successor to the<em>Release It!</em> reputation. Development managers and team 
leads should get a copy for the junior developers on their team as a Christmas 
gift, but only after the senior developers have read through it as well. 
(Senior devs, don't despair&mdash;at 190 pages, you can rip through this in a 
single night, and I can almost guarantee that you'll learn a few ideas you can 
put into practice the next morning to boot.)</p> <br>

<p></p> .NET&nbsp;|&nbsp; C#&nbsp;|&nbsp; C++&nbsp;|&nbsp; Development 
Processes&nbsp;|&nbsp; F#&nbsp;|&nbsp; Industry&nbsp;|&nbsp; Java/J2EE
&nbsp;|&nbsp;Languages&nbsp;|&nbsp; LLVM&nbsp;|&nbsp; Mac OS&nbsp;|&nbsp; Parrot
&nbsp;|&nbsp;Python&nbsp;|&nbsp; Reading&nbsp;|&nbsp; Review&nbsp;|&nbsp; Ruby
&nbsp;|&nbsp;Scala&nbsp;|&nbsp; Solaris&nbsp;|&nbsp; Visual Basic&nbsp;|&nbsp; 
Windows&nbsp;|&nbsp; XML Services <br>
<br>
 Sunday, 22 November 2009 23:24:41 
(Pacific Standard Time, UTC-08:00)<br>
Comments [0] &nbsp;|&nbsp; <br>
<br>
<br>
&nbsp;Tuesday, 13 October 2009<br>
Haacked, but not content; agile still treats 
the disease <br>

<p>Phil Haack wrote a thoughtful, insightful and absolutely correct response to
my earlier blog post. But he's still missing the point.</p> 
<p>The short version: Phil's right when he says, &quot;<strong>Agile is less 
about managing the complexity of an application itself and more about managing 
the complexity of building an application</strong>.&quot; Agile is by far the 
best approach to take when building complex software.</p> 
<p>But that's not where I'm going with this. </p> 
<p>As a starting point in the discussion, I'd like to call attention to one of 
Phil's sidebars: I find it curious (and indicative of the larger point) his 
earlier comment about &quot;<em>I have to wonder, why is that little school 
district in western Pennsylvania engaging in custom software development in the 
first place?</em>&quot; At what point does standing a small Access database up 
qualify as &quot;custom software development&quot;? And I take<em>huge</em> 
issue with Phil's comment immediately thereafter: &quot;&quot; That's totally 
untrue, Phil&mdash;you are, in fact, creating custom educational curricula, for 
your children at home. Not for popular usage, not for commercial use, but 
clearly you're educating your children at home, because you'd be a pretty 
crappy parent if you didn't. You also practice an informal form of medicine 
(&quot;Let me kiss the boo-boo&quot;), psychology (&quot;Now, come on, share 
the truck&quot;), culinary arts (&quot;Would you like mac and cheese 
tonight?&quot;), acting (&quot;Aaar! I'm the Tickle Monster!&quot;) and a 
vastly larger array of &quot;professional&quot; skills that any of the 
&quot;professionals&quot; will do vastly better than you.</p> 
<p>In other words, you're not a professional actor/chef/shrink/doctor, you're 
an amateur one, and you want tools that let you practice your amateur 
&quot;professions&quot; as you wish, without requiring the skills and trappings 
(and overhead) of a professional in the same arena.</p> 
<p>Consider this, Phil: your child decides it's time to have a puppy. (We all 
know the kids are the ones who make these choices, not us, right?) So, being 
the conscientious parent that you are, you decide to build a doghouse for the 
new puppy to use to sleep outdoors (forgetting, as all parents do, that the 
puppy will actually end up sleeping in the bed with your child, but that's 
another discussion for another day). So immediately you head on down to Home 
Depot, grab some lumber, some nails, maybe a hammer and a screwdriver, some 
paint, and head on home.</p> 
<p>Whoa, there, turbo. Aren't you forgetting a few things? For starters, you 
need to get the concrete for the foundation, rebar to support the concrete in 
the event of a bad earthquake, drywall, fire extinguishers, sirens for the 
emergency exit doors... And of course, you'll need a foreman to coordinate all 
the work, to make sure the foundation is poured before the carpenters show up 
to put up the trusses, which in turn has to happen before the drywall can go 
up...</p> 
<p>We in this industry have a jealous and irrational attitude towards the 
amateur software developer. This was even apparent in the Twitter comments that 
accompanied the conversation around my blog post: &quot;@tedneward treating the 
disease would mean... have the client have all their ideas correct from the 
start&quot; (from@kelps). In other words, &quot;bad client! No biscuit!&quot;?
</p> 
<p>Why is it that we, IT professionals, consider anything that involves doing 
something other than simply putting content into an application to be 
&quot;custom software development&quot;? Why can't end-users create tools of 
their own to solve their own problems at a scale appropriate to their local 
problem?</p> 
<p>Phil offers a few examples of why end-users creating their own tools is a 
Bad Idea:</p> 
<blockquote> 
<p>I remember one rescue operation for a company drowning in the complexity of 
a &ldquo;simple&rdquo; Access application they used to run their business. It 
was simple until they started adding new business processes they needed to 
track. It was simple until they started<em>emailing copies around </em>and were 
unsure which was the &ldquo;master copy&rdquo;. Not to mention all the data 
integrity issues and difficulty in changing the monolithic procedural 
application code.</p> </blockquote> 
<blockquote> 
<p>I also remember helping a teachers union who started off with a simple 
attendance tracker style app (to use an example Ted mentions) and just scaled 
it up to an atrociously complex Access database with stranded data and manual 
processes where they printed excel spreadsheets to paper, then manually entered 
it into another application.</p> </blockquote> 
<p>And you know what? </p> 
<p>This is not a bad state of affairs. </p> 
<p>Oh, of course, we, the IT professionals, will immediately pounce on all the 
things wrong with their attempts to extend the once-simple application/solution 
in ways beyond its capabilities, and we will scoff at their solutions, but you 
know what? That just speaks to our insecurities, not the effort expended. You 
think Wolfgang Puck isn't going to throw back his head and roar at my lame 
attempts at culinary experimentation? You think Frank Lloyd Wright wouldn't 
cringe in horror at my cobbled-together doghouse? And I'll bet Maya Angelou 
will be so shocked at the ugliness of my poetry that she'll post it somewhere 
on the &quot;So You Think You're A Poet&quot; website.</p> 
<p>Does that mean I need to abandon my efforts to all of these things?</p> 
<p>The agilists' community reaction to my post would seem to imply so. 
&quot;If you aren't a professional, don't even attempt this?&quot; Really? Is 
that the message we're preaching these days?</p> 
<p>End users have just as much a desire and right to be amateur software 
developers as we do at being amateur cooks, photographers, poets, construction 
foremen, and musicians. And what do you do when you want to add an addition to 
your house instead of just building a doghouse? Or when you want to cook for 
several hundred people instead of just your family?</p> 
<p>You hire a professional, and let them do the project professionally.</p> 
<br> 
<p></p> .NET&nbsp;|&nbsp; C#&nbsp;|&nbsp; C++&nbsp;|&nbsp; Conferences
&nbsp;|&nbsp;Development Processes&nbsp;|&nbsp; F#&nbsp;|&nbsp; Flash
&nbsp;|&nbsp;Industry&nbsp;|&nbsp; Java/J2EE&nbsp;|&nbsp; Languages&nbsp;|&nbsp;
LLVM&nbsp;|&nbsp; Mac OS&nbsp;|&nbsp; Parrot&nbsp;|&nbsp; Python&nbsp;|&nbsp; 
Ruby&nbsp;|&nbsp; Scala&nbsp;|&nbsp; Social&nbsp;|&nbsp; Solaris&nbsp;|&nbsp; 
Visual Basic&nbsp;|&nbsp; VMWare&nbsp;|&nbsp; WCF&nbsp;|&nbsp; Windows
&nbsp;|&nbsp;XML Services <br>
<br>
 Tuesday, 13 October 2009 13:42:22 (Pacific 
Daylight Time, UTC-07:00)<br>
Comments [12] &nbsp;|&nbsp; <br>
<br>
<br>

&nbsp;Monday, 23 March 2009<br>
SDWest, SDBestPractices, SDArch&amp;Design: 
RIP, 1975 - 2009 <br>

<p>This email crossed my Inbox last week while I was on the road:</p> 
<blockquote> 
<p>Due to the current economic situation, TechWeb has made the difficult 
decision to discontinue the Software Development events, including SD West, SD 
Best Practices and Architecture &amp; Design World. We are grateful for your 
support during SD's twenty-four year history and are disappointed to see the 
events end.</p> </blockquote> 
<p>This really bums me out, because the SD shows were some of the best shows 
I&rsquo;ve been to, particularly SD West, which always had a great 
cross-cutting collection of experts from all across the industry&rsquo;s big 
technical areas: C++, Java, .NET, security, agile, and more. It was also where 
I got to meet and interview Bjarne Stroustrup, a personal hero of mine from 
back in my days as a C++ developer, where I got to hang out each year with 
Scott Meyers, another personal hero (and now a good friend) as well as editor on
<em>Effective Enterprise Java</em>, and Mike Cohn, another good friend as well 
as a great guy to work for. It was where I first met Gary McGraw, in a rather 
embarrassing fashion&mdash;in the middle of his presentation on security, my 
cell phone went off with a klaxon alarm ring tone loud enough to be heard 
throughout the entire room, and as every head turned to look at me, he 
commented dryly, &ldquo;That&rsquo;s the buffer overrun alarm&mdash;somewhere 
in the world, a buffer overrun attack is taking place.&rdquo;</p> 
<p>On a positive note, however, the email goes on to say that &ldquo;Cloud 
Connect [will] take over SD West's dates in March 2010 at the Santa Clara 
Convention Center&rdquo;, which is good news, since it means (hopefully) that 
I&rsquo;ll still get a chance to make my yearly pilgrimage to In-N-Out....</p> 
<p>Rest in peace, SD. You will be missed.</p> <br>

<p></p> .NET&nbsp;|&nbsp; C#&nbsp;|&nbsp; C++&nbsp;|&nbsp; Conferences
&nbsp;|&nbsp;Development Processes&nbsp;|&nbsp; F#&nbsp;|&nbsp; Flash
&nbsp;|&nbsp;Java/J2EE&nbsp;|&nbsp; Languages&nbsp;|&nbsp; Ruby&nbsp;|&nbsp; 
Security&nbsp;|&nbsp; Visual Basic&nbsp;|&nbsp; WCF&nbsp;|&nbsp; Windows
&nbsp;|&nbsp;XML Services <br>
<br>
 Monday, 23 March 2009 17:22:43 (Pacific 
Daylight Time, UTC-07:00)<br>
Comments [0] &nbsp;|&nbsp; <br>
<br>
<br>

&nbsp;Sunday, 22 February 2009<br>
As for Peer Review, Code Review? <br>

<p>Interesting little tidbit crossed my Inbox today...</p> 
<blockquote> 
<p>Only 8% members of the Scientific Research Society agreed that &quot;peer 
review works well as it is&quot;. (Chubin and Hackett, 1990; p.192).</p> 
<p>&quot;A recent U.S. Supreme Court decision and an analysis of the peer 
review system substantiate complaints about this fundamental aspect of 
scientific research.&quot; (Horrobin, 2001)</p> 
<p>Horrobin concludes that peer review &quot;is a non-validated charade whose 
processes generate results little better than does chance.&quot; (Horrobin, 
2001). This has been statistically proven and reported by an increasing number 
of journal editors.</p> 
<p>But, &quot;Peer Review is one of the sacred pillars of the scientific 
edifice&quot; (Goodstein, 2000), it is a necessary condition in quality 
assurance for Scientific/Engineering publications, and &quot;Peer Review is 
central to the organization of modern science&hellip;why not apply scientific 
[and engineering] methods to the peer review process&quot; (Horrobin, 2001).</p>
<p>... </p> 
<p>Chubin, D. R. and Hackett E. J., 1990, Peerless Science, Peer Review and 
U.S. Science Policy; New York, State University of New York Press.</p> 
<p>Horrobin, D., 2001, &quot;Something Rotten at the Core of Science?&quot; 
Trends in Pharmacological Sciences, Vol. 22, No. 2, February 2001. Also at
http://www.whale.to/vaccine/sci.html and 
http://post.queensu.ca/~forsdyke/peerrev4.htm (both pages were accessed on 
February 1, 2009)</p> 
<p>Goodstein, D., 2000, &quot;How Science Works&quot;, U.S. Federal Judiciary 
Reference Manual on Evidence, pp. 66-72 (referenced in Hoorobin, 2000)</p> 
</blockquote> 
<p>I know that we don't generally cite the scientific process as part of the 
rationale for justifying code reviews, but it seems to have a distinct 
relationship. If the peer review process is similar in concept to the code 
review process, and the scientific types are starting to doubt the efficacy of 
peer reviews, what does that say about the code review?</p>
<p><em>(Note: I'm not a scientist, so my familiarity with peer review is 
third-hand at best; I'm wide open to education here. How are the code review 
and peer review processes different, if in fact, they are different?)</em> </p>
<p>The Horrobin &quot;sacred pillars&quot; quote, in particular, makes me 
curious: Don't we already apply &quot;scientific [and engineering] 
methods&quot; to the peer review process? And can we honestly say that we in 
the software industry apply &quot;scientific [and engineering]&quot; methods to 
the code review process? Can we iterate the list? Or do we just trust that 
intuition and &quot;more eyeballs&quot; will help spot any obvious defects?</p> 
<p>The implications here, when tied up next to the open source fundamental 
principle that states that &quot;more eyeballs is better&quot;, are interesting 
to consider.<em>If</em> review is not a scientifically-proven or 
&quot;engineeringly-sound&quot; principle, then the open source folks are 
kidding themselves in thinking they're more secure or better-engineered.<em>If
</em> we conduct a scientific measurement of code-reviewed code and find that 
it is &quot;a non-validated charade whose processes generate results little 
better than does chance&quot;, we've at least conducted the study, and can 
start thinking about ways to make it better. (I do wish the email author had 
cited sources that provide the background to the statement, &quot;This has been 
statistically proven&quot;, though.)</p> 
<p>I know this is going to seem like a trolling post, but I'm genuinely 
curious--do we, in the software industry, have any scientifically-conducted 
studies with quantifiable metrics that imply that code-reviewed code is better 
than non-reviewed code? Or are we just taking it as another article of faith?
</p> 
<p>(For those who are curious, the email that triggered all this was an 
invitation to a conference on peer review.</p> 
<blockquote> 
<p>This is the purpose of the International Symposium on Peer Reviewing: ISPR (
http://www.ICTconfer.org/ispr) being organized in the context of The 3rd 
International Conference on Knowledge Generation, Communication and Management: 
KGCM 2009 (http://www.ICTconfer.org/kgcm), which will be held on July 10-13, 
2009, in Orlando, Florida, USA.</p> </blockquote> 
<p>I doubt it has any direct relevance to software, but I could be wrong. If 
you go, let me know of your adventures and conclusions. )</p> <br>

<p></p> Conferences&nbsp;|&nbsp; Development Processes&nbsp;|&nbsp; Security
&nbsp;|&nbsp;Windows <br>
<br>
 Sunday, 22 February 2009 10:36:43 (Pacific 
Standard Time, UTC-08:00)<br>
Comments [9] &nbsp;|&nbsp; <br>
<br>
<br>

&nbsp;Saturday, 03 January 2009<br>
Phishing attacks know no boundaries... or 
limits <br>

<p>People are used to the idea of phishing attacks showing up in their email, 
but in glowing testament to the creativity of potential attackers, Twitter 
recently has seen a rash of phishing attacks through Twitter's &quot;direct 
messaging&quot; feature.</p> 
<p>The attack plays out like this: someone on your Twitter followers list 
sends you a direct message saying, &quot;hey! check out this funny blog about 
you... &quot; with a hyperlink to a website, 
&quot;http://jannawalitax.blogspot.com/&quot; . Clicking on the hyperlink takes 
you to a website that redirects to a webpage containing what<em>looks</em> like 
the Twitter login page. This is an attempt to get you to fill in your username, 
and more importantly, your password.</p> 
<p>Needless to say, I'd avoid it. If you do get suckered in (hey, I admit it, 
I did), make sure to change your password immediately after.</p> 
<p>What I find fascinating about this attack is that the direct messages come 
from people that are on my followers list--unless Twitter somehow has a hole in 
it that allows non-followers to direct-message you, it means that this is a 
classic security Ponzi scheme: I use the attack to gather the credentials for 
the people that I'm following directly, then log in and use those credentials 
to attack their followers, then use those gathered credentials to attack their 
followers, and so on. Fixing this is also going to be a pain--literally, 
everybody on Twitter has to change their password, or the scheme can continue 
with the credentials of those who didn't. (Assuming Twitter doesn't somehow lop 
the attack off at the knees, for example, by disallowing hyperlinks or 
something equally draconian.)</p> 
<p>We won't even stop to consider what damage might be done if a Twitter-user 
uses the same password and login name for their Twitter account as they do for 
other accounts (such as email, banking websites, and so on). If you're one of 
those folks, you seriously might want to reconsider the strategy of using the 
same password for all your websites, unless you don't care if they get spoofed.
</p> 
<p>There's two lessons to be learned here. </p> 
<p>One, that as a user of a service--<em>any</em> service--you have to be 
careful about when and how you're entering your credentials. It's easy to 
simply get into the habit of offering them up every time you see something that 
looks familiar, and if supposed &quot;computer experts&quot; (as most of the 
Twitterverse can be described) can be fooled, then how about the casual user?
</p> 
<p>Two, and perhaps the more important lesson for those of us who build 
software, that any time you build a system that enables people to communicate, 
even when you put a lot of energy into making sure that the system is secure, 
there's always an angle that attackers will find that will expose a 
vulnerability, even if it's just a partial one (such as the gathering of 
credentials here). If you don't need to allow hyperlinks, don't. If you don't 
need to allow Javascript, don't. Start from the bare minimum that people need 
to make your system work, and only add new capabilities after they've been 
scrutinized in a variety of ways. (YAGNI sometimes works to our advantage in 
more ways than one, it turns out.)</p> 
<p>Kudos, by the way, to the Twitter-keepers, who had a message describing the 
direct-message phishing attack on the Twitter Home page within hours.</p> <br>

<p></p> .NET&nbsp;|&nbsp; Development Processes&nbsp;|&nbsp; Java/J2EE
&nbsp;|&nbsp;Security <br>
<br>
 Saturday, 03 January 2009 17:22:38 (Pacific 
Standard Time, UTC-08:00)<br>
Comments [0] &nbsp;|&nbsp; <br>
<br>
<br>

&nbsp;Wednesday, 31 December 2008<br>
2009 Predictions, 2008 Predictions 
Revisited <br>

<p>It's once again that time of year, and in keeping with my tradition, I'll 
revisit the 2008 predictions to see how close I came before I start waxing 
prophetic on the coming year. (I'm thinking that maybe the next year--2010's 
edition--I should actually take a shot at predicting the next decade, but I'm 
not sure if I'd remember to go back and revisit it in 2020 to see how I did. 
Anybody want to set a calendar reminder for Dec 31 2019 and remind me, complete 
with URL? )</p> 
<p>Without further preamble, here's what I said for 2008:</p> 
<ul> 
<li><strong>THEN: </strong><em>General</em>: The buzz around building custom 
languages will only continue to build. More and more tools are emerging to 
support the creation of custom programming languages, like Microsoft's Phoenix, 
Scala's parser combinators, the Microsoft DLR, SOOT, Javassist, 
JParsec/NParsec, and so on. Suddenly, the whole &quot;write your own lexer and 
parser and AST from scratch&quot; idea seems about as outmoded as the idea of 
building your own String class. Granted, there are cases where a from-hand 
scanner/lexer/parser/AST/etc is the Right Thing To Do, but there are times when 
building your own String class is the Right Thing To Do, too. Between the rich 
ecosystem of dynamic languages that could be ported to the JVM/CLR, and the 
interesting strides being made on both platforms (JVM and CLR) to make them 
more &quot;dynamic-friendly&quot; (such as being able to reify classes or 
access the call stack directly), the probability that your company will find a 
need that is best answered by building a custom language are only going to rise.
<strong>NOW: </strong>The buzz has definitely continued to build, but buzz can 
only take us so far. There's been some scattershot use of custom languages in a 
few scattershot situations, but it's certainly not &quot;taken the world by 
storm&quot; in any meaningful way yet.</li> 
<li><strong>THEN: </strong><em>General</em>: The hype surrounding 
&quot;domain-specific languages&quot; will peak in 2008, and start to generate 
a backlash. Let's be honest: when somebody looks you straight in the eye and 
suggests that &quot;scattered, smothered and covered&quot; is a domain-specific 
language, the term has lost all meaning. A lexicon unique to an industry is not 
a domain-specific language; it's a lexicon. Period. If you can incorporate said 
lexicon into your software, thus making it accessible to non-technical 
professionals, that's a good thing. But simply using the lexicon doesn't make 
it a domain-specific language. Or, alternatively, if you like, every single API 
designed for a particular purpose is itself a domain-specific language. This 
means that Spring configuration files are a DSL. Deployment descriptors are a 
DSL. The Java language is a DSL (since the domain is that of programmers 
familiar with the Java language). See how nonsensical this can get? Until 
somebody comes up with a workable definition of the term &quot;domain&quot; in 
&quot;domain-specific language&quot;, it's a nonsensical term. The idea is a 
powerful one, mind you--creating something that's more &quot;in tune&quot; with 
what users understand and can use easily is a technique that's been proven for 
decades now. Anybody who's ever watched an accountant rip an entirely new set 
of predictions for the new fiscal outlook based entirely on a few seed numbers 
and a deeply-nested set of Excel macros knows this already. Whether you call 
them domain-specific languages or &quot;little languages&quot; or 
&quot;user-centric languages&quot; or &quot;macro language&quot; is really up 
to you.<strong>NOW:</strong> The backlash hasn't begun, but only because the 
DSL buzz hasn't materialized in much way yet--see previous note. It generally 
takes a year or two of deployments (and hard-earned experience) before a 
backlash begins, and we haven't hit that &quot;deployments&quot; stage yet in 
anything yet resembling &quot;critical mass&quot; yet. But the DSL/custom 
language buzz continues to grow, and the more the buzz grows, the more the 
backlash is likey.</li> 
<li><strong>THEN: </strong><em>General</em>: Functional languages will begin 
to make their presence felt. Between Microsoft's productization plans for F# 
and the growing community of Scala programmers, not to mention the inherently 
functional concepts buried inside of LINQ and the concurrency-friendly 
capabilities of side-effect-free programming, the world is going to find itself 
working its way into functional thinking either directly or indirectly. And 
when programmers start to see the inherent capabilities inside of Scala (such 
as Actors) and/or F# (such as asynchronous workflows), they're going to embrace 
the strange new world of functional/object hybrid and never look back.<strong>
NOW:</strong> Several books on F# and Scala (and even one or two on Haskell!) 
were published in 2008, and several more (including one of my own) are on the 
way. The functional buzz is building, and lots of disparate groups are each 
evaluating it (functional programming) independently.</li> 
<li><strong>THEN: </strong><em>General</em>: MacOS is going to start posting 
some serious market share numbers, leading lots of analysts to predict that 
Microsoft Windows has peaked and is due to collapse sometime within the 
remainder of the decade. Mac's not only a wonderful OS, but it's some of the 
best hardware to run Vista on. That will lead not a few customers to buy Mac 
hardware, wipe the machine, and install Vista, as many of the uber-geeks in the 
Windows world are already doing. This will in turn lead Gartner (always on the 
lookout for an established trend they can &quot;predict&quot; on) to suggest 
that Mac is going to end up with 115% market share by 2012 (.8 probability), 
then sell you this wisdom for a mere price of $1.5 million (per copy).<strong>
NOW:</strong> Can't speak to the Gartner report--I didn't have $1.5 million 
handy--but certainly the MacOS is growing in popularity. More on that later.
</li> 
<li><strong>THEN:</strong> <em>General</em>: Ted will be hired by Gartner... 
if only to keep him from smacking them around so much. .0001 probability, with 
probability going up exponentially as my salary offer goes up exponentially. 
(Hey, I've got kids headed for college in a few years.)<strong>NOW:</strong> 
Well, Gartner appears to have lost my email address and phone number, but I'm 
sure they were planning to make me that offer.</li> 
<li><strong>THEN: </strong><em>General</em>: MacOS is going to start creaking 
in a few places. The Mac OS is a wonderful OS, but it's got its own creaky 
parts, and the more users that come to Mac OS, the more that software packages 
are going to exploit some of those creaky parts, leading to some instability in 
the Mac OS. It won't be widespread, but for those who are interested in finding 
it, they're there. Assuming current trends (of customers adopting Mac OS) hold, 
the Mac OS 10.6 upgrade is going to be a very interesting process, indeed.
<strong>NOW:</strong> Shhh. Don't tell anybody, but I've been seeing it 
starting to happen. Don't get me wrong, Apple still does a pretty good job with 
the OS, but the law of numbers has started to create some bad upgrade scenarios 
for some people.</li> 
<li><strong>THEN: </strong><em>General</em>: Somebody is going to realize that 
iTunes is the world's biggest monopoly on music, and Apple will be forced to 
defend itself in the court of law, the court of public opinion, or both. Let's 
be frank: if this were Microsoft, offering music that can only be played on 
Microsoft music players, the world would be through the roof. All UI goodness 
to one side, the iPod represents just as much of a monopoly in the music player 
business as Internet Explorer did in the operating system business, and if the 
world doesn't start taking Apple to task over this, then &quot;justice&quot; is 
a word that only applies when losers in an industry want to drag down the 
market leader (which I firmly believe to be the case--nobody likes more than to 
pile on the successful guy).<strong>NOW:</strong> Nothing this year.</li> 
<li><strong>THEN: </strong><em>General</em>: Somebody is going to realize that 
the iPhone's &quot;nothing we didn't write will survive the next upgrade 
process&quot; policy is nothing short of draconian. As my father, who gets it 
right every once in a while, says, &quot;If I put a third-party stereo in my 
car, the dealer doesn't get to rip it out and replace it with one of their own 
(or nothing at all!) the next time I take it in for an oil change&quot;. Fact 
is, if I buy the phone, I own the phone, and I own what's on it. Unfortunately, 
this takes us squarely into the realm of DRM and IP ownership, and we all know 
how clear-cut that is... But once the general public starts to understand some 
of these issues--and I think the iPhone and iTunes may just be the vehicle that 
will teach them--look out, folks, because the backlash will be huge. As in, 
&quot;Move over, Mr. Gates, you're about to be joined in infamy by your other 
buddy Steve....&quot;<strong>NOW:</strong> Apple released iPhone 2.0, and with 
it, the iPhone SDK, so at least Apple has opened the dashboard to third-party 
stereos. But the deployment model (AppStore) is still a bit draconian, and 
Apple still jealously holds the reins over which apps can be deployed there and 
which ones can't, so maybe they haven't learned their lesson yet, after all....
</li> 
<li><strong>THEN: </strong><em>Java</em>: The OpenJDK in Mercurial will slowly 
start to see some external contributions. The whole point of Mercurial is to 
allow for deeper control over which changes you incorporate into your build 
tree, so once people figure out how to build the JDK and how to hack on it, the 
local modifications will start to seep across the Internet....<strong>NOW:
</strong> OpenJDK has started to collect contributions from external (to Sun) 
sources, but still in relatively small doses, it seems. None of the local 
modifications I envisioned creeping across the 'Net have begun, that I can see, 
so maybe it's still waiting to happen. Or maybe the OpenJDK is too complicated 
to really allow for that kind of customization, and it never will.</li> 
<li><strong>THEN:</strong> <em>Java</em>: SpringSource will soon be seen as a 
vendor like BEA or IBM or Sun. Perhaps with a bit better reputation to begin, 
but a vendor all the same.<strong>NOW:</strong> SpringSource's acquisition of 
G2One (the company behind Groovy just as SpringSource backs Spring) only 
reinforced this image, but it seems it's still something that some fail to 
realize or acknowledge due to Spring's open-source (?) nature. (I'm not a 
Spring expert by any means, but apparently Spring 3 was pulled back inside the 
SpringSource borders, leading some people to wonder what SpringSource is up to, 
and whether or not Spring will continue to be open source after all.)</li> 
<li><strong>THEN:</strong> <em>.NET</em>: Interest in OpenJDK will bootstrap 
similar interest in Rotor/SSCLI. After all, they're both VMs, with lots of 
interesting ideas and information about how the managed platforms work.<strong>
NOW:</strong> Nope, hasn't really happened yet, that I can see. Not even the 
2nd edition of the SSCLI book (by Joel Pobar and yours truly, yes that was a 
plug) seemed to foster the kind of attention or interest that I'd expected, or 
at least, not on the scale I'd thought might happen.</li> 
<li><strong>THEN: </strong><em>C++/Native</em>: If you've not heard of LLVM 
before this, you will. It's a compiler and bytecode toolchain aimed at the 
native platforms, complete with JIT and GC.<strong>NOW:</strong> Apple sank a 
lot of investment into LLVM, including hosting an LLVM conference at the 
corporate headquarters.</li> 
<li><strong>THEN:</strong> <em>Java</em>: Somebody will create Yet Another 
Rails-Killer Web Framework. 'Nuff said.<strong>NOW:</strong> You know what? I 
honestly can't say whether this happened or not; I was completely not paying 
attention.</li> 
<li><strong>THEN:</strong> <em>Native</em>: Developers looking for a native 
programming language will discover D, and be happy. Considering D is from the 
same mind that was the core behind the Zortech C++ compiler suite, and that D 
has great native platform integration (building DLLs, calling into DLLs easily, 
and so on), not to mention automatic memory management (except for those areas 
where you want manual memory management), it's definitely worth looking into.
www.digitalmars.com <strong>NOW:</strong> D had its own get-together as well, 
and appears to still be going strong, among the group of developers who still 
work on native apps (and aren't simply maintaining legacy C/C++ apps).</li> 
</ul> 
<p>Now, for the 2009 predictions. The last set was a little verbose, so let me 
see if I can trim the list down a little and keep it short and sweet:</p> 
<ul> 
<li><em>General:</em> &quot;Cloud&quot; will become the next &quot;ESB&quot; 
or &quot;SOA&quot;, in that it will be something that everybody will talk 
about, but few will understand and even fewer will do anything with. 
(Considering the widespread disparity in the definition of the term, this seems 
like a no-brainer.)</li> 
<li><em>Java</em>: Interest in Scala will continue to rise, as will the number 
of detractors who point out that Scala is too hard to learn.</li> 
<li><em>.NET</em>: Interest in F# will continue to rise, as will the number of 
detractors who point out that F# is too hard to learn. (Hey, the two really are 
cousins, and the fortunes of one will serve as a pretty good indication of the 
fortunes of the other, and both really seem to be on the same arc right now.)
</li> 
<li><em>General:</em> Interest in all kinds of functional languages will 
continue to rise, and more than one person will take a hint from Bob 
&quot;crazybob&quot; Lee and liken functional programming to AOP, for good and 
for ill. People who took classes on Haskell in college will find themselves 
reaching for their old college textbooks again.</li> 
<li><em>General:</em> The iPhone is going to be hailed as &quot;the enterprise 
development platform of the future&quot;, and companies will be rolling out 
apps to it. Look for Quicken iPhone edition, PowerPoint and/or Keynote iPhone 
edition, along with connectors to hook the iPhone up to a presentation device, 
and (I'll bet) a World of Warcraft iPhone client (legit or otherwise). iPhone 
is the new hotness in the mobile space, and people will flock to it madly.</li> 
<li><em>.NET</em>: Another Oslo CTP will come out, and it will bear only a 
superficial resemblance to the one that came out in October at PDC. Betting on 
Oslo right now is a fools' bet, not because of any inherent weakness in the 
technology, but just because it's way too early in the cycle to be thinking 
about for anything vaguely resembling production code.</li> 
<li><em>.NET</em>: The IronPython and IronRuby teams will find some serious 
versioning issues as they try to manage the DLR versioning story between 
themselves and the CLR as a whole. An initial hack will result, which will be 
codified into a standard practice when .NET 4.0 ships. Then the next release of 
IPy or IRb will have to try and slip around its restrictions in 2010/2011. By 
2012, IPy and IRb will have to be shipping as part of Visual Studio just to put 
the releases back into lockstep with one another (and the rest of the .NET 
universe).</li> 
<li><em>Java</em>: The death of JSR-277 will spark an uprising among the two 
leading groups hoping to foist it off on the Java community--OSGi and 
Maven--while the rest of the Java world will breathe a huge sigh of relief and 
look to see what &quot;modularity&quot; means in Java 7. Some of the alpha 
geeks in Java will start using--if not building--JDK 7 builds just to get a 
heads-up on its impact, and be quietly surprised and, I dare say, perhaps even 
pleased.</li> 
<li><em>Java</em>: The invokedynamic JSR will leapfrog in importance to the 
top of the list.</li> 
<li><em>Windows</em>: Another Windows 7 CTP will come out, and it will spawn 
huge media interest that will eventually be remembered as Microsoft promises, 
that will eventually be remembered as Microsoft guarantees, that will 
eventually be remembered as Microsoft FUD and &quot;promising much, delivering 
little&quot;. Microsoft ain't always at fault for the inflated expectations 
people have--sometimes, yes, perhaps even a lot of times, but not always.</li> 
<li><em>Mac OS</em>: Apple will begin to legally threaten the clone market 
again, except this time somebody's going to get the DOJ involved. (Yes, this is 
the iPhone/iTunes prediction from last year, carrying over. I still expect this 
to happen.)</li> 
<li><em>Languages</em>: Alpha-geek developers will start creating their own 
languages (even if they're obscure or bizarre ones like Shakespeare or Ook#) 
just to have that listed on their resume as the DSL/custom language buzz 
continues to build.</li> 
<li><em>XML Services</em>: Roy Fielding will officially disown most of the 
&quot;REST&quot;ful authors and software packages available. Nobody will 
care--or worse, somebody looking to make a name for themselves will proclaim 
that Roy &quot;doesn't really understand REST&quot;. And they'll be right--Roy 
doesn't understand what<em>they</em> consider to be REST, and the fact that he 
created the term will be of no importance anymore. Being &quot;REST&quot;ful 
will equate to &quot;I did it myself!&quot;, complete with expectations of a 
gold star and a lollipop.</li> 
<li><em>Parrot</em>: The Parrot guys will make at least one more minor point 
release. Nobody will notice or care, except for a few doggedly stubborn Perl 
hackers. They will find themselves having nightmares of previous lives carrying 
around OS/2 books and Amiga paraphernalia. Perl 6 will celebrate it's 
seventh... or is it eighth?... anniversary of being announced, and nobody will 
notice.</li> 
<li><em>Agile</em>: The debate around &quot;Scrum Certification&quot; will 
rise to a fever pitch as short-sighted money-tight companies start looking for 
reasons to cut costs and either buy into agile at a superficial level and watch 
it fail, or start looking to cut the agilists from their company in order to 
replace them with cheaper labor.</li> 
<li><em>Flash</em>: Adobe will continue to make Flex and AIR look more like C# 
and the CLR even as Microsoft tries to make Silverlight look more like Flash 
and AIR. Web designers will now get to experience the same fun that back-end 
web developers have enjoyed for near-on a decade, as shops begin to 
artificially partition themselves up as either &quot;Flash&quot; shops or 
&quot;Silverlight&quot; shops.</li> 
<li><em>Personal</em>: Gartner will still come knocking, looking to hire me 
for outrageous sums of money to do nothing but blog and wax prophetic.</li> 
</ul> 
<p>Well, so much for brief or short. See you all again next year....</p> <br>

<p></p> .NET&nbsp;|&nbsp; C#&nbsp;|&nbsp; C++&nbsp;|&nbsp; Conferences
&nbsp;|&nbsp;Development Processes&nbsp;|&nbsp; F#&nbsp;|&nbsp; Flash
&nbsp;|&nbsp;Java/J2EE&nbsp;|&nbsp; Languages&nbsp;|&nbsp; LLVM&nbsp;|&nbsp; 
Mac OS&nbsp;|&nbsp; Parrot&nbsp;|&nbsp; Ruby&nbsp;|&nbsp; Security&nbsp;|&nbsp; 
Solaris&nbsp;|&nbsp; Visual Basic&nbsp;|&nbsp; Windows&nbsp;|&nbsp; XML Services
<br> <br>
 Wednesday, 31 December 2008 23:54:29 (Pacific Standard Time, 
UTC-08:00)<br>
Comments [5] &nbsp;|&nbsp; <br>
<br>
<br>
&nbsp;Friday, 12 
December 2008<br>
Ruminations on Women in IT <br>

<p>When I was in college, at the University of California, Davis, I lived in 
the International Relations building (D Building in the Tercero dorm area, for 
any other UCD alum out there), and got my first real glimpse of the feminist 
movement up front. It seemed like it was filled with militant, angry members of 
the female half of the species, who insisted that their gender was spelled 
&quot;womyn&quot;, so that it wasn't somehow derived from &quot;man&quot; 
(wo-man, wo-men, get it?), who blamed most of the world's problems on the fact 
that men were running the show, and that therefore, because of my own gender, I 
was to share equally in the blame for its ills.</p> 
<p>Maybe I was--Lord knows I certainly wasn't an entirely nice guy back then 
(and some will chirp from the back of the theater, &quot;back 
then?!?&quot;)--but it still left the whole &quot;feminist&quot; thing as 
something I couldn't really be around, much less support.</p> 
<p>My sister, it turned out, had a different experience at University of 
California, Santa Cruz, and became one.</p> 
<p>Needless to say, this made family get-togethers somewhat awkward.</p> 
<p>Then, a few years later, she asked my help in purchasing a new computer for 
herself. Basically, she just wanted me along to help explain any of the 
technical terms that she wasn't entirely familiar with, and to give her some 
advice on whether they were important to her needs. Not an unreasonable 
request, and not something I wouldn't do for anybody else, male or female 
alike. (I sometimes wish my father would ask my help<em>before</em> buying, but 
that's another story.)</p> 
<p>We went to the store, and I got my first lesson in sexual discrimination. 
</p> 
<p>The entire time we were in the store, despite the fact that it was my 
sister asking the questions, despite the fact that I only answered questions 
that she asked of me directly (in other words, I was there to help her, not to 
help the sales guy sell to her), almost the entire conversation was spent with 
the sales guy talking to me, even if he was answering her question. His body 
language was unquestionably that of, &quot;She's clearly not capable of making 
this decision herself&quot;, and addressed everything to me, despite her 
repeated attempts to catch his eye and have him talk to her, the actual 
purchaser with the question.</p> 
<p>I was a bit taken aback. I don't think the sales guy even noticed. That 
bothered me more than anything.</p> 
<p>Ever since that time, I've been curiously and cautiously trying to figure 
out why there aren't more women in IT.</p> 
<p>Several theories have presented themselves over the years:</p> 
<ol> 
<li>Women, aside from a statistical minority, are structurally incapable of 
mastering IT. This is the &quot;math is hard&quot; argument, and I think we can 
all pretty much agree where this one belongs.</li> 
<li>Women are encouraged/forced down an educational path that leads them away 
from IT until much later in life. I've heard this from a couple of women my 
age, and while I think there may be some validity to it, at least back in the 
day, I don't know if there still is. I'd love to get some feedback from recent 
high school or college grads who can weigh in with some anecdotal evidence one 
way or the other.</li> 
<li>Women are entering IT, but not in the areas that I hang out in. This is 
definitely possible, and I think is happening, to some degree. At an Adobe 
&quot;Flex Camp&quot; last night (I was Chet Haase's roadie for the evening), I 
noticed a far more even split of gender than I'd ever seen at a Java or .NET 
user group, and when I mentioned this to one of the other speakers, he nodded 
and said that women were far more prevalent in the &quot;web design&quot; 
space, which is clearly not a space I play much in. I've also heard that the 
system admin space is much more &quot;female-friendly&quot;, too.</li> 
<li>Women get in to IT, then out of it or stay &quot;hidden&quot; in it. I've 
heard the theory that some women choose to get out of IT because they're not 
willing to put the same kind of time and energy into it as some men are, or 
they choose to remain at the software developer level instead of trying to 
advance the corporate ladder into management or other more visible positions.
</li> 
<li>There are exactly the number of women in IT that want to be there. Hey, 
let's face it, maybe women just don't<em>like</em> software development, and 
that's OK, because there's a lot of jobs I don't like, either.</li> </ol> 
<p>My concern is with theories 1 and 2. There should be no reason whatsoever 
that a woman cannot succeed every bit as much as a man can. This is one of 
those (few?) industries where the principal qualifications are entirely 
intellectual/mental, and that means there's absolutely no reason why one gender 
should be favored over another. (Nursing and teaching are others, for example.)
</p> 
<p>So, without further ado, those of you who are interested, check out Dana 
Coffey's link on the Lego Build event at the MSDN events coming up.</p> <br>

<p></p> Conferences&nbsp;|&nbsp; Development Processes <br>
<br>
 Friday, 12 
December 2008 11:49:12 (Pacific Standard Time, UTC-08:00)<br>
Comments [9] 
&nbsp;|&nbsp;<br>
<br>
<br>
&nbsp;Wednesday, 10 December 2008 <br>
The Myth of 
Discovery <br>

<p>It amazes me how insular and inward-facing the software industry is. And 
how the &quot;agile&quot; movement is reaping the benefits of a very simple 
characteristic.</p> 
<p>For example, consider Jeff Palermo's essay on &quot;The Myth of 
Self-Organizing Teams&quot;. Now, nothing against Jeff, or his post, <em>per se
</em>, but it amazes me how our industry believes that they are somehow 
inventing new concepts, such as, in this case the &quot;self-organizing 
team&quot;. Team dynamics have been a subject of study for decades, and anyone 
with a background in psychology, business, or sales has probably already been 
through much of the material on it. The best teams are those that find their 
own sense of identity, that grow from within, but still accept some leadership 
from the outside--the classic example here being the championship sports team. 
Most often, that sense of identity is born of a string of successes, which is 
why teams without a winning tradition have such a hard time creating the<em>
esprit de corps</em> that so often defines the difference between success and 
failure.</p> 
<blockquote> 
<p><em>(Editor's note: Here's a free lesson to all of you out there who want 
to help your team grow its own sense of identity: give them a chance to win a 
few successes, and they'll start coming together pretty quickly. It's not 
always that easy, but it works more often than not.)</em></p> </blockquote> 
<p>How many software development managers--much less technical leads or 
project managers--have actually gone and looked through the management aisle at 
the local bookstore?</p> 
<p>Tom and Mary Poppendieck have been spending years now talking about 
&quot;lean&quot; software development, which itself (at a casual glance) seems 
to be a refinement of the concepts Toyota and other Japanese manufacturers were 
pursuing close to two decades ago. &quot;Total quality management&quot; was a 
concept introduced in those days, the idea that anyone on the production line 
was empowered to stop the line if they found something that wasn't right. (My 
father was one of those &quot;lean&quot; manufacturing advocates back in the 
80's, in fact, and has some great stories he can tell to its successes, and 
failures.)</p> 
<p>How many software development managers or project leads give their 
developers the chance to say, &quot;No, it's not right yet, we can't 
ship&quot;, and back them on it? Wouldn't you, as a developer, feel far more 
involved in the project if you knew you had that power--and that responsibility?
</p> 
<p>Or consider the &quot;agile&quot; notion of customer involvement, the 
classic XP &quot;On-Site Customer&quot; principle. Sales people have known for 
years, even decades (if not centuries), that if you involve the customer in the 
process, they are much more likely to feel an ownership stake sooner than if 
they just take what's on the lot or the shelf. Skilled salespeople have done 
the &quot;let's walk through what you<em>might</em> buy, if you were buying, of 
course&quot; trick countless numbers of times, and ended up with a sale where 
the customer didn't even intend to buy.</p> 
<p>How many software development managers or project leads have read a book on 
basic salesmanship? And yet, isn't that notion of extracting what the customer 
wants endemic to both software development and basic sales (of anything)?</p> 
<p>What is it about the software industry that just collectively refuses to 
accept that there might be lots of interesting research on topics that aren't 
technical yet still something that we can use? Why do we feel so compelled to 
trumpet our own &quot;innovations&quot; to ourselves, when in fact, they've 
been long-known in dozens of other contexts? When will we wake up and realize 
that we can learn a lot more if we cross-train in other areas... like, for 
example, getting your MBA?</p> <br>

<p></p> .NET&nbsp;|&nbsp; C#&nbsp;|&nbsp; C++&nbsp;|&nbsp; Development 
Processes&nbsp;|&nbsp; F#&nbsp;|&nbsp; Flash&nbsp;|&nbsp; Java/J2EE&nbsp;|&nbsp;
Languages&nbsp;|&nbsp; LLVM&nbsp;|&nbsp; Mac OS&nbsp;|&nbsp; Parrot&nbsp;|&nbsp;
Reading&nbsp;|&nbsp; Ruby&nbsp;|&nbsp; Solaris&nbsp;|&nbsp; Visual Basic
&nbsp;|&nbsp;VMWare&nbsp;|&nbsp; Windows&nbsp;|&nbsp; XML Services <br>
<br>
 
Wednesday, 10 December 2008 07:48:45 (Pacific Standard Time, UTC-08:00)<br>

Comments [8] &nbsp;|&nbsp; <br>
<br>
<br>
&nbsp;Monday, 15 September 2008 <br>

Apparently I'm #25 on the Top 100 Blogs for Development Managers <br>

<p>The full list is here. It's a pretty prestigious group--and I'm totally 
floored that I'm there next to some pretty big names.</p> 
<p>In homage to Ms. Sally Fields, of so many years ago... &quot;You like me, 
you really like me&quot;. Having somebody come up to me at a conference and 
tell me how much they like my blog is second on my list of &quot;fun things to 
happen to me at a conference&quot;, right behind having somebody come up to me 
at a conference and tell me how much they like my blog, except for that one 
entry, where I said something<em>totally</em> ridiculous (and here's why) ....
</p> 
<p>What I find most fascinating about the list was the means by which it was 
constructed--the various calculations behind page rank, technorati rating, and 
so on. Very cool stuff.</p> 
<p>Perhaps it's trite to say it, but it's still true: readers are what make 
writing blogs worthwhile. Thanks to all of you.</p> <br>

<p></p> .NET&nbsp;|&nbsp; C++&nbsp;|&nbsp; Conferences&nbsp;|&nbsp; 
Development Processes&nbsp;|&nbsp; F#&nbsp;|&nbsp; Flash&nbsp;|&nbsp; Java/J2EE
&nbsp;|&nbsp;Languages&nbsp;|&nbsp; LLVM&nbsp;|&nbsp; Mac OS&nbsp;|&nbsp; Parrot
&nbsp;|&nbsp;Reading&nbsp;|&nbsp; Review&nbsp;|&nbsp; Ruby&nbsp;|&nbsp; Security
&nbsp;|&nbsp;Solaris&nbsp;|&nbsp; Visual Basic&nbsp;|&nbsp; VMWare&nbsp;|&nbsp; 
Windows&nbsp;|&nbsp; XML Services <br>
<br>
 Monday, 15 September 2008 04:29:19 
(Pacific Daylight Time, UTC-07:00)<br>
Comments [4] &nbsp;|&nbsp; <br>
<br>
<br>
&nbsp;Tuesday, 19 August 2008<br>
An Announcement <br>

<p>For those of you who were at the Cinncinnati NFJS show, please continue on 
to the next blog entry in your reader--you've already heard this. For those of 
you who weren't, then allow me to make the announcement:</p> 
<p>Hi. My name's Ted Neward, and I am now a ThoughtWorker.</p> 
<p>After four months of discussions, interviews, more discussions and more 
interviews, I can finally say that ThoughtWorks and I have come to a meeting of 
the minds, and starting 3 September I will be a Principal Consultant at 
ThoughtWorks. My role there will be to consult, write, mentor, architect and 
speak on Java, .NET, XML Services (and maybe even a little Ruby), not to 
mention help ThoughtWorks' clients achieve IT success in other general ways.</p>
<p>Yep, I'm basically doing the same thing I've been doing for the last five 
years. Except now I'm doing it with a TW logo attached to my name.</p> 
<blockquote> 
<p><em>By the way, ThoughtWorkers get to choose their own titles, and I'm 
curious to know what readers think my title should be. Send me your 
suggestions, and if one really strikes home, I'll use it and update this entry 
to reflect the choice. I have a few ideas, but I'm finding that other people 
can be vastly more creative than I, and I'd love to have a title that rivals 
Neal's &quot;Meme Wrangler&quot; in coolness.</em></p> 
<p><em>Oh, and for those of you who were thinking this, &quot;Seat 
Warmer&quot; has already been taken, from what I understand.</em></p> 
</blockquote> 
<p>Honestly, this is a connection that's been hovering at the forefront of my 
mind for several years. I like ThoughtWorks' focus on success, their 
willingness to explore new ideas (both methodologies and technologies), their 
commitment to the community, their corporate values, and their overall attitude 
of &quot;work hard, play hard&quot;. There have definitely been people who came 
away from ThoughtWorks with a negative impression of the company, but they're 
the minority. Any company that encourages T-shirts and jeans, XBoxes in the 
office, and wants to promote good corporate values is a winner in my book. In 
short, ThoughtWorks is, in many ways, the consulting company that I would want 
to build, if I were going to build a consulting firm. I'm not a wild fan of the 
travel commitments, mind you, but I am definitely no stranger to travel, we've 
got some ideas about how I can stay at home a bit more, and frankly I've been 
champing at the bit to get injected into more agile and team projects, so it 
feels like a good tradeoff. Plus, I get to think about languages and platforms 
in a more competitive and hostile way--not that TW is a competitive and hostile 
place, mind you, but in that my new fellow ThoughtWorkers will not let stupid 
thoughts stand for long, and will quickly find the holes in my arguments even 
faster, thus making the arguments as a whole that much stronger... or shooting 
them down because they really are stupid. (Either outcome works pretty well for 
me.)</p> 
<p>What does this mean to the rest of you? Not much change, really--I'm still 
logging lots of hours at conferences, I'm still writing (and blogging, when the 
muse strikes), and I'm still available for consulting/mentoring/speaking; the 
big difference is that now I come with a thousand-strong developers of proven 
capability at my back, not to mention two of the more profound and articulate 
speakers in the industry (inNeal and Martin) as peers. So if you've got some 
.NET, Java, or Ruby projects you're thinking about, and you want a team to come 
in and make it happen, you know how to reach me.</p> <br>

<p></p> .NET&nbsp;|&nbsp; C++&nbsp;|&nbsp; Conferences&nbsp;|&nbsp; 
Development Processes&nbsp;|&nbsp; F#&nbsp;|&nbsp; Flash&nbsp;|&nbsp; Java/J2EE
&nbsp;|&nbsp;Languages&nbsp;|&nbsp; Mac OS&nbsp;|&nbsp; Parrot&nbsp;|&nbsp; Ruby
&nbsp;|&nbsp;Security&nbsp;|&nbsp; Solaris&nbsp;|&nbsp; Visual Basic
&nbsp;|&nbsp;Windows&nbsp;|&nbsp; XML Services <br>
<br>
 Tuesday, 19 August 
2008 11:24:39 (Pacific Daylight Time, UTC-07:00)<br>
Comments [9] &nbsp;|&nbsp; 
<br> <br>
<br>
&nbsp;Thursday, 14 August 2008 <br>
The Never-Ending Debate of 
Specialist v. Generalist <br>

<p>Another DZone newsletter crosses my Inbox, and again I feel compelled to 
comment. Not so much in the uber-aggressive style of my previous attempt, since 
I find myself more on the fence on this one, but because I think it's a 
worthwhile debate and worth calling out.</p> 
<p>The article in question is &quot;5 Reasons Why You Don't Want A 
Jack-of-all-Trades Developer&quot;, by Rebecca Murphey. In it, she talks about 
the all-too-common want-ad description that appears on job sites and mailing 
lists:</p> 
<blockquote> 
<p>I've spent the last couple of weeks trolling Craigslist and have been 
shocked at the number of ads I've found that seem to be looking for an entire 
engineering team rolled up into a single person. Descriptions like this aren't 
at all uncommon:</p> 
<blockquote> 
<p>Candidates must have 5 years experience defining and developing data driven 
web sites and have solid experience with ASP.NET, HTML, XML, JavaScript, CSS, 
Flash, SQL, and optimizing graphics for web use. The candidate must also have 
project management skills and be able to balance multiple, dynamic, and 
sometimes conflicting priorities. This position is an integral part of 
executing our web strategy and must have excellent interpersonal and 
communication skills.</p> </blockquote> </blockquote> 
<p>Her disdain for this practice is the focus of the rest of the article:</p> 
<blockquote> 
<p>Now I don't know about you, but if I were building a house, I wouldn't want 
an architect doing the work of a carpenter, or the foundation guy doing the 
work of an electrician. But ads like the one above are suggesting that a single 
person can actually do all of these things, and the simple fact is that these 
are fundamentally different skills. The foundation guy may build a solid base, 
but put him in charge of wiring the house and the whole thing could, well, burn 
down. When it comes to staffing a web project or product, the principle isn't 
all that different -- nor is the consequence.</p> </blockquote> 
<p>I'll admit, when I got to this point in the article, I was fully ready to 
start the argument right here and now--developers<em>have</em> to have a 
well-rounded collection of skills, since anecdotal evidence suggests that 
trying to go the route of programming specialization (along the lines of 
medical specialization) isn't going to work out, particularly given the 
shortage of programmers in the industry right now to begin with. But she goes 
on to make an interesting point:</p> 
<blockquote> 
<p>The thing is, the more you know, the more you find out you don't know. A 
year ago I'd have told you I could write PHP/MySQL applications, and do the 
front-end too; now that I've seen what it means to be truly skilled at the 
back-end side of things, I realize the most accurate thing I can say is that I 
understand PHP applications and how they relate to my front-end development 
efforts. To say that I can write them myself is to diminish the good work that 
truly skilled PHP/MySQL developers are doing, just as I get a little bent when 
a back-end developer thinks they can do my job.</p> </blockquote> 
<p>She really caught me eye (and interest) with that first statement, because 
it echoes something Bjarne Stroustrup told me almost 15 years ago, in an email 
reply sent back to me (in response to my rather audacious cold-contact email 
inquiry about the costs and benefits of writing a book): &quot;The more you 
know, the more you know you don't know&quot;. What I think also caught my 
eye--and, I admit it, earned respect--was her admission that she maybe isn't as 
good at something as she thought she was before. This kind of reflective 
admission is a good thing (and missing far too much from our industry, IMHO), 
because it leads not only to better job placements for us as well as the 
companies that want to hire us, but also because the more honest we can be 
about our own skills, the more we can focus efforts on learning what needs to 
be learned in order to grow.</p> 
<p>She then turns to her list of 5 reasons, phrased more as a list of 
suggestions to companies seeking to hire programming talent; my comments are in 
italics:</p> 
<blockquote> 
<p>So to all of those companies who are writing ads seeking one magical person 
to fill all of their needs, I offer a few caveats before you post your next 
Craigslist ad:</p> 
<p>1. If you're seeking a single person with all of these skills, make sure 
you have the technical expertise to determine whether a person's skills match 
their resume. Outsource a tech interview if you need to. Any developer can tell 
horror stories about inept predecessors, but when a front-end developer like 
myself can read PHP and think it's appalling, that tells me someone didn't do a 
very good job of vetting and got stuck with a programmer who couldn't deliver 
on his stated skills.</p> 
<p><em>(T: I cannot stress this enough--the technical interview process 
practiced at most companies is a complete sham and travesty, and usually only 
succeeds in making sure the company doesn't hire a serial killer, would-be 
terrorist, or financially destitute freeway-underpass resident. I seriously 
think most companies should outsource the technical interview process entirely.)
</em> </p> 
<p>2. A single source for all of these skills is a single point of failure on 
multiple fronts. Think long and hard about what it will mean to your project if 
the person you hire falls short in some aspect(s), and about the mistakes that 
will have to be cleaned up when you get around to hiring specialized people. I 
have spent countless days cleaning up after back-end developers who didn't 
understand the nuances and power of CSS, or the difference between a div, a 
paragraph, a list item, and a span. Really.</p> 
<p><em>(T: I'm not as much concerned about the single point of failure 
argument here, to be honest. Developers will always have &quot;edges&quot; to 
what they know, and companies will constantly push developers to that edge for 
various reasons, most of which seem to be financial--&quot;Why pay two people 
to do what one person can do?&quot; is a really compelling argument to the CFO, 
particularly when measured against an unquantifiable, namely the quality of the 
project.)</em> </p> 
<p>3. Writing efficient SQL is different from efficiently producing 
web-optimized graphics. Administering a server is different from 
troubleshooting cross-browser issues. Trust me. All are integral to the 
performance and growth of your site, and so you're right to want them all -- 
just not from the same person. Expecting quality results in every area from the 
same person goes back to the foundation guy doing the wiring. You're playing 
with fire.</p> 
<p><em>(T: True, but let's be honest about something here. It's not so much 
that the company wants to play with fire, or that the company has a manual 
entitled &quot;Running a Dilbert Company&quot; that says somewhere inside it, 
&quot;Thou shouldst never hire more than one person to run the IT 
department&quot;, but that the company is dealing with limited budgets and 
headcount. If you only have room for one head under the budget, you want the 
maximum for that one head. And please don't tell me how wrong that practice of 
headcount really is--you're preaching to the choir on that one. The people you 
want to preach to are the Jack Welches of the world, who apparently aren't 
listening to us very much.)</em> </p> 
<p>4. Asking for a laundry list of skills may end up deterring the candidates 
who will be best able to fill your actual need. Be precise in your ad: about 
the position's title and description, about the level of skill you're expecting 
in the various areas, about what's nice to have and what's imperative. If 
you're looking to fill more than one position, write more than one ad; if you 
don't know exactly what you want, try harder to figure it out before you click 
the publish button.</p> 
<p><em>(T: Asking people to think before publishing? Heresy! Truthfully, I 
don't think it's a question of not knowing what they want, it's more trying to 
find what they want. I've seen how some of these same job ads get generated, 
and it's usually because a programmer on the team has left, and they had some 
degree of skill in all of those areas. What the company wants, then, is 
somebody who can step into exactly what that individual was doing before they 
handed in their resignation, but ads like, &quot;Candidate should look at Joe 
Smith's resume on Dice.com (http://...) and have exactly that same skill set. 
Being named Joe Smith a desirable 'plus', since then we won't have to have the 
sysadmins create a new login handle for you.&quot; won't attract much 
attention. Frankly, what I've found most companies want is to just not lose the 
programmer in the first place.)</em> </p> 
<p>5. If you really do think you want one person to do the task of an entire 
engineering team, prepare yourself to get someone who is OK at a bunch of 
things and not particularly good at any of them. Again: the more you know, the 
more you find out you don't know. I regularly team with a talented back-end 
developer who knows better than to try to do my job, and I know better than to 
try to do his. Anyone who represents themselves as being a master of 
front-to-back web development may very well have no idea just how much they 
don't know, and could end up imperiling your product or project -- front to 
back -- as a result.</p> 
<p><em>(T: Or be prepared to pay a lot of money for somebody who is an expert 
at all of those things, or be prepared to spend a lot of time and money growing 
somebody into that role. Sometimes the exact right thing to do is have one 
person do it all, but usually it's cheaper to have a small team work together.)
</em></p> </blockquote> 
<p>(On a side note, I find it amusing that she seems to consider PHP a 
back-end skill, but I don't want to sound harsh doing so--that's just a matter 
of perspective, I suppose. (I can just imagine the guffaws from the mainframe 
guys when I talk about EJB, message-queue and Spring systems being 
&quot;back-end&quot;, too.) To me, the whole &quot;web&quot; thing is front-end 
stuff, whether you're the one generating the HTML from your PHP or servlet/JSP 
or ASP.NET server-side engine, or you're the one generating the CSS and 
graphics images that are sent back to the browser by said server-side engine. 
If a user sees something I did, it's probably because something bad happened 
and they're looking at a stack trace on the screen.)</p> 
<p>The thing I find interesting is that HR hiring practices and job-writing 
skills haven't gotten any better in the near-to-two-decades I've been in this 
industry. I can still remember a fresh-faced wet-behind-the-ears 
Stroustrup-2nd-Edition-toting job candidate named Neward looking at job 
placement listings and finding much the same kind of laundry list of skills, 
including those with the impossible number of years of experience. (In 1995, I 
saw an ad looking for somebody who had &quot;10 years of C++ experience&quot;, 
and wondering, &quot;Gosh, I guess they're looking to hire Stroustrup or 
Lippmann&quot;, since those two are the only people who could possibly have 
filled that requirement at the time. This was right before reading the ad that 
was looking for 5 years of Java experience, or the ad below it looking for 15 
years of Delphi....)</p> 
<p>Given that it doesn't seem likely that HR departments are going to 
&quot;get a clue&quot; any time soon, it leaves us with an interesting 
question: if you're a developer, and you're looking at these laundry lists of 
requirements, how do you respond?</p> 
<p>Here's my own list of things for programmers/developers to consider over 
the next five to ten years:</p> 
<ol> 
<li><em>These &quot;laundry list&quot; ads are not going away any time soon.
</em> We can rant and rail about the stupidity of HR departments and hiring 
managers all we want, but the basic fact is, this is the way things are going 
to work for the forseeable future, it seems. Changing this would require a 
&quot;sea change&quot; across the industry, and sea change doesn't happen 
overnight, or even within the span of a few years. So, to me, the right 
question to ask isn't, &quot;How do I change the industry to make it easier for 
me to find a job I can do?&quot;, but &quot;How do I change what I do when 
looking for a job to better respond to what the industry is doing?&quot;</li> 
<li><em>Exclusively focusing on a single area of technology is the Kiss of 
Death.</em> If all you know is PHP, then your days are numbered. I mean no 
disrespect to the PHP developers of the world--in fact, were it not too 
ambiguous to say it, I would rephrase that as &quot;If all you know is<em>X</em>
, your days are numbered.&quot; There is no one technical skill that will be as 
much in demand in ten years as it is now. Technologies age. Industry evolves. 
Innovations come along that completely change the game and leave our 
predictions of a few years ago in the dust. Bill Gates (he of the &quot;640K 
comment&quot;) has said, and I think he's spot on with this, &quot;We routinely 
overestimate where we will be in five years, and vastly underestimate where we 
will be in ten.&quot; If you put all your eggs in the PHP basket, then when PHP 
gets phased out in favor of<em>(insert new &quot;hotness&quot; here)</em>, 
you're screwed. Unless, of course, you want to wait until you're the last man 
standing, which seems to have paid off well for the few COBOL developers still 
alive.... but not so much for the Algol, Simula, or RPG folks....</li> 
<li><em>Assuming that you can stop learning is the Kiss of Death.</em> Look, 
if you want to stop learning at some point and coast on what you know, be 
prepared to switch industries. This one, for the forseeable future, is one 
that's predicated on radical innovation and constant change. This means we have 
to accept that everything is in a constant state of flux--you can either rant 
and rave against it, or roll with it. This doesn't mean that you don't have to 
look back, though--anybody who's been in this industry for more than 10 years 
has seen how we keep reinventing the wheel, particularly now that the 
relationship between Ruby and Smalltalk has been put up on the big stage, so to 
speak. Do yourself a favor: learn stuff that's already &quot;done&quot;, too, 
because it turns out there's a lot of lessons we can learn from those who came 
before us. &quot;Those who cannot remember the past are condemned to repeat 
it&quot; (George Santanyana). Case in point: if you're trying to get into XML 
services, spend some time learning CORBA and DCOM, and compare how they do 
things against WSDL and SOAP. What's similar? What's different? Do some 
Googling and see if you can find comparison articles between the two, and what 
XML services were supposed to &quot;fix&quot; from the previous two. You don't 
have to write a ton of CORBA or DCOM code to see those differences (though 
writing at least a little CORBA/DCOM code will probably help.)</li> 
<li><em>Find a collection of people smarter than you.</em> Chad Fowler calls 
this &quot;Being the worst player in any band you're in&quot; (<em>My Job Went 
to India (and All I Got Was This Lousy Book)</em>, Pragmatic Press<em>)</em>. 
The more you surround yourself with smart people, the more of these kinds of 
things (tools, languages, etc) you will pick up merely by osmosis, and find 
yourself more attractive to those kind of &quot;laundry list&quot; job reqs. If 
nothing else, it speaks well to you as an employee/consultant if you can say, 
&quot;I don't know the answer to that question, but I know people who do, and I 
can get them to help me&quot;.</li> 
<li><em>Learn to be at least self-sufficient in related, complementary 
technologies.</em> We see laundry list ads in &quot;clusters&quot;. Case in 
point: if the company is looking for somebody to work on their website, they're 
going to rattle off a list of five or so things they want he/she to know--HTML, 
CSS, XML, JavaScript and sometimes Flash (or maybe now Silverlight), in 
addition to whatever server-side technology they're using (ASP.NET, servlets, 
PHP, whatever). This is a pretty reasonable request, depending on the depth of 
each that they want you to know. Here's the thing: the company does<em>not</em> 
want the guy who says he knows ASP.NET (and nothing but ASP.NET), when asked to 
make a small HTML or CSS change, to turn to them and say, &quot;I'm sorry, 
that's not in my job description. I only know ASP.NET. You'll have to get your 
HTML guy to make that change.&quot; You should at least be comfortable with the 
basic syntax of all of the above (again, with possible exception for Flash, 
which is the odd man out in that job ad that started this piece), so that you 
can at least make sure the site isn't going to break when you push your changes 
live. In the case of the ad above, learn the things that &quot;surround&quot; 
website development: HTML, CSS, JavaScript, Flash, Java applets, HTTP (!!), 
TCP/IP, server operating systems, IIS or Apache or Tomcat or some other server 
engine (including the necessary admin skills to get them installed and up and 
running), XML (since it's so often used for configuration), and so on. These 
are all &quot;complementary&quot; skills to being an ASP.NET developer (or a 
servlet/JSP developer). If you're a C# or Java programmer, learn different 
programming languages, a la F# (.NET) or Scala (Java), IronRuby (.NET) or JRuby 
(Java), and so on. If you're a Ruby developer, learn either a JVM language or a 
CLR language, so you can &quot;plug in&quot; more easily to the large corporate 
enterprise when that call comes.</li> 
<li><em>Learn to &quot;read&quot; the ad at a higher level.</em> It's often 
possible to &quot;read between the lines&quot; and get an idea of what they're 
looking for, even before talking to anybody at the company about the job. For 
example, I read the ad that started this piece, and the internal dialogue that 
went on went something like this:
<blockquote> Candidates must have 5 years experience <em>(No entry-level 
developers wanted, they want somebody who can get stuff done without having 
their hand held through the process)</em>defining and developing data driven 
<em>(they want somebody who's comfortable with SQL and databases) </em>web sites
<em>(wait for it, the &quot;web cluster&quot; list is coming) </em>and have 
solid experience with ASP.NET<em>(OK, they're at least marginally a Microsoft 
shop, that means they probably also want some Windows Server and IIS experience)
</em>, HTML, XML, JavaScript, CSS <em>(the &quot;web cluster&quot;, knew that 
was coming)</em>, Flash <em>(OK, I wonder if this is because they're building 
rich internet/intranet apps already, or just flirting with the idea?)</em>, SQL 
<em>(knew that was coming)</em>, and optimizing graphics for web use <em>(OK, 
this is another wrinkle--this smells of &quot;we don't want our graphics-heavy 
website to suck&quot;)</em>. The candidate must also have project management 
skills<em>(in other words, &quot;You're on your own, sucka!&quot;--you're not 
part of a project team)</em>and be able to balance multiple, dynamic, and 
sometimes conflicting priorities<em>(in other words, &quot;You're own your own 
trying to balance between the CTO's demands and the CEO's demands, 
sucka!&quot;, since you're not part of a project team; this also probably means 
you're not moving into an existing project, but doing more maintenance work on 
an existing site)</em>. This position is an integral part of executing our web 
strategy<em>(in other words, this project has public visibility and you can't 
let stupid errors show up on the website and make us all look bad)</em>and must 
have excellent interpersonal and communication skills<em>(what job </em>doesn't 
<em> need excellent interpersonal and communication skills?)</em>. </blockquote>
See what I mean? They want an ASP.NET dev. My guess is that they're thinking a 
lot about Silverlight, since Silverlight's closest competitor is Flash, and so 
theoretically an ASP.NET-and-Flash dev would know how to use Silverlight well. 
Thus, I'm guessing that the HTML, CSS, and JavaScript don't need to be 
&quot;Adept&quot; level, nor even &quot;Master&quot; level, but 
&quot;Journeyman&quot; is probably necessary, and maybe you could get away with 
&quot;Apprentice&quot; at those levels, if you're working as part of a team. 
The SQL part will probably have to be &quot;Journeyman&quot; level, the XML 
could probably be just &quot;Apprentice&quot;, since I'm guessing it's only 
necessary for the web.config files to control the ASP.NET configuration, and 
the &quot;optimizing web graphics&quot;, push-come-to-shove, could probably be 
forgiven if you've had some experience at doing some performance tuning of a 
website.</li> 
<li><em>Be insightful.</em> I know, every interview book ever written says you 
should &quot;ask questions&quot;, but what they're really getting at is 
&quot;Demonstrate that you've thought about this company and this 
position&quot;. Demonstrating insight about the position and the company and 
technology as a whole is a good way to prove that you're a neck above the other 
candidates, and will help keep the job once you've got it.</li> 
<li><em>Be honest about what you know.</em> Let's be honest--we've all met 
developers who claimed they were &quot;experts&quot; in a particular tool or 
technology, and then painfully demonstrated how far from &quot;expert&quot; 
status they really were. Be honest about yourself: claim your skills on a 
simple four-point scale. &quot;Apprentice&quot; means &quot;I read a book on 
it&quot; or &quot;I've looked at it&quot;, but &quot;there's no way I could do 
it on my own without some serious help, and ideally with a Master looking over 
my shoulder&quot;. &quot;Journeyman&quot; means &quot;I'm competent at it, I 
know the tools/technology&quot;; or, put another way, &quot;I can do 80% of 
what anybody can ask me to do, and I know how to find the other 20% when those 
situations arise&quot;. &quot;Master&quot; means &quot;I not only claim that I 
can do what you ask me to do with it, I can optimize systems built with it, I 
can make it do things others wouldn't attempt, and I can help others learn it 
better&quot;. Masters are routinely paired with Apprentices as mentors or 
coaches, and should expect to have this as a major part of their 
responsibilities. (Ideally, anybody claiming &quot;architect&quot; in their 
title should be a Master at one or two of the core tools/technologies used in 
their system; or, put another way, architects should be<em>very</em> dubious 
about architecting with something they can't reasonably claim at least 
Journeyman status in.) &quot;Adept&quot;, shortly put, means you are not only 
fully capable of pulling off anything a Master can do, but you routinely take 
the tool/technology way beyond what anybody else thinks possible, or you know 
the depth of the system so well that you can fix bugs just by thinking about 
them. With your eyes closed. While drinking a glass of water. Seriously, Adept 
status is not something to claim lightly--not only had you better know the guys 
who created the thing personally, but you should have offered up suggestions on 
how to make it better and had one or more of them accepted.</li> 
<li><em>Demonstrate that you have relevant skills beyond what they asked for.
</em> Look at the ad in question: they want an ASP.NET dev, so any familiarity 
with IIS, Windows Server, SQL Server, MSMQ, COM/DCOM/COM+, WCF/Web services, 
SharePoint, the CLR, IronPython, or IronRuby should be listed prominently on 
your resume, and brought up at least twice during your interview. These are 
(again) complementary technologies, and even if the company doesn't have a need 
for those things right now, it's probably because Joe didn't know any of those, 
and so they couldn't use them without sending Joe to a training class. If you 
bring it up during the interview, it can also show some insight on your part: 
&quot;So, any questions for us?&quot; &quot;Yes, are you guys using Windows 
Server 2008, or 2003, for your back end?&quot; &quot;Um, we're using 2003, why 
do you ask?&quot; &quot;Oh, well, when I was working as an ASP.NET dev for my 
previous company, we moved up to 2008 because it had the Froobinger 
Enhancement, which let us...., and I was just curious if you guys had a similar 
need.&quot; Or something like that. Again, be entirely honest about what you 
know--if you helped the server upgrade by just putting the CDs into the drive 
and punching the power button, then say as much.</li> 
<li><em>Demonstrate that you can talk to project stakeholders and users.</em> 
Communication is huge. The era of the one-developer team is long since 
over--you have to be able to meet with project champions, users, other 
developers, and so on. If you can't do that without somebody being offended at 
your lack of tact and subtlety (or your lack of personal hygiene), then don't 
expect to get hired too often.</li> 
<li><em>Demonstrate that you understand the company, its business model, and 
what would help it move forward.</em> Developers who actually understand 
business are surprisingly and unfortunately rare. Be one of the rare ones, and 
you'll find companies highly reluctant to let you go.</li> </ol> 
<p>Is this an exhaustive list? Hardly. Is this list guaranteed to keep you 
employed forever? Nope. But this seems to be working for a lot of the people I 
run into at conferences and client consulting gigs, so I humbly submit it for 
your consideration.</p> 
<p>But in no way do I consider this conversation completely over, either--feel 
free to post your own suggestions, or tell me why I'm full of crap on any (or 
all) of these.</p> <br>

<p></p> .NET&nbsp;|&nbsp; C++&nbsp;|&nbsp; Development Processes&nbsp;|&nbsp; 
F#&nbsp;|&nbsp; Flash&nbsp;|&nbsp; Java/J2EE&nbsp;|&nbsp; Languages&nbsp;|&nbsp;
Reading&nbsp;|&nbsp; Ruby&nbsp;|&nbsp; Visual Basic&nbsp;|&nbsp; Windows
&nbsp;|&nbsp;XML Services <br>
<br>
 Thursday, 14 August 2008 15:38:42 (Pacific 
Daylight Time, UTC-07:00)<br>
Comments [4] &nbsp;|&nbsp; <br>
<br>
<br>

&nbsp;Friday, 01 August 2008<br>
From the &quot;Team-Building Exercise&quot; 
Department <br>

<p>This crossed my Inbox, and I have to say, I'm stunned at this incredible 
display of teamwork. Frankly... well,see for yourself.</p> <br>

<p></p> .NET&nbsp;|&nbsp; Development Processes&nbsp;|&nbsp; Review
&nbsp;|&nbsp;Windows <br>
<br>
 Friday, 01 August 2008 00:23:11 (Pacific 
Daylight Time, UTC-07:00)<br>
Comments [1] &nbsp;|&nbsp; <br>
<br>
<br>

&nbsp;Friday, 25 July 2008<br>
From the &quot;Gosh, You Wanted Me to Quote 
You?&quot; Department... <br>

<p>This comment deserves response:</p> 
<blockquote> 
<p>First of all, if you're quoting my post, blocking out my name, and 
attacking me behind my back by calling me &quot;our intrepid troll&quot;, you 
could have shown the decency of linking back to my original post. Here it is, 
for those interested in the real discussion:</p> 
<p>
http://www.agilesoftwaredevelopment.com/blog/jurgenappelo/professionalism-knowledge-first
</p> </blockquote> 
<p>Well, frankly, I didn't get your post from your blog, I got it from an 
email 'zine (as indicated by the comment &quot;This crossed my Inbox...&quot;), 
and I didn't really think that anybody would have any difficulty tracking down 
where it came from, at least in terms of the email blast that put it into my 
Inbox. Coupled with the fact that, quite honestly, I don't generally make a 
practice of using peoples' names without their permission (and my email to the 
author asking if I could quote the post with his name attached generated no 
response), so I blocked out the name. Having said that, I'm pleased to offer 
full credit as to its source.</p> 
<blockquote> 
<p>Now, let's review some of your remarks: </p> 
<p>&quot;COBOL is (at least) twenty years old, so therefore any use of COBOL 
must clearly be as idiotic.&quot;</p> 
<p>I never talked about COBOL, or any other programming language. I was 
talking about old practices that are nowadays considered harmful and seriously 
damaging. (Like practising waterfall project management, instead of agile 
project management.) I don't see how programming in COBOL could seriously 
damage a business. Why do you compare COBOL with lobotomies? I don't 
understand. I couldn't care less about programming languages. I care about 
management practices.</p> </blockquote> 
<p>Frankly, the distinction isn't very clear in your post, and even more 
frankly, to draw a distinction here is a bit specious. &quot;I didn't mean we 
should throw away the<em>good</em> stuff that's twenty years old, only the <em>
bad</em> stuff!&quot; doesn't seem much like a defense to me. There are cases 
where waterfall style development is<em>exactly</em> the right thing to do a 
more agile approach is<em>exactly</em> the wrong thing to do--the difference, 
as I'm fond of saying, lies entirely in the context of the problem. 
Analogously, there are cases where keeping an existing COBOL system up and 
running is the wrong thing to do, and replacing it with a new system is the 
right thing to do. It all depends on context, and for that reason, any dogmatic 
suggestion otherwise is flawed.</p> 
<blockquote> 
<p>&quot;How can a developer honestly claim to know &quot;what it can be good 
for&quot;, without some kind of experience to back it?&quot;</p> 
<p>I'm talking about gaining knowledge from the experience of others. If I 
hear 10 experts advising the same best practice, then I still don't have any 
experience in that best practice. I only have knowledge about it. That's how 
you can apply your knowledge without your experience.</p> </blockquote> 
<p>Leaving aside the notion that there is no such thing as best practices 
(another favorite rant of mine), what you're suggesting is that you, the 
individual, don't necessarily have to have experience in the topic but others 
have to, before we can put faith into it. That's a very different scenario than 
saying &quot;We don't need no stinkin' experience&quot;, and is still vastly 
more dangerous than saying, &quot;I have used this, it works.&quot; I (and lots 
of IT shops, I've found) will vastly prefer the latter to the former.</p> 
<blockquote> 
<p>&quot;Knowledge, apparently, isn't enough--experience still matters&quot; 
</p> 
<p>Yes, I never said experience doesn't matter. I only said it has no value 
when you don't have gained the appropriate knowledge (from other sources) on 
when to apply it, and when not.</p> </blockquote> 
<p>You said it when you offered up the title, &quot;Knowledge, not 
Experience&quot;.</p> 
<blockquote> 
<p>&quot;buried under all the ludicrous hyperbole, he has a point&quot; </p> 
<p>Thanks for agreeing with me.</p> </blockquote> 
<p>You're welcome!  Seriously, I think I understand better what you were 
trying to say, and it's not the horrendously dangerous comments that I thought 
you were saying, so I will apologize here and now for believing you to be a 
wet-behind-the-ears/lets-let-technology-solve-all-our-problems/dangerous-to-the-extreme 
developer that I've run across far too often, particularly in startups. So, 
please, will you accept my apology?</p> 
<blockquote> 
<p>&quot;developers, like medical professionals, must ensure they are on top 
of their game and spend some time investing in themselves and their knowledge 
portfolio&quot;</p> 
<p>Exactly.</p> </blockquote> 
<p>Exactly. </p> 
<blockquote> 
<p>&quot;this doesn't mean that everything you learn is immediately 
applicable, or even appropriate, to the situation at hand&quot;</p> 
<p>I never said that. You're putting words into my mouth. </p> 
<p>My only claim is that you need to KNOW both new and old practices and 
understand which ones are good and which ones can be seriously damaging. I 
simply don't trust people who are bragging about their experience. What if a 
manager tells me he's got 15 years of experience managing developers? If he's a 
micro-manager I still don't want him. Because micro-management is considered 
harmful these days. A manager should KNOW that.</p> </blockquote> 
<p>Again, this was precisely the read I picked up out of the post, and my 
apologies for the misinterpretation. But I stand by the idea that this is one 
of those posts that<em>could</em> be read in a highly dangerous fashion, and 
used to promote evil, in the form of &quot;Well, he runs a company, so 
therefore he must know what he's doing, and therefore having any kind of 
experience isn't really necessary to use something new, so... see, Mr. CEO 
boss-of-mine? We're safe! Now get out of my way and let me use Software 
Factories to build our next-generation mission-critical core-of-the-company 
software system, even though nobody's ever done it before.&quot;</p>
<p>To speak to your example for a second, for example: Frankly, there are 
situations where a micro-manager is a<em>good</em> thing. Young, inexperienced 
developers, for example, need more hand-holding and mentoring than older, more 
senior, more experienced developers do (speaking stereotypically, of course). 
And, quite honestly, the guy with 15 years managing developers is<em>far</em> 
more likely to know how to manage developers than the guy who's never managed 
developers before at all. The former is the safer bet; not a guarantee, 
certainly, but often the safer bet, and that's sometimes the best we can do in 
this industry.</p> 
<blockquote> 
<p>&quot;And we definitely shouldn't look at anything older than five years 
ago and equate it to lobotomies.&quot;</p> 
<p>I never said that either. Why do you claim that I said this? I don't have a 
problem with old techniques. The daily standup meeting is a 80-year old best 
practice. It was already used by pilots in the second world war. How could I be 
against that? It's fine as it is.</p> </blockquote> 
<p>Um... because you used the term &quot;lobotomies&quot; first? And because 
your title pretty clearly implies the statement, perhaps? (And let's lose the 
term &quot;best practice&quot; entirely, please? There is no such thing--not 
even the daily standup.)</p> 
<blockquote> 
<p>It's ok you didn't like my article. Sure it's meant to be provocative, and 
food for thought. The article got twice as many positive votes than negative 
votes from DZone readers. So I guess I'm adding value. But by taking the 
discussion away from its original context (both physically and conceptually), 
and calling me names, you're not adding any value for anyone.</p> </blockquote> 
<p>I took it in exactly the context it was given--a DZone email blast. I can't 
help it if it was taken out of context, because that's how it was handed to me. 
What's worse, I can see a development team citing this as an &quot;expert 
opinion&quot; to their management as a justification to try untested approaches 
or technologies, or as inspiration to a young developer, who reads 
&quot;knowledge, not experience&quot;, and thinks, &quot;Wow, if I know all the 
cutting-edge latest stuff, I don't need to have those 10 years of experience to 
get that job as a senior application architect.&quot; If your article was aimed 
more clearly at the development process side of things, then I would wish it 
had appeared more clearly in the arena of development processes, and made it 
more clear that your aim was to suggest that managers (who aren't real big on 
reading blogs anyway, I've sadly found) should be a bit more pragmatic and 
open-minded about who they hire.</p> 
<p>Look, I understand the desire for a provocative title--for me, the author 
of &quot;The Vietnam of Computer Science&quot;, to cast stones at another 
author for choosing an eye-catching title is so far beyond hypocrisy as to move 
into sheer wild-eyed audacity. But I have seen, first-hand, how that article 
has been used to justify the most incredibly asinine technology decisions, and 
it moves me now to say &quot;Be careful what you wish for&quot; when choosing 
titles that meant to be provocative and food for thought. Sure, your piece got 
more positive votes than negative ones. So too, in their day, did articles on 
client-server, on CORBA, on Six-Sigma, on the necessity for big up-front 
design....</p> 
<p>&nbsp;</p> 
<p>Let me put it to you this way. Assume your child or your wife is sick, and 
as you reach the hospital, the admittance nurse offers you a choice of the two 
doctors on call. Who do you want more: the doctor who just graduated fresh from 
medical school and knows all the latest in holistic and unconventional 
medicine, or the doctor with 30 years' experience and a solid track record of 
healthy patients?</p> <br>

<p></p> .NET&nbsp;|&nbsp; C++&nbsp;|&nbsp; Conferences&nbsp;|&nbsp; 
Development Processes&nbsp;|&nbsp; F#&nbsp;|&nbsp; Java/J2EE&nbsp;|&nbsp; 
Languages&nbsp;|&nbsp; LLVM&nbsp;|&nbsp; Mac OS&nbsp;|&nbsp; Parrot&nbsp;|&nbsp;
Ruby&nbsp;|&nbsp; Visual Basic&nbsp;|&nbsp; Windows&nbsp;|&nbsp; XML Services 
<br> <br>
 Friday, 25 July 2008 00:03:40 (Pacific Daylight Time, UTC-07:00) <br>
Comments [9] &nbsp;|&nbsp; <br>
<br>
<br>
&nbsp;Thursday, 24 July 2008 <br>

From the &quot;You Must Be Trolling for Hits&quot; Department... <br>

<p>Recently this little gem crossed my Inbox....</p> 
<blockquote> 
<p><strong>Professionalism = Knowledge First, Experience Last</strong><br>
By 
J----- A-----</p> 
<p><br>
Do you trust a doctor with diagnosing your mental problems if the 
doctor tells you he's got 20 years of experience? Do you still trust that 
doctor when he picks up his tools, and asks you to prepare for a lobotomy?</p> 
<p>Would you still be impressed if the doctor had 20 years of experience in 
carrying out lobotomies?</p> 
<p>I am always skeptic when people tell me they have X years of experience in 
a certain field or discipline, like &quot;5 years of experience as a .NET 
developer&quot;, &quot;8 years of experience as a project manager&quot; or 
&quot;12 years of experience as a development manager&quot;. It is as if 
people's professional levels need to be measured in years of practice.</p> 
<p>This, of course, is nonsense.</p> 
<p><strong>Professionalism is measured by what you are going to do <em>now</em>
...</strong></p> 
<p>Are you going to use some discredited technique from half a century ago?<br>
&bull;&nbsp;&nbsp;&nbsp; Are you, as a .NET developer, going to use 
Response.Write, because you've got 5 years of experience doing exactly that?<br>
&bull;&nbsp;&nbsp;&nbsp; Are you, as a project manager, going to create Gantt 
charts, because that's what you've been doing for 8 years?<br>

&bull;&nbsp;&nbsp;&nbsp; Are you, as a development manager, going to 
micro-manage your team members, as you did in the 12 years before now?</p> 
<p>If so, allow me to illustrate the value of your experience...</p> 
<p>(Photo of &quot;Zero&quot; signs)</p> 
<p>Here's an example of what it means to be a professional:</p> 
<p>There's a concept called Kanban making headlines these days in some parts 
of the agile community. I honestly and proudly admit that I have no experience 
at all in applying Kanban. But that's just a minor inconvenience. Because I 
have attained the knowledge of what it is and what it can be good for. And now 
there are some planning issues in our organization for which this Kanban-stuff 
might be the perfect solution. I'm sure we're going to give it a shot, in a 
controlled setting, with time allocated for a pilot and proper evaluations 
afterwards. That's the way a professional tries to solve a problem.</p> 
<p><strong>Professionals don't match problems with their experiences. They 
match them with their knowledge.</strong></p> 
<p>Sure, experience is useful. But only when you already have the knowledge in 
place. Experience has no value when there's no knowledge to verify that you are 
applying the right experience.</p> 
<p><strong>Knowledge Comes First, Experience Comes Last</strong></p> 
<p>This is my message to anyone who wants to be a professional software 
developer, a professional project manager, or a professional development 
manager.</p> 
<p>You must gain and apply knowledge first, and experience will help you after 
that. Professionals need to know about the latest developments and techniques.
</p> 
<p>They certainly don't bother measuring years of experience.</p> 
<p>Are you still practicing lobotomies?</p> </blockquote> 
<p>Um....</p> 
<p>Wow.</p> 
<p>Let's start with the logical fallacy in the first section. Do I trust a 
doctor with diagnosing my mental problems if he tells me he's got 20 years of 
experience? Generally, yes, unless I have reasons to doubt this. If the guy 
picks up a skull-drill and starts looking for a place to start boring into my 
skull, sure, I'll start to question his judgement.... But what does this have 
to do with anything? I wouldn't trust the guy if he picked up a chainsaw and 
started firing<em>it</em> up, either.</p> 
<p>Look, I get the analogy: &quot;Doctor has 20 years of experience using 
outdated skills&quot;, har har. Very funny, very memorable, and totally 
inappropriate metaphor for the situation. To stand here and suggest that 
developers who aren't using the latest-and-greatest, 
so-bleeding-edge-even-saying-the-name-cuts-your-skin tools or languages or 
technologies are somehow practicing lobotomies (which, by the way, are still a 
recommended practice in certain mental disorder cases, I understand) in order 
to solve any and all mental-health issues, is a gross mischaracterization--and 
the worst form of negligence--I've ever heard suggested.</p> 
<p>And it comes as no surprise that it's coming from the CIO of a consulting 
company. (Note to self: here's one more company I don't want anywhere<em>near 
</em>my clients' IT infrastructure.)</p> 
<p>Let's take this analogy to its logical next step, shall we?</p> 
<p>COBOL is (at least) twenty years old, so therefore any use of COBOL must 
<em>clearly</em> be as idiotic as drilling holes in your skull to let the 
demons out. So any company currently using COBOL has no real option other than 
to immediately upgrade<em>all</em> of their currently-running COBOL 
infrastructure (despite the fact that it's tested, works, and cashes most of 
the US banking industry's checks on a daily basis) with something vastly 
superior and totally untested (since we don't need experience, just knowlege), 
like... oh, I dunno.... how about Ruby? Oh, no, wait, that's at least 10 years 
old. Ditto for Java. And let's not even<em>think</em> about C, Perl, Python....
</p> 
<p>I know; let's rewrite the entire financial industry's core backbone in 
Groovy, since it's only, what, 6 years old at this point? I mean, sure, we'll 
have to do all this over again in just four years, since that's when Groovy 
will turn 10 and thus obviously begin it's long slide into mediocrity, 
alongside the &quot;four humors&quot; of medicine and Aristotle's (completely 
inaccurate) anatomical depictions, but hey, that's progress, right? Forget 
experience, it has<em>no value</em> compared to the &quot;knowledge&quot; that 
comes from reading the documentation on a brand-new language, tool, library, or 
platform....</p> 
<p>What I find most appalling is this part right here:</p> 
<blockquote> 
<p>I honestly and proudly admit that I have no experience at all in applying 
Kanban.<em>But that's just a minor inconvenience. Because I have attained the 
knowledge of what it is and what it can be good for.</em> </p> </blockquote> 
<p>How can a developer honestly claim to <em>know</em> &quot;what it can be 
good for&quot;, without some kind of experience to back it? (Hell, I'll even 
accept that you have familiarity and experience with something vaguely<em>
relating</em> to the thing at hand, if you've got it--after all, experience in 
Java makes you a pretty damn good C# developer, in my mind, and vice versa.)</p>
<p>And, to make things even <em>more</em> interesting, our intrepid troll, 
having established the attention-gathering headline, then proceeds to step away 
from the chasm, by backing away from this &quot;knowledge-not-experience&quot; 
position in the same paragraph, just one sentence later:</p> 
<blockquote> 
<p>I'm sure we're going to give it a shot, in a controlled setting, with time 
allocated for a pilot and proper evaluations afterwards.</p> </blockquote> 
<p>Ah... In other words, he and his company are going to experiment with this 
new technique, &quot;in a controlled setting, with time allocated for a pilot 
and proper evaluations afterwards&quot;, in order to gain<em>experience</em> 
with the technique and see how it works and how it doesn't.</p> 
<p>In other words....</p> 
<p>.... experience matters. </p> 
<p>Knowledge, apparently, isn't enough--experience still matters, and it 
matters a lot earlier than his &quot;knowledge first, experience last&quot; 
mantra seems to imply. Otherwise, once you &quot;know&quot; something, why not 
apply it immediately to your mission-critical core?</p> 
<p>At the end of the day, buried under all the ludicrous hyperbole, he has a 
point: developers, like medical professionals, must ensure they are on top of 
their game and spend some time investing in themselves and their knowledge 
portfolio. Jay Zimmerman takes great pains to point this out at every No Fluff 
Just Stuff show, and he's right: those who spend the time to invest in their 
own knowledge portfolio, find themselves the last to be fired and the first to 
be hired. But this doesn't mean that everything you learn is immediately 
applicable, or even appropriate, to the situation at hand. Just because you 
learned Groovy last weekend in Austin doesn't mean you have the right--or the 
responsibility--to immediately start slipping Groovy in to the production 
servers. Groovy has its share of good things, yes, but it's got its share of 
negative consequences, too, and you'd better<em>damn</em> well know what they 
are before you start betting the company's future on it. (No, I will not tell 
you what those negative consequences are--that's<em>your</em> job, because what 
if it turns out I'm wrong, or they don't apply to your particular situation?)
<em>Every</em> technology, language, library or tool has a positive/negative 
profile to it, and if you can't point out the pros as well as the cons, then 
you don't understand the technology and you have<em>no</em> business using it 
on anything except maybe a prototype that never leaves your local hard disk. 
Too many projects were built with &quot;flavor-of-the-year&quot; tools and 
technologies, and a few years later, long after the original 
&quot;bleeding-edge&quot; developer has gone on to do a new project with a new 
&quot;bleeding-edge&quot; technology, the IT guys left to admin and upkeep the 
project are struggling to find ways to keep this thing afloat.</p> 
<p>If you're languishing at a company that seems to resist anything and 
everything new, try this exercise on: go down to the IT guys, and<em>ask</em> 
them why they resist. Ask them to show you a data flow diagram of how 
information feeds from one system to another (assuming they even have one). Ask 
them how many different operating systems they have, how many different 
languages were used to create the various software programs currently running, 
what tools they have to know when one of those programs fails, and how many 
different data formats are currently in use. Then go find the guys currently 
maintaining and updating and bug-fixing those current programs, and ask to see 
the code. Figure out how long it would take you to rewrite the whole thing, and 
keep the company in business while you're at it.</p> 
<p>There is a reason &quot;legacy code&quot; exists, and while we shouldn't be 
afraid to replace it, we shouldn't be cavalier about tossing it out, either.</p>
<p>And we <em>definitely</em> shouldn't look at anything older than five years 
ago and equate it to lobotomies. COBOL had some good ideas that still echo 
through the industry today, and Groovy and Scala and Ruby and F# undoubtedly 
have some buried mines that we will, with benefit of ten years' hindsight, look 
back at in 2018 and say, &quot;Wow, how dumb were we to think that this was the 
last language we'd ever have to use!&quot;.</p> 
<p>That's experience talking. </p> 
<p>And the funny thing is, it seems to have served us pretty well. When we 
don't listen to the guys claiming to know how to use something effectively that 
they've never actually used before, of course.</p> 
<p><em>Caveat emptor.</em></p> <br>

<p></p> .NET&nbsp;|&nbsp; C++&nbsp;|&nbsp; Development Processes&nbsp;|&nbsp; 
F#&nbsp;|&nbsp; Java/J2EE&nbsp;|&nbsp; Languages&nbsp;|&nbsp; LLVM&nbsp;|&nbsp; 
Mac OS&nbsp;|&nbsp; Parrot&nbsp;|&nbsp; Ruby&nbsp;|&nbsp; Visual Basic
&nbsp;|&nbsp;Windows&nbsp;|&nbsp; XML Services <br>
<br>
 Thursday, 24 July 
2008 00:53:02 (Pacific Daylight Time, UTC-07:00)<br>
Comments [9] &nbsp;|&nbsp; 
<br> <br>
<br>
&nbsp;Sunday, 06 April 2008 <br>
More on Paradise <br>

<p>A couple of people have commented on the previous entry, citing, 
essentially, that Google needs to do this to be &quot;the best&quot;. I 
understand the argument completely: Google wants to attract the top talent, or 
retain the top talent, or at least entice the top talent, not to mention give 
them every reason to be horribly productive, so all of that extravagance is a 
justifiable--and some might argue necessary--expense.</p> 
<p>Thing is, I don't buy into that argument for a second. Talent wants to be 
rewarded, granted, but think about this for a moment: what kind of hours are 
these employees buying into by working there? There's an implicit tradeoff 
here, one that says, &quot;If you are<em>insanely</em> productive, then the 
cost of this office is justified&quot;, meaning the pressure is on. Having an 
off day? Better pull the all-nighter to make up for it. Got stuck on something 
you didn't anticipate? Better pull the all-weekender to compensate. You're not 
in the bush leagues any more, sonny--you're at Google, and we paid a<em>lot</em>
 of money to make this office your home away from home, so snap to it!</p> 
<p>I'm not suggesting that Goole is <em>explicitly demanding</em> this of 
their employees... but neither did Microsoft, back in the day.</p> 
<p>See, all of this--including the justification arguments--is eerily 
reminiscent of Microsoft in their heyday, with the best example being the 
original Windows NT team. The hours they pulled over the last few months (some 
say years) of that project were nothing short of marathon sprints, and 
Microsoft laid everything they could at the feet of these developers (though<em>
nothing</em> like what Google has built in Zurich, mind you) to help them focus 
on shipping the project.</p> 
<p>The Wall Street Journal ran an article about the whole thing, and one quote 
from that article stuck with me: that the pressure to work the insanely long 
hours didn't come from upper management, but from the other developers on the 
team. &quot;Are you signed up for this thing or not?&quot; was a euphemism for 
&quot;Why the hell are you leaving at 9PM? And you're not back until 8AM? What 
are you, some kind of slacker?&quot; (I felt like screaming back, &quot;Just 
say no!&quot;, and I wasn't even there.) The<em>peer pressure</em> was insane, 
and drove several members of the team to get outta Dodge as the first 
opportunity. Or some took off for bike rides across the country to recharge. Or 
some just... broke.</p> 
<p>Microsoft doesn't do this anymore. Nobody is expected to put in 60 hour 
work weeks as a matter of course; now, the average is around 45, which I 
believe (though I have no factual evidence to support this) is about average 
for the industry as a whole. (C'mon, admit it, even if you're a strict 
9-to-5'er, you still do a little reading at home or stick around after hours to 
help with the big rollout. It needs to be done, and you're professional enough 
to want to see it done right.)</p> 
<p>In college, I learned a lot about startups and established companies. Like 
most of the folks I hung out with in college, I used to stay up way too late 
with friends hanging out at Woodstock's (the local pizzeria) arguing 
politics/sex/religion/operating systems or playing role-playing games*, then 
come home and bang out a 5-page paper in a few hours before cracking open my 
notes to study for the final that morning. I could do this without any real 
penalty, but I usually ended up taking the final, then coming back to my 
apartment and passing out for a few hours, only to awake to my roommates 
chanting &quot;Piz-za! Piz-za! Piz-za!&quot; and starting the whole thing over 
again. I was young, I had energy, I was fundamentally stupid, and<em>I can't do 
that anymore</em>. I can't sprint like that and still be able to function 
coherently over time, and as you get older, you realize that while college can 
be managed as a series of sprints, life requires you to have a more marathon 
attitude, particularly because you can't know when the sprints are coming, like 
they do in college.</p> 
<p>As companies grow larger, their initial lifecycle is a series of sprints: 
roll the first release out, take a breather while the sales guys gather in the 
customer(s) and figure out what the next iteration will be, then do the whole 
thing over again. This effect is even<em>more</em> pronounced if the company 
has that one Really Big Customer, the one that represents some significant 
(over 50%) of the company's revenue; it's that company that drives the feature 
set and its delivery date. Meeting their needs and challenges becomes the 
source of the sprints to come.</p> 
<p>As time goes on, however, and assuming the company has somehow managed to 
find success, they find that the Really Big Customer is actually now just one 
of several, and the features and the timing of the releases need to be balanced 
across the entire customer set. In other words, while some sprints are still 
necessary, the frequency and intensity begins to smooth out and the focus 
shifts to structure, pace, and consistency.</p> 
<p>That is, if the company has successfully transitioned from 
&quot;startup&quot; to &quot;established&quot;. Some startups never do, and try 
to sprint themselves from one scenario to the another, and eventually run 
themselves into the ground. Managing this transition isn't easy, and is 
something that generally only comes from having lived through it once or 
twice... or three or four times... Ah, good times.</p> 
<p>Remember that whole &quot;work/life balance&quot; thing? We're discovering, 
over and over again, that having a good work/life balance is a key part of 
maintaining a sound outlook on life, much less your basic sanity. Creating a 
&quot;home away from home&quot; where employees can put in insane amount of 
hours is<em>not</em> healthy &quot;work/life balance&quot;... <em>unless</em> 
you presume your company will be staffed with fresh-from-college 
twenty-something singles who have nobody to go home to and all their friends in 
the office. And that, folks, is not a sustainable model.</p> 
<p>Point is, Google's extravagance here smacks of startups, sprints, and 
fevered intensity. What's worse, I hear little bits and pieces of rumors that 
Google<em>reveres</em> the eleventh-hour developer-god who swoops in, pulls the 
all-weeker to get the release out the door, to high praise from management and 
his/her peers.</p> 
<p>That's not sustainable, and the sooner Google--or any other company, for 
that matter--realizes that, the better they will be in the long term.</p> 
<p>&nbsp;</p> 
<p>&nbsp;</p> 
<p>&nbsp;</p> 
<p>* - Does this really surprise you? Yes, I am a <em>huge</em> geek.</p> <br>

<p></p> Development Processes <br>
<br>
 Sunday, 06 April 2008 00:49:56 
(Pacific Daylight Time, UTC-07:00)<br>
Comments [3] &nbsp;|&nbsp; <br>
<br>
<br>
&nbsp;Thursday, 03 April 2008<br>
Developer paradise? <br>

<p>Check out this video. No, go on, watch it. The rest of this won't make much 
sense until you do.</p> 
<p>Now that you've seen it, take a moment, do the &quot;WOW&quot; thing in 
your head, imagine how cool it would be to work there, all of it. Go on, I know 
you want to, I did too when I first saw it. Go ahead, take a moment; you'll be 
distracted until you do, and you'll miss the rest of the point of this blog 
entry, and then I'll be sad. Go on, now. Here, I'll do it with you, even.</p> 
<p>Mmmmmm. Slide to lunch. Ahhhh. Massage chair in front of the fish tank. 
Wow, just<em>think</em> of how cool it must be to work at Google. I mean, they 
work hard and all, but still... now<em>there's</em> a company that knows how to 
take care of its engineers, right?</p> 
<p>OK, daydreaming done? Let's think about this for a moment.</p> 
<p>First, how can anybody get anything <em>done</em> with all that noise 
surrounding them? Oh, I don't mean actual audio noise, I know they've created 
quiet zones and all that, I mean the myriad distractions that float around that 
office building. I'll be honest--I find myself getting work done better in an 
environment<em>without</em> that additional stimulus and excitement (legacy of 
my ADD, I'm sure). Knowing that I could just nip on over to the video game room 
to spend some &quot;thinking time&quot; in front of an all-you-can-play Galaga 
machine would drive me batty.</p> 
<p>Maybe that's just me, and others are just <em>begging</em> to be given the 
chance to prove me wrong, and if that's the case, then by all means, please 
feel free. But I've heard this same experience from lots of people doing the 
work-at-home thing, and I don't think the anecdotal evidence here is widely 
skewed. Sometimes you want work to be... just work. Vanilla, boring, and 
predictable.</p> 
<p>Don't get me wrong--I don't exactly look forward to my next engagement that 
plops me down in the middle of the cube farm--there's a continuum here, and 
Google is clearly far on the opposite end of that spectrum from the 
Dilbert-esque cubicle prairie as anyone can get. But had I my personal 
preference here, it would be a desk, fairly plain, comfortable, yet focused 
more on the functional than the &quot;fun&quot;.</p> 
<p>But second, there's a deeper concern that I have, one which I worry a <em>
lot</em> more about than just peoples' preference in work space.</p> 
<p>When's the last time you saw this kind of extravagance being lavished on 
developers? For me, it was at a number of different Silicon Valley firms during 
the dot-com boom of the late 90's... and<em>all</em> of those firms are 
dessicated remains of what they once were, or else dried up completely into 
dust and have long since blown away with the coastal breeze. This was classic 
startup behavior: drop a<em>ton</em> of money </p> 
<p>I'll call it: If Google sees nothing wrong with this kind of extravagance 
in setting up an office, then they have just done their first evil.</p> 
<p>Pause for a moment to think about the costs involved in setting up that 
office. I submit to you, dear reader, that Google is being financially 
irresponsible with that office, all nice perks aside. Google's money machine 
isn't going to last forever--nobody's ever does--and the company (desperately, 
IMHO) needs to find something else to prove to Wall Street and Developer Street 
that they're still a company that knows how to write cool software<em>and</em> 
make money. (Plenty of companies write cool software, and close their doors a 
few years later, and plenty of companies know how to make money, but having a 
company who can do both is a real rarity.)</p> 
<p>Look at Google's habits right now: they're <em>pouring</em> money out left 
and right in an effort to maintain or improve the Google &quot;image&quot;; 
tons of giveaways at conferences, tons of offices all across the world, 
incredible office spaces like the one in the video, and a ton of projects 
created by Google engineers just because said engineers think it's cool. While 
that's a developer's dream,<em>it doesn't pay the rent</em>. I want to work for 
a company that offers me a creative, productive work environment, true, but 
more than that, I want to work for a company that knows how to make sure my 
checks still cash. (Yes, I remember the late 90's well,<em>and</em> the 
collapse that followed.)</p> 
<p>I'm worried about Google--they appear to be on a dangerous arc, spending in 
what would seem to be far greater excess of what they're taking in, and that's 
not even considering some of the companies they would be well-advised to 
consider buying in order to flush out more of their corporate profile (which is 
its own interesting discussion, one for a later day). What is Google's 
principal source of income right now? Near as I can tell, it's AdWords, and I 
just can't believe that the AdWords gravy train will run any longer than...</p> 
<p>... well, than the DOS gravy train. Granted, that train ran for a long 
time, but eventually it ran out, and the company had to find alternative 
sources of income. Microsoft did, and now it's Google's turn to prove they can 
put money back into their corporate coffers.</p> 
<p>The parallels between Google and Microsoft are staggering, IMHO.</p> <br>

<p></p> Development Processes <br>
<br>
 Thursday, 03 April 2008 03:02:54 
(Pacific Daylight Time, UTC-07:00)<br>
Comments [11] &nbsp;|&nbsp; <br>
<br>

<br> &nbsp;Saturday, 22 March 2008 <br>
Reminder <br>

<p>A couple of people have asked me over the last few weeks, so it's probably 
worth saying out loud:</p> 
<p>No, I don't work for a large company, so yes, I'm available for consulting 
and research projects. If you've got one of those burning questions like, 
&quot;How would our company/project/department/whatever make use of 
JRuby-and-Rails, and what would the impact to the rest of the system be&quot;, 
or &quot;Could using F# help us write applications faster&quot;, or &quot;How 
would we best integrate Groovy into our application&quot;, or &quot;How does 
the new Adobe Flex/AIR move help us build richer client apps&quot;, or 
&quot;How do we improve the performance of our Java/.NET app&quot;, or other 
questions along those lines, drop me a line and let's talk. Not only will I 
cook up a prototype describing the answer, but I'll meet with your management 
and explain the consequences of the research, both pro and con, for them to 
evaluate.</p> 
<p>Shameless call for consulting complete, now back to the regularly-scheduled 
programming.</p> <br>

<p></p> .NET&nbsp;|&nbsp; C++&nbsp;|&nbsp; Conferences&nbsp;|&nbsp; 
Development Processes&nbsp;|&nbsp; Flash&nbsp;|&nbsp; Java/J2EE&nbsp;|&nbsp; 
Languages&nbsp;|&nbsp; LLVM&nbsp;|&nbsp; Mac OS&nbsp;|&nbsp; Parrot&nbsp;|&nbsp;
Reading&nbsp;|&nbsp; Ruby&nbsp;|&nbsp; Security&nbsp;|&nbsp; Solaris
&nbsp;|&nbsp;VMWare&nbsp;|&nbsp; Windows&nbsp;|&nbsp; XML Services <br>
<br>
 
Saturday, 22 March 2008 03:43:18 (Pacific Daylight Time, UTC-07:00)<br>

Comments [0] &nbsp;|&nbsp; <br>
<br>
<br>
&nbsp;Saturday, 15 March 2008 <br>

The reason for conferences <br>

<p>People have sometimes asked me if it's really worth it to go to a 
conference these days, given that so much material is appearing online via 
blogs, webcasts, online publications and Google. I think the answer is an 
unqualified &quot;yes&quot; (what else would you expect from a guy who spends a 
significant part of his life speaking at conferences?), but not necessarily for 
the reasons you might think.</p> 
<p>A long time ago, Billy Hollis said something very profound to me: 
&quot;Newbies go to conferences for the technical sessions. Seasoned veterans 
go to conferences for the people.&quot; At the time, I thought this was Billy's 
way of saying that the sessions really weren't &quot;all that&quot; at most 
conferences (JavaOne and TechEd come to mind, for example--whatever scheduling 
gods that think project managers on a particular project make good technical 
speakers on that subject really needs to be taken out back and shot), and that 
you're far better off spending the time networking to improve your social 
network. Now I think it's for a different reason. By way of explanation, allow 
me to recount a brief travel anecdote.</p> 
<p>I spend a lot of time on airplanes, as you might expect. Periodically, 
while staring out the window (trying to rearrange words in my head in order to 
make them sound coherent for the current email, blog entry, book chapter or 
article), I will see another commercial aircraft traveling in the same air 
traffic control lane going the other way. Every time I see it, I'm simply 
floored at how fast they appear to be going--they usually don't stay within my 
visibility for more than a few seconds. &quot;Whoosh&quot; is the first thought 
that goes through my easily-amused consciousness, and then, &quot;Damn, they're 
really<em>moving</em>.&quot; Then I realize, &quot;Wow--somebody on that plane 
over there is probably looking at this plane I'm on, and thinking the exact 
same thing.&quot;</p> 
<p>This is why you go to conferences.</p> 
<p>In the agile communities, they talk about velocity, the amount of work a 
team can get done in a particular iteration. But I think teams need to have a 
sense of their velocity<em>relative to the rest of the industry</em>, too. It 
helps put things into perspective. All too often, I find teams that look at me 
in meetings and conference calls and say, &quot;Surely the rest of the industry 
isn't this bad, right?&quot; or &quot;Surely, somebody else has found a 
solution to this problem by now, right?&quot; or &quot;Please, dear God,<em>tell
</em> me this kind of WTF-style of project management is unique to my 
company&quot;. While I am certainly happy to answer those questions, the fact 
of the matter is, at the end of the day they're still left taking my word for 
it, and let's be blunt: my answer can really only cover those companies and/or 
project teams I've had direct contact with. I can certainly tell you what I've 
heard from others (usually at conferences), but even that's now getting into 
secondhand information, which to you will be third-hand. (And that, of course, 
assumes I'm getting it from the source in the first place.)</p> 
<p>This isn't just about project management styles--agile or waterfall or 
WHISCEY (Why the Hell Isn't Somebody Coding Everything Yet) or 
what-have-you--but also about technical trends. Is Ruby taking off? Is Scala 
becoming more mainstream? Is JRuby worth exploring? Is C++ making a comeback? 
What are peoples' experiences with Spring 2.5? Has Grails reached a solid level 
of performance and/or stability? Sure, I'm happy to come to your company, meet 
with your team, talk about what I've seen and heard and done--but sending your 
developers (and managers, though *ahem* preferably to conferences that aren't 
in Las Vegas) to a conference like No Fluff Just Stuff or JavaOne or TechEd or 
SD West can give them some opportunities to swap stories and gain some 
important insights about your team's (and company's) velocity relative to the 
rest of the industry.</p> 
<p>All of which is to say, yes, Billy was right: it's about the people. Which 
means, boss, it's OK to let the developers go to the parties and maybe sleep in 
and miss a session or two the next morning.</p> <br>

<p></p> Conferences&nbsp;|&nbsp; Development Processes <br>
<br>
 Saturday, 15 
March 2008 08:58:52 (Pacific Daylight Time, UTC-07:00)<br>
Comments [1] 
&nbsp;|&nbsp;<br>
<br>
<br>
Mort means productivity <br>

<p>Recently, a number of folks in the Java space have taken to openly 
ridiculing Microsoft's use of the &quot;Mort&quot; persona, latching on to the 
idea that Mort is somehow equivalent to &quot;Visual Basic programmer&quot;, 
which is itself somehow equivalent to &quot;stupid idiot programmer who doesn't 
understand what's going on and just clicks through the wizards&quot;. This 
would be a mischaracterization, one which I thinkNikhilik's definition helps to 
clear up:</p> 
<blockquote> 
<p>Mort, the opportunistic developer, likes to create quick-working solutions 
for immediate problems and focuses on productivity and learn as needed. Elvis, 
the pragmatic programmer, likes to create long-lasting solutions addressing the 
problem domain, and learn while working on the solution. Einstein, the paranoid 
programmer, likes to create the most efficient solution to a given problem, and 
typically learn in advance before working on the solution. ....</p> 
<p>The description above is only rough summarization of several 
characteristics collected and documented by our usability folks. During the 
meeting a program manager on our team applied these personas in the context of 
server controls rather well (I think), and thought I should share it. Mort 
would be a developer most comfortable and satisfied if the control could be 
used as-is and it just worked. Elvis would like to able to customize the 
control to get the desired behavior through properties and code, or be willing 
to wire up multiple controls together. Einstein would love to be able to deeply 
understand the control implementation, and want to be able to extend it to give 
it different behavior, or go so far as to re-implement it.</p> </blockquote> 
<p>Phrased this way, it's fairly easy to recognize that it's possible that 
these are more roles than actual categorizations for programmers as 
individuals--sometimes you want to know how to create the most efficient 
solution, and sometimes you just want the damn thing (whatever it is) to get 
out of your way and let you move on.</p> 
<p>Don Box called this latter approach (which is a tendency on the part of <em>
all</em> developers, not just the VB guys) the <em>selective ignorance bit</em>
: none of us have the time or energy or intellectual capacity to know how<em>
everything</em> works, so for certain topics, a programmer flips the selective 
ignorance bit and just learns enough to make it work. For myself, a lot of the 
hardware stuff sees that selective ignorance bit flipped on, as well as a lot 
of the frou-frou UI stuff like graphics and animation and what-not. Sure, I'm 
curious, and I'd love to know how it works, but frankly, that's way down the 
priority list.</p> 
<p>If trying to find a quick-working solution to a particular problem means 
labeling yourself as a Mort, then call me Mort. After all, raise your hand if 
you didn't watch a team of C++ guys argue for months over the most efficient 
way to create a reusable framework for an application that they ended up not 
shipping because they couldn't get the framework done in time to actually start 
work on the application as a whole....</p> <br>

<p></p> .NET&nbsp;|&nbsp; C++&nbsp;|&nbsp; Development Processes&nbsp;|&nbsp; 
Java/J2EE&nbsp;|&nbsp; Languages&nbsp;|&nbsp; Ruby <br>
<br>
 Saturday, 15 
March 2008 08:57:39 (Pacific Daylight Time, UTC-07:00)<br>
Comments [1] 
&nbsp;|&nbsp;<br>
<br>
<br>
&nbsp;Tuesday, 19 February 2008 <br>
The Fallacies 
Remain.... <br>

<p>Just recently, I got this bit in an email from the Redmond Developer News 
ezine:</p> 
<blockquote> 
<p>TWO IF BY SEA </p> 
<p>In the course of just over a week starting on Jan. 30, a total of five 
undersea data cables linking Europe, Africa and the Middle East were damaged or 
disrupted. The first two cables to be lost link Europe with Egypt and terminate 
near the Port of Alexandria.</p> 
<p>http://reddevnews.com/columns/article.aspx?editorialsid=2502 </p> 
<p>Early speculation placed the blame on ship anchors that might have dragged 
across the sea floor during heavy weather. But the subsequent loss of cables in 
the Persian Gulf and the Mediterranean has produced a chilling numbers game. 
Someone, it seems, may be trying to sabotage the global network.</p> 
<p>It's a conclusion that came up at a recent International Telecommunication 
Union (ITU) press conference. According to an Associated Press report, ITU head 
of development Sami al-Murshed isn't ready to &quot;rule out that a deliberate 
act of sabotage caused the damage to the undersea cables over two weeks 
ago.&quot;</p> 
<p>http://tinyurl.com/3bjtdg </p> 
<p>You think? </p> 
<p>In just seven or eight days, five undersea cables were disrupted. </p> 
<p>Five. All of them serving or connecting to the Middle East. And thus far, 
only one cable cut -- linking Oman and the United Arab Emirates -- has been 
identified as accidental, caused by a dragging ship anchor.</p> 
<p>So what does it mean for developers? A lot, actually. Because it means that 
the coming wave of service-enabled applications needs to take into account the 
fact that the cloud is, literally, under attack.</p> 
<p>This isn't new. For as long as the Internet has been around, concerns about 
attacks on the network have centered on threats posed by things like 
distributed denial of service (DDOS) and other network-borne attacks. Twice -- 
once in 2002 and again in 2007 -- DDOS attacks have targeted the 13 DNS root 
servers, threatening to disrupt the Internet.</p> 
<p>But assaults on the remote physical infrastructure of the global network 
are especially concerning. These cables lie hundreds or even thousands of feet 
beneath the surface. This wasn't a script-kiddie kicking off an ill-advised DOS 
attack on a server. This was almost certainly a sophisticated, well-planned, 
well-financed and well-thought-out effort to cut off an entire section of the 
world from the global Internet.</p> 
<p>Clearly, efforts need to be made to ensure that the intercontinental cable 
infrastructure of the Internet is hardened. Redundant, geographically dispersed 
links, with plenty of excess bandwidth, are a good start.</p> 
<p>But development planners need to do their part, as well. Web-based 
applications shouldn't be crafted with the expectation of limitless bandwidth. 
Services and apps must be crafted so that they can fail gracefully, shift to 
lower-bandwidth media (such as satellite) and provide priority to 
business-critical operations. In short, your critical cloud-reliant apps must 
continue to work, when almost nothing else will.</p> 
<p>And all this, I might add, as the industry prepares to welcome the second 
generation of rich Internet application tools and frameworks.</p> 
<p>Silverlight 2.0 will debut at MIX08 next month. Adobe is upping the ante 
with its latest offerings. Developers will enjoy a major step up in their 
ability to craft enriched, Web-entangled applications and environments.</p> 
<p>But as you make your plans and write your code, remember this one thing: 
The people, organization or government that most likely sliced those four or 
five cables in the Mediterranean and Persian Gulf -- they can do it again.</p> 
</blockquote> 
<p>There's a couple of things to consider here, aside from the geopolitical 
ramifications of a concerted attack on the global IT infrastructure (which does 
more to damage corporations and the economy than it does to disrupt military 
communications, which to my understanding are mostly satellite-based).</p> 
<p>First, this attack on the global infrastructure raises a <em>huge</em> 
issue with respect to outsourcing--if you lose touch with your development 
staff for a day, a week, a month (just how long<em>does</em> it take to lay 
down new trunk cable, anyway?), what sort of chaos is this going to strike with 
your project schedule? In<em>The World is Flat</em>, Friedman mentions that a 
couple of fast-food restaurants have outsourced the drive-thru--you drive up to 
the speaker, and as you place your order, you're talking to somebody half a 
world way who's punching it into a computer that's flashing the data back to 
the fast-food join in question for harvesting (it's not like they<em>make</em> 
the food when you order it, just harvest it from the fields of pre-cooked 
burgers ripening under infrared lamps in the back) and disbursement as you pull 
forward the remaining fifty feet to the first window.</p> 
<p>The ludicrousness of this arrangement notwithstanding, this means that the 
local fast-food joint is now dependent on the global IT infrastructure in the 
same way that your ERP system is. Aside from the obvious &quot;geek 
attraction&quot; to a setup like this, I find it fascinating that at no point 
did somebody stand up and yell out, &quot;What happened to<em>minimizing</em> 
the risks?&quot; Effective project development relies heavily on the ability to 
touch base with the customer every so often to ensure things are progressing in 
the way the customer was anticipating. When the development team is one ocean 
and two continents away in one direction, or one ocean and a whole pile of 
islands away in the other direction, or even just a few states over, that vital 
communication link is now at the mercy of every single IT node in between them 
and you.</p> 
<p>We can make huge strides, but at the end of the day, the huge distances 
involved can only be &quot;fractionalized&quot;, never eliminated.</p> 
<p>Second, as Desmond points out, this has a huge impact on the design of 
applications that are assuming a 100% or 99.9% Internet uptime. Yes, I'm 
looking at you, GMail and Google Calendar and the other so-called 
&quot;next-generation Internet applications&quot; based on technologies like 
AJAX. (I categorically refuse to call them &quot;Web 2.0&quot; 
applications--there is no such thing as &quot;Web 2.0&quot;.) As much as we 
keep looking to the future for an &quot;always-on&quot; networking 
infrastructure, the more we delude ourselves to the practical realities of life:
<em>there is no such thing as &quot;always-on&quot; infrastructure</em>. 
Networking or otherwise.</p> 
<p>I know this personally, since last year here in Redmond, some 
stronger-than-normal winter storms knocked down a whole slew of power lines and 
left my house without electricity for a week. To<em>very</em> quickly discover 
how much of modern Western life depends on &quot;always-on&quot; assumptions, 
go without power to the house for a week. We were fortunate--parts of Redmond 
and nearby neighborhoods got power back within 24 hours, so if I needed to 
recharge the laptop or get online to keep doing business, much less get a hot 
meal or just find a place where it was warm, it meant a quick trip down to the 
local strip mall where a restaurant with WiFi (Canyon's, for those of you that 
visit Redmond) kept me going. For others in Redmond, the power outage meant a 
brief vacation down at the Redmond Town Center Marriott, where power was 
available pretty much within an hour or two of its disruption.</p> 
<p>The First Fallacy of Enterprise Systems states that &quot;The network is 
reliable&quot;. The network is only as reliable as the infrastructure around 
it, and not just the infrastructure that your company lays down from your 
workstation to the proxy or gateway or cable modem. Take a 
&quot;traceroute&quot; reading from your desktop machine to the server on which 
your application is running--if it's not physically in the building as you, 
then you're probably looking at 20 - 30 &quot;hops&quot; before it reaches the 
server. Every single one of those &quot;hops&quot; is a potential point of 
failure. Granted, the architecture of TCP/IP suggests that we should be able to 
route around any localized points of failure, but how many of those points are, 
in fact, to your world view, completely unroutable? If your gateway machine 
goes down, how does TCP/IP try to route around that? If your ISP gets hammered 
by a Denial-of-Service attack, how do clients reach the server?</p> 
<p>If we cannot guarantee 100% uptime for electricity, something we've had 
close to a century to perfect, then how can you assume similar kinds of 
guarantees for network availability? And before any of you point out that 
&quot;Hey, most of the time, it just works so why worry about it?&quot;, I 
humbly suggest you walk into your Network Operations Center and ask the helpful 
IT people to point out the Uninterruptible Power Supplies that fuel the servers 
there &quot;just in case&quot;.</p> 
<p>When they in turn ask you to point out the &quot;just in case&quot; 
infrastructure around the application, what will you say?</p> 
<p>Remember, the Fallacies only bite you when you ignore them:</p> 
<blockquote> 
<p>1) The network is reliable </p> 
<p>2) Latency is zero </p> 
<p>3) Bandwidth is infinite </p> 
<p>4) The network is secure </p> 
<p>5) Topology doesn't change </p> 
<p>6) There is one administrator </p> 
<p>7) Transport cost is zero </p> 
<p>8) The network is homogeneous </p> 
<p>9) The system is monolithic </p> 
<p>10) The system is finished</p> </blockquote> 
<p>Every project needs, at some point, to have somebody stand up in the room 
and shout out, &quot;But how do we<em>minimize</em> the risks?&quot; If this is 
truly a &quot;mission-critical&quot; application, then somebody needs the 
responsibility of cooking up &quot;What if?&quot; scenarios and answers, even 
if the answer is to say, &quot;There's not much we can reasonably do in that 
situation, so we'll just accept that the company shuts its doors in that 
case&quot;.</p> <br>

<p></p> .NET&nbsp;|&nbsp; C++&nbsp;|&nbsp; Development Processes&nbsp;|&nbsp; 
Java/J2EE&nbsp;|&nbsp; Ruby&nbsp;|&nbsp; Security&nbsp;|&nbsp; XML Services <br>
<br> Tuesday, 19 February 2008 21:25:03 (Pacific Standard Time, UTC-08:00) <br>

Comments [1] &nbsp;|&nbsp; <br>
<br>
<br>
&nbsp;Wednesday, 23 January 2008 <br>

Can Dynamic Languages Scale? <br>

<p>The recent &quot;failure&quot; of the Chandler PIM project generated the 
question,&quot;Can Dynamic Languages Scale?&quot; on TheServerSide, and, as is 
all too typical these days, it turned into a &quot;You suck&quot;/&quot;No you 
suck&quot; flamefest between a couple of posters to the site.</p> 
<p>I now make the perhaps vain attempt to address the question meaningfully.
</p> 
<h3>What do you mean by &quot;scale&quot;? </h3> 
<p>There's an implicit problem with using the word &quot;scale&quot; here, in 
that we can think of a language scaling in one of two very orthogonal 
directions:</p> 
<ol> 
<li>Size of project, as in lines-of-code (LOC) </li> 
<li>Capacity handling, as in &quot;it needs to scale to 100,000 requests per 
second&quot;</li> </ol> 
<p>Part of the problem I think that appears on the TSS thread is that the 
posters never really clearly delineate the differences between these two. 
Assembly language can scale(2), but it can't really scale(1) very well. Most 
people believe that C scales(2) well, but doesn't scale(1) well. C++ scores 
better on scale(1), and usually does well on scale(2), but you get into all 
that icky memory-management stuff. (Unless, of course, you're using the Boehm 
GC implementation, but that's another topic entirely.)</p> 
<p>Scale(1) is a measurement of a language's ability to extend or enhance the 
<em>complexity budget</em> of a project. For those who've not heard the term 
&quot;complexity budget&quot;, I heard it first from Mike Clark (though I can't 
find a citation for it via Google--if anybody's got one, holler and I'll slip 
it in here), he of Pragmatic Project Automation fame, and it's essentially a 
statement that says &quot;Humans can only deal with a fixed amount of 
complexity in their heads. Therefore, every project has a fixed complexity 
budget, and the more you spend on infrastructure and tools, the less you have 
to spend on the actual business logic.&quot; In many ways, this is a reflection 
of the ability of a language or tool to raise the level of abstraction--when 
projects began to exceed the abstraction level of assembly, for example, we 
moved to higher-level languages like C to help hide some of the complexity and 
let us spend more of the project's complexity budget on the program, and not 
with figuring out which register needed to have the value of the interrupt to 
be invoked. This same argument can be seen in the argument against EJB in favor 
of Spring: too much of the complexity budget was spent in getting the details 
of the EJB beans correct, and Spring reduced that amount and gave us more room 
to work with. Now, this argument is at the core of the Ruby/Rails-vs-Java/JEE 
debate, and implicitly it's obviously there in the middle of the room in the 
whole discussion over Chandler.</p> 
<p>Scale(2) is an equally important measurement, since a project that cannot 
handle the expected user load during peak usage times will have effectively 
failed just as surely as if the project had never shipped in the first place. 
Part of this will be reflected in not just the language used but also the tools 
and libraries that are part of the overall software footprint, but choice of 
language can obviously have a major impact here: Erlang is being tossed about 
as a good choice for high-scale systems because of its intrinsic Actors-based 
model for concurrent processing, for example.</p> 
<p>Both of these get tossed back and forth rather carelessly during this 
debate, usually along the following lines:</p> 
<ol> 
<li>Pro-Java (and pro-.NET, though they haven't gotten into this particular 
debate so much as the Java guys have) adherents argue that a dynamic language 
cannot scale(1) because of the lack of type-safety commonly found in dynamic 
languages. Since the compiler is not there to methodically ensure that 
parameters obey a certain type contract, that objects are not asked to execute 
methods they couldn't possibly satisfy, and so on. In essence, strongly-typed 
languages are theorem provers, in that they take the assertion (by the 
programmer) that this program is type-correct, and validate that. This means 
less work for the programmer, as an automated tool now runs through a series of 
tests that the programmer doesn't have to write by hand; as one contributor to 
the TSS threadput it: 
<blockquote> &quot;With static languages like Java, we get a select subset of 
code tests, with 100% code coverage, every time we compile. We get those tests 
for &quot;free&quot;. The price we pay for those &quot;free&quot; tests is 
static typing, which certainly has hidden costs.&quot;</blockquote>Note that 
this argument frequently derails into the world of IDE support and refactoring 
(as its witnessed on the TSS thread), pointing out that Eclipse and IntelliJ 
provide powerful automated refactoring support that is widely believed to be 
impossible on dynamic language platforms.</li> 
<li>Pro-Java adherents also argue that dynamic languages cannot scale(2) as 
well as Java can, because those languages are built on top of their own 
runtimes, which are arguably vastly inferior to the engineering effort that 
goes into the garbage collection facilities found in the JVM Hotspot or CLR 
implementations.</li> 
<li>Pro-Ruby (and pro-Python, though again they're not in the frame of this 
argument quite so much) adherents argue that the dynamic nature of these 
languages means less work during the creation and maintenance of the codebase, 
resulting in a far fewer lines-of-code count than one would have with a more 
verbose language like Java, thus implicitly improving the scale(1) of a dynamic 
language.
<p>On the subject of IDE refactoring, scripting language proponents point out 
that the original refactoring browser was an implementation built for (and 
into) Smalltalk, one of the world's first dynamic languages.</p> </li> 
<li>Pro-Ruby adherents also point out that there are plenty of web 
applications and web sites that scale(2) &quot;well enough&quot; on top of the 
MRV (Matz's Ruby VM?) interpreter that comes &quot;out of the box&quot; with 
Ruby, despite the widely-described fact that MRV Ruby Threads are what Java 
used to call &quot;green threads&quot;, where the interpreter manages thread 
scheduling and management entirely on its own, effectively using one native 
thread underneath.</li> 
<li>Both sides tend to get caught up in &quot;you don't know as much as me 
about this&quot; kinds of arguments as well, essentially relying on the idea 
that the less you've coded in a language, the less you could possibly know 
about that language, and the more you've coded in a language, the more 
knowledgeable you must be. Both positions are fallacies: I know a great deal 
about D, even though I've barely written a thousand lines of code in it, 
because D inherits much of its feature set and linguistic expression from both 
Java and C++. Am I a certified expert in it? Hardly--there are likely dozens of 
D idioms that I don't yet know, and certainly haven't elevated to the state of 
intuitive use, and those will come as I write more lines of D code. But that 
doesn't mean I don't already have a deep understanding of how to design D 
programs, since it fundamentally remains, as its genealogical roots imply, an 
object-oriented language. Similar rationale holds for Ruby and Python and 
ECMAScript, as well as for languages like Haskell, ML, Prolog, Scala, F#, and 
so on: the more you know about &quot;neighboring&quot; languages on the 
linguistic geography, the more you know about that language in particular. If 
two of you are learning Ruby, and you're a Python programmer, you already have 
a leg up on the guy who's never left C++. Along the other end of this 
continuum, the programmer who's written half a million lines of C++ code and 
still never uses the &quot;private&quot; keyword is<em>not</em> an expert C++ 
programmer, no matter what his checkin metrics claim. (And believe me, I've met 
way too many of these guys, in more than just the C++ domain.)</li> </ol> 
<p>A couple of thoughts come to mind on this whole mess.</p> 
<h3>Just how refactorable are you?</h3> 
<p> First of all, it's a widely debatable point as to the actual 
refactorability of dynamic languages. On NFJS speaker panels, Dave Thomas (he 
of the PickAxe book) would routinely admit that not all of the refactorings 
currently supported in Eclipse were possible on a dynamic language platform 
given that type information (such as it is in a language like Ruby) isn't 
present until runtime. He would also take great pains to point out that simple 
search-and-replace across files, something any non-trivial editor supports, 
will do many of the same refactorings as Eclipse or IntelliJ provides, since 
type is no longer an issue. Having said that, however, it's relatively easy to 
imagine that the IDE could be actively &quot;running&quot; the code as it is 
being typed, in much the same way that Eclipse is doing constant compiles, 
tracking type information throughout the editing process. This is an area I 
personally expect the various IDE vendors will explore in depth as they look 
for ways to capture the dynamic language dynamic (if you'll pardon the pun) 
currently taking place.</p> 
<h3>Who exactly are you for?</h3> 
<p>What sometimes gets lost in this discussion is that not all dynamic 
languages need be for programmers; a tremendous amount of success has been 
achieved by creating a core engine and surrounding it with a scripting engine 
that non-programmers use to exercise the engine in meaningful ways. Excel and 
Word do it, Quake and Unreal (along with other equally impressively-successful 
games) do it, UNIX shells do it, and various enterprise projects I've worked on 
have done it, all successfully. A model whereby core components are written in 
Java/C#/C++ and are manipulated from the UI (or other 
&quot;top-of-the-stack&quot; code, such as might be found in nightly batch 
execution) by these less-rigorous languages is a powerful and effective 
architecture to keep in mind, particularly in combination with the next 
point....</p> 
<h3>Where do you run again?</h3> 
<p>With the release of JRuby, and the work on projects like IronRuby and 
Ruby.NET, it's entirely reasonable to assume that these dynamic languages can 
and will now run on top of modern virtual machines like the JVM and the CLR, 
completely negating arguments 2 and 4. While a dynamic language will usually 
take some kind of performance and memory hit when running on top of VMs that 
were designed for statically-typed languages, work on the DLR and the MLVM, as 
well as enhancements to the underlying platform that will be more beneficial to 
these dynamic language scenarios, will reduce that. Parrot may change that in 
time, but right now it sits at a 0.5 release and doesn't seem to be making huge 
inroads into reaching a 1.0 release that will be attractive to anyone outside 
of the &quot;bleeding-edge&quot; crowd.</p> 
<h3>So where does that leave us?</h3> 
<p>The allure of the dynamic language is strong on numerous levels. Without 
having to worry about type details, the dynamic language programmer can 
typically slam out more work-per-line-of-code than his statically-typed 
compatriot, given that both write the same set of unit tests to verify the 
code. However, I think this idea that the statically-typed developer must 
produce the same number of unit tests as his dynamically-minded coworker is a 
fallacy--a large part of the point of a compiler is to provide those same 
tests, so why duplicate its work? Plus we have the guarantee that the compiler 
will always execute these tests, regardless of whether the programmer using it 
remembers to write those tests or not.</p> 
<p>Having said that, by the way, I think today's compilers (C++, Java and C#) 
are pretty weak in the type expressions they require and verify. 
Type-inferencing languages, like ML or Haskell and their modern descendents, F# 
and Scala, clearly don't require the degree of verbosity currently demanded by 
the traditional O-O compilers. I'm pretty certain this will get fixed over 
time, a la how C# has introducedimplicitly typed variables.</p> 
<p>Meanwhile, why the rancor between these two camps? It's eerily reminiscent 
of the ill-will that flowed back and forth between the C++ and Java communities 
during Java's early days, leading me to believe that it's more a concern over 
job market and emplyability than it is a real technical argument. In the end, 
there will continue to be a ton of Java work for the rest of this decade and 
well into the next, and JRuby (and Groovy) afford the Java developer lots of 
opportunities to learn those dynamic languages and still remain relevant to her 
employer.</p> 
<p>It's as Marx said, lo these many years ago: &quot;From each language, 
according to its abilities, to each project, according to its needs.&quot;</p> 
<p>Oh, except Perl. Perl just sucks, period. </p> 
<h3>PostScript</h3> 
<p>I find it deeply ironic that the news piece TSS cited at the top of the 
discussion claims that the Chandler project failed due to mismanagement, not 
its choice of implementation language. It doesn't even mention what language 
was used to build Chandler, leading me to wonder if anybody even read the piece 
before choosing up their sides and throwing dirt at one another.</p> <br>

<p></p> .NET&nbsp;|&nbsp; C++&nbsp;|&nbsp; Development Processes&nbsp;|&nbsp; 
Java/J2EE&nbsp;|&nbsp; Languages&nbsp;|&nbsp; Ruby <br>
<br>
 Wednesday, 23 
January 2008 23:51:02 (Pacific Standard Time, UTC-08:00)<br>
Comments [15] 
&nbsp;|&nbsp;<br>
<br>
<br>
&nbsp;Tuesday, 15 January 2008 <br>
My Open 
Wireless Network <br>

<p>People visiting my house have commented from time to time on the fact that 
at my house, there's no WEP key or WPA password to get on the network; in fact, 
if you were to park your car in my driveway and open up your notebook, you can 
jump onto the network and start browsing away. For years, I've always shrugged 
and said, &quot;If I can't spot you sitting in my driveway, you deserve the 
opportunity to attack my network.&quot; Fortunately, Bruce Schneier, author of 
the insanely-good-readingCrypto-Gram newsletter, is in the same camp as I:</p> 
<blockquote> 
<p>My Open Wireless Network </p> 
<p>Whenever I talk or write about my own security setup, the one thing that 
surprises people -- and attracts the most criticism -- is the fact that I run 
an open wireless network at home.</p> 
<p>There's no password. There's no encryption. Anyone with wireless capability 
who can see my network can use it to access the internet.</p> 
<p>To me, it's basic politeness. Providing internet access to guests is kind 
of like providing heat and electricity, or a hot cup of tea. But to some 
observers, it's both wrong and dangerous.</p> 
<p>I'm told that uninvited strangers may sit in their cars in front of my 
house, and use my network to send spam, eavesdrop on my passwords, and upload 
and download everything from pirated movies to child pornography. As a result, 
I risk all sorts of bad things happening to me, from seeing my IP address 
blacklisted to having the police crash through my door.</p> 
<p>While this is technically true, I don't think it's much of a risk. I can 
count five open wireless networks in coffee shops within a mile of my house, 
and any potential spammer is far more likely to sit in a warm room with a cup 
of coffee and a scone than in a cold car outside my house. And yes, if someone 
did commit a crime using my network the police might visit, but what better 
defense is there than the fact that I have an open wireless network? If I 
enabled wireless security on my network and someone hacked it, I would have a 
far harder time proving my innocence.</p> 
<p>This is not to say that the new wireless security protocol, WPA, isn't very 
good. It is. But there are going to be security flaws in it; there always are.
</p> 
<p>I spoke to several lawyers about this, and in their lawyerly way they 
outlined several other risks with leaving your network open.</p> 
<p>While none thought you could be successfully prosecuted just because 
someone else used your network to commit a crime, any investigation could be 
time-consuming and expensive. You might have your computer equipment seized, 
and if you have any contraband of your own on your machine, it could be a 
delicate situation. Also, prosecutors aren't always the most technically savvy 
bunch, and you might end up being charged despite your innocence. The lawyers I 
spoke with say most defense attorneys will advise you to reach a plea agreement 
rather than risk going to trial on child-pornography charges.</p> 
<p>In a less far-fetched scenario, the Recording Industry Association of 
America is known to sue copyright infringers based on nothing more than an IP 
address. The accused's chance of winning is higher than in a criminal case, 
because in civil litigation the burden of proof is lower. And again, lawyers 
argue that even if you win it's not worth the risk or expense, and that you 
should settle and pay a few thousand dollars.</p> 
<p>I remain unconvinced of this threat, though. The RIAA has conducted about 
26,000 lawsuits, and there are more than 15 million music downloaders. Mark 
Mulligan of Jupiter Research said it best: &quot;If you're a file sharer, you 
know that the likelihood of you being caught is very similar to that of being 
hit by an asteroid.&quot;</p> 
<p>I'm also unmoved by those who say I'm putting my own data at risk, because 
hackers might park in front of my house, log on to my open network and 
eavesdrop on my internet traffic or break into my computers.</p> 
<p>This is true, but my computers are much more at risk when I use them on 
wireless networks in airports, coffee shops and other public places. If I 
configure my computer to be secure regardless of the network it's on, then it 
simply doesn't matter. And if my computer isn't secure on a public network, 
securing my own network isn't going to reduce my risk very much.</p> 
<p>Yes, computer security is hard. But if your computers leave your house, you 
have to solve it anyway. And any solution will apply to your desktop machines 
as well.</p> 
<p>Finally, critics say someone might steal bandwidth from me. Despite 
isolated court rulings that this is illegal, my feeling is that they're welcome 
to it. I really don't mind if neighbors use my wireless network when they need 
it, and I've heard several stories of people who have been rescued from 
connectivity emergencies by open wireless networks in the neighborhood.</p> 
<p>Similarly, I appreciate an open network when I am otherwise without 
bandwidth. If someone were using my network to the point that it affected my 
own traffic or if some neighbor kid was dinking around, I might want to do 
something about it; but as long as we're all polite, why should this concern 
me? Pay it forward, I say.</p> 
<p>Certainly this does concern ISPs. Running an open wireless network will 
often violate your terms of service. But despite the occasional 
cease-and-desist letter and providers getting pissy at people who exceed some 
secret bandwidth limit, this isn't a big risk either. The worst that will 
happen to you is that you'll have to find a new ISP.</p> 
<p>A company called Fon has an interesting approach to this problem. Fon 
wireless access points have two wireless networks: a secure one for you, and an 
open one for everyone else. You can configure your open network in either 
&quot;Bill&quot; or &quot;Linus&quot; mode: In the former, people pay you to 
use your network, and you have to pay to use any other Fon wireless network.</p>
<p>In Linus mode, anyone can use your network, and you can use any other Fon 
wireless network for free. It's a really clever idea.</p> 
<p>Security is always a trade-off. I know people who rarely lock their front 
door, who drive in the rain (and, while using a cell phone), and who talk to 
strangers. In my opinion, securing my wireless network isn't worth it. And I 
appreciate everyone else who keeps an open wireless network, including all the 
coffee shops, bars and libraries I have visited in the past, the Dayton 
International Airport where I started writing this, and the Four Points 
Sheraton where I finished. You all make the world a better place.</p> 
</blockquote> 
<p>I'll admit that he's gone to far greater lengths to justify the open 
wireless network than I; frankly, the idea that somebody might try to sit in my 
driveway in order to hack my desktop machine and store kitty porn on it had 
never occurred to me. I was always far more concerned that somebody might sit 
on my ISP's server, hack my desktop machine's IP from there and store kitty 
porn on it. Which is why, like Schneier, I keep any machine that's in my house 
as up to date as possible. Granted, that doesn't protect me against a zero-day 
exploit, but if an attacker is that determined to put kitty porn on my machine, 
I probably couldn't stop them from breaking down my front door while we're all 
at work and school and loading it on via a CD-ROM, either.</p>
<p>And, at least in my neighborhood, I can (barely) find the signal for a few 
other wireless networks that are wide open, too, so I know I'm not the only 
target of opportunity here.So the prospective kitty porn bandit has his choice 
of machines to attack, and frankly I'll take the odds of my machines being the 
more hardened targets over my neighbors' machines any day. (Remember, computer 
security is often an exercise in convincing the bad guy to go play in somebody 
else's yard. I wish it were otherwise, but until we have effective response and 
deterrence mechanisms, it's going to remain that way for a long time.)</p>
<p>I've known a lot of people who leave their front doors unlocked--my 
grandparents lived in rural Illinois for sixty some-odd years in the same 
house, leaving the front door pretty much unlocked all the time, and the keys 
to their cars in the drivers' side sun shade, and never in all that time did 
any seedy character &quot;break in&quot; to their home or steal their car. 
(Hell, after my grandfather died a few years ago, the kids--my mom and her 
siblings--descended on the place to get rid of a ton of the junk he'd collected 
over the years. I think they would have<em>welcomed</em> a seedy character 
trying to make off with the stuff at that point.)</p>
<p>Point is, as Schneier points out in the last paragraph, security is always 
a trade-off, and we must never lose sight of that fact. Remember, dogma is the 
root of all evil, and should never be considered a substitute for reasoned 
thought processes.</p>
<p>And meanwhile, friends, when you come to my house to visit, enjoy the 
wireless, the heat, and the electricity. If you're nice, we may even let you 
borrow chair for a while, too.</p> <br>

<p></p> Development Processes&nbsp;|&nbsp; Mac OS&nbsp;|&nbsp; Security
&nbsp;|&nbsp;Windows <br>
<br>
 Tuesday, 15 January 2008 09:45:10 (Pacific 
Standard Time, UTC-08:00)<br>
Comments [3] &nbsp;|&nbsp; <br>
<br>
<br>

&nbsp;Saturday, 29 December 2007<br>
I Refused to be Terrorized <br>

<p>Bruce Schneier has a great blog post on this. I'm joining the movement, 
with this declaration:</p> 
<blockquote> 
<p>I am not afraid of terrorism, and I want you to stop being afraid on my 
behalf. Please start scaling back the official government war on terror. Please 
replace it with a smaller, more focused anti-terrorist police effort in keeping 
with the rule of law. Please stop overreacting. I understand that it will not 
be possible to stop all terrorist acts. I accept that. I am not afraid.</p> 
</blockquote> 
<p>In fact, I would amend this a little to include more than just the 
politically-correct discussion of terrorism and the government:</p> 
<blockquote> 
<p>I am not afraid of security discussions, and I want you to stop being 
afraid on my behalf. Please start scaling back the draconian requirements on my 
passwords and connection options. Not everything has to run over HTTPS and 
require passwords that must be 12 characters long and contain an upper-case 
letter, a lower-case letter, a number, a punctuation mark, and a letter from 
the Klingon alphabet. Please replace it with a smaller, more focused security 
effort in keeping with the risk involved. Please stop overreacting. I 
understand that it will not be possible to stop all acts of security attack. I 
accept that. I am not afraid.</p> </blockquote> 
<p>I want companies not to abandon their security efforts, but to put the 
effort into more targeted efforts. Don't spend millions instituting a VPN; 
instead, spend that time and money getting developers to find and fix all the 
command injection and/or cross-site scripting attacks that plague web 
applications.</p> <br>

<p></p> .NET&nbsp;|&nbsp; C++&nbsp;|&nbsp; Development Processes&nbsp;|&nbsp; 
Java/J2EE&nbsp;|&nbsp; Windows&nbsp;|&nbsp; XML Services <br>
<br>
 Saturday, 
29 December 2007 16:46:57 (Pacific Standard Time, UTC-08:00)<br>
Comments [0] 
&nbsp;|&nbsp;<br>
<br>
<br>
&nbsp;Saturday, 15 December 2007 <br>
Let the JDK 
Hacking Begin... <br>

<p>OpenJDK, the open-source JDK 7 release (and no, I don't know if there's any 
practical difference between the two) has officially opened for business with 
the promotion of the&quot;real, live&quot; Mercurial repositories. These are 
the real deal, the same repositories that Sun employees will be working on as 
they modify the code... which means, in all reality, that there is a<em>very 
tiny</em> window of opportunity for you to check out code between changesets 
that are dependent on one another due to the way they've got the forest set 
up--if you get weird build errors, try re-fetching... but more on that later.
</p> 
<p>Think about it for a second--now, thanks to the way Mercurial handles 
source (a distributed source code system is different from a centralized 
client/server one like SVN or CVS), you can hack away on your own private build 
of the JDK, and still keep up to date with what Sun's doing. And, if Sun likes 
what you're doing, you could end up contributing back to the JDK as a whole.</p>
<p>You could make changes to the langtools bundle to start adding new features 
you think should be in Java.</p> 
<p>You could make changes to the hotspot bundle to start exploring ways to 
optimize GC better.</p> 
<p>You could fix bugs you find in the various Java libraries.</p> 
<p>You could even add new features to the core classes that you've been saying 
needed to be there since Day 1.</p> 
<p>You can start exploring the new Modules system (JSR-277) strawman 
implementation.</p> 
<p>Then, you can post your diffs, or just tell people where your changesets 
are, and others can play with them.</p> 
<p>Cool.</p> 
<h2>Getting Started</h2> 
<p>Meanwhile, for those of you looking to get started with hacking on the JDK, 
there's a couple of things you need to do:</p> 
<ol> 
<li>Fetch the prereqs. </li> 
<li>Fetch the source through Mercurial. </li> 
<li>Set up your build prompt. </li> 
<li>Type &quot;make&quot; and go away for an hour or two.</li> </ol> 
<p>Naturally, things are just a <em>teeny</em> bit more complicated than that, 
particularly if you're on a Windows box. (<strong>Update:</strong> Volker 
Simonis has blogged, similar to this post,his experience in configuring a Suse 
Enterprise Linux 9.3 box to build the OpenJDK.) Thus, as one who's labored 
through this process on said platform, I'll show you how I got it all up and 
running on my machine (well, technically, pseudo-machine, it's a VMWare image 
with Windows XP SP2 in it). If you have questions, you're free to send me 
email, but chances are I'll just redirect you to build-dev, where the amazing 
Kelly O'Hair hangs out and answers build questions. (MAJOR kudos again to Kelly 
for getting this done!)</p> 
<p>Note that Sun is looking at other platforms, and has some good docs on 
building for Ubuntu 6.x and 7.x in the README already known--I've managed that 
with very little difficulty, so if you're starting from scratch, I'd suggest 
grabbing the free VMWare Player, an Ubuntu 7.04 Desktop image, and use that as 
a JDK build environment.</p> 
<p>While we're at it, along the way, I'll be pointing out areas that I think 
Sun could use and would appreciate some community contributions. (People are 
always asking me this at conferences, as a way of making their name more 
well-known and building credibility in the industry.) Note that I have no 
&quot;pull&quot; with Sun and can't begin to guess where they<em>want</em> the 
help, I'm simply suggesting where I think the help would be useful--to the 
community if not to Sun directly.</p> 
<p>By the way, the official document for build process requirements is the 
OpenJDK Build README; I only offer this up as a supplement and to offer up my 
own experiences. Note that I didn't go &quot;all the way&quot;, in that I don't 
care about building the Java Plug-In or some of the other ancillary stuff, such 
as the JDK installer itself, so I didn't necessarily install all the bits 
documented on the OpenJDK README page. I wanted source that I could hack, step 
into, debug, and so on.</p> 
<p>Without further ado....</p> 
<h1></h1> 
<h2>1. Fetch the prereqs</h2> 
<p>This sort of goes without saying, but in order to build the JDK, you need 
to have the necessary tools in place.</p> 
<ul> 
<li><strong><em>Environment.</em></strong> The build needs to be portable 
across both Windows and *nix, obviously, so OpenJDK needs a *nix-like 
environment on the Windows platform. Originally, back in the JDK 1.2 era, this 
was a commercial toolkit Sun purchased, the MKS Toolkit, but obviously that 
doesn't work for open source, so they've chosen to useCygwin. You'll need to 
make sure you have a couple of tools explicitly documented by Sun (ar.exe, 
cpio.exe, make.exe, file.exe, and m4.exe), but realistically you'll want the 
rest of the Cygwin developer tools just to have a complete environment. Cygwin 
afficionados will (hopefully) weigh in with what the best packages mix to grab 
are; I just grabbed everything that looked tasty at the time, including stuff 
that had nothing to do with building OpenJDK. Note that there is a big problem 
with the make.exe that comes with Cygwin, however...&nbsp; (<strong>Update:
</strong> Poonam's Weblog notes, &quot;Along with the default installation, we 
need to install Devel, Interpreters and Utils packages.&quot;)
<ul> 
<li>On my system, I put this in C:\Prg\cygwin. </li> 
<li>CONTRIBUTION: There's a lot of MSYS developers who would love it if you 
could figure out how to make the build work with their toolchain, too.... and 
don't ask me how, because I have no real idea; I'm totally n00b when it comes 
to the differences between the two.</li> </ul> </li> 
<li><strong><em>GNU Make 3.78.1 to 3.80.</em></strong> NOT the 3.81 or later 
version, because apparently there's problems with DOS-like paths, like C:/ 
(which is obviously going to be a factor on Windows; see some ofKelly's 
problems with shells for examples of what he's had to wrestle with). Getting 
this turned out to be a pain, and required some searching; Kelly pointed outa 
patched make.exe on Cygwin that should work. Make sure this make appears <em>
before</em> the regular Cygwin make in your shell prompt (later). (<strong>
Update:</strong> Find the right one here, as well.) 
<ul> 
<li>On my system, I created a directory for the OpenJDK project, in 
C:\Prg\OpenJDK, and put GNU make in C:\Prg\OpenJDK\bin.</li> 
<li>CONTRIBUTION: Either fix the damn PATH issues (&quot;Dear Mr. Gates, Would 
you<em>please</em> tell your developers to use the right kind of slash, 
already?&quot;), or else fix Cygwin's make to recognize C:/ (note the forward 
slash) paths. Not sure what else could be done here.</li> </ul> </li> 
<li><strong><em>Compiler.</em></strong> Sun originally was using the Microsoft 
Visual C++ 6.0 environment (which made sense at the time, since they were a 
company and could pay for it); with the open-source release... well... turns 
out that they're still using the Microsoft tools, only they've also added 
Microsoft Visual Studio 2003 to the list of supported compilers. (I'm using 
MSVS2003 myself.) This is a pain because both of those environments are 
commercial, and the Visual C++ 2005 Express (which is free) doesn't seem to 
work yet. People have suggested other compilers (Watcom's OpenC++, Cygwin's 
gcc, and so on), but right now that doesn't sound like a high priority for Sun. 
Of course, it's fair to suggest that if you're building for Windows, you 
probably have a MSVS installation somewhere, but still, it'd be nice....
<ul> 
<li>On my system, I put MSVS2003 in C:\Prg\MSVS2003. Note that I *just* 
installed the Visual C++ bits, and left out the rest of the .NET stuff. (I do 
.NET in another VMWare image, so I don't need it here.) To make our life 
simpler, reister the environment variables globally. (This is an install 
option.)</li> 
<li>CONTRIBUTION: Port the makefiles to use Visual C++ 2005 Express, or one of 
the other free compilers. I would think the easiest transition would be to 
VC2005Ex, but there may be some tools missing from the Express editions that 
the build needs; I haven't spent much time here researching what's working and 
not. This would likely be (much) harder for other compilers, owing to the 
differences in toolchains.</li> 
<li>CONTRIBUTION: Port the makefiles to use Visual Studio 2008.</li> </ul> 
</li> 
<li><strong><em>DirectX SDK.</em></strong> Yes, you need the DirectX SDK, 
specifically theSummer 2004 9.X Update, for building some of the advanced 
graphics stuff. The Build README has it linked there, as well, and the link, as 
of this writing, is still good. Install it, then take note of the DXSDK 
environment variable--we'll need it later.
<ul> 
<li>On my system, I put DirectX in C:\Prg\DirectX9SDK_062005.</li> </ul> </li> 
<li><strong><em>FreeType 2.</em></strong> This is something to do with fonts, 
and is needed to remove a commercial dependency that was holding up the OpenJDK 
from building about a year ago.Grab it, install it, and note the headers and 
lib directory. The FreeTypedownload page has some links to pre-built stuff, but 
in the end, you just want freetype.dll, freetype.lib, and the various FreeType 
headers.
<ul> 
<li>On my system, I put these in C:\Prg\OpenJDK\deps\freetype-igor (named 
after the helpful soul on build-dev who was kind enough to send me his 
pre-built FreeType bits). Note that underneath that directory, I have 
windows/freetype-i586/headers and /lib, which I'll use later for environment 
variables.</li> 
<li>CONTRIBUTION: Put a &quot;JDK-specific&quot; bundle of FreeType somewhere 
on the Web for people to download and not have to build.</li> </ul> </li> 
<li><strong><em>A &quot;bootstrap&quot; JDK.</em></strong> Go grab JDK 1.6; 
you'll need this for building the Java bits. (I would hope this isn't a problem 
for you; if it is, you<em>may</em> want to quickly go to some other Web page. 
<em>Any</em> web page.) 
<ul> 
<li>On my system, this resides in C:\Prg\jdk1.6.0.</li> </ul> </li> 
<li><strong><em>Apache Ant.</em></strong> At least version 1.6.3, I'm using 1.7
 without a problem.
<ul> 
<li>On my system, this resides in C:\Prg\apache-ant-1.7.0.</li> </ul> </li> 
<li><strong><em>The &quot;binary plugs&quot;.</em></strong> As much work has 
Sun has done to unencumber the JDK, there's still little bits and pieces that 
are commercial that they can't release the source to, so they put them into a 
&quot;binary plugs&quot; package and have you install it, then point to it in 
the build scripts, and copy those files over. They get versioned every time Sun 
releases a built JDK 7 bundle, but I don't think you need tograb this more than 
once; just the same, keep an eye on that as time goes on, and if you get weird 
build errors, check build-dev to see if the plugs changed.
<ul> 
<li>On my system, this resides in C:\Prg\OpenJDK\deps\openjdk-binary-plugs. 
(The .jar file is an executable jar, so just &quot;java -jar&quot; it and tell 
it to install in C:\Prg\OpenJDK\deps; it adds the rest. It's done in 3 seconds, 
and consists of 1 file. Now you see why I wouldn't worry too much about this.)
</li> </ul> </li> 
<li><strong><em>Mercurial.</em></strong> This is a distributed revision 
control system, and there's lots more to say about that at theMercurial website
. Its commands look somewhat similar to SVN, though definitely read
&quot;Distributed Revision Control with Mercurial&quot; if you're going to be 
keeping up with the source trees as they go. You *want* the &quot;forest&quot; 
extension as part of your Mercurial binary, so grab the&quot;Batteries 
Included&quot; installer version. I went with the non-TortoiseHG version, and 
had to download all four of the released files off that page and install and 
uninstall and install and uninstall until I found one that worked (the 
&quot;win32extrasp1.exe&quot; version in the &quot;dec&quot; release list on 
Sourceforge).
<ul> 
<li>On my system, Mercurial lives in C:\Prg\Mercurial. Put in on the PATH so 
you have access to &quot;hg.exe&quot;.</li> 
<li>CONTRIBUTION: Figure out what the differences are and post it 
someplace--how to get the &quot;forest&quot; extension installed and turned on 
in Mercurial was a<em>pain</em>; Google was of little to no help here. (Tag it 
as a comment to this blog entry, if you like, and I'll update the entry itself 
once I see it.)</li> 
<li><strong>Update:</strong> Daniel Fuchs blogs about how to get Mercurial's 
&quot;forest&quot; extension installed in your installation, in case you don't 
get the &quot;Batteries Included&quot; version:
<blockquote> 
<p>I simply cloned the forest repository in <code>c:\Mercurial</code>. In a 
cygwin terminal:</p> 
<pre> cd c:/Mercurial hg clone http://www.terminus.org/hg/hgforest hgforest 
</pre> 
<p>Then I edited <code>c:/Mercurial/Mercurial.ini</code> and added the lines:
</p> 
<pre> [extensions] forest=c:/Mercurial/hgforest/forest.py </pre> 
<p>as documented in the Mercurial Wiki. </p> </blockquote></li> </ul> </li> 
<li><strong><em>Optional: FindBugs.</em></strong> The build will start using 
FindBugs to do source-analysis to find bugs before they happen, so it's not a 
bad idea to have that as well.
<ul> 
<li>On my system, this resides in C:\Prg\findbugs-1.2.1.</li> </ul></li> </ul> 
<p>At this point, you should be ready to go.</p> 
<h2>2. Fetch the source.</h2> 
<p>Ivan's got it (mostly) right: just do this:</p> 
<blockquote> 
<p>cd C:\Prg\OpenJDK</p> 
<p>md jdk7</p> 
<p>hg fclone http://hg.openjdk.java.net/jdk7/jdk7/ jdk7</p> </blockquote> 
<p>(Don't use MASTER as he does, I don't think that works--that was just for 
the experimental repositories.) Don't forget the trailing slash, either, or 
you'll get an error saying something along the lines of 
http://hg.openjdk.java.net/jdk7%5Ccorba is not a valid URL or somesuch.</p> 
<p>If your Mercurial doesn't have the &quot;forest&quot; extension, 
&quot;fclone&quot; won't work; Ivan's got tips on how to pull down the 
sub-repositories by hand, but I get nervous doing that because what if the list 
changes and I wasn't paying attention?</p> 
<p>Your network will go wild for about twenty minutes or so, maybe longer, 
pulling lots of stuff down from the URL above. The sources should now reside in 
C:\Prg\OpenJDK\jdk7. Go browse 'em for a bit, if you feel so inclined. Get a 
good rush going, because this next step can be the tricky one.</p> 
<p><strong>Update:</strong> Volker Simonis ran into some issues with using 
Mercurial and an HTTP proxy, and found it difficult to find assistance across 
the Web. In the interests of making it easier for others, he's allowed me to 
republish his experience here:</p> 
<blockquote> 
<p>I just had a real hard time to get the forest extension working and finally 
found out that it was because the forest extension doesn't honor the 
&quot;http_proxy&quot; environment variable. So I thought I'll post it here in 
case anybody else will face the same problem in order to save him some time. 
(If you're only interested in the solution of the problem, you can skip the 
next paragraphs and jump right to the end of this post).</p> 
<p>I installed Mercurial and the forest extension as described in various 
places, here on the list and on the Web - that was the easy part:) Afterwards I 
could do:</p> 
<pre>/share/software/OpenJDK&gt; hg clone 
http://hg.openjdk.java.net/jdk7/jdk7/ jdk7 requesting all changes adding 
changesets adding manifests adding file changes added 2 changesets with 26 
changes to 26 files<br>
26 files updated, 0 files merged, 0 files removed, 0 
files unresolved</pre> 
<p>So everything worked fine! Note that I'm behind a firewall, but Mercurial 
correctly picked up my http proxy from the &quot;http_proxy&quot; environment 
variable!</p> 
<p>But now, everytime I tried 'fclone', I got the following error: </p> 
<pre>/share/software/OpenJDK&gt; hg fclone 
http://hg.openjdk.java.net/jdk7/jdk7/ jdk7 [.] abort: error: Name or service 
not known</pre> 
<p>That was not amusing. First I thought I got something wrong during the 
installation of the forest extension. I than used the '--traceback' option to 
&quot;hg&quot; which told me that the error was in keepalive.py:</p> 
<pre>/share/software/OpenJDK&gt; hg --traceback fclone 
http://hg.openjdk.java.net/jdk7/jdk7/ jdk7 [.] Traceback (most recent call 
last): ... File 
&quot;/share/software/Python-2.5.1_bin/lib/python2.5/site-packages/mercurial/keepalive.py&quot;, 
line 328, in _start_transaction raise urllib2.URLError(err) URLError: 
&lt;urlopen error (-2, 'Name or service not known')&gt; abort: error: Name or 
service not known</pre> 
<p>So I enabled the debugging output in keepalive.py and realized, that the 
first two connections to hg.openjdk.java.net where made trough the proxy, while 
the third one (the first that actually fetches files), wants to go directly to 
hg.openjdk.java.net, which badly fails:</p> 
<pre>/share/software/OpenJDK&gt; hg fclone 
http://hg.openjdk.java.net/jdk7/jdk7/ jdk7 keepalive.py - creating new 
connection to proxy:8080 (1078835788) keepalive.py - STATUS: 200, OK 
keepalive.py - re-using connection to proxy:8080 (1078835788) keepalive.py - 
STATUS: 200, OK [.] keepalive.py - creating new connection to 
hg.openjdk.java.net (1078970092) abort: error: Name or service not known</pre> 
<p>The problem can be fixed by adding the proxy settings to your .hgrc file, 
like this:</p> 
<pre>[http_proxy] host=proxy:8080 </pre> 
<p>where you have to replace &quot;proxy:8080&quot; with the name and the port 
of your real proxy host!</p> </blockquote> 
<p>Volker's original email came from the buid-dev list, and if you have 
further questions about Mercrial and HTTP proxies, I'd suggest that as a 
resource.</p> 
<p></p> 
<h2>3. Set up your environment.</h2> 
<p>This is where you'll spend a fair amount of time, because getting this 
right can be ugly. There's some environment variables that tell the build 
script where to find things, and we have to point out those things, like 
compiler location and such. If you installed everything in the same places I 
did, then the following, which I put into C:\Prg\OpenJDK\build_shell.sh,<em>
should</em> work for you:</p> 
<blockquote> 
<p>#!/bin/sh </p> 
<p># &quot;External&quot; bits (outside of OpenJDK path structure)<br>
#<br>

export ALT_BOOTDIR=C:/Prg/jdk1.6.0<br>
export ANT_HOME=c:/Prg/apache-ant-1.7.0
<br>export FINDBUGS_HOME=/cygdrive/c/Prg/findbugs-1.2.1 </p> 
<p># OpenJDK flag (to make FreeType check pass)<br>
#<br>
export OPENJDK=true 
</p> 
<p>export OPENJDK_HOME=C:/Prg/OpenJDK<br>
openjdkpath=$(cygpath --unix 
$OPENJDK_HOME) </p> 
<p># OpenJDK-related bits<br>
# (ALT_JDK_IMPORT_PATH fixes a corba bug; remove 
it later)<br>
#<br>
export ALT_JDK_IMPORT_PATH=$(cygpath --unix C:/Prg/jdk1.6.0)
<br>export ALT_BINARY_PLUGS_PATH=$OPENJDK_HOME/openjdk-binary-plugs<br>
export 
ALT_UNICOWS_DLL_PATH=$OPENJDK_HOME/deps/unicows<br>
export 
ALT_FREETYPE_LIB_PATH=$OPENJDK_HOME/deps/freetype-igor/windows/freetype-i586/lib
<br>export 
ALT_FREETYPE_HEADERS_PATH=$OPENJDK_HOME/deps/freetype-igor/windows/freetype-i586/include/freetype2
<br>. $openjdkpath/jdk7/jdk/make/jdk_generic_profile.sh </p> 
<p># Need GNU make in front of Cygwin's; this is the only practical way to do 
it<br>
#<br>
PATH=$openjdkpath/bin:$PATH<br>
export PATH </p> 
<p># Let people know this is an OpenJDK-savvy prompt<br>
#<br>
export 
PS1='OpenJDK:\[\e]0;\w\a\]\[\e[32m\]\u@${COMPUTERNAME}:\[\e[33m\]\w\[\e[0m\]\n\$ 
'</p> </blockquote> 
<p>  Note the UNICOWS_DLL thing in the middle; this was necessary in earlier 
builds, and I don't know if it still is. For now I'm not messing with it; if 
you discover that you need it, the Build README has links. (<strong>Update:
</strong> Kelly confirmed that this is no longer necessary in the OpenJDK 
build. Yay!)</p> 
<p>Note that I set the COMPILER_VERSION flag to tell the build script which 
compiler I'm using--if that's not set, the build fails pretty quickly, 
complaining that &quot;COMPILER_VERSION&quot; cannot be empty. (<strong>Update:
</strong> Kelly mentions, &quot;I suspect the reason you are having the 
COMPILER_VERSION problem is that the makefiles are trying to run cl.exe to get 
the version, and if the PATH/LIB/INCLUDE env variables are not setup right, 
cl.exe fails. Several people have run into that problem.&quot;)</p> 
<p>Note that OPENJDK must be set, or the build process thinks this is a 
commercial build, and an early sanity-check to see what version of FreeType is 
running will fail. (Specfically, Sun builds a tool just to see if the code 
compiles; if it fails to compile, chances are you forgot to set this flag. 
That's been my problem, each and every time I tried to rebuild the OpenJDK 
build space. Hopefully I never forget it again.)</p> 
<p>Note that I call into jdk_generic_profile.sh to do some more setup work; 
this gets all the MSVS2003 stuff into the PATH as well.</p> 
<p>Be very careful with which path you use; sometimes the build wants C:/Prg 
style paths, and sometimes it wants /cygdrive/c/Prg style paths. Don't assume 
the script above is perfect--I'm still testing it, and will update this entry 
as necessary as I find bugs in it.</p> 
<p>(<strong>Update:</strong> Kelly mentions, &quot;Be careful putting cygwin 
before VS2003 in the PATH, cygwin has a link.exe and so does VS2003, you need 
the one from VS2003.&quot;)</p> 
<p>From a Cygwin bash prompt,</p> 
<blockquote> 
<p>cd /cygdrive/c/Prg/OpenJDK</p> 
<p>. ./build_shell.sh</p> 
<p>cd jdk7</p> 
<p>make sanity</p> </blockquote> 
<p>It will churn, think, text will go flying by, and you will (hopefully) see 
&quot;Sanity check passed&quot;. If not, look at the (voluminous) output to 
figure out what paths are wrong, and correct them. Note that certain paths may 
be reported as warnings and yet the buld will still succeed, that's OK, as far 
as I can tell.</p> 
<p>And no, I don't know what all of those environment variables are for. Kelly 
might, but I suspect there's a lot of built-up cruft from over the years that 
they'd like to start paring down. Let's hope.</p> 
<h2>4. Type &quot;make&quot; and go away for a while.</h2> 
<p>Specifically, type &quot;make help&quot; to see the list of targets.</p> 
<blockquote> 
<p>OpenJDK:Ted@XPJAVA:/cygdrive/c/Prg/OpenJDK/jdk7<br>
$ make help<br>

Makefile for the JDK builds (all the JDK).</p> 
<p>--- Common Targets ---<br>
all -- build the core JDK (default target)<br>

help -- Print out help information<br>
check -- Check make variable values for 
correctness<br>
sanity -- Perform detailed sanity checks on system and settings
<br>fastdebug_build -- build the core JDK in 'fastdebug' mode (-g -O)<br>

debug_build -- build the core JDK in 'debug' mode (-g)<br>
clean -- remove all 
built and imported files<br>
clobber -- same as clean </p> 
<p>--- Common Variables ---<br>
ALT_OUTPUTDIR - Output directory<br>

OUTPUTDIR=./build/windows-i586<br>
ALT_PARALLEL_COMPILE_JOBS - Solaris/Linux 
parallel compile run count<br>
PARALLEL_COMPILE_JOBS=2<br>
ALT_SLASH_JAVA - 
Root of all build tools, e.g. /java or J:<br>
SLASH_JAVA=J:<br>
ALT_BOOTDIR - 
JDK used to boot the build<br>
BOOTDIR=C:/Prg/jdk1.6.0<br>
ALT_JDK_IMPORT_PATH 
- JDK used to import components of the build<br>

JDK_IMPORT_PATH=c:/Prg/JDK16~1.0<br>
ALT_COMPILER_PATH - Compiler install 
directory<br>
COMPILER_PATH=C:/Prg/MSVS2003/Common7/Tools/../../Vc7/Bin/<br>

ALT_CACERTS_FILE - Location of certificates file<br>

CACERTS_FILE=/lib/security/cacerts<br>
ALT_DEVTOOLS_PATH - Directory containing 
zip and gnumake<br>
DEVTOOLS_PATH=/usr/bin/<br>
ALT_DXSDK_PATH - Root directory 
of DirectX SDK<br>
DXSDK_PATH=C:/Prg/DIRECT~1<br>
ALT_MSDEVTOOLS_PATH - Root 
directory of VC++ tools (e.g. rc.exe)<br>

MSDEVTOOLS_PATH=C:/Prg/MSVS2003/Common7/Tools/../../Vc7/Bin/<br>

ALT_MSVCRT_DLL_PATH - Directory containing mscvrt.dll<br>

MSVCRT_DLL_PATH=C:/WINDOWS/system32<br>
WARNING: SLASH_JAVA does not exist, try 
make sanity<br>
WARNING: CACERTS_FILE does not exist, try make sanity </p> 
<p>--- Notes ---<br>
- All builds use same output directory unless overridden 
with<br>
ALT_OUTPUTDIR=&lt;dir&gt;, changing from product to fastdebug you may 
want<br>
to use the clean target first.<br>
- JDK_IMPORT_PATH must refer to a 
compatible build, not all past promoted<br>
builds or previous release JDK 
builds will work.<br>
- The fastest builds have been when the sources and the 
BOOTDIR are on<br>
local disk. </p> 
<p>--- Examples ---<br>
make fastdebug_build<br>
make 
ALT_OUTPUTDIR=/tmp/foobar all<br>
make ALT_OUTPUTDIR=/tmp/foobar fastdebug_build
<br>make ALT_OUTPUTDIR=/tmp/foobar all<br>
make ALT_BOOTDIR=/opt/java/jdk1.5.0
<br>make ALT_JDK_IMPORT_PATH=/opt/java/jdk1.6.0 </p> 
<p>OpenJDK:Ted@XPJAVA:/cygdrive/c/Prg/OpenJDK/jdk7<br>
$</p> </blockquote> 
<p>The one I want is &quot;make fastdebug_build&quot; or &quot;make 
debug_build&quot; (so I have debug symbols and can go spelunking). Do it.</p> 
<p>Watch the stuff go flying by.</p> 
<p>Get bored.</p> 
<p>Get lunch.</p> 
<p>Come back, it might be done. </p> 
<p>If it's a successful build, you'll have &quot;stuff&quot; in the 
C:\Prg\OpenJDK\jdk7\build directory, corresponding to the kind of build you 
kicked off; for a &quot;fastdebug&quot; build, for example, there'll be a 
&quot;windows-i586-fastdebug&quot; directory in which you'll find a 
&quot;j2sdk-image&quot; directory in which there<em>should</em> be a complete 
JDK environment. Try running Java and see if it works (make sure your PATH is 
updated to point to the right place before you do!)</p> 
<p>If not, it's debugging time.  Note that the &quot;build&quot; directory is 
completely built from scratch, so if you get a partial build and want to start 
over from scratch, just &quot;rd /s build&quot; from the jdk7 directory. (It's 
easier than &quot;make clean&quot; or &quot;make clobber&quot;, I've found.)</p>
<h2>More About Builds</h2> 
<p>When building the JDK, you may want to build bits &quot;underneath&quot; 
the top-level directory, but doing this is a tad tricky. I asked about this on 
the build-dev list, and Kelly responded with a great email about the build 
process, specifically about launching &quot;sub&quot; makes within the system:
</p> 
<blockquote> 
<p>Due to history, a build directly from the jdk/make directories uses a 
default OUTPUTDIR of jdk/build/* but if FASTDEBUG=true, it's 
jdk/build/*-fastdebug, or if a plain debug build with just VARIANT=DBG it would 
be jdk/build/*-debug The variant builds leave results in a completely separate 
outputdir.</p> 
<p>If you used the very top level makefile (which came from the now defunct 
control/make area) the default OUTPUTDIR is ./build/* (at the very top of the 
repositories). When this top level Makefile runs the jdk/make Makefiles, it 
passes in a ALT_OUTPUTDIR to refer to this top level build result area because 
it's default outputdir is not the same place.</p> 
<p>I don't know the complete history as to why this was done this way, but my 
tendency is to try and get us back to a single default OUTPUTDIR for all the 
repositories. Someday...</p> 
<p>This is what I do when I work on just the jdk repository:<br>
cd jdk/make 
&amp;&amp; gnumake</p> 
<p>That primes the outputdir area, then I can drop down in:<br>
cd 
jdk/make/java &amp;&amp; gnumake</p> 
<p>Or even drop in and clean an area and re-build it:<br>
cd jdk/make/jpda 
&amp;&amp; gnumake clean &amp;&amp; gnumake</p> 
<p>Or just repeat the entire build (incremental build)<br>
cd jdk/make 
&amp;&amp; gnumake</p> 
<p>If I wanted the jdk image (j2sdk-image), I would need to:<br>
cd jdk/make 
&amp;&amp; gnumake image</p> 
<p>But the output by default will go to jdk/build/* and a different directory 
if VARIANT=DBG or FASTDEBUG=true.</p> </blockquote> 
<p>This should help having to go through the whole process for incremental 
updates. (Note that on my system, I had to call it &quot;make&quot; instead of 
&quot;gnumake&quot;.)</p> 
<h2>Futures</h2> 
<p>As time goes on, I'll hopefully find the time to blog about how to find 
various little things in the JDK and make source-modifications to prove that 
they work, and use that as a springboard from which to let you start hacking on 
the JDK. In the meantime,<em>please</em>, if you run into trouble and find 
fixes to any of the above, comment or email me, and I'll correct this.</p> 
<h2>Contributions/Suggested TODOs</h2> 
<ul> 
<li>Have the make system automatically capture the results of the build in a 
log file, for easier debugging. Granted, you can &quot;make &gt;| 
build.output&quot;, but that seems tedious when it could easily be captured 
automagically for you each time. (&quot;make sanity&quot; does this, capturing 
the results in build/windows-i586-fastdebug/sanityCheckMessages.txt and 
sanityCheckWarnings.txt.</li> 
<li>Documentation, documentation, documentation. This thing does a lot of 
recursive makes and invokes a lot of tools (some of them built as part of the 
build process itself), and it would be<em>much</em> easier to debug and 
understand if the process were better documented. Even a simple flowchart/tree 
of the various Make invocations and what each does (in practice) would be 
helpful when trying to track down issues.</li> 
<li>Add support for all output to be captured into a build log file. This can 
obviously be done at the command-line via &quot;make |&amp; tee build.log&quot; 
or &quot;&gt;&amp; build.log&quot;, but I think it'd be nice if it were somehow 
folded in as part of the build process so it happened automatically.</li> </ul> 
<h2>Updates </h2> 
<ol> 
<li>Kelly OHair sent me some updated information, such as the right link to 
use for the README file (from the HG repository instead of the SVN one).</li> 
<li>Kelly also mentioned that the plugin and installers are not part of the 
OpenJDK build yet, so that's not something I could build even if I wanted to. 
Which is fine, for me. </li> 
<li>Kelly confirmed that UNICOWS is not needed in OpenJDK. </li> 
<li>Kelly mentions that link.exe on the PATH must be VS2003's, not Cygwin's. 
</li> 
<li>Kelly mentions the COMPILER_VERSION problem might be PATH issues. </li> 
<li>Kelly notes, &quot;On the C:\ vs C:/ vs. /cyg*/ paths, I try and use the 
C:/ all the time everywhere, and anywhere in the Makefiles where I know I need 
C:\ or the /cyg*/ paths, I try and use cygpath or MKS dosname to force them 
into the right form. NMAKE.EXE only likes C:\ paths, and cygwin PATH only wants 
/cyg*/ paths, and Windows/Java/MKS never want /cyg*/ paths. :^( I wish we had a 
better solution on Windows for this shell/path mania.&quot;</li> 
<li>Poonam's Weblog has a good page on building the OpenJDK on Windows with 
NetBeans, from which I've stolen... ahem, <em>leveraged</em>... some links. His 
webpage has a link for theUNICOWS download page, but that only includes the 
DLL, not the .LIB, which you will also need. (It's an import library for the 
DLL; you need both.) The only place I know to get the .LIB is from the Platform 
SDK, and you need an old one, circa November 2001 or so. I don't know if it's 
kosher to redistribute any other way (meaning, Microsoft won't let us, as far 
as I know).</li> 
<li>Added Volker Simonis' experiences with HTTP proxies in Mercurial </li> 
<li>Added the &quot;sub&quot; make build discussion from Kelly on build-dev 
</li> 
<li>Added the link to Volker's blog about building on Suse Linux</li> </ol> 
<br> 
<p></p> C++&nbsp;|&nbsp; Development Processes&nbsp;|&nbsp; Java/J2EE <br>
<br>
 Saturday, 15 December 2007 02:35:59 (Pacific Standard Time, UTC-08:00)<br>

Comments [2] &nbsp;|&nbsp; <br>
<br>
<br>
&nbsp;Saturday, 08 December 2007 <br>

Quotes on writing <br>

<p>This is, without a doubt, the most accurate quote <em>ever</em> about the 
&quot;fun&quot; of writing a book:</p> 
<blockquote> 
<p>Writing a book is an adventure. To begin with, it is a toy and an 
amusement; then it becomes a mistress, and then it becomes a master, and then a 
tyrant. The last phase is that just as you are about to be reconciled to your 
servitude, you kill the monster, and fling him out to the public. (Source: 
Winston Churchill)</p> </blockquote> 
<p>Keep that in mind, all you who are considering authoring as a career or 
career supplement.</p> 
<p>Were I to offer my own, it would be like so:</p> 
<blockquote> 
<p>Writing a book is like having a child. </p> 
<p>Trying is the best part, in some ways. You have this idea, this burning 
sensation in your heart, that just<em>has</em> to get out into the world. But 
you need a partner, a publisher who will help you bring your vision to life. 
You write proposals, you write tables of contents, you imagine the book cover 
in your mind. Then, YES! You get a publisher to agree. You sign the contract, 
fax it in, and you are on the way! We are<em>authoring!</em></p> 
<p>At first, it is wonderful and exciting and full of potential. You run into 
a few hangups, a few periods of nausea&nbsp;as you realize the magnitude of 
what you're really doing. You resolve to press on. As you continue, you begin 
to feel like you're in control again, but you start to get this sense like it's 
an albatross, a weight around your neck. Before long, you're dragging your 
feet, you can't seem to muster the energy to do anything, just get this thing
<em>done</em>. The deadline approaches, the sheer horror of what's left to be 
done paralyzes you. You look your editor in the eye (literally or figuratively) 
and say, &quot;I can't do this.&quot; The editor says, &quot;Push&quot;. You 
whimper, &quot;Don't make me do this, just cancel the contract.&quot; The 
editor says, &quot;Push&quot;. You scream at them, &quot;This is YOUR fault, 
you MADE me do this!&quot; The editor says, &quot;Push&quot;. Then, all of a 
sudden, it's done, it's out, it's on the shelf, and you take photos and show it 
off to all the friends, neighbors and family, who look at you a little 
sympathetically, and don't mention how awful you really look in that photo.</p> 
<p>As the book is out in the world, you feel a sense of pride an joy at it. 
You imagine it profoundly changing the way people look at the world. You 
imagine it reaching bestseller lists. You're already practicing the speech for 
the Nobel. You're sitting in your study, you reach out and grab one of the free 
copies still sitting on your desk, and you open to a random page. Uh, oh. 
There's a typo, or a mistake, or something that clearly got past you and the 
technical reviewers and the copyeditors. Damn. Oh, well, one mistake can't make 
that much difference.</p> 
<p>Then the reviews come in on Amazon. People like it. People post good 
reviews. One of them is not positive. You get angry: this is your<em>baby</em> 
they are attacking. How DARE they. You make plans to find large men with 
Italian names and track down that reviewer. You suddenly realize your 
overprotectiveness. You laugh at yourself weakly. You try to convince yourself 
that there's no pleasing some people.</p> 
<p>Then someone comes up to you at a conference or interview or other 
gathering, and says, &quot;Wow, you wrote that? I have that book on my 
shelf!&quot; and suddenly it's all OK. It may not be perfect, but it's yours, 
and you love it all the same, warts and all.</p> </blockquote> 
<p>Nearly a dozen books later, it's always the same.</p> <br>

<p></p> .NET&nbsp;|&nbsp; C++&nbsp;|&nbsp; Conferences&nbsp;|&nbsp; 
Development Processes&nbsp;|&nbsp; Java/J2EE&nbsp;|&nbsp; Reading&nbsp;|&nbsp; 
Ruby&nbsp;|&nbsp; Windows&nbsp;|&nbsp; XML Services <br>
<br>
 Saturday, 08 
December 2007 02:48:51 (Pacific Standard Time, UTC-08:00)<br>
Comments [1] 
&nbsp;|&nbsp;<br>
<br>
<br>
&nbsp;Wednesday, 05 December 2007 <br>
A Dozen 
Levels of Done <br>

<p>Michael Nygard (author of the <em>great</em> book <em>Release It!</em>), 
writes that &quot;[his] definition of 'done' continues to expand&quot;. 
Currently, his definition reads:</p> 
<blockquote> 
<p>A feature is not &quot;done&quot; until all of the following can be said 
about it:</p> 
<ol> 
<li>All unit tests are green. </li> 
<li>The code is as simple as it can be. </li> 
<li>It communicates clearly. </li> 
<li>It compiles in the automated build from a clean checkout. </li> 
<li>It has passed unit, functional, integration, stress, longevity, load, and 
resilience testing.</li> 
<li>The customer has accepted the feature. </li> 
<li>It is included in a release that has been branched in version control. 
</li> 
<li>The feature's impact on capacity is well-understood. </li> 
<li>Deployment instructions for the release are defined and do not include a 
&quot;point of no return&quot;.</li> 
<li>Rollback instructions for the release are defined and tested. </li> 
<li>It has been deployed and verified. </li> 
<li>It is generating revenue.</li> </ol> 
<p>Until all of these are true, the feature is just unfinished inventory.</p> 
</blockquote> 
<p>As much as I agree with the first 11, I'm not sure I agree with #12. Not 
because it's not important--too many software features are added with no 
positive result--but because it's too hard to measure the revenue a particular 
program, much less a particular software<em>feature</em>, is generating. </p>
<p>My guess is that this is also conflating the differences between 
&quot;features&quot; and &quot;releases&quot;, since they aren't always one and 
the same, and that not all &quot;features&quot; will be ones mandated by the 
customer (making #6 somewhat irrelevant). Still, this is an important point to 
any and all development shops:</p>
<p>What do <em>you</em> call &quot;done&quot;?</p> <br>

<p></p> Development Processes&nbsp;|&nbsp; Reading <br>
<br>
 Wednesday, 05 
December 2007 02:44:41 (Pacific Standard Time, UTC-08:00)<br>
Comments [4] 
&nbsp;|&nbsp;<br>
<br>
<br>
&nbsp;Monday, 03 December 2007 <br>
Anybody know of 
a good WebDAV client library ... <br>

<p>... for Ruby,&nbsp;or PowerShell/.NET? I'm looking for something to make it 
easier to use WebDAV from a shell scripting language on Windows; Ruby and 
PowerShell are the two that come to mind as the easiest to use on Windows. For 
some reason, Google doesn't yield much by way of results, and I've<em>got</em> 
to believe there's better WebDAV support out there than what I'm finding.</p> 
<p>(Yes, I could write one, but why bother, if one is out there that already 
exists? DRY!)</p> 
<p>BTW, anybody who finds one and wants credit for it, I'll be happy to post 
here. If you're a commercial vendor and you send me a license to go with it, 
I'll post that, too, with some code and explanation on how I'm using it, though 
I doubt it's going to be all that different from how anybody else would use it.
</p> <br>

<p></p> .NET&nbsp;|&nbsp; C++&nbsp;|&nbsp; Development Processes&nbsp;|&nbsp; 
Java/J2EE&nbsp;|&nbsp; Ruby&nbsp;|&nbsp; Windows <br>
<br>
 Monday, 03 December 
2007 17:03:19 (Pacific Standard Time, UTC-08:00)<br>
Comments [1] &nbsp;|&nbsp; 
<br> <br>
<br>
&nbsp;Monday, 29 October 2007 <br>
Welcome to the Shitty Code 
Support Group <br>

<p>&quot;Hi. My name's Ted, and I write shitty code.&quot;</p> 
<p>With this opening, a group of us&nbsp;earlier this year opened a panel 
(back in March, as I recall) at the No Fluff Just Stuff conference in 
Minneapolis. Neal Ford started the idea, whispering it to me as we sat down for 
the panel, and I immediately followed his opening statement in the same vein. 
Poor Charles Nutter, who was new to the tour, didn't get the 
whispered-down-the-line instruction, and tried valiantly to recover the panel's 
apparent collective discard of dignity--&quot;Hi, I'm Charles, and I write<em>
Ruby</em> code&quot;--to no avail. (He's since stopped trying.)</p> 
<p>The reason for our declaration of impotent implementation, of course, was, 
as this post states so well, a Zen-like celebration of our inadequacies: <em>To 
be a Great Programmer, you must admit that you are a Terrible Programmer.</em>
</p> 
<p>To those who count themselves as the uninitiated into our particular brand 
of philosophy (or religion, or just plain weirdness), the declaration is a 
statement of anti-Perfectionism. &quot;I am human, therefore I make mistakes. 
If I make mistakes, then I cannot assume that I will write code that has no 
mistakes. If I cannot write code that has no mistakes, then I must assume that 
mistakes are rampant within the code. If mistakes are rampant within the code, 
then I must find them. But because I make mistakes, then I must also assume 
that I make mistakes trying to identify the mistakes in the code.<em>Therefore, 
I will seek the best support I can find in helping me find the mistakes in my 
code.</em>&quot;</p> 
<p>This support can come in a variety of forms. The Terrible Programmer cites 
several of his favorites:&nbsp;use of the&nbsp;Statically-Typed Language (in 
his case, Ada), Virulent Assertions, Testing Masochism, the Brutally Honest 
Code Review, and Zeal for the Visible Crash. Myself, I like to consider other 
tools as well: the Static Analysis Tool Suite,&nbsp;a Commitment to Confront 
the Uncomfortable Truth, and the Abject Rejection of Best Practices.</p> 
<p>By this point in time, most developers have at least heard of, if not 
considered adoption of, the Masochistic Testing meme. Fellow NFJS'ers Stuart 
Halloway and Justin Gehtland have founded a consultancy firm, Relevance, that 
sets a high bar as a corporate cultural standard: 100% test coverage of their 
code. Neal Ford has reported that ThoughtWorks makes similar statements, though 
it's my understanding that clients sometimes put accidental obstacles in their 
way of achieving said goal. It's amibtious, but as the ancient American Indian 
proverb is said to state,<em>If you aim your arrow at the sun, it will fly 
higher and father than if you aim it at the ground</em>.</p> 
<p>In fact, on a panel this weekend in Dallas, fellow NFJS'er David Bock 
attributed the rise of interest in dynamic languages to the growth of the 
unit-testing meme, since faith in the quality of code authored in a dynamic 
language can only follow if the code has been rigorously tested, since we have 
no tool checking the syntactic or semantic correctness before it is executed.
</p> 
<p>Among the jet-setting Ruby crowd, this means a near-slavish devotion to 
unit tests. Interestingly enough, I find this attitude curiously incomplete: if 
we assume that we make mistakes, and that we therefore require unit tests to 
prove to ourselves (and, by comfortable side effect, the world around us) that 
the code is correct, would we not also benefit from the automated--and in some 
ways far more comprehensive--checking that a static-analysis tool can provide? 
Stu Halloway once stated, &quot;In five years, we will look back upon the Java 
compiler as a weak form of unit testing.&quot; I have no doubt that he's 
right--but I draw an entirely different conclusion from his statement: we need 
better static analysis tools, not to abandon them entirely.&nbsp;</p> 
<p>Consider, if you will, the static-analysis tool known as FindBugs. Fellow 
NFJS'er&nbsp;(and&nbsp;author of&nbsp;the JavaOne bestselling book<em>Java 
Concurrency in Practice</em>)&nbsp;Brian Goetz offered a presentation last year 
on the use of FindBugs in which he cited the use of the tool over the existing 
JDK Swing code base. Swing has been in use since 1998, has had close to a 
decade of debugging driven by actual field use, and (despite what personal 
misgivings the average Java programmer may have about building a &quot;rich 
client&quot; application instead of a browser-based one) can legitimately call 
itself a successful library in widespread use.</p> 
<p>If memory serves, Brian's presentation noted that when run over the Swing 
code base, FindBugs discovered 70-something concurrency errors that remained in 
the code base, some since JDK 1.2.<em>In close to a decade of use, 70-something 
concurrency bugs had been neither fixed nor found.</em> Even if I misremember 
that number, and it is off by an order of magnitude--7 bugs instead of 70--the 
implication remains clear:<em>simple usage cannot reveal all bugs.</em></p> 
<p>The effect of this on the strength of the unit-testing argument cannot be 
understated--if the quality of dynamic-language-authored code rests on the 
exercise of that code under constrained circumstances in order to prove its 
correctness, then we have a major problem facing us, because the interleaving 
of execution paths that define concurrency bugs remain beyond the ability of 
most (arguably all) languages and/or platforms to control.<em>It thus follows 
that concurrent code cannot be effectively unit tested, and thus the premise 
that dynamic-language-authored code can be proven correct by simple use of unit 
tests is flawed.</em></p> 
<p>Some may take this argument to imply a rejection of unit tests. Those who 
do would be seeking evidence to support a position that is equally untenable, 
that unit-testing is somehow &quot;wrong&quot; or unnecessary. No such 
statement is implied; quite&nbsp;the opposite, in fact. We can neither reject 
the&nbsp;necessitary of unit testing any more than we can the&nbsp;beneficence 
of static analysis tools; each is, in its own way, incomplete in its ability to 
prove code correct, at least in current incarnations. In fact, although I will 
not speak for them, many of the Apostles of UnitTestitarianism in fact 
indirectly support this belief, arguing that unit tests do not obviate the need 
for a quality-analysis phase after a development iteration, because correct 
unit tests cannot imply completely correct code.</p> 
<p>Currently much research is taking place in the statically-typed languages 
to make them more typesafe without sacrificing the productivity enhancements 
seen in the dynamically-typed language world. Scala, for example, makes heavy 
use of type-inferencing to reduce the burden of type declarations by the 
programmer, requiring such declarations only when the inferencing yields 
ambiguity. As a result, Scala's syntax--at a first glance--looks remarkably 
similar in places to Ruby's, yet the Scala compiler is still fully type-safe, 
ensuring that accidental coercion doesn't yield confusion. Microsoft is 
pursuing much the same route as part of their LINQ strategy for Visual Studio 
2008/.NET 3.5 (the &quot;Orcas&quot; release), and some additional research is 
being done around functional languages in the form of F#.</p> 
<p>At the end of the day, the fact remains that I write shitty code. That 
means I need all the help I can get--human-centric or otherwise--to make sure 
that my code is correct. I will take that help in the form of unit tests that I 
force myself (and others around me force myself) to write. But I also have to 
accept the possibility that my unit tests themselves may be flawed, and that 
therefore other tools--which do not suffer from my inherent human capacity to 
forget or brush off--are a powerful and necessary component of my toolbox.</p> 
<br> 
<p></p> .NET&nbsp;|&nbsp; C++&nbsp;|&nbsp; Conferences&nbsp;|&nbsp; 
Development Processes&nbsp;|&nbsp; Java/J2EE&nbsp;|&nbsp; Ruby <br>
<br>
 
Monday, 29 October 2007 17:12:32 (Pacific Daylight Time, UTC-07:00)<br>

Comments [4] &nbsp;|&nbsp; <br>
<br>
<br>
&nbsp;Sunday, 07 October 2007 <br>
A 
Book Every Developer Must Read <br>

<p>This is not a title I convey lightly, but Michael Nygard's <em>Release It!
</em> deserves the honor. It's the first book I've ever seen that addresses the 
issues of building software that's Production-friendly and 
sysadmin-approachable. He describes a series of antipatterns describing a 
variety of software failures, and offers up a series of solutions (patterns, if 
you will) to building software systems designed to combat said failures.</p> 
<p>From the back cover:</p> 
<blockquote> 
<p>Every website project is really an enterprise integration project: the 
stakes are high and the projects complex. In this world where good marketing 
can be fatal to yor website, where networks are unreliable, and where 
astronomically unlikely coincidences happen daily, you need all the help you 
can get.</p> 
<p>...</p> </blockquote> 
<blockquote> 
<p>You're a whiz at development. But 80% of typical project lifecyle cost can 
occur in production--not in development.</p> </blockquote> 
<p>Although Michael's personal experience stems mostly from the Java space, 
the lessons and stories he offers up are equally relevant to Java, .NET, C++, 
Ruby, PHP, and any other language or platform you can imagine. Michael Nygard 
not only knows the Ten Fallacies of Enterprise Development, he<em>breathes</em> 
them.</p> 
<p>Go. Now. Buy. Read. Don't write another line of code until you do.</p> <br>

<p></p> .NET&nbsp;|&nbsp; C++&nbsp;|&nbsp; Development Processes&nbsp;|&nbsp; 
Java/J2EE&nbsp;|&nbsp; Reading&nbsp;|&nbsp; Ruby&nbsp;|&nbsp; Windows
&nbsp;|&nbsp;XML Services <br>
<br>
 Sunday, 07 October 2007 17:41:29 (Pacific 
Daylight Time, UTC-07:00)<br>
Comments [3] &nbsp;|&nbsp; <br>
<br>
<br>

&nbsp;Thursday, 20 September 2007<br>
Hard Questions About Architects <br>

<p>I get e-mail from blog readers, and this one--literally--stopped me in my 
tracks as I was reading. Rather than interpret, I'll just quote (with 
permission) the e-mail and respond afterwards</p> 
<blockquote> 
<p>Hi Ted,</p> 
<p>I had a job interview last Friday which I wanted to share with you. It was 
for a &ldquo;Solutions Architect&rdquo; role with a large Airline here in New 
Zealand. I had a preliminary interview with the head Architect which went 
extremely well, and I was called in a few days later for an interview with the 
other three guys on the Architecture team.</p> 
<p>The second interview started off with the usual pleasantries, and then the 
technical grilling began with:</p> 
<p><b>&ldquo;What are the best practices that you would put in place on a 
project?&rdquo;</b> </p> 
<p>I replied &ldquo;I&rsquo;ve come to realise that there is no such thing as 
&rsquo;Best Practice&rsquo; in architecture &ndash; everything is 
contextual.&rdquo;</p> 
<p>Well, it went down like a sh*t sandwich! The young German guy who asked the 
question looked at me like some sort of heretic and said &ndash; &ldquo; I 
disagree&rdquo;. I thought to myself, no damn it &ndash; I&rsquo;m going to 
push it to see where this guys argument goes, so I said &ldquo;I can take any 
best practice you can think of, and by changing the context, I can render that 
best practice a worst practice&rdquo;. He didn&rsquo;t like that very much, but 
I thought, &lsquo;bugger him, I know I&rsquo;m right&rsquo;.</p> 
<p>He then came out with the next question &ldquo;What do you do when your 
architectural principles are compromised&rdquo;. I asked &ldquo;what do you 
mean&rdquo;, and he indicated that an application he designed recently, was 
found to be far too expensive to implement So he asked again &lsquo;what would 
you do&rsquo;?</p> 
<p>I replied &ldquo;redesign it&rdquo;. He scoffed at this answer, and 
reiterated &ldquo;but what if the redesign was compromising your architectural 
principles&rdquo;? So I asked &ldquo;what is more important. Your principles, 
or achieving a business objective?&rdquo;. I don&rsquo;t remember exactly what 
his answer was, but it was along the lines of &ldquo;you have to maintain a 
corporate standard&rdquo;.</p> 
<p>His next attack was at extreme programming. Having seen that I had used 
extreme programming one of my recent projects (in addition to also using 
waterfall, more recently, on others), he asked &ldquo;don&rsquo;t you think 
that the very risky nature of extreme programming is at odds with it&rsquo;s 
ability to deliver software consistently?&rdquo;. This was a bit of a stunner. 
I indicated that, once again, it was contextual. XP is appropriate on some 
projects, but it is not on others. On the XP project I worked on, it was 
entirely appropriate. We delivered early, within budget, the client got what he 
wanted, and we got a few million dollars worth of work out of it. Not 
surprisingly, he didn&rsquo;t have a lot to say to me after that.</p> 
<p>Having worked as a consultant for a number of years now, I have been 
entirely focused on adding<b>business value</b>. I was stunned to hear first 
hand, how divorced from the business this &ldquo;architect&rdquo; was. Clearly 
he has to maintain some sort of structure with their corporate systems, but 
surely each business solution should be assessed primarily in the context of 
it&rsquo;s own business objectives.</p> 
<p>The interview was good in the respect that I was able to quickly establish 
that it wasn&rsquo;t the place for me, but it did leave me with some unanswered 
questions:</p> 
<ol> 
<li>How could their idea of an architect (being the policemen of corporate 
best practice) be so far removed from someone like myself, who aims to make 
case by case judgements based on pragmatism and experience?</li> 
<li>Is architecture supposed to be facilitative or restrictive? </li> 
<li>What relevance do architects have today? Are they just overpaid, out of 
touch developers?</li> </ol> 
<p>Regards, </p> 
<p>Shane Paterson </p> 
<p>Hands on Architect Type </p> 
<p>(for lack of a more relevant title)</p> </blockquote> 
<p>Wow. </p>
<p>For starters, Shane, kudos to you for sticking to your guns, and for 
figuring out really quickly that this was clearly not a place you wanted to 
work--a lot of developers have a mentality that says that they need the company 
more than the company needs them, sort of a &quot;job at any price&quot; 
mindset. Interviews aren't supposed to be the place where candidates grovel and 
say whatever the company wants them to hear--an interview is supposed to be a 
vetting process for both sides.</p>
<p>But on to your questions: </p>
<p><em>How could their idea of an architect be so far removed from someone 
like myself?</em> I can't answer this one solidly, but I can say that the 
definition of an architect seems to be vague and indiscriminate a term, only 
exceeded in opacity by the term &quot;software&quot; itself. For some companies 
I've worked for, the &quot;architect&quot; was as you describe yourself, 
someone whose hands were dirty with code, acting as technical lead, developer, 
sometimes-project-manager, and always focused on customer/business value as 
well as technical details. At other places, the architect (or &quot;architect 
team&quot;) was a group of developers who had to be promoted (usually due to 
longevity) with no clear promotion path available to them other than 
management. This &quot;architect team&quot; then lays down &quot;corporate 
standards&quot;, usually based on &quot;industry standards&quot;, with little 
to no feedback as to the applicability of their standards to the problems faced 
by the developers underneath them. A friend of mine on the NFJS tour, Brian 
Sletten, tells a story of how he consulted on a project, implementing the 
(powerful) 1060 Netkernel toolkit at the core of the system, to resounding 
success. Then, on deployment, the &quot;architecture team&quot; took a look, 
pronounced the system to be incompatible with their &quot;official 
standards&quot;, and<em>forced new development of a working product</em>. In 
other words, the fact that it worked (and could easily be turned to 
interoperate with their SOAP-based standard, of which there were zero existing 
services) was in no way going to stand as an impediment to their enforcement of 
the corporate standard.</p> 
<p><em>Is architecture supposed to be facilitative or restrictive?</em> Ah, 
this is a harder one to answer. In essence, both. Now, before the crowd starts 
getting out their torches and pitchforks to have a good old-fashioned lynching, 
hear me out.</p> 
<p>Architecture is intended to be facilitative, of course, in that a good 
architecture should enable developers to build applications quickly and easily, 
without having to spend significant amounts of time re-inventing similar 
infrastructure across multiple projects. A good architecture will also 
facilitate interoperability across applications, ensure a good code quality, 
ensure good maintainability, provide for future extensibility, and so on. All 
of this, I would argue, falls under the heading of &quot;facilitation&quot;.</p>
<p>But an architecture is also intended to be restrictive, in that it should 
channel software developers in a direction that leads to all of these 
successes, and away from potential decisions that would lead to prolems later. 
In other words, as Microsoft's CLR architect Rico Mariani put it, a good 
architecture should enable developers to &quot;fall into the pit of 
success&quot;, where if you just (to quote the proverbial surfer) &quot;go with 
the flow&quot;, you make decisions that lead to all of those good qualities we 
just discussed.</p> 
<p>This is asking a lot of an architecture, granted. But that's the ideal.</p> 
<p><em>What relevance do architects have today?</em> Well, this is a dangerous 
question, in that you're asking it of one who considers himself an architect 
and technologist, so take this with the usual grain of salt. Are we just 
overpaid out-of-touch developers? God, I hope not. Fowler talks about 
architecture being irrelevant in an agile project, but I disagree with that 
notion pretty fundamentally: an architect is the captain of the ship, making 
the decisions that cross multiple areas of concern (navigation, engineering, 
and so on), taking final responsibility for the overall health of the ship and 
its crew (project and its members), able to step into any station to perform 
those duties as the need arises (write code for any part of the project should 
they lose a member). He has to be familiar with the problem domain,&nbsp;the 
technology involved, and keep an eye out on new technologies that might make 
the project easier or answer new customers' feature requests.</p> 
<p>And if anybody stands up at this point and says, &quot;Hey, wait a minute, 
that's a pretty tall order for anybody to fill!&quot;, then you start to get an 
idea of why architects do, frequently, get paid more than developers do. Having 
to know the business, the technology at a high and low level of detail, keeping 
your hands in the code, and watching the horizon for new developments in 
industry, is a pretty good way to burn out any free time you might have thought 
you'd have.</p> 
<p>Granted, all of these answers notwithstanding, there's a large number of 
&quot;architects&quot; out there whose principal goal is to simply remain 
employed. To do that, they cite &quot;best practices&quot; established by 
&quot;industry experts&quot; as a cover for making decisions of their own, 
because nobody ever gets fired for choosing what industry &quot;best 
practices&quot; dictate. That's partly why I hate that term: it's a cop-out. 
It's basically relying on articles on popular websites and magazines to do your 
thinking for you. Inevitably, when somebody at a conference says the word, 
&quot;Best Practice&quot;, listeners' minds turn off, their pens turn on, and 
they dutifully enscribe this bit of knowledge into their projects at home,<em>
without considering the applicability to their project or corporate culture</em>
. Nothing, not a single technology, not a single development methodology, not 
even a single tool, is<em>always</em> the right answer.</p> 
<p>In the end, I think what Shane ran into was an &quot;architect&quot; with 
an agenda and an alpha-geek complex. He refused to consider somebody with a 
competing point of view, because God forbid somebody show him<em>not</em> to be 
the expert he's hoodwinked everybody else at work to think he is. Unfortunately 
I've run across this phenomenon too often to call it statistical error, and the 
only thing you can do is to do exactly what you did, Shane: get the hell out of 
Dodge.</p> <br>

<p></p> .NET&nbsp;|&nbsp; C++&nbsp;|&nbsp; Development Processes&nbsp;|&nbsp; 
Java/J2EE&nbsp;|&nbsp; Ruby&nbsp;|&nbsp; Windows&nbsp;|&nbsp; XML Services <br>

<br> Thursday, 20 September 2007 04:11:38 (Pacific Daylight Time, UTC-07:00) 
<br> Comments [5] &nbsp;|&nbsp; <br>
<br>
<br>
&nbsp;Saturday, 28 July 2007 <br>
The First Strategy: Declare War on Your Enemies (The Polarity Strategy) <br>

<em></em> 
<p><em>Software is endless battle and conflict, and you cannot develop 
effectively unless you can identify the enemies of your project. Obstacles are 
subtle and evasive, sometimes appearing to be strengths and not distractions. 
You need clarity. Learn to smoke out your obstacles, to spot them by the signs 
and patterns that reveal hostility and opposition to your success. Then, once 
you have them in your sights, have your team declare war. As the opposite poles 
of a magnet create motion, your enemies--your opposites--can fill you with 
purpose and direction. As people and problems that stand in your way, who 
represent what you loathe, oppositions to react against, they are a source of 
energy. Do not be naive: with some problems, there can be no compromise, no 
middle ground.</em></p> 
<p>In the early 1970s, Margaret Thatcher shook up British politics by refusing 
to take the style of the politicians before her: where they were smooth and 
conciliatory, she was confrontational, attacking her opponents directly. She 
bucked the conventional wisdom, attacking her opponents mercilessly where 
historically politicians sought to reassure and compromise. Her opponents had 
no choice but to respond in kind. Thatcher's approach polarized the population, 
seizing the attention, attracting the undecided and winning a sizable victory.
</p> 
<p>But now she had to rule, and as she continued down her obstinate, all-in 
style of &quot;radicalism&quot; politics, she seemed to gain more enemies than 
any one politician could hold off. Then, in 1982, Argentina attacked the 
British Falkland Islands in the South Atlantic. Despite the distance--the 
Falklands were almost diametrically opposite the globe from the British home 
islands, off the tip of South America--Thatcher never hesitated, dispatching 
the British armed forces to respond to the incursion with deadly force, and the 
Argentinians were beaten. Suddenly, her obstinacy and radicalism were seen in a 
different light: courage, nobility, resolute and confident.</p> 
<p>Thatcher, as an outsider (a middle-class woman and a right-wing radical), 
chose not to try and &quot;fit in&quot; with the crowd, but to stake her 
territory clearly and loudly.&nbsp;Life as an outsider can be hard, but she 
knew that if&nbsp;she&nbsp;tried to blend in, she could easily be replaced. 
Instead, she set herself&nbsp;up at every turn as one woman against an army of 
men.</p> 
<p>As Greene notes, </p> 
<blockquote> 
<p>We live in an era&nbsp;in which people are seldom directly&nbsp;hostile. 
The rules of engagement--social, political, military--have changed, and so must 
your notion of the enemy. An up-front enemy is rare now and is actually a 
blessing. ... Although the world &nbsp;is more competitive than ever, outward 
aggression is discouraged, so people have learned to go underground, to attack 
unpredictably and craftily. ... Understand: the word 'enemy'--from the Latin<em>
inimicus</em>, &quot;not a friend&quot;--has been demonized and politicized. 
Your first task as a strategist is to widen your concept of the enemy, to 
include in that group those who are working against you, thwarting you, even in 
subtle ways.</p> </blockquote> 
<p>Software projects face much the same kind of problem: there are numerous 
forces that are at work trying to drag the project down into failure. While 
agile books love to assume an environment in which the agile methodology is 
widely accepted and embraced, reality often intrudes in very rude 
ways.&nbsp;Sometimes management's decision to &quot;go agile&quot; is based not 
to try and&nbsp;deliver software more successfully, but to simply take the 
latest &quot;fad&quot; and throw it into the mix, such that when it fails, 
management can say, &quot;But we followed industry best practices, it clearly 
can't be<em>management</em> at fault.&quot; (This is the same idea behind the 
statement, &quot;Nobody ever got fired for buying IBM (or Microsoft or Java or 
...).&quot; Sometimes the users are not as committed to the project as we might 
hope, and at times, the users are even contradictory to one another, as each 
seeks to promote their own agenda within the project.</p> 
<p>As a software development lead (or architect, or technical lead, or project 
manager, or agile coach, or whatever), you need to learn how to spot these 
enemies to your project, identify them clearly, and make it clear that you see 
them as an enemy that will not be tolerated. This doesn't mean always treat 
them openly hostile--sometimes the worst enemies can be turned into the best 
friends, if you can identify what drives them to the position that they take, 
and work to meet their needs. Case in point: system administrators frequently 
find themselves at odds with developers, because the developer seeks (by 
nature) to change the system, and sysadmins seek (by nature) to keep everything
<em>status quo</em>. Recognizing that sysadmins have historically been 
blindsided by projects that essentially ignored<em>their</em> needs (such as a 
need to know that the system is still running, or the need to be able to easily 
add users, change security policies, or diagnose failures quickly at 3AM in the 
morning) means that we as developers can either treat them as enemies to be 
overcome, or win them as friends by incorporating their needs into the project. 
But they are either &quot;for us&quot; or &quot;against us&quot;, and not just 
a group to be ignored.</p> 
<p>Other enemies are not to be tolerated at any level: apathy, sloth, or 
ignorance are all too common among developer teams. Ignorance of how underlying 
technologies work. Apathy as to the correctness of the code being created. 
Sloth in the documentation or tests. These are enemies that, given enough time 
and inattention, will drag the project down into the tar pits of potential 
failure. They cannot be given any quarter. Face them squarely, with no 
compromise. Your team, if they hold these qualities, must be shown that there 
is no tolerance for them. Hold brown-bag lunches once a week to talk about new 
technologies, and their poential impact on the team or company or project. 
Conduct code reviews religiously, during active development (rather that at the 
end as a token gesture), with no eye towards criticizing the author of the 
code, but the code itself. Demand perfection in the surrounding artifacts of 
the project: the help files, the user documentation, the graphics used for 
buttons and backdrops and menus.</p> 
<p>Do not wait for the enemies of your project to show themselves, 
either--actively seek them out and crush them. Take ignorance, for example. Do 
not just &quot;allow&quot; each of your developers to research new 
technologies, but demand it of them: have each one give a brown-bag 
presentation in turn. In a four-man team, this means that each developer will 
have a month in which to find something new to discover, analyze, discuss and 
present. They do not have to have all the answers to the technology, and in 
fact, if the technology is sufficiently large or interesting, they can spend 
the next month investigating a new element or new use of the technology. 
Demanding this of your developers means they are forced to educate themselves, 
and forced to raise their own awareness of the changing world. (Naturally, 
developers must be given time to do this research; anecdotally, giving them 
Friday afternoons to do this experimentation and research, when energy and 
interest in the work week is already typically at an ebb, works well.)</p> 
<p>Wherever possible, avoid enemies that are large and hard to pinpoint. 
Simply saying &quot;We need to have better quality code&quot; is too amorphous 
and too vague. Developers have nothing to measure against. Personalize your 
enemies, eyeball to eyeball. Put names to them, make them clearly visible to 
all involved. &quot;We will have 100% code coverage in our unit tests&quot; is 
a clearly-defined goal, and anything that prevents that goal from being reached 
will be clearly visible. &quot;We will not ship code that fails to pass any 
unit test&quot; is another clear goal, but must be paired with&nbsp;something 
that avoids the natural &quot;Well, then, we'll not write any unit tests, and 
we'll have met that goal!&quot; response. Demanding a ratio of 
unit-test-lines-to-lines ratio is a good start: &quot;We will have three lines 
of unit test code per line of code&quot; now offers a measurable, identifiable 
enemy that can be stared in the face. Go so far as to make a ceremony out of 
it: call the developers into a room, put a poster on a wall, and make your 
intentions clear. Motivate them. &quot;When we presented the release of the 
payroll system to the HR department last year, the users called it 'barely 
acceptable' and 'hard to use'. I<em>refuse</em> to allow that to happen again. 
The system we build for them this year will be amazing. It will be reliable. It 
will have those features they need to get their job done<em>(and be specific 
here)</em>, and we will accept no excuse otherwise.&quot;</p> 
<p>One of the world's most successful software companies, Microsoft is no 
stranger to the polarity strategy. The company as a whole as declared war on 
its enemies in a variety of fields, and with few exceptions, has won in almost 
every conflict. Microsoft actively courts conflict and confrontation. The 
presence of a well-established competitor in a particular field is no barrier 
to entry; Microsoft has routinely entered fields with dominant competitors and 
come out ahead in the end. Witness their entries into the&nbsp;word-processor 
and spreadsheet&nbsp;markets held at the time by dominant competitors 
WordPerfect and Lotus 1-2-3, their entry into the&nbsp;video-game console 
market against well-established competitors Sega and Nintendo, and more 
recently, the mobile entertainment device market (the Zune) against the iPod. 
In the latter, the battle has just begun, and the market remains firmly in the 
hands of the iPod, but let it not be forgotten that Microsoft is not one to 
retreat quickly from a battle.</p> 
<p>Microsoft is also known to do this within their projects; developers who 
are committed to a project yet seem hesitant or lax in their work are asked if 
they are really &quot;on board&quot; with this project. The legends of 
Microsoft developers putting in 80-plus hours a week on a project are somewhat 
true, but not because Microsoft management expects it of them, but because 
developers have been willing to put that kind of time into the project in order 
to succeed.</p> 
<p>And Microsoft management itself has declared war on its enemies, time and 
again, looking to eliminate any and all opposition the successful release of 
software. Distractions? Every developer gets his own office. Household chores? 
Microsoft has been known to offer their developers laundry services at work. 
Computing resources? It's not at all uncommon to walk into a Microsoft 
developers' office and see multiple CPUs (under desks, in corners, laptops, and 
so on) and monitors all over the room. Bodily needs? Refrigerators on every 
floor, stocked with sodas of every variety, water, juices, anything that the 
thirsty developer could need, all free for the taking. Even fatigue is an 
enemy: Microsoft buildings have video game consoles, foosball tables, bean bag 
chairs, and more&nbsp;tools of relaxational&nbsp;activities&nbsp;to help the 
developer take a break when necessary, so that they can resume the fight 
refreshed.</p> 
<p>Greene notes further,</p> 
<blockquote> 
<p>There are always hostile people and destructive relationships. The only way 
to break out of a negative dynamic is to confront it.&nbsp;Repressing your 
anger, avoiding the person threatening you, always looking to conciliate--these 
common strategies spell ruin.&nbsp;Avoidance of conflict becomes a habit, and 
you lose the taste for battle. Feeling guilty is pointless; it is not your 
fault you have enemies. Feeling wronged or victimized is equally futile. In 
both cases you are looking inward, concentrating on yourself and your feelings. 
Instead of internalizing a bad situation, externalize it and face your enemy. 
It is the only way out.</p> </blockquote> 
<p>To adapt this to software, instead of simply talking about the hopeless 
situation in which you find yourself--your company has no interest in agile, 
your team is just too &quot;inexperienced&quot; to tackle the kinds of projects 
you are being given, and so on--externalize it. Face the enemy. Your company 
has no interest in agile? Fine--instead of trying to talk them into it, take 
the radical approach,<em>do</em>&nbsp;a project in an agile fashion (even 
without upper management's knowledge if necessary), and show them the results. 
Can't get the IT budget to allow for a source-control server or continuous 
integration server? Use your desktop machine instead. Face the enemy, confront 
it, and defeat it.</p> 
<p>Enemies are not evil, and should not be seen as something to avoid. Enemies 
give us something against which to measure ourselves, to use as a foil against 
which to better ourselves. They motivate and focus your beliefs. Have a 
co-worker who refuses to see the benefits of dynamic languages? Avoiding him 
simply avoids an opportunity for you to learn from him and to better your 
arguments and approaches. Have a boss who doesn't see what the big deal behind 
a domain-specific language is? Have conversations on the subject, to understand 
her reluctance and opposition, and build a small DSL to show her the benefits 
of doing so. Don't avoid these people, for they offer you opportunities to 
better yourself and your understanding.</p> 
<p>Enemies also give you a standard against which to judge yourself. It took 
Joe Frazier to make Muhammad Ali a truly great boxer. The samurai of Japan had 
no guage of their excellence unless they challenged the best swordsmen they 
could find. For the Indianapolis Colts of last year, each victory was hollow 
unless they could beat their arch-rivals, the New England Patriots. The bigger 
the opponent, the greater your reward, even in defeat, for if you lose, you 
have opportunities to analyze the results and discover how and why you lost, 
then correct your strategy for the next time. For there will<em>always</em> be 
a next time.</p> 
<p>Don't seek to eschew enemies, either. Enemies give us options and purpose. 
Julius Caesar identified Pompey as his enemy early in his rise to the throne. 
Everything he did from then on was measured against Pompey, to put him in a 
stronger position to meet his enemy. Once he defeated Pompey, however, Caesar 
lost his way, and eventually fell because he viewed himself as a god and above 
all other men. Enemies force on you a sense of realism and humility.</p> 
<p>Remember, enemies surround you and your project, and sometimes even within 
your project. Keep a sharp eye out, so that once spotted, they can be 
identified, analyzed, and handled. Show no quarter to those enemies: they must 
either join you to help you in your quest to build great software, or be 
ruthlessly eliminated from your path. They can either benefit from the project, 
or they can be removed from the battlefield entirely. Some enemies--ignorance, 
apathy, sloth--are not easily defeated, nor once defeated will they remain so. 
Never lay down your arms against them or trust your arms to someone else--you 
are the last line of your own defense.</p> <br>

<p></p> Development Processes <br>
<br>
 Saturday, 28 July 2007 15:42:31 
(Pacific Daylight Time, UTC-07:00)<br>
Comments [1] &nbsp;|&nbsp; <br>
<br>
<br>
&nbsp;Saturday, 14 July 2007<br>
Yellow Journalism Meets The Web... again... 
<br> 
<p>For those who aren't familiar with the term, &quot;yellow journalism&quot; 
was&nbsp;a moniker applied to journalism (newspapers, at the time) articles 
that were written with little attention to the facts, and maximum attention to 
gathering attention and selling newspapers. Articles were sensationalist, 
highly incorrect or unvalidated, seeking to draw at the emotional strings the 
readers would fear or want pulled. Popular at the turn of the last century, 
perhaps the most notable example of yellow journalism was the sinking of the<em>
Maine</em>, a US battleship that exploded in harbor while visiting Cuba (then, 
ironically, a very US-friendly place). Papers at the time attributed the 
explosion to sabotage work by Spain, despite the fact that no cause or proof of 
sabotage was ever produced, leading the US to declare war on the Spanish, seize 
several Spanish colonies (including the Phillipines in the Pacific, which would 
turn out to be important to US Pacific Naval interests during World War Two), 
and in general pronouce anything Spanish to be &quot;enemies of the state&quot; 
and all that.</p> 
<p>Vaguely reminiscent of Fox News, now that I think of it.</p> 
<p>In this case, however, yellow journalism meets the Web in two recent 
&quot;IT magazine&quot; pieces that have come to my attention:this one, which 
blasts Sun for not rolling out updates in a more timely fashion to its 
consumers, despite the many issues that constant update rollouts pose for those 
same consumers, but more flagrantly,this one, which states that Google 
researchers have found a vulnerability in the Java Runtime Environment 
that&nbsp;&quot;threatens the security of all platforms, browsers, and even 
mobile devices&quot;. As if that wasn't enough, check out these 
&quot;sky-is-falling&quot; quotes:</p> 
<blockquote> 
<p>&quot; 'It&rsquo;s a pretty significant weakness, which will have a 
considerable impact if the exploit codes come to fruition quickly. It could 
affect a lot of organizations and users.'</p> 
<p>&quot;...&nbsp;anyone using the Java Runtime Environment or Java 
Development Kit is at risk.</p> 
<p>&quot; 'Delivery of exploits in this manner is attractive to attackers 
because even though the browser may be fully patched, some people neglect to 
also patch programs invoked by browsers to render specific types of content.'
</p> 
<p>&quot;... the bugs threaten pretty much every modern device.</p> 
<p>&quot; '... this exploit is browser independent, as long as it invokes a 
vulnerable Java Runtime Environment.'</p> 
<p>&quot;... the problem is compounded by the slim chance of an enterprise 
patching Java Runtime vulnerabilities.</p> </blockquote> 
<p>Now, I have no problems with the media reporting security vulnerabilities; 
in fact, I encourage it (as any security professional should), because 
consumers and administrators can only take action to protect against 
vulnerabilities when we know about them. But here's the thing:<em>nowhere</em>, 
not one place in the article, describes what the vulnerability actually<em>is
</em>. Is this a class verifier problem? Is this a buffer overflow attack? A 
luring attack? A flaw in the platform security model? A flaw in how Java parses 
and consumes image formats (a la the infamous &quot;picture attachment 
attack&quot; that bedevils Outlook every so often)?</p> 
<p>No details are given in this article, just fear, uncertainty and doubt. No 
quote, no vague description of how the vulnerability can be exploited, not even 
a link to&nbsp;the original&nbsp;report from Google's Security team.</p> 
<p>Folks, that is sensationalist journalism at its best. Or worst, if you 
prefer.</p> 
<p>Mr. Tung, who authored the article, should have titled it &quot;The Sky is 
Falling! The Sky is Falling!&quot; instead. Frankly, if I were Mr. Tung's 
editor, this drivel would never have been published. If I were given the 
editor's job tomorrow, I'd thank Mr. Tung for his efforts and send him over to 
a competitor's publication. Blatant, irresponsible, and reckless.</p> 
<p>Now, if you'll excuse me, I'm going to try and find some hard data on this 
vulnerability. Any vulnerability that can somehow strike across every JVM ever 
written (according to the article above) must be some kinda doozy. After all, I 
need to learn how to defend myself before al Qaeda gets hold of this and takes 
over &quot;pretty much every modern device&quot; and uses them to take over the 
world, which surely must be next....</p> <br>

<p></p> Development Processes&nbsp;|&nbsp; Java/J2EE&nbsp;|&nbsp; Reading <br>

<br> Saturday, 14 July 2007 23:07:48 (Pacific Daylight Time, UTC-07:00) <br>

Comments [4] &nbsp;|&nbsp; <br>
<br>
<br>
&nbsp;Friday, 13 July 2007 <br>
The 
Strategies of Software Development <br>

<p>At a software conference not too long ago, I was asked what book I was 
currently reading that I'd recommend, and I responded, &quot;Robert Greene's<em>
The 33 Strategies of War&quot;</em>. When asked why I'd recommend this, the 
response was pretty simple: &quot;Because I believe that there's more parallels 
to what we do in military history than in constructing buildings.&quot;</p> 
<p>Greene's book is an attempt at a distillation of what all the most 
successful generals and military leaders throughout history used to make them 
so successful. A lot of these concepts and ideas are just generally good 
practices, but a fair amount of them actually apply pretty directly to software 
development (whether you call it &quot;agile&quot; or not). Consider this 
excerpt from the Preface, for example:</p> 
<blockquote> 
<p>The war [that exists in the real world] exists on several levels. Most 
obviously, we have our rivals on the other side. The world has become 
increasingly competitive and nasty. In politics, business, even the arts, we 
face opponents who will do almost anything to gain an edge. More troubling and 
complex, however, are the battles we face with those who are supposedly on our 
side. There are those who outwardly play the team game, who act very friendly 
and agreeable, but who sabotage us behind the scenes, ues the group to promote 
their own agenda. Others, more difficult to spot, play subtle games of passive 
aggression, offering help that never comes, instilling guilt as a secret 
weapon. On the surface everything seems peaceful enough, but just below it, it 
is every man and woman for him- or herself, this dynamic infecting even 
families and relationships. The culture may deny this reality and promote a 
gentler picture, but we know it and feel it, in our battle scars.</p> 
</blockquote> 
<p>Without trying to paint a paranoid picture, this &quot;dynamic of war&quot; 
frequently infects software development teams and organizations; developers vs. 
management, developers vs. system adminstrators, developers vs. DBAs, even 
developers vs. architects or developers vs. developers. His book, then, 
suggests that we need to face this reality and learn how to deal with it:</p> 
<blockquote> 
<p>What we need are not impossible and inhuman ideals of peace and cooperation 
to live up to, and the confusion that brings us, but rather practical knowledge 
on how to deal with conflict and the daily battles we face. And this knowledge 
is not about how to be more forceful in getting what we want or defending 
ourselves but rather how to be more rational and strategic when it comes to 
conflict, channeling our aggressive impulses instead of denying or repressing 
them. If there is an ideal to aim for, it should be that of the strategic 
warrior, the man or woman who manages difficult situations and people through 
deft and intelligent maneuver.</p> </blockquote> 
<p>... and I want that man or woman heading up my project team.</p> 
<p>It may seem incongruous to draw parallels between war and software 
development, because in war there is an obvious &quot;enemy&quot;, an obvious 
target for our aggression and intentions and strategies and tactics. It turns 
out, however, that the &quot;enemy&quot; in software development is far more 
nebulous and amorphous, that of &quot;failure&quot;, which can be just as 
tenacious and subversive. This enemy won't ever try to storm your cubicles and 
kill you or try to hold you for ransom, but a lot of the strategies that Greene 
talks about aren't so much about how to kill people, but how to think 
strategically, which is, to my mind, something we all of us have to do more of.
</p> 
<p>Consider this, for example; Greene suggests &quot;six fundamental ideals 
you should aim for in transforming yourself into a strategic warrior in daily 
life&quot;:</p> 
<ul> 
<li><em>Look at things as they are, not as your emotions color them</em>. Too 
often, it's easy to &quot;lose your head&quot; and see the situation in 
emotional terms, rather than rational ones. &quot;Fear will make you 
overestimate the enemy and act too defensively&quot;; in other words, fear will 
cause you to act too conservatively and resist taking the necessary gamble on a 
technology or idea that will lead to success. &quot;Anger and impatience will 
draw you into rash actions that will cut off your options&quot;; or, anger and 
impatience will cause you to act rashly with respect to co-workers (such as 
DBAs and sysadmins) or technology decisions that may leave you with no clear 
path forward. &quot;The only remedy is to be aware that the pull of emotion is 
inevitable, to notice it when it is happening, and to compensate for it.&quot;
</li> 
<li><em>Judge people by their actions</em>. &quot;What people say about 
themselves [on resumes, in meetings, during conversations] does not matter; 
people will say anything. Look at what they have done; deeds do not lie.&quot; 
Which means, you have to have a way by which to measure those deeds, meaning 
you have to have a good &quot;feel&quot; for what's going on in your 
department--simply listening to reports in meetings is often not enough. 
&quot;In looking back at a defeat [failed project], you must identify the 
things you could have done differently. It is your own bad strategies, not the 
unfair opponent [or management decisions or unhelpful IT department, or 
whatever], that are to blame for your failures. You are responsible for the 
good and bad in your life.&quot;</li> 
<li><em>Depend on your own arms</em>. &quot;... people tend to rely on things 
that seem simple and easy or that have worked before. ... But true strategy is 
psychological--a matter of intelligence, not material force. ... But if your 
mind is armed with the art of war, there is no power that can take that away. 
In the middle of a crisis, your mind will find its way to the right solution. 
... As Sun-tzu says, 'Being unconquerable lies with yourself.' &quot;</li> 
<li><em>Worship Athena, not Ares</em>. This one probably doesn't translate 
directly; Athena was the goddess of war in its form seen in guile, wisdom, and 
cleverness, whereas Ares was the god of war in its direct and brutal form. 
Athena always fought with the utmost intelligence and subtlety; Ares fought for 
the sheer joy of blood. Probably the closest parallel here would be to suggest 
that we seek subtle solutions, not brute force ones, meaning look for answers 
that don't require hiring thousands of consultants and developers. But that's a 
stretch.</li> 
<li><em>Elevate yourself above the battlefield</em>. &quot;In war, strategy is 
the art of commanding the entire miliary operation. Tactics, on the other hand, 
is the skill of forming up the army for battle [project] itself and dealing 
with the immediate needs of the battlefield. Most of us in life are tacticians, 
not strategists.&quot; Too many project managers (and team members) never look 
beyond the immediate project in front of them to consider the wider 
implications of their actions. &quot;To have the power that only strategy can 
bring, you must be able to elevate yourself above the battlefield, to focus on 
your long-term objectives, to craft an entire campaign, to get out of the 
reactive mode that so many battles in life lock you into. Keeping your overall 
goals in mind, it becomes much easier to decide when to fight [or accept a job 
or accept a project] and when to walk away.&quot;</li> 
<li><em>Spiritualize your warfare</em>. &quot;... the greatest battle is with 
yourself--your weaknesses, your emotions, your lack of resolution in seeing 
things through to the end. You must declare unceasing war on yourself. As a 
warrior in life, you welcome combat and conflict as ways to prove yourself, to 
better your skills, to gain courage, confidence and experience.&quot; That 
means we should never let fear or doubt stop us from tackling a new challenge 
(but, similarly, we shouldn't risk others' welfare on wild risks). &quot;You 
want more challenges, and you invite more war [or projects].&quot;</li> </ul> 
<p>Granted, it's not a complete 1-to-1 match, but there's a lot that the 
average developer can learn from the likes of Sun-Tzu, MacArthur, Julies 
Caesar, Genghis Khan, Miyamoto Musashi, Erwin Rommel, or Carl von Clausewitz.
</p> 
<p>Just for reference purposes, the original 33 strategies (some of which may 
not be easy or even possible to adapt) are:</p> 
<ol> 
<li>Declare war on your enemies: The Polarity Strategy</li> 
<li>Do not fight the last war: The Guerilla-War-of-the-Mind Strategy</li> 
<li>Amidst the turmoil of events, do not lose your presence of mind: The 
Counterbalance Strategy</li> 
<li>Create a sense of urgency and desperation: The Death-Ground Strategy</li> 
<li>Avoid the snares of groupthink: The Command-and-Control Strategy</li> 
<li>Segment your forces: The Controlled-Chaos Strategy</li> 
<li>Transform your war into a crusade: Morale Strategies</li> 
<li>Pick your battles carefully: The Perfect-Economy Strategy</li> 
<li>Turn the Tables: The Counterattack Strategy</li> 
<li>Create a threatening presence: Deterrence Strategies</li> 
<li>Trade space for time: The Nonengagement Strategy</li> 
<li>Lose battles but win the war: Grand Strategy</li> 
<li>Know your enemy: The Intelligence Strategy</li> 
<li>Overwhelm resistance with speed and suddenness: The Blitzkrieg Strategy
</li> 
<li>Control the dynamic: Forcing Strategies</li> 
<li>Hit them where it hurts: The Center-of-Gravity Strategy</li> 
<li>Defeat them in detail: The Divide-and-Conquer Strategy</li> 
<li>Expose and attack your opponent's soft flank: The Turning Strategy</li> 
<li>Envelop the enemy: The Annihiliation Strategy</li> 
<li>Maneuver them into weakness: The Ripening-for-the-sickle Strategy</li> 
<li>Negotiate while advancing: The Diplomatic-War Strategy</li> 
<li>Know how to end things: The Exit Strategy</li> 
<li>Weave a seamless blend of fact and fiction: Misperception Strategies</li> 
<li>Take the line of least expectation: The Ordinary Extraordinary Strategy
</li> 
<li>Occupy the moral high ground: The Righteous Strategy</li> 
<li>Deny them targets: The Strategy of the Void</li> 
<li>Seem to work for the interests of others while furthering your own: The 
Alliance Strategy</li> 
<li>Give your rivals enough rope to hang themselves: The One-Upmanship Strategy
</li> 
<li>Take small bites: The Fait Accompli Strategy</li> 
<li>Penetrate their minds: Communication Strategies</li> 
<li>Destroy them from within: The Inner-Front Strategy</li> 
<li>Dominate while seeming to submit: The Passive-Aggression Strategy</li> 
<li>Sow uncertainty and panic through acts of terror: The Chain-Reaction 
Strategy</li> </ol> 
<p>What I'm planning to do, then, is go through the 33 strategies of war, 
analogize as necessary/possible, and publishthe results. Hopefully people find 
it useful, but even if you don't think it's going to help, it'll help me 
internalize the elements I want to through the process just for my own use. 
And, in the end, that's the point of &quot;spiritualize your warfare&quot;: 
trying to continuously enhance yourself.</p> 
<p>Naturally, I invite comment and debate; in fact, I'd really encourage it, 
because I'm not going to promise that these are 100%-polished ideas or 
concepts, at least as how they apply to software. So please, feel free to 
comment, either publicly on the blog or privately through email, whether you 
agree or not.&nbsp;(Particularly if you don't agree--the more the idea is 
tested, the better it stands, or the sooner it gets refactored.)</p> <br>

<p></p> Conferences&nbsp;|&nbsp; Development Processes&nbsp;|&nbsp; Reading 
<br> <br>
 Friday, 13 July 2007 22:41:02 (Pacific Daylight Time, UTC-07:00) <br>
Comments [8] &nbsp;|&nbsp; <br>
<br>
<br>
&nbsp;Sunday, 15 April 2007 <br>

Would you still love AJAX if you knew it was insecure? <br>

<p>From Bruce Schneier's latest Crypto-Gram:</p> 
<blockquote> 
<p>JavaScript Hijacking</p> 
<p>JavaScript hijacking is a new type of eavesdropping attack against 
Ajax-style Web applications.&nbsp; I'm pretty sure it's the first type of 
attack that specifically targets Ajax code.&nbsp; The attack is possible 
because Web browsers don't protect JavaScript the same way they protect HTML; 
if a Web application transfers confidential data using messages written in 
JavaScript, in some cases the messages can be read by an attacker.</p> 
<p>The authors show that many popular Ajax programming frameworks do nothing 
to prevent JavaScript hijacking.&nbsp; Some actually *require* a programmer to 
create a vulnerable server in order to function.</p> 
<p>Like so many of these sorts of vulnerabilities, preventing the class of 
attacks is easy.&nbsp; In many cases, it requires just a few additional lines 
of code.&nbsp; And like so many software security problems, programmers need to 
understand the security implications of their work so they can mitigate the 
risks they face.&nbsp; But my guess is that JavaScript hijacking won't be 
solved so easily, because programmers don't understand the security 
implications of their work and won't prevent the attacks.</p> 
<p>Paper:<br>

http://www.fortifysoftware.com/servlet/downloads/public/JavaScript_Hijacking.pdf
<br>or http://tinyurl.com/28nzje</p> 
<p>Responses to many of the blog comments, by one of the paper's co-authors:
<br>http://www.schneier.com/blog/archives/2007/04/javascript_hija_1.html#c160667
<br>or http://tinyurl.com/yqaoz5</p> </blockquote> 
<p>It would be an interesting comparison, to see a rich-client app using 
&quot;traditional&quot; calls back to a server (via RMI, .NET Remoting, or some 
kind of messaging system like JMS or MSMQ) weighed against an AJAX app, 
compared on security holes. My gut instinct tells me that the rich client app 
would be more secure, but only because using the binary RPC/messaging toolkit 
obfuscates the wire traffic enough to dissuade the 'casual' attacker, not 
because it's inherently more secure.</p> 
<p>By the way, if you're not receiving Crypto-Gram via email or RSS, you are 
seriously at risk&nbsp;of writing insecure apps. Think it's all dry and boring 
security threat alerts? Hardly--check out the &quot;Second Annual Move-Plot 
Threat Contest&quot;. Then tell me whether you think it's funny--or just 
sad--that there will not only be a real winner to this contest, but that the 
TSA will, in all likelihood, react the way Bruce predicts, particularly when 
the major news outlets report the story and it joins the list of fears the 
public already receives on a daily basis.</p> 
<p>More people die every day from automobile accidents than from terrorism. 
Hell, I'd even bet that on September 11, 2001, more people died from automobile 
accidents that day than from the Twin Towers attack. (I don't have the 
statistics to verify that, but I imagine it's fairly easy to find out; right or 
wrong, kudos to whomever takes the ten or fifteen minutes to research it and 
send it to me for posting here.)</p> 
<p>Ban the automobile! Protect your children from the evil terrorists at Ford, 
GM, Saturn, Toyota, DaimlerChryseler, and more! Send in the troops to arrest 
these fiendish perpetrators of unnecessary and senseless deaths to innocent 
American citizens! (And for God's sake, don't ask how many people die from 
peanut allergies each year, or we'll lose Skippy and Reese's Peanut Butter Cups 
too!)</p> <br>

<p></p> .NET&nbsp;|&nbsp; C++&nbsp;|&nbsp; Development Processes&nbsp;|&nbsp; 
Java/J2EE&nbsp;|&nbsp; Ruby&nbsp;|&nbsp; Windows&nbsp;|&nbsp; XML Services <br>

<br> Sunday, 15 April 2007 01:46:33 (Pacific Daylight Time, UTC-07:00) <br>

Comments [6] &nbsp;|&nbsp; <br>
<br>
<br>
&nbsp;Tuesday, 30 January 2007 <br>

Important/Not-so-important <br>

<p>Frank Kelly posted some good ideas on his entry, &quot;Java: Are we 
worrying about the wrong things?&quot;, but more interestingly, he suggested 
(implicitly) a new format for weighing in on trends and such, his 
&quot;Important/Not-so-important&quot; style. For example,</p> 
<blockquote> 
<p><strong>NOT SO IMPORTANT</strong>: Web 2.0<br>
<strong>IMPORTANT</strong>: 
Giving users a good, solid user experience. Web 2.0 doesn't make sites better 
by itself - it provides powerful technologies but it's no silver bullet. There 
are so many terrible web sites out there with issues such as<br>
- Too much 
content / too clutteredhttp://jdj.sys-con.com/<br>
- Too heavy for the many 
folks still on dial-up<br>
- Inconsistent labeling- etc. (See Jakob Nielsen's 
sitefor some great articles )<br>
Sometimes you have to wonder if some web site 
designers actually care about their intended audience?</p> </blockquote> 
<p>I love this format--it helps cut through the B/S and get to the point. 
Frank, I freely admit that I'm going to steal this idea from you, so I hope 
you're watching Trackbacks or blog links or whatever. :)</p> <br>

<p></p> .NET&nbsp;|&nbsp; C++&nbsp;|&nbsp; Conferences&nbsp;|&nbsp; 
Development Processes&nbsp;|&nbsp; Java/J2EE&nbsp;|&nbsp; Reading&nbsp;|&nbsp; 
Ruby&nbsp;|&nbsp; Windows&nbsp;|&nbsp; XML Services <br>
<br>
 Tuesday, 30 
January 2007 03:17:23 (Pacific Standard Time, UTC-08:00)<br>
Comments [1] 
&nbsp;|&nbsp;<br>
<br>
<br>
&nbsp;Friday, 26 January 2007 <br>
More on Ethics 
<br> 
<p>While traveling not too long ago, I saw a great piece on ethics, and wished 
I'd kept the silly magazine (I couldn't remember which one) because it was just 
a really good summation of how to live the ethical life. While wandering around 
the Web with Google tonight,I found it&nbsp;(scroll down a bit, to after the 
bits on Prohibition and Laughable Laws); in summary, the author advocates a 
life around five basic points:</p> 
<ol> 
<li>Do no harm</li> 
<li>Make things better</li> 
<li>Respect others</li> 
<li>Be fair</li> 
<li>Be loving</li> </ol> 
<p>Seems pretty simple, no? The problems occur, of course, in the 
interpretation and execution. For example, how exactly do we define 
&quot;better&quot;, when we seek to make things better? Had I the power, I 
would create a world where all people are free to practice whatever religious 
beliefs they hold, but clearly if those religious beliefs involve human 
sacrifice, then it's of dubious belief that my actions made the world 
&quot;better&quot;. (Of course, said practitioners would probably disagree.)</p>
<p>It's also pretty hard to actually follow through on these on a daily basis. 
The author, Bruce Weinstein, makes this pretty clear in this example:</p> 
<blockquote> 
<p>For example, how often do we really keep &ldquo;do no harm&rdquo; in mind 
during our daily interactions with people? If a clerk at the grocery store is 
nasty to us, don&rsquo;t we return the nastiness and tell ourselves, 
&ldquo;Serves them right?&rdquo;&nbsp; We may, but if we do, we harm the other 
person. In so doing, we harm our own soul&mdash;and this is one of the reasons 
why we shouldn&rsquo;t return nastiness with more of the same.</p> </blockquote>
<p>Ouch. Guilty as charged.</p> 
<p>There's a quiz attached to the article, and I highly suggest anyone who 
cares about their own ethical behavior take it; some of the questions are 
pretty clear-cut (at least to me), but some of them fall into that category of 
&quot;Well, I know what I *should* say I would do, but...&quot;, and some of 
them are just downright surprising.</p> 
<p>Personally, I think these five points are points that every developer 
should also advocate and life their life by, since, quite honestly, I think we 
as an industry do a pretty poor job on all five points. Clearly we violate #1 
when we're not careful with security measures in the code; too 
many&nbsp;programmers (and projects)&nbsp;fail to realize that 
&quot;better&quot; in #2 is from the customers' perspective, not our own; too 
many programmers look down on anyone who's not technical in some way, or even 
those who disagree with them, thus violating #3; too many consultants I've met 
(thankfully none I can call &quot;friends&quot;) will take any excuse to 
overbill a client (#4); and so on, and so on, and so on.</p> 
<p>Maybe I'm getting negative in my old age, but it just seems to me that 
there's too much shouting and posturing going on (*cough* Fleury *cough*) and 
not enough focus on the people to whom we are ultimately beholden: our 
customers. Do what's right for them, even if it's not the easy thing to do, 
even when they don't think they need it (such as the incapcitated friend in the 
quiz), and you can never go wrong.</p> <br>

<p></p> .NET&nbsp;|&nbsp; C++&nbsp;|&nbsp; Conferences&nbsp;|&nbsp; 
Development Processes&nbsp;|&nbsp; Java/J2EE&nbsp;|&nbsp; Reading&nbsp;|&nbsp; 
Ruby&nbsp;|&nbsp; Windows&nbsp;|&nbsp; XML Services <br>
<br>
 Friday, 26 
January 2007 17:34:23 (Pacific Standard Time, UTC-08:00)<br>
Comments [3] 
&nbsp;|&nbsp;<br>
<br>
<br>
Programming Promises (or, the Professional 
Programmer's Hippocratic Oath) <br>

<p>Michael.NET, apparently inspired by my &quot;Check Your Politics At The 
Door&quot; post, and equally peeved atanother post on blogs.msdn.com, hit a 
note of pure inspiration when he created his list of&quot;Programming 
Promises&quot;, which I repeat below:</p> 
<ul> 
<li>I promise to get the job done. </li> 
<li>I promise to use whatever tools I need to, regardless of politics. </li> 
<li>I promise to listen to the Closed Source and Open Source zealots equally, 
and then dismiss them.</li> 
<li>I promise to support, as long as I am able, any closed source applications 
I may release.</li> 
<li>I promise to release open source any applications I can not, or will not, 
support.</li> 
<li>I promise to learn as many languages and libraries as possible, regardless 
of politics.</li> 
<li>I promise to engage with as many other programmers as possible, both in 
person and online, in order to learn from them; regardless of politics.</li> 
<li>I promise to not bash Microsoft nor GNU, nor others like them, everyone 
has a place in our industry.</li> 
<li>I promise to use both Windows and Linux, both have their uses. </li> 
<li>I promise to ask questions when I don't know the answer, and answer 
questions when I do.</li> 
<li>I promise to learn from my mistakes, and to try to the first time. </li> 
<li>I promise to listen to any idea, however crazy it may sound.</li> </ul> 
<p>In many ways, this strikes me as fundamentally similar to the Hippocratic 
Oath that all doctors must take as part of their acceptance into the ranks of 
the medical profession. For most, this isn't just a bunch of words they recite 
as entry criteria, this is something they firmly believe and adhere to, almost 
religiously. It seems to me that our discipline could use something similar. 
Thus, do I swear by, and encourage others to similarly adopt, the Oath of the 
Conscientious Programmer:</p> 
<blockquote> 
<p>I swear to fulfill, to the best of my ability and judgment, this covenant: 
</p> 
<p>I will respect the hard-won scientific gains of those&nbsp;programmers and 
researchers&nbsp;in whose steps I walk, and gladly share such knowledge as is 
mine with those who are to follow. That includes respect for both those who 
prefer to keep their work to themselves, as well as those who seek improvement 
through the open community.</p> 
<p>I will apply, for the benefit of the customer, all measures [that] are 
required, avoiding those twin traps of&nbsp;gold-plating and&nbsp;computing 
nihilism.</p> 
<p>I will remember that there is humanity to&nbsp;programming as well as 
science, and that warmth, sympathy, and understanding&nbsp;will 
far&nbsp;outweigh the programmer's&nbsp;editor or the vendor's tool.</p> 
<p>I will not be ashamed to say &quot;I know not,&quot; nor will I fail to 
call in my colleagues when the skills of another are needed for a system's 
development, nor will I hold in lower estimation those colleagues who ask of my 
opinions or skills.</p> 
<p>I will respect the privacy of my customers, for their problems are not 
disclosed to me that the world may know. Most especially must I tread with care 
in matters of life and death, or of customers' perceptions of the same. If it 
is given me to save a project or a company, all thanks. But it may also be 
within my power to kill a project, for the company's greater good; this awesome 
responsibility must be faced with great humbleness and awareness of my own 
frailty. Above all, I must not play at God, and remain open to others' ideas or 
opinions.</p> 
<p>I will remember that I do not create a report, or a data entry screen, 
but&nbsp;tools for&nbsp;human beings, whose&nbsp;problems may affect the 
person's family and economic stability. My responsibility includes these 
related problems, if I am to care adequately for those who are technologically 
impaired.</p> 
<p>I will actively seek to avoid problems that are time-locked, for I know 
that software written today will still be running long after I was told it 
would be replaced.</p> 
<p>I will remember that I remain a member of society, both our own and of the 
one surrounding all of us, with special obligations to all my fellow human 
beings, those sound of mind and body as well as the clueless.</p> 
<p>If I do not violate this oath, may I enjoy life and art, respected while I 
live and remembered with affection thereafter. May I always act so as to 
preserve the finest traditions of my calling and may I long experience the joy 
of&nbsp;the thanks and praise from those who seek my help.</p> </blockquote> 
<p></p> 
<p>I, Ted Neward,&nbsp;so solemnly swear.</p> <br>

<p></p> .NET&nbsp;|&nbsp; C++&nbsp;|&nbsp; Conferences&nbsp;|&nbsp; 
Development Processes&nbsp;|&nbsp; Java/J2EE&nbsp;|&nbsp; Reading&nbsp;|&nbsp; 
Ruby&nbsp;|&nbsp; Windows&nbsp;|&nbsp; XML Services <br>
<br>
 Friday, 26 
January 2007 16:51:53 (Pacific Standard Time, UTC-08:00)<br>
Comments [2] 
&nbsp;|&nbsp;<br>
<br>
<br>
&nbsp;Monday, 15 January 2007 <br>
The Root of All 
Evil <br>

<p>At&nbsp;a No Fluff Just Stuff&nbsp;conference not that long ago, Brian 
Goetz and I were hosting a BOF on &quot;Java Internals&quot; (I think it was), 
and he tossed off a one-liner that just floored me; I forget the exact 
phrasology, but it went something like:</p> 
<blockquote> 
<p>Remember that part about premature optimization being the root of all evil? 
He was referring to programmer career lifecycle, not software development 
lifecycle.</p> </blockquote> 
<p>... and the more I thought about it, the more I think Brian was absolutely 
right. There are some projects, no matter how mature or immature, that I simply 
don't want any developer on the team to &quot;optimize&quot;, because I know 
what their optimizations will be like: trying to avoid method calls because 
&quot;they're expensive&quot;, trying to avoid allocating objects because 
&quot;it's more work for the GC&quot;, and completely ignoring network 
traversals because they just don't realize the cost of going across the wire 
(or else they think it really can't be all<em>that</em> bad). And then there 
are those programmers I've met who are &quot;optimizing&quot; from the very 
get-go, because they work to avoid network round-trips, or write SQL statements 
that don't need later optimization, simply because they got it right the first 
time (where &quot;right&quot; means &quot;correct&quot;<em>and</em> 
&quot;fast&quot;).</p> 
<p>It made me wish there was a &quot;Developer Skill&quot; setting I could 
throw on the compiler/IDE, something that would pick up the following 
keystrokes...</p> 
<blockquote> 
<p>for (int x = 10; x &gt; 0; x--)</p> </blockquote> 
<p>... and immediately pop Clippy up (yes, the annoying paperclip from Office) 
who then says, &quot;It looks like you're doing a decrementing loop count as a 
premature optimization--would you like me to help you out?&quot; and promptly 
rewrites the code as...</p> 
<blockquote> 
<p>// QUIT BEING STUPID, STUPID!</p> 
<p>for (int x = 0; x &lt; 10; x++)</p> </blockquote> 
<p>... because the JVM and CLR actually better understand and therefore JIT 
better code when your code is more clear than &quot;hand-optimized&quot;.</p> 
<p>And before any of those thirty-year crusty old curmudgeons start to stand 
up and shout &quot;See? I<em>told</em> you young whippersnappers to start 
listening to me, we should have wrote it all in COBOL and we would have<em>liked
</em> it!&quot;, let me be very quick to point out that years of experience in 
a developer are very subjective things--I've met developers with less than two 
years experience that I would qualify as &quot;senior&quot;, and I've met 
developers with more than thirty that I wouldn't feel safe to code &quot;Hello 
World&quot;.</p> 
<p>Which, naturally, then brings up the logical question, &quot;How do I know 
if I'm ready to start optimizing?&quot; For our answer, we turn to that ancient 
Master, Yoda:</p> 
<blockquote> 
<p>YODA: Yes, a Jedi's strength flows from the Force. But beware of the dark 
side. Anger, fear, aggression; the dark side of the Force are they. Easily they 
flow, quick to join you in a fight. If once you start down the dark path, 
forever will it dominate your destiny, consume you it will, as it did Obi-Wan's 
apprentice.<br>
LUKE: Vader... Is the dark side stronger? <br>
YODA: No, no, 
no. Quicker, easier, more seductive.<br>
LUKE: But how am I to know the good 
side from the bad?<br>
YODA: You will know... when you are calm, at peace, 
passive. A Jedi uses the Force for knowledge and defense, never for attack.</p> 
</blockquote> 
<p>What he refers to, of course, is that most ancient of all powers, the 
Source. When you feel calm, at peace, while you look through the Source, and 
aren't scrambling through it looking for a quick and easy answer to your 
performance problem, then you know you are channelling the Light Side of the 
Source. Remember, a Master uses the Source for knowledge and defense, never for 
a hack.</p> 
<p><em>(Few people realize that Yoda, in addition to being a great Jedi 
Master,&nbsp;was also a great Master of the Source. Go back and read your 
Empire Strikes Back if you don't believe me--most of his teaching to Luke 
applies to programming just as much as it does to righting evils in the galaxy.)
</em></p> 
<p>All humor bits aside, the time to learn about performance and JIT 
compilation is<em>not</em> the eleventh hour; spend some time cruisng the 
Hotspot FAQ and the various performance-tuning books, and most importantly, if 
you see a result that doesn't jibe with your experience, ask yourself 
&quot;why&quot;.</p> <br>

<p></p> .NET&nbsp;|&nbsp; C++&nbsp;|&nbsp; Conferences&nbsp;|&nbsp; 
Development Processes&nbsp;|&nbsp; Java/J2EE&nbsp;|&nbsp; Reading&nbsp;|&nbsp; 
Ruby&nbsp;|&nbsp; Windows&nbsp;|&nbsp; XML Services <br>
<br>
 Monday, 15 
January 2007 14:26:29 (Pacific Standard Time, UTC-08:00)<br>
Comments [6] 
&nbsp;|&nbsp;<br>
<br>
<br>
&nbsp;Friday, 05 January 2007 <br>
A Time for a 
Change <br>

<p>I've had The Blog Ride up for almost two years now, and it seems the latest 
fad to change your blog title to match whatever your particular focus is at the 
moment. Given my tech predictions for 2007, and how I believe that 
interoperability is going to become a Big Deal (well, I guess in one sense it 
was already, but now I think it's going to become a Bigger Deal), and that hey, 
this is my schtick anyway, I've decided to rename the blog from &quot;The Blog 
Ride&quot; (which was kinda a lame name to begin with) to ...</p> 
<p>Truth be told, I thought about squatting on Jason Whittington's old blog 
title (&quot;Managed Space&quot;), given that a lot of where my focus centers 
these days is around managed environments (Java and .NET, principally), but I 
didn't like that idea because (a) it was his idea first, and I don't like 
&quot;me-too&quot; kinds of faked creativity, and (b) I do a lot more than just 
managed code, so...</p> 
<p>Welcome to &quot;Interoperability Happens&quot;.</p> 
<p>One of the things I've set as a resolution for the new year is to post some 
concrete interoperability tips (very similar to the ones I'd been posting to 
TechTarget's &quot;tssblog&quot; site) ranging on all sorts of interop topics 
from XML services, to using the proprietary communication toolkits, to using 
IKVM, to some concrete examples (authenticating from Java against a Microsoft 
Active Directory or ADAM service, hosting Workflow inside of Spring, or writing 
Office Action Panes that talk to Java back-end servers, and so on) of 
interoperability &quot;in the field&quot;. I won't promise that I'll have a new 
one up every other week or so, but that's the goal. And the interop hopefully 
won't be limited to just Java and .NET; I plan to start exploring the Java/Ruby 
and .NET/Ruby interop space, as well as other pairings (Python, Tcl, maybe a 
few other languages or environments, like perhaps Parrot) that appeal to me. 
(That said, I've got&nbsp;a list of about 20 or 30 or so topics on just 
Java/.NET, so&nbsp;any delays or significant pauses aren't for lack of material 
or ideas.)</p> 
<p>And if there's any particular interoperability topic or question you've 
got, you know how to reach me.</p> 
<p>Catch ya around in 2007.</p> <br>

<p></p> .NET&nbsp;|&nbsp; C++&nbsp;|&nbsp; Development Processes&nbsp;|&nbsp; 
Java/J2EE&nbsp;|&nbsp; Ruby&nbsp;|&nbsp; Windows&nbsp;|&nbsp; XML Services <br>

<br> Friday, 05 January 2007 14:04:53 (Pacific Standard Time, UTC-08:00) <br>

Comments [0] &nbsp;|&nbsp; <br>
<br>
<br>
&nbsp;Friday, 24 March 2006 <br>
Why 
programmers shouldn't fear offshoring <br>

<p>Recently, while engaging in my other passion (international relations), I 
was reading the latest issue of Foreign Affairs, and ran across an interesting 
essay regarding the increasing outsourcing--or, the term they introduce which I 
prefer in this case, &quot;offshoring&quot;--of technical work, and I found 
some interesting analysis there that I think solidifies why I think programmers 
shouldn't fear offshoring, but instead embrace it and ride the wave to a better 
life for both us and consumers. Permit me to explain.</p> 
<p>The essay, entitled &quot;Offshoring: The Next Industrial Revolution?&quot; 
(by Alan S. Blinder), opens with an interesting point, made subtly, that 
offshoring (or &quot;offshore outsourcing&quot;), is really a natural economic 
consequence:</p> 
<blockquote> 
<p>In February 2004, when N. Gregory Mankiw, a Harvard professor then serving 
as chairman of the White House Council of Economic Advisers, caused a national 
uproar with a &quot;textbook&quot; statement about trade, economists rushed to 
his defense. Mankiw was commenting on the phenomenon that has been clumsily 
dubbed &quot;offshoring&quot; (or &quot;offshore outsourcing&quot;)--the 
migration of jobs, but not the people who perform them, from rich countries to 
poor ones. Offshoring, Mankiw said, is only &quot;the latest manifestation of 
the gains from trade that economists have talked about at least since Adam 
Smith. ... More things are tradable than were tradable in the past, and that's 
a good thing.&quot; Although Democratic and Republican politicians alike 
excoriated Mankiw for his callous attitude toward American jobs, economists 
lined up to support his claim that offshoring is simply international business 
as usual.</p> 
<p>Their economics were basically sound: the well-known principle of 
comparative advantage implies that trade in new kinds of products will bring 
overall improvements in productivity and well-being. But Mankiw and his 
defenders underestimated both the importance of offshoring and its disruptive 
effect on wealthy countries. Sometimes a quantitative change is so large that 
it brings qualitative changes, as offshoring likely will. We have so far barely 
seen the tip of the offshoring iceberg, the eventual dimensions of which may be 
staggering.</p> </blockquote> So far, you're not likely convinced that this is 
a good thing, and Blinder's article doesn't really offer much reassurance as 
you go on:
<blockquote> 
<p>To be sure, the furor over Mankiw's remark was grotesquely out of 
proportion to the current importance of offshoring, which is still largely a 
prospective phenomenon. Although there are no reliable national data, 
fragmentary studies indicate that well under a million service-sector jobs have 
been lost to offshoring to date. (A million seems impressive, but in the 
gigantic and rapidly churning U.S. labor market, a million jobs is less than 
two weeks' worth of normal gross job losses.)1 However, constant improvements 
in technology and global communications will bring much more offshoring of 
&quot;impersonal services&quot;--that is, services that can be delivered 
electronically over long distances, with little or no degradation in quality.
</p> 
<p>That said, we should not view the coming wave of offshoring as an impending 
catastrophe. Nor should we try to stop it. The normal gains from trade mean 
that the world as a whole cannot lose from increases in productivity, and the 
United States and other industrial countries have not only weathered but also 
benefited from comparable changes in the past. But in order to do so again, the 
governments and societies of the developed world must face up to the massive, 
complex, and multifaceted challenges that offshoring will bring. National data 
systems, trade policies, educational systems, social welfare programs, and 
politics must all adapt to new realities. Unfortunately, none of this is 
happening now.</p> </blockquote> Phrases like &quot;the world cannot lose from 
increases in productivity&quot; are hardly comforting to programmers who are 
concerned about their jobs, and hearing &quot;nor should we try to stop&quot; 
the impending wave of offshoring is not what most programmers want to hear. But 
there's an interesting analytical point that I think Blinder misses about the 
software industry, and in order to make the point I have to walk through his 
argument a bit to get to it. I'm not going to quote the entirety of the article 
to you, don't worry, but I do have to walk through a few points to get there. 
Bear with me, it's worth the ride, I think.
<p></p> 
<h3>Why Offshoring</h3> 
<p>Blinder first describes the basics of &quot;comparative advantage&quot; and 
why it's important in this context:</p> 
<blockquote> 
<p>Countries trade with one another for the same reasons that individuals, 
businesses and regions do: to exploit their comparative advantages. Some 
advantages are &quot;natural&quot;: Texas and Saudi Arabia sit atop massive 
deposits of oil that are entirely lacking in New York and Japan, and nature has 
conspired to make Hawaii a more attractive tourist destination than Greenland. 
Ther eis not much anyone can do about such natural advantages.</p> 
<p>But in modern economics, nature's whimsy is far less important than it was 
in the past. Today, much comparative advantage derives from human effort rather 
than natural conditions. The concentration of computer companies around Silicon 
Valley, for example, has nothing to do with bountiful natural deposits of 
silicon; it has to do with Xerox's fabled Palo Alto Research Center, the 
proximity of Stanford University, and the arrival of two young men named 
Hewlett and Packard. Silicon Valley could have sprouted up anywhere.</p> 
<p>One important aspect of this modern reality is that patterns of man-made 
comparative advantage can and do change over time. The economist Jagdish 
Bhagwait has labeled this phenomenon &quot;kaleidoscopic comparative 
advantage&quot;, and it is critical to understanding offshoring. Once upon a 
time, the United Kingdom had a comparative advantage in textile manufacturing. 
Then that advantage shifted to New England, and so jobs were moved from the 
United Kingdom to the United States.2 Then the comparative advantage in textile 
manufacturing shifted once again--this time to the Carolinas--and jobs migrated 
south within the United States.3 Now the comparative advantage in textile 
manufacturing resides in China and other low-wage countries, and what many are 
wont to call &quot;American jobs&quot; have been moved there as a result.</p> 
<p>Of course, not everything can be traded across long distances. At any point 
in time, the available technology--especially in transportation and 
communications4--largely determines what can be traded internationally and what 
cannot. Economic theorists accordingly divide the world's goods and services 
into two bins: tradable and non-tradable. Traditionally, any item that could be 
put in a box and shipped (roughly, manufactured goods) was considered tradable, 
and anything that could not be put into a box (such as services) or was too 
heavy to ship (such as houses) was thought of as nontradable. But because 
technology is always improving and transportation is becoming cheaper and 
easier, the boundary between what is tradable and what is not is constantly 
shifting. And unlike comparative advantage, this change is not kaleidoscopic; 
it moves in only one direction, with more and more items becoming tradable.</p> 
<p>The old assumption that if you cannot put it in a box, you cannot trade it 
is thus hopelessly obsolete. Because packets of digitized information play the 
role that boxes used to play, many more services are now tradable and many more 
will surely become so. In the future, and to a great extent already, the key 
distinction will no longer be between things that can be put in a box and 
things that cannot. Rather, it will be between services that can be delivered 
electronically and those that cannot.</p> </blockquote> Blinder goes on to 
describe the three industrial revolutions, the first being the one we all 
learned in school, coming at the end of the 18th century and marked by Adam 
Smith's<i>The Wealth of Nations</i> in 1776. It was a massive shift in the 
economic system, as workers in industrializing countries migrated from farm to 
factory. &quot;It has been estimated that in 1810, 84 percent of the U.S. work 
force was engaged in agriculture, compared to a paltry 3 percent in 
manufacturing. By 1960, manufacturing's share had rised to almost 25 percent 
and agriculture's had dwindled to just 8 percent. (Today, agriculture's share 
is under 2 percent.)&quot; (This statistic is important, by the way--keep it in 
mind as we go.) He goes on to point out the second Revolution, the shift from 
manufacturing to services:
<blockquote> 
<p>Then came the second Industrial Revolution, and jobs shifted once 
again--this time away from manufacturing and toward services. The shift to 
services is still viewed with alarm in the United States and other rich 
countries, where people bemoan rather than welcome the resulting loss of 
manufacturing jobs5. But in reality, new service-sector jobs have been created 
far more rapidly than old manufacturing jobs have disappeared. In 1960, about 
35 percent of nonagricultural workers in the United States produced goods and 
65 percent produced services. By 2004, only about one-sixth of the United 
States' nonagricultural jobs were in goods-producing industries, while 
five-sixths produced services. This trend is worldwide and continuing.</p> 
</blockquote> It's also important to point out that the years from 1960 to 2004 
saw a meteoric rise in the average standard of living for the United States, on 
a scale that's basically unheard of in history. In fact, it was SUCH a huge 
rise that it became an expectation that your children would live better than 
you did, and the inability to keep that basic expectation in place (which has 
become a core part of the so-called &quot;American Dream&quot;) that creates 
major societal angst on the part of the United States today.
<blockquote> 
<p>We are now i nthe arly stages of a third Industrial Revolution--the 
information age. The cheap and easy flow of information around the globe has 
vastly expanded the scope of tradable services, and there is much more to come. 
Industrial revolutions are big deals. And just like the previous two, the third 
Industrial Revolution will require vast and usettling adjustments in the way 
Americans and residents of other developed countries work, live, and educate 
their children.</p> </blockquote> Wow, nothing unsettles people more than 
statements like &quot;the world you know will cease to exist&quot; and the 
like. But think about this for a second: despite the basic &quot;growing 
pains&quot; that accompanied the transitions themselves, on just about every 
quantifiable scale imaginable, we live a much better life today than our 
forebears did just two hundred years ago, and orders of magnitude better than 
our forebears did three hundred or more years ago (before the first Industrial 
Revolution). And if you still hearken back to the days of the &quot;American 
farmer&quot; with some kind of nostalgia, you never worked on a farm. Trust me 
on this.
<p></p> 
<h3>So what does this mean?</h3> 
<p>But now we start to come to the interesting part of the article. </p> 
<blockquote> But a bit of historical perspective should help temper fears of 
offshoring. The first Industrial Revolution did not spell the end of 
agriculture, or even the end of food production, in the United States. It jus 
tmean that a much smaller percentage of Americans had to work on farms to feed 
the population. (By charming historical coincidence, the actual number of 
Americans working on farms today--around 2 million--is about what it was in 
1810.) The main reason for this shift was not foreign trade, but soaring farm 
productivity. And most important, the massive movement of labor off the farms 
did not result in mass unemployment. Rather, it led to a large-scale 
reallocation of labor to factories.</blockquote> Here's where we get to the 
&quot;hole&quot; in the argument. Most readers will read that paragraph, do the 
simple per-capita math, and conclude that thanks to soaring productivity gains 
in the programming industry (cite whatever technology you want here--Ruby, 
objects, hardware gains, it really doesn't matter what), the percentage of 
programmers in the country is about to fall into a black hole. After all, if we 
can go from 84 percent of the population involved in agriculture to less than 
2% or so, thanks to that soaring productivity, why wouldn't it happen here 
again?
<p></p> 
<p>Therein lies the flaw in the argument: <i>the amount of productivity 
required to achieve the desired ends is constant in the agriculture industry, 
yet a constantly-changing dynamic value in software</i>. This is also known as 
what I will posit as the Groves-Gates Maxim: &quot;What Andy Groves giveth, 
Bill Gates taketh away.&quot;</p> 
<h3>The Groves-Gates Maxim</h3> 
<p>The argument here is simple: the process of growing food is a pretty 
constant one: put seed in ground, wait until it comes up, then harvest the 
results and wait until next year to start again. Although we might have 
numerous tools that can help make it easier to put seeds into the ground, or 
harvesting the results, or even helping to increase the yield of the crop when 
it comes up, the basic amount of productivity required is pretty much constant. 
(My cousin, the FFA Farmer of the Year from some years back and a seed hybrid 
researcher in Iowa might disagree with me, mind you.) Compare this with the 
software industry: the basic differences between what's an acceptable 
application to our users today, compared to even ten years ago, is an order of 
magnitude different. Gains in productivity have not yielded the ability to 
build applications faster and faster, but instead have created a situation 
where users and managers ask more of us with each successive application.</p> 
<p>The Groves-Gates Maxim is an example of that: every time Intel (where Andy 
Groves is CEO) releases new hardware that accelerates the power and potential 
of what the &quot;average&quot; computer (meaning, priced at somewhere between 
$1500-$2000) is capable of, it seems that Microsoft (Mr. Gates' little firm) 
releases a new version of Windows that sucks up that power by providing a 
spiffier user interface and &quot;eye-candy&quot; features, be they 
useful/important or not. In other words, the more the hardware creates 
possibilities, the more software is created to exploit and explore those 
possibilities. The additional productivity is spent not in reducing the time 
required to produce the thing desired (food in the case of agriculture, an 
operating system or other non-trivial program in the case of software), but in 
the expansion of the functionality of the product.</p> 
<p>This basic fact, the Groves-Gates Maxim, is what saves us from the bloody 
axe of forced migration. Because what's expected of software is constantly on 
the same meteoric rise as what productivity gains provide us, the need for 
programmer time remains pretty close to constant. Now, once the desire for 
exponentially complicated features starts to level off, the exponentially 
increasing gains in productivity will have the same effect as they did in the 
agricultural industry, and we will start seeing a migration of programmers into 
other, &quot;personal service&quot; industries (which are hard to offshore, as 
opposed to &quot;impersonal service&quot; industries which can be easily 
shipped overseas).</p> 
<h3>Implications</h3> 
<p>What does this mean for programmers? For starters, as Dave Thomas has 
already frequently pointed out on NFJS panels, programmers need to start 
finding ways to make their service a &quot;personal service&quot; position 
rather than an &quot;impersonal service&quot; one. Blinder points out that the 
services industry is facing a split down the middle along this distinction, and 
it's not necessarily a high-paying vs low-paying divide:</p> 
<blockquote> 
<p>Many people blithely assume that the critical labor-market distinction is, 
and will remain, between highly educated (or highly-skilled) people and 
less-educated (or less-skilled) people--doctors versus call-center operators, 
for example. The supposed remedy for the rich countries, accordingly, is more 
education and a general &quot;upskilling&quot; of the work force. But this view 
may be mistaken. Other things being equal, education and skills are, of course, 
good things; education yields higher returns in advanced societies, and more 
schooling probably makes workers more flexible and more adaptable to change. 
But the problem with relying on education as the remedy for potential job 
losses is that &quot;other things&quot; are not remotely close to equal. The 
critical divide in the future may instead be between those types are work that 
are easily deliverable through a wire (or via wireless connections) with little 
or no diminution in quality and those that are not. And this unconventional 
divide does not correspond well to traditional distinctions between jobs that 
require high levels of education and jobs that do not.</p> 
<p>A few disparate examples will illustrate just how complex--or, rather, how 
untraditional--the new divide is. It is unlikely that the services of either 
taxi drivers or airline pilots will ever be delivered electronically over long 
distances. The first is a &quot;bad job&quot; with negligible educational 
requirements; the second is quite the reverse. On the other hand, typing 
services (a low-skill job) and security analysis (a high-skill job) are already 
being delivered electronically from India--albeit on a small scale so far. Most 
physicians need not fear that their jobs will be moved offshore, but 
radiologists are beginning to see this happening already. Police officers will 
not be replaced by electronic monitoring, but some security guards will be. 
Janitors and crane operators are probably immune to foreign competition; 
accountants and computer programmers are not. In short, the dividing line 
between the jobs that produce services that are suitable for electronic 
delivery (and are thus threatened by offshoring) and those that do not does not 
correspond to traditional distinctions between high-end and low-end work.</p> 
</blockquote> What's the implications here for somebody deep in our industry? 
Pay close attention to Blinder's conclusion, that computer programmers are 
highly vulnerable to foreign competition, based on the assumption that the 
product we deliver is easily transferable across electronic media. But there is 
hope:
<blockquote> 
<p>There is currently not even a vocabulary, much less any systematic data, to 
help society come to grips with the coming labor-market reality. So here is 
some suggested nomenclature. Service that cannot be delivered electronically, 
or that are notably inferior when so delivered, have one essential 
characteristic: personal, face-to-face contact is either imperative or highly 
desirable. Think of hte waiter who serves you dinner, the doctor you gives you 
your annual physical, or the cop on the beat. Now think of any of those tasks 
being performed by robots controlled from India--not quite the same. But such 
face-to-face human contact is not necessary in the relationship you have with 
the telephone operator who arranges your conference call or the clerk who takes 
your airline reservation over the phone. He or she may be in India already.</p> 
<p>The first group of tasks can be called personally-delivered services, or 
simply personal services, and the second group of impersonally delivered 
services, or impersonal services. In the brave new world of globalized 
electronic commerce, impersonal services have more in common with manufactured 
goods that can be put in boxes than they do with personal services. Thus, many 
impersonal services are destined to become tradable and therefore vulnerable to 
offshoring. By contrast, most personal services have attributes that cannot be 
transmitted through a wire. Some require face-to-face contact (child care), 
some are inherently &quot;high-risk&quot; (nursing), some involve high levels 
of personal trust (psychotherapy), and some depend on location-specific 
attributes (lobbying).</p> </blockquote> In other words, programmers that want 
to remain less vulnerable to foreign competition need to find ways to stress 
the personal, face-to-face contact between themselves and their clients, 
regardless of whether you are a full-time employee of a company, a contractor, 
or a consultant (or part of a team of consultants) working on a project for a 
client. Look for ways to maximize the four cardinalities he points out:
<ul> 
<li><b>Face-to-face contact.</b> Agile methodologies demand that customers be 
easily accessible in order to answer questions regarding implementation 
decisions or to resolve lack of understanding of the requirements. Instead of 
demanding customers be present at your site, you may find yourself in a better 
position if you put yourself close to your customers.</li> 
<li><b>&quot;High-risk&quot;.</b> This is a bit harder to do with software 
projects--either the project is inherently high-risk in its makeup (perhaps 
this is a mission-critical app that the company depends on, such as the 
e-commerce portal for an online e-tailer), or it's not. There's not much you 
can do to change this, unless you are politically savvy enough to 
&quot;sell&quot; your project to a group that would make it mission-critical.
</li> 
<li><b>High levels of personal trust.</b> This is actually easier than you 
might think--trust in this case refers not to the privileged nature of 
therapist-patient communication, but in the credibility the organization has in 
you to carry out the work required. One way to build this trust is to 
understand the business domain of the client, rather than remaining aloof and 
&quot;staying focused on the technology&quot;. This trust-based approach is 
already present in a variety of forms outside our industry--regardless of the 
statistical ratings that might be available, most people find that they have a 
favorite auto repair mechanic or shop not for any quantitatively-measurable 
reason, but beceause the mechanic &quot;understands&quot; them somehow. The 
best customer-service shops understand this, and have done so for years. The 
restaurant that recognizes me as a regular after just a few visits and has my 
Diet Coke ready for me at my favorite table is far likelier to get my business 
on a regular basis than the one that never does. Learn your customers, learn 
their concerns, learn their business model and business plan, and get yourself 
into the habit of trying to predict what they might need next--not so you can 
build it already, but so that you can demonstrate to them that you understand<i>
them</i>, and by extension, their needs.</li> 
<li><b>Location-specific attributes.</b> Sometimes, the software being built 
is localized to a particular geographic area, and simply being in that same 
area can yield significant benefits, particularly when heroic efforts are 
called for. (It's very hard to flip the reset switch on a server in Indiana 
from a console in India, for example.)</li> </ul> In general, what you're 
looking to do is demonstrate how your value to the company arises out of more 
than just your technical skill, but also some other qualities that you can 
provide in better and more valuable form than somebody in India (or China, or 
Brazil, or across the country for that matter, wherever the offshoring goes). 
It's not a guarantee that you might still be offshored--some management folks 
will just see bottom-line dollars and not recognize the intangible value-add 
that high levels of personal trust or locality provides--but it'll minimize it 
on the whole.
<p></p> 
<p>But even if this analysis doesn't make you feel a little more comfortable, 
consider this: there are 1 billion people in China alone, and close to the same 
in India. Instead of seeing them as potential competition, imagine what happens 
when the wages from the offshored jobs start to create a demand for goods and 
services in those countries--if you think the software market in the U.S. was 
hot a decade ago, where only a half-billion (across both the U.S. and Europe) 
people were demanding software, now think about it when four<i>times</i> that 
many start looking for it.</p> 
<h3>Footnotes</h3> 
<p>1 Which in of itself is an interesting statistic--it implies that 
offshoring is far less prevalent than some of people worried about it believe 
it to be, including me.</p> 
<p>2 Interesting bit of trivia--part of the reason that advantage shifted was 
because the US stole (yes, stole, as in industrial espionage, one of the first 
recorded cases of modern industrial espionage) the plans for modern textile 
machinery from the UK. Remember that, next time you get upset at China's rather 
loose grip of intellectual property law....</p> 
<p>3 Which, by the way, was a large part of the reason we fought the Civil War 
(the &quot;War Between the States&quot; to some, or the &quot;War of Northern 
Aggression&quot; to others)--the Carolinas depended on slave labor to pick 
their cotton cheaply, and couldn't acquire Northern-made machinery cheaply to 
replace the slaves. Hence, for that (and a lot of other reasons), war followed.
</p> 
<p>4 An interesting argument--is there any real difference between 
transportation and communications? One ships &quot;stuff&quot;, the other 
&quot;data&quot;, beyond that, is there any difference?</p> 
<p>5 And, I'd like to point out, the shrinking environmental damage that can 
arise from a manufacturing-based economy. Services rarely generate pollution, 
which is part of the clash between the industrialized &quot;Western&quot; 
nations and the developing &quot;Southern&quot; ones over environmental issues.
</p> 
<h3>Resources</h3> 
<p><i>&quot;Offshoring: The Next Industrial Revolutoin?&quot;</i>, by Alan S. 
Blinder, Foreign Affairs (March/April 2006), pp 113 - 128.</p> <br>

<p></p> .NET&nbsp;|&nbsp; C++&nbsp;|&nbsp; Development Processes&nbsp;|&nbsp; 
Java/J2EE&nbsp;|&nbsp; Reading&nbsp;|&nbsp; Ruby&nbsp;|&nbsp; XML Services <br>

<br> Friday, 24 March 2006 02:43:00 (Pacific Daylight Time, UTC-07:00) <br>

Comments [6] &nbsp;|&nbsp; <br>
<br>
<br>
&nbsp;Sunday, 05 March 2006 <br>

Check it out... <br>

<p>The new home page is alive and kicking...</p> <br>

<p></p> .NET&nbsp;|&nbsp; C++&nbsp;|&nbsp; Conferences&nbsp;|&nbsp; 
Development Processes&nbsp;|&nbsp; Java/J2EE&nbsp;|&nbsp; Ruby&nbsp;|&nbsp; XML 
Services <br>
<br>
 Sunday, 05 March 2006 07:35:08 (Pacific Standard Time, 
UTC-08:00)<br>
Comments [1] &nbsp;|&nbsp; <br>
<br>
<br>
&nbsp;Friday, 03 March 
2006<br>
Don't fall prey to the latest social engineering attack <br>

<p>My father, whom I've often used (somewhat disparagingly...) as an example 
of the classic &quot;power user&quot;, meaning 
&quot;he-thinks-he-knows-what-he's-doing-but-usually-ends-up-needing-me-to-fix-his-computer-afterwards&quot; 
(sorry Dad, but it's true...), often forwards me emails that turn out to be one 
hoax or another. This time, though, he found a winner--he sent methis article, 
warning against the latest caller identity scam: this time, they call claiming 
to be clerks of the local court, threatening that because the victim hasn't 
reported in for jury duty, arrest warrants have been issued. When the victim 
protests, the &quot;clerk&quot; asks for confidential info to verify the 
records. Highly credible attack, if you ask me.</p> 
<p>Net result (from the article): </p> 
<ul> 
<li>Court workers will not telephone to say you've missed jury duty or that 
they are assembling juries and need to pre-screen those who might be selected 
to serve on them, so dismiss as fraudulent phones call of this nature. About 
the only time you would hear by telephone (rather than by mail) about anything 
having to do with jury service would beafter you have mailed back your 
completed questionnaire, and even then only rarely.</li> 
<li>Do not give out bank account, social security, or credit card numbers over 
the phone if you didn't initiate the call, whether it be to someone trying to 
sell you something or to someone who claims to be from a bank or government 
department. If such callers insist upon &quot;verifying&quot; such information 
with you, have them read the data to you from their notes, with you saying yea 
or nay to it rather than the other way around.</li> 
<li>Examine your credit card and bank account statements every month, keeping 
an eye peeled for unauthorized charges. Immediately challenge items you did not 
approve.</li> </ul> In other words, don't assume the voice on the other end of 
the phone is actually who they say they are. I think it's fairly reasonable to 
ask to speak to a supervisor or ask for a phone # to call back on after you've 
&quot;assembled the appropriate records&quot; and what-not. Who knows? Some 
scammers might even be dumb enough to give you the phone # back, and then it's 
&quot;Hello, Police...?&quot;, baby....
<p></p> 
<p>Remember, it's always acceptable to ask for verification of THEIR identity 
if they're asking for confidential information. And most credible organizations 
are taking great pains to not ask for that information over the phone in the 
first place. Practice the same discretion over the phone that you would over IM 
or email; the phone can be just as anonymous as the Internet can.</p> <br>

<p></p> .NET&nbsp;|&nbsp; C++&nbsp;|&nbsp; Conferences&nbsp;|&nbsp; 
Development Processes&nbsp;|&nbsp; Java/J2EE&nbsp;|&nbsp; Reading&nbsp;|&nbsp; 
Ruby&nbsp;|&nbsp; XML Services <br>
<br>
 Friday, 03 March 2006 22:00:57 
(Pacific Standard Time, UTC-08:00)<br>
Comments [2] &nbsp;|&nbsp; <br>
<br>
<br>
&nbsp;Sunday, 01 January 2006<br>
2006 Tech Predictions <br>

<p>In keeping with the tradition, I'm suggesting the following will take place 
for 2006:</p> 
<ol> 
<li>The hype surrounding Ajax will slowly fade, as people come to realize that 
there's really nothing new here, just that DHTML is cool again. AsDion points 
out, Ajax will become a toolbox that you use in web development without 
thinking that &quot;I am doing Ajax&quot;. Just as we don't think about 
&quot;doing HTML&quot; vs &quot;doing DOM&quot;.</li> 
<li>The release of EJB 3 may actually start people thinking about EJB again, 
but hopefully this time in a more pragmatic and less hype-driven fashion. (Yes, 
EJB does have its place in the world, folks--it's just a much smaller place 
than most of the EJB vendors and book authors wanted it to be.)</li> 
<li>Vista will be slipped to 2007, despite Microsoft's best efforts. In the 
meantime, however, WinFX (which is effectively .NET 3.0) will ship, and people 
will discover that Workflow (WWF) is by far the more interesting of the 
WPF/WCF/WWF triplet. Notice that I don't say &quot;powerful&quot; or 
&quot;important&quot;, but &quot;interesting&quot;.</li> 
<li>Scripting languages will hit their peak interest period in 2006; Ruby 
conversions will be at its apogee, and its likely that somewhere in the latter 
half of 2006 we'll hear about the first major Ruby project failure, most likely 
from a large consulting firm that tries to duplicate the success of Ruby's 
evangelists (Dave Thomas, David Geary, and the other Rubyists I know of from 
the NFJS tour) by throwing Ruby at a project without really understanding it. 
In other words, same story, different technology, same result. By 2007 the Ruby 
Backlash will have begun.</li> 
<li>Interest in building languages that somehow bridge the gap between static 
and dynamic languages will start to grow, most likely beginning with E4X, the 
variant of ECMAScript (Javascript to those of you unfamiliar with the 
standards) that integrates XML into the language.</li> 
<li>Java developers will start gaining interest in building rich Java apps 
again. (Freely admit, this is a long shot, but the work being done by the Swing 
researchers at Sun, not least of which is Romain Guy, will by the middle of 
2006 probably be ready for prime-time consumption, and there's some seriously 
interesting sh*t in there.)</li> 
<li>Somebody at Microsoft starts seriously hammering on the CLR team to 
support continuations. Talk emerges about supporting it in the 4.0 (post-WinFX) 
release.</li> 
<li><i>Effective Java</i> (2nd Edition) will ship. (Hardly a difficult 
prediction to make--Josh said as much in the Javapolis interview I did with him 
and Neal Gafter.)</li> 
<li><i>Effective .NET</i> will ship.</li> 
<li><i>Pragmatic XML Services</i> will ship.</li> 
<li>JDK 6 will ship, and a good chunk of the Java community self-proclaimed 
experts and cognoscente will claim it sucks.</li> 
<li>Java developers will seriously begin to talk about what changes we 
want/need to Java for JDK 7 (&quot;Dolphin&quot;). Lots of ideas will be put 
forth. Hopefully most will be shot down. With any luck, Joshua Bloch and Neal 
Gafter will still be involved in the process, and will keep tight rein on the 
more... aggressive... ideas and turn them into useful things that won't break 
the spirit of the platform.</li> 
<li>My long-shot hope, rather than prediction, for 2006: Sun comes to realize 
that the Java platform isn't about the<i>language</i>, but the <i>platform</i>, 
and begin to give serious credence and hope behind a multi-linguistic JVM 
ecosystem.</li> 
<li>My long-shot dream: JBoss goes out of business, the JBoss source code goes 
back to being maintained by developers whose principal interest is in 
maintaining open-source projects rather than making money, and it all gets 
folded together with what the Geronimo folks are doing. In other words, the 
open-source community stops the infighting and starts pulling oars in the same 
direction at the same time. For once.</li> </ol> Flame away....
<p></p> <br>

<p></p> .NET&nbsp;|&nbsp; C++&nbsp;|&nbsp; Conferences&nbsp;|&nbsp; 
Development Processes&nbsp;|&nbsp; Java/J2EE&nbsp;|&nbsp; Reading&nbsp;|&nbsp; 
Ruby&nbsp;|&nbsp; XML Services <br>
<br>
 Sunday, 01 January 2006 00:25:56 
(Pacific Standard Time, UTC-08:00)<br>
Comments [97] &nbsp;|&nbsp; <br>
<br>

<br> &nbsp;Sunday, 30 October 2005 <br>
Porting legacy code <br>

<p>Matt Davey poses an interesting question: </p> 
<blockquote> The problem: <br>

<ul> 
<li>C++ Corba legacy codebase (5+ years old, 1 million lines) </li> 
<li>No unit tests </li> 
<li>Little test data </li> 
<li>Limited knowledge transfer from the original development team. </li> 
<li>A flake environment to run the application in.</li> </ul> The requirement: 
<br> 
<ul> 
<li>Port the C++ result accumulation and session management code to Java</li> 
</ul> Do you: <br>

<ol> 
<li>Write C+ unit tests to understand the current system, then write Java 
equivalent code using TDD</li> 
<li>Write Java tests using TDD based on your understanding of the C++ code 
</li> 
<li>Hope you understand the C++ code, and JFDI in Java </li> 
<li>Give up and go home </li> 
<li>Get the original development team to do the work</li> </ol> </blockquote> 
Ah, I love the smell of legacy code in the morning.
<p></p> 
<p>My answer: depends. (Typical.) Here's what I mean: </p> 
<ul> 
<li>Option 1 is clearly the &quot;best&quot; answer if the goal is to produce 
code that will most accurately match what the current C++ code is doing, but 
also represents the greatest time and energy commitment, as well as making the 
fundamental assumption that what the C++ code does today is correct in the 
first place.</li> 
<li>Option 2 is the approach to take if the time crunch is a bit tighter 
and/or if the C++ unit tests can't be sold to management (&quot;You're just 
going to throw them away anyway!&quot;), particularly if the team working on 
the port has many or all of the original C++ devs. It also allows for the 
inevitable &quot;You know, we always wanted to change how that code worked, so 
why don't we....&quot; requirements changes.</li> 
<li>Option 3 is probably appropriate in those shops where WHISKEY (Why the 
Hell Isn't Somebody Koding Everything Yet) is considered an acceptable 
development methodology, but the lack of unit tests for the Java port will 
catch up to you someday (as it always does).</li> 
<li>Option 4 is probably best if the company you work for is seriously 
considering Option 3.</li> 
<li>Option 5 is only viable if the original development team is available (not 
going to happen if you outsourced it, by the way), able to work on it (meaning 
they've flipped the switch to Java at<i>both</i> a syntactic and semantic 
level), and isn't otherwise engaged on another project (which is probably the 
dealbreaker).</li> </ul> Matt also left out a few options: 
<ul> 
<li>6. Let management believe in the whizzy-bang code conversion wizard that 
such-and-such company is trying to sell them on that &quot;guarantees&quot; 99% 
code translation and compatibility</li> 
<li>7. Let management outsource the port, and let <i>them</i> worry about it
</li> 
<li>8. Give it all up and start from scratch--who needs that system anyway? 
It's not like anybody ever really<i>used</i> it, right?</li> </ul> 
<p></p> 
<p>Porting legacy code is one of the least-favorite projects of any software 
developer, but what few developers seem to realize is that they're also the 
least-favorite of management, too: it's a project that has no discernible ROI 
beyond that of &quot;getting us out of the Stone Age&quot;. You might argue 
that the code becomes more maintainable if it's written in 
whatever-the-latest-technology-flavor-is-today, but the truth of the matter is, 
today's hot language is tomorrow's legacy language, subject to being rewritten 
in tommorrow's hot language. (Any programmer who's been writing code for more 
than five years probably already knows this, and any programmer who's been 
writing code for more than 10 years almost certainly knows this.)</p> 
<p>Companies have been on this hamster wheel for far too long. Having gone 
through several transitions, particularly the C++-to-COM/CORBA-to-Java/EJB 
transitions over the last decade--and they're starting to resist if not 
outright reject the idea. Instead, they're preferring to find ways to create 
interoperable solutions rather than ported solutions--hence the huge interest 
in Web services when they first came out (and the interest in CORBA when it 
first came out, and the interest in middleware products in general like Tuxedo 
when they first came out, and so on). Integration still remains the &quot;hard 
problem&quot; of our industry, one that none of the new languages or platforms 
seem to want to address until they have to. Witness, for example, Sun's 
reluctance to really adopt any sort of external-facing technology into Java 
until they had to (meaning the Java Connector Architecture; their adoption of 
CORBA was half-hearted at best and a PR move at worst). .NET suffers the same 
problem, though fortunately Microsoft was wise enogh to realize that shipping 
.NET without a good Win32/COM interop story was going to kill it before it left 
the gate. C++ at least had the advantage of being call-compatible with C (if 
you declared the prototypes correctly), and so could automatically interop 
against the operating system's libraries pretty easily. In fact, it could be 
argued that C has long been the de-facto call-level compatibility 
interoperability standard (Python has C bindings, Ruby has C bindings, Java 
reluctantly, it seems, support C bindings through JNI, and so on), but of 
course that only works to a given platform/OS, since C offers so little by way 
of standardization and the operating systems have never been able to create a 
portable OS layer beyond the simple stuff; POSIX was arguably the closest they 
came, and many's the POSIX programmer who will tell you just how successful 
THAT was.</p> 
<p>My point? I hereby declare a rule that any new language developed should 
think<i>first</i> about its interoperability bindings, and developers 
contemplating the adoption of a new language must flesh out, in concrete form, 
how they will integrate the hot new language into their existing architecture, 
or else they can't use it. (Yes, this applies equally to Ruby, Java, .NET, C++, 
and all the rest, even FORTRAN--no exceptions.) If you can't describe how it'll 
integrate into your current stuff, then you're just fascinated with the bright 
shiny new toy and need to grow up. It doesn't really matter to me<i>how</i> it 
integrates--through a database, through files on a filesystem, through a 
message-passing interface like JMS, or through a call-level interface, just 
have SOME kind of plan for hooking your new &lt;technology X&gt; project into 
the rest of the enterprise. (And yes, those answers are there for each of those 
languages/platforms; the test is not whether such answers exist, but how they 
map into<i>your</i> existing infrastructure.)</p> 
<p>What's more, I hereby rededicate this blog to finding interoperabilty 
solutions across the technology spectrum--got an interop problem you're not 
sure how to solve? Email me and (with your permission) I'll post the 
response--sort of an &quot;Ann Landers&quot; for interop geeks.</p> 
<p>By the way, this conundrum can be genericized pretty easily using 
generics/templates:</p> 
<blockquote> 
<pre> enum Q { No, Bad, Little, Flakey, Untouchable }; enum technology { C, 
C++, Java, C#, C++/CLI, VB 6, VB 7, VB 8, FORTRAN, COBOL, Smalltalk, Lisp, ... 
}; Problem&lt;technology X, technology Y, type T extends AbstractTest, enum 
Q&gt;: {
<ul> 
<li>&lt;X&gt; legacy codebase (&lt;int N where N &gt; 1&gt; years old, &lt;int 
L where L &gt; 1000&gt; lines)</li> 
<li>No &lt;type T&gt; tests</li> 
<li>&lt;Q&gt; test data</li> 
<li>&lt;Q&gt; knowledge transfer from the original development team</li> 
<li>&lt;Q&gt; environment to run the application in.</li> </ul> } returning 
requirement:
<ul> 
<li>Port the &lt;X&gt; project to &lt;Y&gt;</li> </ul> </pre> </blockquote> (I 
thought about doing it in Schema, but this seemed geekier... and easier, given 
all the angle-brackets XSD would require. )
<p></p> <br>

<p></p> .NET&nbsp;|&nbsp; C++&nbsp;|&nbsp; Development Processes&nbsp;|&nbsp; 
Java/J2EE&nbsp;|&nbsp; Ruby&nbsp;|&nbsp; XML Services <br>
<br>
 Sunday, 30 
October 2005 13:17:33 (Pacific Daylight Time, UTC-07:00)<br>
Comments [19] 
&nbsp;|&nbsp;<br>
<br>
<br>
&nbsp;Saturday, 17 September 2005 <br>
No, John, 
software really *does* evolve <br>

<p>John Haren, of CodeSnipers.com, recently blogged about something I feel 
pretty strongly about:</p> 
<p> </p> 
<blockquote> There's a common trope in CS education that goes something like 
this:<em>&quot;All software evolves, so be prepared for it.&quot; </em> 
<p>Far be it from me to imply that one shouldn't be able to respond to change; 
that's not my intention. But the idea expressed above contains a flaw:<em>
software does not evolve</em>.</p> 
<p>Duh, John&hellip; everyone knows that software changes. Features creep. 
Scope broadens. New platforms and whizbangs are targeted. Get with it!</p> 
<p>I concede the obvious: of course software changes. But repeat after me: 
software does not evolve. Because change != evolution.</p> 
<p>Evolution is a blind, natural process; the result of random mutations in an 
organism. Now it may just so happen that the result of the mutation is 
beneficial to the reproductive success of the organism, meaning we&rsquo;d 
expect to see creatures with such a trait outperform others without it. That's 
how traits are selected for. In the overwhelming majority of cases, mutations 
are detrimental, and they don't stick around for long (since there are many, 
many more ways of being dead than alive).</p> 
<p>Now in order to say that software evolves, you'd have to accept that your 
development process goes something like this: Developer opens a file at random, 
positions the cursor at random, punches a few keys at random. Developer then 
recompiles and sees what happens, hoping for the staggering luck that the 
resulting change actually does improve the software, and everybody loves it, so 
they buy it, and you'd expect to see more of it.</p> 
<p>Okay, insert joke here about how your development process seems that way 
from time to time.</p> 
<p>Jocularity aside, there's more at issue here than a flawed analogy. Of more 
significance is the type of thinking it can engender. Nothing &quot;just 
happens&quot; in software. Whatever it is, somebody made it happen. Someone 
decided. They may very well have decided in error, but they decided. They 
decided &quot;well, let's just try and fit that feature in; it shouldn't cause 
too many problems if it goes out only 70% tested... if it breaks, we'll deal 
with it then.&quot; Or they think &quot;yeah, a talking paperclip&hellip; why 
not?&quot; In other words, magical thinking. Don&rsquo;t do that.</p> 
<p>And CS departments should stop teaching that. Let them stress peopleware 
instead.</p> </blockquote> 
<p></p> 
<p>His presumption here, which may seem fair at first, is that all evolution 
is basically random. And, frankly, that's not entirely without truth. But what 
he sees as the randomness in the system is different from the randomness that I 
see, and that's that the<em>users</em> are what bring the randomness into the 
system.</p> 
<p>Look, how many times hasn't a user told you, &quot;We need this 
feature&quot;, only to discover six months after shipping that feature that 
nobody's really used it, but that it in turn sort of answered a different 
problem that you ended up providing for them as a new feature? See, the software
<em>itself</em> doesn't evolve randomly, but the users' <em>interactions</em> 
with the software do. That's evolution, that's healthy, and that's how software 
evolves.</p> 
<p>In short, it's recognizing that the users are part of the system, too, part 
of the organism that makes up this bizaare and wonderful world we live in, and 
their input is often exactly that: random. Which is probably why it's so 
important to have the on-site customer, as per agile development's 
recommendation, because you never know when randomness will strike and make 
your life better/faster/easier.</p> <br>

<p></p> Development Processes <br>
<br>
 Saturday, 17 September 2005 02:28:47 
(Pacific Daylight Time, UTC-07:00)<br>
Comments [1] &nbsp;|&nbsp; <br>
<br>
<br>
&nbsp;Sunday, 28 August 2005<br>
Best practices, redux <br>

<p>Jared Richardson took issue with my assertion that there's no such thing as 
best practices, stating that, in essence, it's not kosher for me to deny 
existence of something that I have to define:</p> 
<blockquote> They said &quot;There are no best practices&quot; and then they 
had to define the term... but they defined it wrong! A best practice isn't a 
required practice or a universally dominant practice. It's just one of the best 
ones. It's used quite often, but not everywhere and not always. (No such thing 
as best practices? Sure there are!) </blockquote> 
<p></p> 
<p>Unfortunately, Jared, the semantics of the term &quot;best practice&quot; 
implies exactly that: something that's used everywhere and always, for when 
would anyone choose<em>not</em> to use a &quot;best practice&quot;? And if it's 
not used everywhere and always, then it's not &quot;best&quot;, implying 
&quot;better than all alternatives&quot;, ya?</p> 
<p>How about, at the end of the day, we just drop the term &quot;best 
practice&quot; altogether, and stick with &quot;useful practice&quot; instead? 
Would that satisfy everyone's sensibilities?</p> <br>

<p></p> Development Processes <br>
<br>
 Sunday, 28 August 2005 10:23:29 
(Pacific Daylight Time, UTC-07:00)<br>
Comments [6] &nbsp;|&nbsp; <br>
<br>
<br>
<br> <br>

<p> </p> 
</body>
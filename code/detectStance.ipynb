{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11472"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "file = \"../data/stance-dataset.csv\"\n",
    "\n",
    "df = pd.read_csv(file, encoding='utf-8')\n",
    "# df = df[df['ds']!='yahoo']\n",
    "# df.info()\n",
    "df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11472\n",
      "401\n",
      "24856\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ds</th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_stance</th>\n",
       "      <th>answer_stance_object</th>\n",
       "      <th>object_count</th>\n",
       "      <th>object_1</th>\n",
       "      <th>mask_pos_1</th>\n",
       "      <th>object_2</th>\n",
       "      <th>mask_pos_2</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>softwareengineering.stackexchange</td>\n",
       "      <td>10373</td>\n",
       "      <td>What's the better way to charge for a cloud pl...</td>\n",
       "      <td>Like all good questions, the answer depends. I...</td>\n",
       "      <td>2</td>\n",
       "      <td>simple but more expensive</td>\n",
       "      <td>2</td>\n",
       "      <td>simple but more expensive</td>\n",
       "      <td>[[231, 252], [389, 393], [605, 619], [753, 757]]</td>\n",
       "      <td>complicated but cheaper</td>\n",
       "      <td>[]</td>\n",
       "      <td>like good questions answer depends depends app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>softwareengineering.stackexchange</td>\n",
       "      <td>18838</td>\n",
       "      <td>Haskell AND Lisp vs. Haskell OR Lisp</td>\n",
       "      <td>I suggest learning both, Haskell first, then C...</td>\n",
       "      <td>2</td>\n",
       "      <td>Haskell AND Lisp</td>\n",
       "      <td>2</td>\n",
       "      <td>Haskell AND Lisp</td>\n",
       "      <td>[[25, 56]]</td>\n",
       "      <td>Haskell OR Lisp</td>\n",
       "      <td>[]</td>\n",
       "      <td>suggest learning haskell first common lisp epe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>softwareengineering.stackexchange</td>\n",
       "      <td>19392</td>\n",
       "      <td>When is it better to offload work to the RDBMS...</td>\n",
       "      <td>You want to do all set-based operations in the...</td>\n",
       "      <td>3</td>\n",
       "      <td>do it in code</td>\n",
       "      <td>2</td>\n",
       "      <td>offload work to the RDBMS</td>\n",
       "      <td>[[40, 55], [232, 248]]</td>\n",
       "      <td>do it in code</td>\n",
       "      <td>[[165, 175], [578, 598]]</td>\n",
       "      <td>want setbased operations database performance ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>softwareengineering.stackexchange</td>\n",
       "      <td>20653</td>\n",
       "      <td>Is it better to specialize in a single field I...</td>\n",
       "      <td>Specialise if you enjoy it  As you are aware, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>expand into other fields to broaden my horizons</td>\n",
       "      <td>2</td>\n",
       "      <td>to specialize in a single field I like</td>\n",
       "      <td>[[53, 63], [404, 410], [512, 519]]</td>\n",
       "      <td>expand into other fields to broaden my horizons</td>\n",
       "      <td>[[892, 933]]</td>\n",
       "      <td>specialise enjoy aware specialise automaticall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>softwareengineering.stackexchange</td>\n",
       "      <td>21186</td>\n",
       "      <td>Microsoft SDE Interview vs Microsoft SDET Inte...</td>\n",
       "      <td>Unfortunately, those are both myths. SDEs and ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "      <td>Microsoft SDE Interview</td>\n",
       "      <td>[[37, 41], [283, 295], [950, 962]]</td>\n",
       "      <td>Microsoft SDET Interview</td>\n",
       "      <td>[[46, 51], [341, 354], [832, 836], [938, 945]]</td>\n",
       "      <td>unfortunately myths sdes sdets get starting sa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                 ds     id  \\\n",
       "0           0  softwareengineering.stackexchange  10373   \n",
       "1           1  softwareengineering.stackexchange  18838   \n",
       "2           2  softwareengineering.stackexchange  19392   \n",
       "3           3  softwareengineering.stackexchange  20653   \n",
       "4           4  softwareengineering.stackexchange  21186   \n",
       "\n",
       "                                            question  \\\n",
       "0  What's the better way to charge for a cloud pl...   \n",
       "1               Haskell AND Lisp vs. Haskell OR Lisp   \n",
       "2  When is it better to offload work to the RDBMS...   \n",
       "3  Is it better to specialize in a single field I...   \n",
       "4  Microsoft SDE Interview vs Microsoft SDET Inte...   \n",
       "\n",
       "                                              answer  answer_stance  \\\n",
       "0  Like all good questions, the answer depends. I...              2   \n",
       "1  I suggest learning both, Haskell first, then C...              2   \n",
       "2  You want to do all set-based operations in the...              3   \n",
       "3  Specialise if you enjoy it  As you are aware, ...              3   \n",
       "4  Unfortunately, those are both myths. SDEs and ...              1   \n",
       "\n",
       "                              answer_stance_object  object_count  \\\n",
       "0                        simple but more expensive             2   \n",
       "1                                 Haskell AND Lisp             2   \n",
       "2                                    do it in code             2   \n",
       "3  expand into other fields to broaden my horizons             2   \n",
       "4                                          Neutral             2   \n",
       "\n",
       "                                 object_1  \\\n",
       "0               simple but more expensive   \n",
       "1                        Haskell AND Lisp   \n",
       "2               offload work to the RDBMS   \n",
       "3  to specialize in a single field I like   \n",
       "4                 Microsoft SDE Interview   \n",
       "\n",
       "                                         mask_pos_1  \\\n",
       "0  [[231, 252], [389, 393], [605, 619], [753, 757]]   \n",
       "1                                        [[25, 56]]   \n",
       "2                            [[40, 55], [232, 248]]   \n",
       "3                [[53, 63], [404, 410], [512, 519]]   \n",
       "4                [[37, 41], [283, 295], [950, 962]]   \n",
       "\n",
       "                                          object_2  \\\n",
       "0                          complicated but cheaper   \n",
       "1                                  Haskell OR Lisp   \n",
       "2                                    do it in code   \n",
       "3  expand into other fields to broaden my horizons   \n",
       "4                         Microsoft SDET Interview   \n",
       "\n",
       "                                       mask_pos_2  \\\n",
       "0                                              []   \n",
       "1                                              []   \n",
       "2                        [[165, 175], [578, 598]]   \n",
       "3                                    [[892, 933]]   \n",
       "4  [[46, 51], [341, 354], [832, 836], [938, 945]]   \n",
       "\n",
       "                                                text  \n",
       "0  like good questions answer depends depends app...  \n",
       "1  suggest learning haskell first common lisp epe...  \n",
       "2  want setbased operations database performance ...  \n",
       "3  specialise enjoy aware specialise automaticall...  \n",
       "4  unfortunately myths sdes sdets get starting sa...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = text.lower() \n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) \n",
    "    text = BAD_SYMBOLS_RE.sub('', text)  \n",
    "    text = text.replace('x', '')\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    return text\n",
    "\n",
    "words = []\n",
    "print(df.size)\n",
    "df1 = df.copy()\n",
    "texts = []\n",
    "max_text_word_len = 0\n",
    "for i, document in df.iterrows():\n",
    "    text = document.answer + ' ' + document['object_1']\n",
    "    text = clean_text(text).replace('\\d+', '')\n",
    "    texts.append(text)\n",
    "    max_text_word_len = len(list(set(text.split(' ')))) if len(list(set(text.split(' ')))) > max_text_word_len else max_text_word_len\n",
    "    \n",
    "df['text'] = texts\n",
    "\n",
    "print(max_text_word_len)\n",
    "texts = []\n",
    "for i, document in df1.iterrows():\n",
    "    text = document.answer + ' ' + document['object_2']\n",
    "    text = clean_text(text).replace('\\d+', '')\n",
    "    texts.append(text)\n",
    "df1['text'] = texts\n",
    "df = pd.concat([df, df1])\n",
    "print(df.size)\n",
    "                 \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12447\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAX_NB_WORDS = 50000\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "EMBEDDING_DIM = 100\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(df['text'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (1912, 500)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "X = tokenizer.texts_to_sequences(df['text'].values)\n",
    "X = keras.preprocessing.sequence.pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (1912, 4)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(df['answer_stance']).values\n",
    "\n",
    "print('Shape of label tensor:', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1720, 500) (1720, 4)\n",
      "(192, 500) (192, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.1, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# print(X.shape[1])\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "print(X_train.shape,Y_train.shape)\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "Y_pred = model.predict(X_test)\n",
    "predicted = []\n",
    "expected = []\n",
    "for i, preds in enumerate(Y_pred):\n",
    "    predicted.append([i for i, pred in enumerate(preds) if pred == max(preds)])\n",
    "    expected.append([index for index , val in enumerate(Y_test[i]) if val == 1])\n",
    "print(classification_report(expected, predicted, digits=3, zero_division=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../models/lst_stance_detection_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "df = pd.read_json(\"../data/documents_query.json\")\n",
    "\n",
    "words = []\n",
    "df1 = df\n",
    "texts = []\n",
    "max_text_word_len = 0\n",
    "for i, document in df.iterrows():\n",
    "    text = document.answer + ' ' + document['Object1'] \n",
    "    text = clean_text(text).replace('\\d+', '')\n",
    "    texts.append(text)\n",
    "    # max_text_word_len = len(list(set(text.split(' ')))) if len(list(set(text.split(' ')))) > max_text_word_len else max_text_word_len\n",
    "print(len(texts))  \n",
    "df['answer'] = texts\n",
    "\n",
    "texts = []\n",
    "for i, document in df.iterrows():\n",
    "    text = document.answer + ' ' + document['Object2']\n",
    "    text = clean_text(text).replace('\\d+', '')\n",
    "    texts.append(text)\n",
    "df1['answer'] = texts\n",
    "df = pd.concat([df, df1])\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(df['answer'].values)\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X = tokenizer.texts_to_sequences(df['answer'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "predictions = model.predict(X)\n",
    "\n",
    "expected = []\n",
    "prediction = []\n",
    "for i, preds in enumerate(predictions):\n",
    "    prediction.append([i for i, pred in enumerate(preds) if pred == max(preds)])\n",
    "    # expected.append([index for index , val in enumerate(Y_test[i]) if val == 1])\n",
    "    \n",
    "df['Prediction'] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1 = df[:961]\n",
    "results1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results2 = df[961:]\n",
    "results1['Obj1_pred'] = results1['Prediction']\n",
    "results1['Obj2_pred'] = results2['Prediction']\n",
    "results2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results1[results1['Obj1_pred'] == results1['Obj2_pred']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict(X_test, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1,use_multiprocessing=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['acc'], label='train')\n",
    "plt.plot(history.history['val_acc'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN + LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Flatten, Conv1D, SpatialDropout1D, MaxPooling1D,AveragePooling1D, Bidirectional, merge, concatenate, Input, Dropout, LSTM\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "config = tf.compat.v1.ConfigProto(gpu_options=gpu_options)\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "\n",
    "y_dim=4\n",
    "num_filters=200\n",
    "filter_sizes=[3, 4, 5] \n",
    "pool_padding='valid' \n",
    "dropout=0.5\n",
    "\n",
    "embed_input = Input(shape=(X.shape[1],))\n",
    "x = Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1])(embed_input)\n",
    "pooled_outputs = []\n",
    "for i in range(len(filter_sizes)):\n",
    "    conv = Conv1D(num_filters, kernel_size=filter_sizes[i], padding='valid', activation='relu')(x)\n",
    "    conv = MaxPooling1D(pool_size=EMBEDDING_DIM-filter_sizes[i]+1)(conv)           \n",
    "    pooled_outputs.append(conv)\n",
    "merge = concatenate(pooled_outputs)\n",
    "    \n",
    "x = Dense(30, activation='relu')(merge)\n",
    "x = Dropout(dropout)(x)\n",
    "x = Bidirectional(LSTM(100, return_sequences=True, dropout=0.5, recurrent_dropout=0.1))(x)\n",
    "x = Dense(30, activation='relu')(x)\n",
    "x = Dropout(dropout)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(y_dim, activation='sigmoid')(x)\n",
    "\n",
    "model1 = Model(inputs=embed_input,outputs=x)\n",
    "\n",
    "#   model.compile(optimizer='adam',loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "model1.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])\n",
    "\n",
    "print(model1.summary())\n",
    "\n",
    "epochs = 3\n",
    "batch_size = 64\n",
    "\n",
    "history = model1.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "df = pd.read_json(\"../data/documents_query.json\")\n",
    "\n",
    "words = []\n",
    "df1 = df\n",
    "texts = []\n",
    "max_text_word_len = 0\n",
    "for i, document in df.iterrows():\n",
    "    text = document.answer + ' ' + document['Object1'] \n",
    "    text = clean_text(text).replace('\\d+', '')\n",
    "    texts.append(text)\n",
    "    # max_text_word_len = len(list(set(text.split(' ')))) if len(list(set(text.split(' ')))) > max_text_word_len else max_text_word_len\n",
    "print(len(texts))  \n",
    "df['answer'] = texts\n",
    "\n",
    "texts = []\n",
    "for i, document in df.iterrows():\n",
    "    text = document.answer + ' ' + document['Object2']\n",
    "    text = clean_text(text).replace('\\d+', '')\n",
    "    texts.append(text)\n",
    "df1['answer'] = texts\n",
    "df = pd.concat([df, df1])\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(df['answer'].values)\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X = tokenizer.texts_to_sequences(df['answer'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "predictions = model1.predict(X)\n",
    "\n",
    "expected = []\n",
    "prediction = []\n",
    "for i, preds in enumerate(predictions):\n",
    "    prediction.append([i for i, pred in enumerate(preds) if pred == max(preds)][0])\n",
    "    # expected.append([index for index , val in enumerate(Y_test[i]) if val == 1])\n",
    "    \n",
    "df['Prediction'] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1 = df[:961]\n",
    "results1[results1['Prediction'] == 2].head()\n",
    "# results1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['acc'], label='train')\n",
    "plt.plot(history.history['val_acc'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
